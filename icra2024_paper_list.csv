Under Pressure: Learning-Based Analog Gauge Reading in the Wild,"Maurits Reitsma, Julian Keller, Kenneth Blomqvist, Roland Siegwart",ETH Zurich,Automation,"We propose an interpretable framework for reading analog gauges that is deployable on real world robotic systems. Our framework splits the reading task into distinct steps, such that we can detect potential failures at each step. Our system needs no prior knowledge of the type of gauge or the range of the scale and is able to extract the units used. We show that our gauge reading algorithm is able to extract readings with a relative reading error of less than 2%."
MORALS: Analysis of High-Dimensional Robot Controllers Via Topological Tools in a Latent Space,"Ewerton Vieira, Aravind Sivaramakrishnan, Sumanth Tangirala, Edgar Granados, Konstantin Mischaikow, Kostas E. Bekris","Rutgers University,Rutgers University, New Brunswick,Rutgers,Rutgers, the State University of New Jersey",Automation,"Estimating the region of attraction RoA for a robot controller is essential for safe application and controller composition. Many existing methods require a closed-form expression that limit applicability to data-driven controllers. Methods that operate only over trajectory rollouts tend to be data-hungry. In prior work, we have demonstrated that topological tools based on Morse Graphs (directed acyclic graphs that combinatorially represent the underlying nonlinear dynamics) offer data-efficient RoA estimation without needing an analytical model. They struggle, however, with high-dimensional systems as they operate over a state-space discretization. This paper presents Morse Graph-aided discovery of Regions of Attraction in a learned Latent Space (MORALS). The approach combines auto-encoding neural networks with Morse Graphs. MORALS shows promising predictive capabilities in estimating attractors and their RoAs for data-driven controllers operating over high-dimensional systems, including a 67-dim humanoid robot and a 96-dim 3-fingered manipulator. It first projects the dynamics of the controlled system into a learned latent space. Then, it constructs a reduced form of Morse Graphs representing the bistability of the underlying dynamics, i.e., detecting when the controller results in a desired versus an undesired behavior. The evaluation on high-dimensional robotic datasets indicates data efficiency in RoA estimation."
TinyMPC: Model-Predictive Control on Resource-Constrained Microcontrollers,"Khai Nguyen, Sam Schoedel, Anoushka Alavilli, Brian Plancher, Zachary Manchester","Carnegie Mellon University,Barnard College, Columbia University",Automation,"Model-predictive control (MPC) is a powerful tool for controlling highly dynamic robotic systems subject to complex constraints. However, MPC is computationally demanding, and is often impractical to implement on small, resource-constrained robotic platforms. We present TinyMPC, a high-speed MPC solver with a low memory footprint targeting the microcontrollers common on small robots. Our approach is based on the alternating direction method of multipliers (ADMM) and leverages the structure of the MPC problem for efficiency. We demonstrate TinyMPCâ€™s effectiveness by benchmarking against the state-of-the-art solver OSQP, achieving nearly an order of magnitude speed increase, as well as through hardware experiments on a 27 gram quadrotor, demonstrating high-speed trajectory tracking and dynamic obstacle avoidance."
A Movable Microfluidic Chip with Gap Effect for Manipulation of Oocytes,"Shuzhang Liang, Satoshi Amaya, Hirotaka Sugiura, Hao Mo, Yuguo Dai, Fumihito Arai",The University of Tokyo,Automation,"This study proposes a novel movable microfluidic chip in which a microfluidic chip is integrated into a robotic manipulator for manipulating oocytes. The microfluidic device has the ability to release a single cell with gap effect. The robotic manipulator can control the position of the microfluidic chip. The microfluidic chip with a pipette tip is directly fabricated using 3D printing. Xenopus oocyte was used in the experiment. When oocytes move from bottom side of the channel to the tip side, they generate gaps between each other. The gap distance can reach about 16 times the diameter of oocyte. In addition, a capacitive sensor was used to detect oocytes in the manipulation processes. The results showed that oocytes were successfully released one by one with no deformation in shape using the movable microfluidic chip. The method has significant advantages in biomedicine engineering and micro-nano-manipulation."
Efficient Composite Learning Robot Control under Partial Interval Excitation,"Tian Shi, Weibing Li, Haoyong Yu, Yongping Pan","Sun Yat-Sen University,Sun Yat-sen University,National University of Singapore",Automation,"Parameter convergence in adaptive control is crucial for improving the stability and robustness of robotic systems. Nevertheless, a stringent condition named persistent excitation (PE) needs to be satisfied to ensure parameter convergence in the conventional adaptive robot control. Composite learning robot control (CLRC) is an innovative methodology that guarantees parameter convergence under a condition of interval excitation (IE) that is strictly weaker than PE. This paper puts forward a time-division multi-channel (TDMC) CLRC strategy such that parameter convergence is achieved even without the IE condition. In the TDMC mechanism, a filtered regressor is integrated with multiple time intervals to generate a generalized prediction error for parameter update, such that excitation information of regressor channels at different instants is exploited more effectively and efficiently to achieve fast and accurate parameter estimation. Global exponential stability with parameter convergence of the closed-loop system is achieved under a partial IE condition that is much weaker than IE. Experiments on a collaborative robot with 7 degrees of freedom have demonstrated the superiority of the proposed approach in both parameter estimation and trajectory tracking compared to start-of-the-art approaches."
Learning Vision-Based Bipedal Locomotion for Challenging Terrain,"Helei Duan, Bikram Pandit, Mohitvishnu S. Gadde, Bart Jaap Van Marum, Jeremy Dao, Chanho Kim, Alan Fern",Oregon State University,Cognitive Robotics,"Reinforcement learning (RL) for bipedal locomotion has recently demonstrated robust gaits over moderate terrains using only proprioceptive sensing. However, such blind controllers will fail in environments where robots must anticipate and adapt to local terrain, which requires visual perception. In this paper, we propose a fully-learned system that allows bipedal robots to react to local terrain while maintaining commanded travel speed and direction. Our approach first trains a controller in simulation using a heightmap expressed in the robot's local frame. Next, data is collected in simulation to train a heightmap predictor, whose input is the history of depth images and robot states. We demonstrate that with appropriate domain randomization, this approach allows for successful sim-to-real transfer with no explicit pose estimation and no fine-tuning using real-world data. To the best of our knowledge, this is the first example of sim-to-real learning for vision-based bipedal locomotion over challenging terrains."
Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-To-End,"Chong Zhang, Jin Jin, Jonas Frey, Nikita Rudin, Matias Mattamala, Cesar D. Cadena Lerma, Marco Hutter","ETH Zurich,Tongji University,ETH Zurich, NVIDIA,University of Oxford",Cognitive Robotics,"Autonomous robots must navigate reliably in unknown environments even under compromised exteroceptive perception, or perception failures. Such failures often occur when harsh environments lead to degraded sensing, or when the perception algorithm misinterprets the scene due to limited generalization. In this paper, we model perception failures as invisible obstacles and pits, and train a reinforcement learning (RL) based local navigation policy to guide our legged robot. Unlike previous works relying on heuristics and anomaly detection to update navigational information, we train our navigation policy to reconstruct the environment information in the latent space from corrupted perception and react to perception failures end-to-end. To this end, we incorporate both proprioception and exteroception into our policy inputs, thereby enabling the policy to sense collisions on different body parts and pits, prompting corresponding reactions. We validate our approach in simulation and on the real quadruped robot ANYmal running in real-time ("
Vision-Language Frontier Maps for Zero-Shot Semantic Navigation,"Naoki Yokoyama, Sehoon Ha, Dhruv Batra, Jiuguang Wang, Bernadette Bucher","Georgia Institute of Technology,Georgia Tech / Facebook AI Research,Boston Dynamics AI Institute,University of Michigan",Cognitive Robotics,"Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM's zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real world deployment can be viewed at naoki.io/vlfm."
NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration,"Ajay Sridhar, Dhruv Shah, Catherine Glossop, Sergey Levine","University of California, Berkeley,UC Berkeley",Cognitive Robotics,"Robotic learning for navigation in unfamiliar environments needs to provide policies for both task-oriented navigation (i.e., reaching a goal that the robot has located), and task-agnostic exploration (i.e., searching for a goal in a novel setting). Typically, these roles are handled by separate models, for example by using subgoal proposals, planning, or separate navigation strategies. In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the latter providing the ability to search novel environments, and the former providing the ability to reach a user-specified goal once it has been located. We show that this unified policy results in better overall performance when navigating to visually indicated goals in novel environments, as compared to approaches that use subgoal proposals from generative models, or prior methods based on latent variable models. We instantiate our method by using a large-scale Transformer- based policy trained on data from multiple ground robots, with a diffusion model decoder to flexibly handle both goal- conditioned and goal-agnostic navigation. Our experiments, conducted on a real-world mobile robot platform, show effective navigation in unseen environments in comparison with five alternative methods, and demonstrate significant improvements in performance and lower collision rates, despite utilizing smaller models than state-of-the-art approaches."
Learning Continuous Control with Geometric Regularity from Robot Intrinsic Symmetry,"Shengchao Yan, Baohe Zhang, Yuan Zhang, Joschka Boedecker, Wolfram Burgard","University of Freiburg,University of Technology Nuremberg",Cognitive Robotics,"Geometric regularity, which leverages data symmetry, has been successfully incorporated into deep learning architectures such as CNNs, RNNs, GNNs, and Transformers. While this concept has been widely applied in robotics to address the curse of dimensionality when learning from high-dimensional data, the inherent reflectional and rotational symmetry of robot structures has not been adequately explored. Drawing inspiration from cooperative multi-agent reinforcement learning, we introduce novel network structures for single-agent control learning that explicitly capture these symmetries. Moreover, we investigate the relationship between the geometric prior and the concept of Parameter Sharing in multi-agent reinforcement learning. Last but not the least, we implement the proposed framework in online and offline learning methods to demonstrate its ease of use. Through experiments conducted on various challenging continuous control tasks on simulators and real robots, we highlight the significant potential of the proposed geometric regularity in enhancing robot learning capabilities."
MUI-TARE: Cooperative Multi-Agent Exploration with Unknown Initial Position,"Jingtian Yan, Lin Xingqiao, Zhongqiang Ren, Shiqi Zhao, Jieqiong Yu, Chao Cao, Peng Yin, Ji Zhang, Sebastian Scherer","Carnegie Mellon University,Shanghai Jiao Tong University,University of California San Diego",Planning under Uncertainty I,"Multi-agent exploration of a bounded 3D environment with unknown initial positions of agents is a challenging problem. It requires quickly exploring the environments and robustly merging the sub-maps built by the agents. Most of the current exploration strategies directly merge two sub-maps built by different agents when a single frame of overlap is detected, which can lead to incorrect merging due to the false-positive detection of the overlap and is thus not robust. Meanwhile, some state-of-the-art place recognition methods use sequence matching for more robust data association. However, naively applying these sequence-based matching methods to multi-agent exploration may require one agent to repeat a large amount of another agent's history trajectory so that a sequence of matched observation can be established, which reduces the overall exploration time efficiency. To intelligently balance the robustness of sub-map merging and exploration efficiency, we develop a new approach for lidar-based multi-agent exploration, which can direct one agent to repeat another agent's trajectory adaptively based on the quality indicator of the sub-map merging process. Additionally, our approach extends the recent single-agent hierarchical exploration strategy to multiple agents cooperatively by planning for agents with merged sub-maps to further improve exploration efficiency. Our experiments show that our approach is up to 50% more efficient than the baselines on average while merging"
Safe Planning in Dynamic Environments Using Conformal Prediction,"Lars Lindemann, Matthew Cleaveland, Gihyun Shim, George J. Pappas","University of Southern California,University of Pennsylvania",Planning under Uncertainty I,"We propose a framework for planning in unknown dynamic environments with probabilistic safety guarantees using conformal prediction. Particularly, we design a model predictive controller (MPC) that uses i) trajectory predictions of the dynamic environment, and ii) prediction regions quantifying the uncertainty of the predictions. To obtain prediction regions, we use conformal prediction, a statistical tool for uncertainty quantification, that requires availability of offline trajectory data - a reasonable assumption in many applications such as autonomous driving. The prediction regions are valid, i.e., they hold with a user-defined probability, so that the MPC is provably safe. We illustrate the results in the self-driving car simulator CARLA at a pedestrian-filled intersection. The strength of our approach is compatibility with state of the art trajectory predictors, e.g., RNNs and LSTMs, while making no assumptions on the underlying trajectory-generating distribution. To the best of our knowledge, these are the first results that provide valid safety guarantees in such a setting."
Wasserstein Distributionally Robust Chance Constrained Trajectory Optimization for Mobile Robots within Uncertain Safe Corridor,"Shaohang Xu, Haolin Ruan, Wentao Zhang, Yi'an Wang, Lijun Zhu, Chin Pang Ho","Huazhong University of Science and Technology,City University of Hong Kong",Planning under Uncertainty I,"Safe corridor-based Trajectory Optimization (TO) presents an appealing approach for collision-free path planning of autonomous robots, offering global optimality through its convex formulation. The safe corridor is constructed based on the perceived map, however, the non-ideal perception induces uncertainty, which is rarely considered in trajectory generation. In this paper, we propose Distributionally Robust Safe Corridor Constraints (DRSCCs) to consider the uncertainty of the safe corridor. Then, we integrate DRSCCs into the trajectory optimization framework using Bernstein basis polynomials. Theoretically, we rigorously prove that the trajectory optimization problem incorporating DRSCCs is equivalent to a computationally efficient, convex quadratic program. Compared to the nominal TO, our method enhances navigation safety by significantly reducing the infeasible motions in presence of uncertainty. Moreover, the proposed approach is validated through two robotic applications, a micro Unmanned Aerial Vehicle (UAV) and a quadruped robot Unitree A1."
Shield Model Predictive Path Integral: A Computationally Efficient Robust MPC Method Using Control Barrier Functions,"Ji Yin, Charles Dawson, Chuchu Fan, Panagiotis Tsiotras","Georgia Institute of Technology,MIT,Massachusetts Institute of Technology,Georgia Tech",Planning under Uncertainty I,"Model Predictive Path Integral (MPPI) control is a type of sampling-based model predictive control that simulates thousands of trajectories and uses these trajectories to synthesize optimal controls on-the-fly. In practice, however, MPPI encounters problems limiting its application. For instance, it has been observed that MPPI tends to make poor decisions if unmodeled dynamics or environmental disturbances exist, preventing its use in safety-critical applications. Moreover, the multi-threaded simulations used by MPPI require significant onboard computational resources, making the algorithm inaccessible to robots without modern GPUs. To alleviate these issues, we propose a novel (Shield-MPPI) algorithm that provides robustness against unpredicted disturbances and achieves real-time planning using a much smaller number of parallel simulations on regular CPUs. The novel Shield-MPPI algorithm is tested on an aggressive autonomous racing platform both in simulation and in hardware. The results show that the proposed controller greatly reduces the number of constraint violations compared to state-of-the-art robust MPPI variants and stochastic MPC methods."
Distributionally Robust CVaR-Based Safety Filtering for Motion Planning in Uncertain Environments,"Sleiman Safaoui, Tyler Summers","The University of Texas at Dallas,University of Texas at Dallas",Planning under Uncertainty I,"Safety is a core challenge of autonomous robot motion planning, especially in the presence of dynamic and uncertain obstacles. Many recent results use learning and deep learning-based motion planners and prediction modules to predict multiple possible obstacle trajectories and generate obstacle-aware ego robot plans. However, planners that ignore the inherent uncertainties in such predictions incur collision risks and lack formal safety guarantees. In this paper, we present a computationally efficient safety filtering solution to reduce the collision risk of ego robot motion plans using multiple samples of obstacle trajectory predictions. The proposed approach reformulates the collision avoidance problem by computing safe halfspaces based on obstacle sample trajectories using distributionally robust optimization (DRO) techniques. The safe halfspaces are used in a model predictive control (MPC)-like safety filter to apply corrections to the reference ego trajectory thereby promoting safer planning. The efficacy and computational efficiency of our approach are demonstrated through numerical simulations."
Monte Carlo Planning in Hybrid Belief POMDPs,"Moran Barenboim, Moshe Shienman, Vadim Indelman","Technion - Israel Institute of Technology,Israel Institute of Technology",Planning under Uncertainty I,"Real-world problems often require reasoning about hybrid beliefs, over both discrete and continuous random variables. Yet, such a setting has hardly been investigated in the context of planning. Moreover, existing online Partially Observable Markov Decision Processes (POMDPs) solvers do not support hybrid beliefs directly. In particular, these solvers do not address the added computational burden due to an increasing number of hypotheses with the planning horizon, which can grow exponentially. As part of this work, we present a novel algorithm, Hybrid Belief Monte Carlo Planning (HB-MCP) that utilizes the Monte Carlo Tree Search (MCTS) algorithm to solve a POMDP while maintaining a hybrid belief. We illustrate how the upper confidence bound (UCB) exploration bonus can be leveraged to guide the growth of hypotheses trees alongside the belief trees. We then evaluate our approach in highly aliased simulated environments where unresolved data association leads to multi-modal belief hypotheses."
Data Association Aware POMDP Planning with Hypothesis Pruning Performance Guarantees,"Moran Barenboim, Idan Lev Yehudi, Vadim Indelman",Technion - Israel Institute of Technology,Planning under Uncertainty I,"Autonomous agents that operate in the real world must often deal with partial observability, which is commonly modeled as partially observable Markov decision processes (POMDPs). However, traditional POMDP models rely on the assumption of complete knowledge of the observation source, known as fully observable data association. To address this limitation, we propose a planning algorithm that maintains multiple data association hypotheses, represented as a belief mixture, where each component corresponds to a different data association hypothesis. However, this method can lead to an exponential growth in the number of hypotheses, resulting in significant computational overhead. To overcome this challenge, we introduce a pruning-based approach for planning with ambiguous data associations. Our key contribution is to derive bounds between the value function based on the complete set of hypotheses and the value function based on a pruned subset of the hypotheses, enabling us to establish a trade-off between computational efficiency and performance. We demonstrate how these bounds can both be used to certify any pruning heuristic in retrospect and propose a novel approach to determine which hypotheses to prune in order to ensure a predefined limit on the loss. We evaluate our approach in simulated environments and demonstrate its efficacy in handling multi-modal belief hypotheses with ambiguous data associations."
Safe POMDP Online Planning Via Shielding,"Shili Sheng, David Parker, Lu Feng","University of Virginia,University of Oxford",Planning under Uncertainty I,"Partially observable Markov decision processes (POMDPs) have been widely used in many robotic applications for sequential decision-making under uncertainty. POMDP online planning algorithms such as Partially Observable Monte-Carlo Planning (POMCP) can solve very large POMDPs with the goal of maximizing the expected return. But the resulting policies cannot provide safety guarantees which are imperative for real-world safety-critical tasks (e.g., autonomous driving). In this work, we consider safety requirements represented as almost-sure reach-avoid specifications (i.e., the probability to reach a set of goal states is one and the probability to reach a set of unsafe states is zero). We compute shields that restrict unsafe actions which would violate the almost-sure reach-avoid specifications. We then integrate these shields into the POMCP algorithm for safe POMDP online planning. We propose four distinct shielding methods, differing in how the shields are computed and integrated, including factored variants designed to improve scalability. Experimental results on a set of benchmark domains demonstrate that the proposed shielding methods successfully guarantee safety (unlike the baseline POMCP without shielding) on large POMDPs, with negligible impact on the runtime for online planning."
Generating Sparse Probabilistic Graphs for Efficient Planning in Uncertain Environments,"Yasmin Veys, Martina Stadler, Nicholas Roy",Massachusetts Institute of Technology,Planning under Uncertainty I,"Environments with regions of uncertain traversability can be modeled as roadmaps with probabilistic edges for efficient planning under uncertainty. We would like to generate roadmaps that enable planners to efficiently find paths with expected low costs through uncertain environments. The roadmap must be sparse so that the planning problem is tractable, but still contain edges that are likely to contribute to low-cost plans under various realizations of the environmental uncertainty. Determining the optimal set of edges to add to the roadmap without considering an exponential number of traversability scenarios is challenging. We propose the use of a heuristic that bounds the ratio between the expected path cost in our graph and the expected path cost in an optimal graph to determine whether a given edge should be added to the roadmap. We test our approach in several environments, demonstrating that our uncertainty-aware roadmaps effectively trade off between plan quality and planning efficiency for uncertainty-aware agents navigating in the graph."
"Magnetic Gear-Based Actuator: A Framework of Design, Optimization, and Disturbance Observer-Based Torque Control","Hangyeol Song, Edgar Lee, Hyung-tae Seo, Seokhwan Jeong","Sogang University,Kyonggi University,Mechanical Eng., Sogang University",Mechanism Design I,"This letter presents a design framework and novel control strategy for a compact coaxial magnetic-gear-based actuation module suitable for small-to-mid-sized mechanical and robotic applications. The proposed actuation module adopts a non-contact magnetic coupling mechanism to transmit rotational power with a predetermined gear ratio, in contrast to traditional mechanical gear-based transmissions. This approach offers several advantages such as enhanced backdrivability, hardware safety, and transparency when compared to conventional contact-based transmissions. Furthermore, the magnetic coupling effect provides a spring-like characteristic that can be utilized to implement a series elastic actuation enabling sensorless torque control. The design of the magnetic gear was optimized using a differential evolution method, and a dynamic model was formulated to specify its dynamic characteristics. Finally, a composite disturbance observer-based torque control algorithm was developed, which capitalizes on the features of the magnetic spring. The proposed control algorithm was validated through several experiments."
Johnsen-Rahbek Capstan Clutch: A High Torque Electrostatic Clutch,"Timothy Amish, Jeffrey Auletta, Chad C. Kessens, Joshua R. Smith, Jeffrey Lipton","University of Washington,US DEVCOM Army Research Laboratory,United States Army Research Laboratory,Northeastern University",Mechanism Design I,"In many robotic systems, the holding state con- sumes power, limits operating time, and increases operating costs. Electrostatic clutches have the potential to improve robotic performance by generating holding torques with low power consumption. A key limitation of electrostatic clutches has been their low specific shear stresses which restrict gen- erated holding torque, limiting many applications. Here we show how combining the Johnsen-Rahbek (JR) effect with the exponential tension scaling capstan effect can produce clutches with the highest specific shear stress in the literature. Our system generated 31.3 N/cm2 sheer stress and a total holding torque of 7.1 NÂ·m while consuming only 2.5 mW/cm2 at 500 V. We demonstrate a theoretical model of an electrostatic adhesive capstan clutch and demonstrate how large angle (Î¸ > 2Ï€) designs increase efficiency over planar or small angle (Î¸ < Ï€) clutch designs. We also report the first unfilled polymeric material, polybenzimidazole (PBI), to exhibit the JR-effect"
Research on Bionic Foldable Wing for Flapping Wing Micro Air Vehicle,"Shengjie Xiao, Kai Hu, Yuhong Sun, Yun Wang, Bo Qin, Huichao Deng, Xuan Wu, Xilun Ding","Beijing University of Aeronautics and Astronautics,Beihang University,Beihang university,China Nanhu Academy of Electronics and Information Technology,Beijing Univerisity of Aeronautics and Astronautics",Mechanism Design I,"This paper presents a bionic foldable wing that imitates the hind wing of ladybirds. Based on the folding mechanism of the hind wing of ladybirds and the theory of origami, the motion model of the bionic foldable wing is established, yield the motion law of the crease angles and the variation relationship between the panels are obtained. Bionic foldable wings utilise shape memory alloy to drive wings to fold, and embedded torsion springs to release energy to realize the function of wing unfolding. In the experiments of the vehicle equipped with foldable wings, the lift and attitude torque of bionic foldable wings are measured by the F/T sensor. The experimental results indicated that its aerodynamic performance is basically close to that of our optimized non-foldable wings. Moreover, the vehicle with foldable wings has been able to overcome gravity to achieve flight, which provides a novel concept for the research on flapping wing."
Magnetic Field-Driven Bristle-Bots,"Lukáš Supik, Kateřina Stránská, Miroslav Kulich, Libor Přeučil, Michael Somr, Karel Kosnar","Czech Technical University in Prague,Czech Institute of Informatics, Robotics and Cybernetics Czech T,Czech Technical University in Prague, CIIRC,Czech Technical University in Prague, Faculty of Civil Engineeri",Mechanism Design I,"Widespread applications for mobile robots are creating a large demand for developing new driving mechanisms that can handle diverse environments. Bristle-bot-like robot designs, mainly studied over the past decade, are based on vibration mechanisms built on flexible legs that enable motion on the ground. However, creating scalable and steerable bristle-bots remains a challenge. Here, we focus on developing a new kind of magnetically-driven bristle-bots with a wireless control and power supply that can be steered and downscaled. In experiments, we verified our concept with 3D-printed bristle-bot units equipped with body-embedded permanent magnets actuated via torque imposed by an external magnetic field. An AC-powered Helmholtz coil generated the bristle-botâ€™s driving field, providing 2D input control, field amplitude, and frequency. A variable number of legs on each side of a bristle-botâ€™s body was used to ensure that each side of the bristle-bot's body has a different frequency response. This asymmetry introduces steerability by a rich set of control commands: rotations with simultaneous forward and backward locomotion. We also observed and controlled a new side locomotion phenomenon not yet described in previous studies. The results presented were supported with data from numerous experiments and thorough statistical analysis, which indicated promising directions for future developments."
A Scalable Monolithic 3D Printable Variable Stiffness Mechanism,"Paul Baisamy, Adam A. Stokes, Francesco Giorgio-Serchi","The University of Edinburgh,University of Edinburgh",Mechanism Design I,"Variable Stiffness Mechanisms (VSM) are becoming ubiquitous in mechatronics given the benefit they provide in terms of safety and performance. Despite these assets, VSMs remain fairly complex mechanical devices lacking in compactness, ease of manufacturing and accessibility. In addition, the scarcity of commercially available VSMs requires that such systems are mostly designed in-house. We propose a new type of VSM that improves on the pre-existing Jack Spring concept by making it more compact and robust. The new concept, which we refer to as the Compact Modifier of Active Coils (C-MAC) mechanism, is specifically designed to be manufactured through a monolithic 3D print. This approach enables to modify a minimal set of design features, namely the spring diameter and the coil diameter, to achieve the desired range of stiffness variation. We test the proposed design on six configurations; these show hysteretic energy losses no larger than 35% over the stiffness variation and confirm stiffness to scale according to theory. Stiffness ranging from 0.15 N/mm to 1.02N /mm were measured for an overall device length of 140 mm, including a maximal stroke length of 22 mm. The results confirm excellent scalability and manufacturability of the proposed design, providing a versatile mechanism for fast prototyping."
Modular Growing Mechanism with Multi-Axis Deformation,"Dongdong Du, Emanuela Del Dottore, Alessio Mondini, Edoardo Sinibaldi, Barbara Mazzolai","Zhejiang University,Istituto Italiano di Tecnologia,Istituito Italiano di Tecnologia",Mechanism Design I,"Plant cells expand and elongate. Their cumulative actuation defines organ morphing. Inspired by this modular transformability, this study proposes a modular concept for growing robots that will be able to grow by adding at their tip Transformable Modules (TMs). We provide a two-module implementation to evaluate the concept viability. We designed and characterized Shape-Retention Bellows (SRBs) that constitute the TM and are used to maintain the shape once the extension force is relaxed. We demonstrate module radial expansion and axial elongation in a straight and bent configuration (up to âˆ¼4â—¦). This is the first concept of growing robots to enact the robotâ€™s modularity and Transferability for future deployment in distributed growing systems capable of acting in various scenarios."
Design and Experimental Characterisation of a Novel Quasi-Direct Drive Actuator for Highly Dynamic Robotic Applications,"Carlos Adrian Perez Diaz, Ignacio Muñoz Planelles, Luis Daniel Martin Hernandez, Carlos Andres Candelo Zuluaga, Iván Jesús Torres-rodríguez, Jordi Marsa, Daniel Sanz Merodio, Miguel López Estévez","Arquimea Research Center,Institut de Robòtica i Informàtica Industrial, CSIC-UPC,ARC",Mechanism Design I,"This paper presents the design and experimen- tal results of a proprioceptive, high-bandwidth quasi-direct drive (QDD) actuator for highly dynamic robotic applications. A comprehensive review of the mechanical design of the PULSE115-60 actuator is presented, with particular focus on the design parameters affecting the dynamic performance of the actuator and a full specification is provided. Fundamental parameters to describe the dynamic behaviour of an actuator are discussed, and an experimental method to determine speed and torque bandwidth of the actuator is presented. A rigorous method to determine backdrive torque is also explained. Finally, experimental results quantifying the dynamic performance of the PULSE115-60 actuator are discussed. The PULSE115-60 actuator has a highly dynamic response, surpassing the torque bandwidth at low torque amplitudes showcased in state-of- the-art literature. The differences between current and torque bandwidth, two concepts often conflated in literature, are elucidated. Experimental procedures detailed in previous work are discussed and a novel standardised procedure is proposed for robust characterisation and fair comparison of different actuation systems. Finally, performance results for PULSE115- 60 are presented, demonstrating a torque bandwidth of 66.3 Hz at an amplitude of 6 NÂ·m, Â±0.11â—¦ of backlash and 0.37 NÂ·m of backdrive torque."
Design and Evaluation of a Reconfigurable 7-DOF Upper Limb Rehabilitation Exoskeleton with Gravity Compensation,"Linliang Zheng, Qingcong Wu, Yanghui Zhu, Qiang Zhang",Nanjing University of Aeronautics and Astronautics,Mechanism Design I,"With the development of society, aging population and the number of stroke patients is increasing year by year. Rehabilitation exoskeleton can help patients to carry out rehabilitation training and improve their activities of daily living (ADL). First of all, a reconfigurable exoskeleton for upper limb rehabilitation is designed in this paper. The exoskeleton combines gravity compensation with left-right arm switching function through its reconfigurability. Secondly, the motion space and singular configuration of the exoskeleton are analyzed. By changing the working mode of the gravity compensation device, the control experiment of the motor is carried out. The influence of gravity compensation device on motor driving torque and energy consumption is analyzed. Finally, the results of experiment show that, in the best case, the gravity compensation device can reduce the energy consumption by 41.15% and the maximum motor current by 33.56% of the driving element."
Flexible Omnidirectional Driving Gear Mechanism with Adaptation Over Arbitrary Curvatures,"Moses Gladson Selvamuthu, Kazuki Abe, Kenjiro Tadakuma, Riichiro Tadakuma","Yamagata University,Tohoku University",Mechanism Design I,"A support structure for flexible displays such as OLED or flexible LEDs was developed using the flexible omnidirectional driving gear mechanism. It is a gear mechanism having two degrees of freedom on one surface. This flexible display mechanism is expected to be placed inside a car dashboard as a human interface and for workspace optimization. In this study, we propose a novel flexible omnidirectional driving gear for supporting flexible displays discussing its design, motion range, repeatability, positional accuracy, and adaptability to any guide surface through magnetic coupling. The experiments showed satisfactory results for positional accuracy and repeatability with adaptability over a wide range of curvatures."
Tactile Robot Programming: Transferring Task Constraints into Constraint-Based Unified Force-Impedance Control,"Kübra Karacan, Robin Kirschner, Hamid Sadeghian, Fan Wu, Sami Haddadin","Technical University of Munich,TU Munich, Institute for Robotics and Systems Intelligence",Formal Methods in Robotics and Automation I,"Flexible manufacturing lines are required to meet the demand for customized and small batch-size products. Even though state-of-the-art tactile robots may provide the versatility for increased adaptability and flexibility, their potential is yet to be fully exploited. To support robotics deployment in manufacturing, we propose a task-based tactile robot programming paradigm that uses an object-centric tactile skill definition that directly links identified object constraints of the task to the definition of constraint-based unified force-impedance control. In this study, we first explain the basic concept of abstracting the task constraints experienced by the object and transferring them to the robot's operational space frame. Second, using the object-centric tactile skill definition, we synthesize unified force-impedance control and formalized holonomic constraints to enable flexible task execution. Later, we propose the quantified analysis metrics for the process by analyzing them as a typical example of flexible manipulation disassembly skills, e.g., levering and unscrew-driving regarding their object requirements. Supported by realistic experimental evaluation using a Franka Emika robot, our tactile robot programming approach for the direct translation between task-level constraints and robot control parameter design is shown to be a viable solution for increased robotic deployment in flexible manufacturing lines."
Online Modifications for Event-Based Signal Temporal Logic Specifications,"David Gundana, Hadas Kress-Gazit",Cornell University,Formal Methods in Robotics and Automation I,"In this paper we present a grammar and control synthesis framework for online modification of Event-based Signal Temporal Logic (STL) specifications, during execution. These modifications allow a user to change the robots' task in response to potential future violations, changes to the environment, or user-defined task design changes. In cases where a modification is not possible, we provide feedback to the user and suggest alternative modifications. We demonstrate our task modification process using a Hello Robot Stretch satisfying an Event-based STL specification."
Sampling-Based Reactive Synthesis for Nondeterministic Hybrid Systems,"Qi Heng Ho, Zachary Sunberg, Morteza Lahijanian","University of Colorado Boulder,University of Colorado",Formal Methods in Robotics and Automation I,"This paper introduces a sampling-based strategy synthesis algorithm for nondeterministic hybrid systems with complex continuous dynamics under temporal and reachability constraints. We model the evolution of the hybrid system as a two-player game, where the nondeterminism is an adversarial player whose objective is to prevent achieving temporal and reachability goals. The aim is to synthesize a winning strategy -- a reactive (robust) strategy that guarantees the satisfaction of the goals under all possible moves of the adversarial player. The approach is based on growing a (search) game-tree in the hybrid space by combining a sampling-based planning method with a novel bandit-based technique to select and improve on partial strategies. We provide conditions under which the algorithm is probabilistically complete, i.e., if a winning strategy exists, the algorithm will almost surely find it. The case studies and benchmark results show that the algorithm is general and consistently outperforms the state of the art."
Safety Verification of Closed-Loop Control System with Anytime Perception,"Lipsy Gupta, Jahid Chowdhury Choton, Pavithra Prabhakar",Kansas State University,Formal Methods in Robotics and Automation I,"In this paper, we consider the problem of safety analysis of a closed-loop control system with anytime perception sensor. We formalize the framework and present a general procedure for safety analysis using reachable set computation. We instantiate the procedure for two concrete classes, namely, the classical discrete-time linear system with linear state feedback controller and an extension with variable update rates. We present an exact computational method based on polyhedral manipulations for the first class and an over-approximate method for the second class. Our experimental results demonstrate the feasibility of the approach."
Model Predictive Robustness of Signal Temporal Logic Predicates,"Yuanfei Lin, Haoxuan Li, Matthias Althoff","Technical University of Munich,Technische Universität München",Formal Methods in Robotics and Automation I,"The robustness of signal temporal logic not only assesses whether a signal adheres to a specification but also provides a measure of how much a formula is fulfilled or violated. The calculation of robustness is based on evaluating the robustness of underlying predicates. However, the robustness of predicates is usually defined in a model-free way, i.e., without including the system dynamics. Moreover, it is often nontrivial to define the robustness of complicated predicates precisely. To address these issues, we propose a notion of model predictive robustness, which provides a more systematic way of evaluating robustness compared to previous approaches by considering model-based predictions. In particular, we use Gaussian process regression to learn the robustness based on precomputed predictions so that robustness values can be efficiently computed online. We evaluate our approach for the use case of autonomous driving with predicates used in formalized traffic rules on a recorded dataset, which highlights the advantage of our approach compared to traditional approaches in terms of expressiveness. By incorporating our robustness definitions into a trajectory planner, autonomous vehicles obey traffic rules more robustly than human drivers in the dataset."
Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning,"Noémie Jaquier, Leonel Rozo, Tamim Asfour","Karlsruhe Institute of Technology (KIT),Bosch Center for Artificial Intelligence",Formal Methods in Robotics and Automation I,"In the realm of robotics, numerous downstream robotics tasks leverage machine learning methods for processing, modeling, or synthesizing data. Often, this data comprises variables that inherently carry geometric constraints, such as the unit-norm condition of quaternions representing rigid-body orientations or the positive definiteness of stiffness and manipulability ellipsoids. Handling such geometric constraints effectively requires the incorporation of tools from differential geometry into the formulation of machine learning methods. In this context, Riemannian manifolds emerge as a powerful mathematical framework to handle such geometric constraints. Nevertheless, their recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the ""single tangent space fallacy"". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algorithm is applied. This paper provides a theoretical elucidation of various misconceptions surrounding this approach and offers experimental evidence of its shortcomings. Finally, it presents valuable insights to promote best practices when employing Riemannian geometry within robot learning applications."
Optimal Control Synthesis with Relaxed Global Temporal Logic Specifications for Homogeneous Multi-Robot Teams,"Disha Kamale, Cristian Ioan Vasile",Lehigh University,Formal Methods in Robotics and Automation I,"In this work, we address the problem of control synthesis for a homogeneous team of robots given a global temporal logic specification and formal user preferences for relaxation in case of infeasibility. The relaxation preferences are represented as a Weighted Finite-state Edit System and are used to compute a relaxed specification automaton that captures all allowable relaxations of the mission specification and their costs. For synthesis, we introduce a Mixed Integer Linear Programming (MILP) formulation that combines the motion of the team of robots with the relaxed specification automaton. Our approach combines automata-based and MILP-based methods and leverages the strengths of both approaches, while avoiding their shortcomings. Specifically, the relaxed specification automaton explicitly accounts for the progress towards satisfaction, and the MILP-based optimization approach avoids the state-space explosion associated with explicit product-automata construction, thereby efficiently solving the problem. The case studies highlight the efficiency of the proposed approach"
An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Temporal Logic Goals and Travel Duration Uncertainty,"Kaier Liang, Gustavo A. Cardona, Cristian Ioan Vasile",Lehigh University,Formal Methods in Robotics and Automation I,"This paper introduces an iterative approach to multi-agent route planning under chance constraints. A heterogeneous team of agents with various capabilities is tasked with a Capability Temporal Logic (CaTL) mission, a fragment of Signal Temporal Logic. The agents' motion is modeled as a finite weighted graph, where the weights represent travel durations. Given the probability distribution over the durations of each edge's traversal, we want to find paths for all agents such that (a) the specification robustness is maximized, (b) travel time is minimized, and (c) the success probability is maximized. We tackle the problem using an iterative approach. In each stage, it selects edges' traversal duration and success probabilities and then solves a multi-agent route planning problem. We use an efficient Mixed-Integer Linear Programming (MILP) encoding for the latter. Our method provides a framework for agents to make informed decisions in choosing the most suitable edge attributes (travel durations and success probabilities) that consider agents' capabilities to perform tasks in the environment. The proposed iterative method leverages graph structure to generate a more efficient search space. The effectiveness of our method is demonstrated through simulated case studies where obtaining the optimal solution would otherwise be computationally expensive. Our approach efficiently explores the solution space, generating better solutions and improving the performance of multi-agent route planning with uncertain travel durations."
Safe Networked Robotics with Probabilistic Verification,"Sai Shankar Narasimhan, Sharachchandra Bhat, Sandeep Chinchali","The University of Texas at Austin,University of Texas at Austin",Formal Methods in Robotics and Automation I,"Autonomous robots must utilize rich sensory data to make safe control decisions. To process this data, compute-constrained robots often require assistance from remote computation, or the cloud, that runs compute-intensive deep neural network perception or control models. However, this assistance comes at the cost of a time delay due to network latency, resulting in past observations being used in the cloud to compute the control commands for the present robot state. Such communication delays could potentially lead to the violation of essential safety properties, such as collision avoidance. This paper develops methods to ensure the safety of robots operated over communication networks with stochastic latency. To do so, we use tools from formal verification to construct a shield, i.e., a run-time monitor, that provides a list of safe actions for any delayed sensory observation, given the expected and maximum network latency. Our shield is minimally intrusive and enables networked robots to satisfy key safety constraints, expressed as temporal logic specifications, with desired probability. We demonstrate our approach on a real F1/10th autonomous vehicle that navigates in indoor environments and transmits rich LiDAR sensory data over congested WiFi links."
Phasic Diversity Optimization for Population-Based Reinforcement Learning,"Jingcheng Jiang, Haiyin Piao, Yu Fu, Yihang Hao, Chuanlu Jiang, Ziqi Wei, Xin Yang","Dalian University of Technology,Northwestern Polytechnical University,Yangzhou Collaborative Innovation Research Institute CO., LTD,Chinese Academy of Science",Multi-Robot Systems I,"Looking back at previous diversity work on Rein-forced learning, diversity is often achieved through the augmented loss function, which is required in the context of reward and diversity. Usually, the diversity optimization algorithm uses the multi-armed bandit algorithm to select the coefficient that predefined space. However, the dynamic distribution of the reward signal or quality of the MAB with diversity limits the performance of these methods. We introduce the Phase Diversity Optimization (PDO) algorithm, a population-based training-based framework that combines reward and diversity training to different stages, rather than optimizing multi-objective functions. In the secondary phase, having poor performance diversification through determinants does not replace the better agents in the archive. Rewards and diversity allow us to use the diversity of positive optimization in the secondary phase, where performance does not degrade. Furthermore, we built an aerial melee scenario agent."
VO-Safe Reinforcement Learning for Drone Navigation,"Feiqiang Lin, Changyun Wei, Raphael Grech, Ze Ji","Cardiff University,Hohai University,Spirent Communications",Multi-Robot Systems I,"This work is focused on reinforcement learning (RL)-based navigation for drones, whose localisation is based on visual odometry (VO). Such drones should avoid flying into areas with poor visual features, as this can lead to deteriorated localization or complete loss of tracking. To achieve this, we propose a hierarchical control scheme, which uses an RL-trained policy as the high-level controller to generate waypoints for the next control step and a low-level controller to guide the drone to reach subsequent waypoints. For the high-level policy training, unlike other RL-based navigation approaches, we incorporate awareness of VO performance into our policy by introducing pose estimation-related punishment. To aid robots in distinguishing between perception-friendly areas and unfavoured zones, we instead provide semantic scenes, as input for decision-making instead of raw images. This approach also helps minimise the sim-to-real application gap."
RoCo: Dialectic Multi-Robot Collaboration with Large Language Models,"Mandi Zhao, Shreeya Jain, Shuran Song","Stanford University,Columbia University",Multi-Robot Systems I,"We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset that evaluates LLMsâ€™ agent representation and reasoning capability. We experimentally demonstrate the effectiveness of our approach â€” it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility â€” in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. Project website: https://project-roco.github.io"
Collision Avoidance and Navigation for a Quadrotor Swarm Using End-To-End Deep Reinforcement Learning,"Zhehui Huang, Zhaojing Yang, Rahul Krupani, Baskin Senbaslar, Sumeet Batra, Gaurav Sukhatme","University of Southern California,NVIDIA,USC",Multi-Robot Systems I,"End-to-end deep reinforcement learning (DRL) for quadrotor control promises many benefits -- easy deployment, task generalization and real-time execution capability. Prior end-to-end DRL-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training RL policies. In this work, we propose an end-to-end DRL approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our work is the first work that demonstrates the possibility of learning neighbor-avoiding and obstacle-avoiding control policies trained with end-to-end DRL that transfers zero-shot to real quadrotors. Our approach scales to 32 robots with 80% obstacle density in simulation and 8 robots with 20% obstacle density in physical deployment. Website: https://sites.google.com/view/obst-avoid-swarm-rl."
C3F: Constant Collaboration and Communication Framework for Graph-Representation Dynamic Multi-Robotic Systems,"Hongda Jia, Zijian Gao, Cheng Yang, Bo Ding, Yuanzhao Zhai, Huaimin Wang",National University of Defense Technology,Multi-Robot Systems I,"Deep reinforcement learning (DRL) methods have been widely applied in distributed multi-robotic systems and successfully realized autonomous learning in many fields. In these fields, robots need to communicate and collaborate with other robots in real time, and reach agreed cognition for task assignment, which puts high requirements on efficiency and stability. However, robots may often get damaged even crash in complex environments, and have to be dynamically substituted. It seems not robust enough for most existing DRL works to make new robots fast adapt to current team policies, causing performance degradation. In this work, we get inspired by the genetic mechanism of social animals' instincts, and propose a robust multi-robotic collaboration and communication framework, C3F. It introduces graph-based representation to discover more features on the relevance among robots, and takes advantage of meta learning mechanism to conclude the general meta policy. When some robots crash and get replaced by new ones, this meta policy will be reused to guide new robots on how to quickly follow the existing collaboration and communication rules, and fast adapt to their roles in the team. The experiments on both the Webots simulator and the Starcraft II platform indicate that our methods have better performance compared with some SOTA methods, showing strong robustness and remarkable adaptability to the dynamic substitution in multi-robotic systems."
Multi-Level Action Tree Rollout (MLAT-R): Efficient and Accurate Online Multiagent Policy Improvement,"Andrea Henshall, Sertac Karaman","MIT,Massachusetts Institute of Technology",Multi-Robot Systems I,"Rollout algorithms are renowned for their abilities to correct for the suboptimalities of offline-trained base policies. In the multiagent setting, performing online rollout can require an exponentially large number of optimizations with respect to the number of agents. One-agent-at-a-time algorithms offer computationally efficient approaches to guaranteed policy improvement; however, this improvement is with respect to a state value estimate derived from a potentially poor base policy. Monte Carlo tree search (MCTS) provably converges to the true state value estimates; however, the exponentially large search space often makes its online use limited. Here, we present the Multi-Level Action Tree Rollout (MLAT-R) algorithm. MLAT-R provides 1) provable improvement over a base policy, 2) policy improvement with respect to the true state value, 3) applicability to any number of agents, and 4) an action space that grows linearly with the number of agents rather than exponentially. In this paper, we outline the algorithm, sketch a proof of its improvement over a base policy, and evaluate its performance on a challenging problem for which the base policy cannot reach a terminal state. Despite the challenging experimental setup, our algorithm reached a terminal state in 86% of all experiments, compared to 31% for state-of-the-art one-agent-at-a-time algorithms. In experiments involving MCTS, MLAT-R reached a terminal state in 99% of experiments compared to 92% for MCTS. MLAT-R achieved these results while considering an exponentially smaller action space than MCTS."
Stimulate the Potential of Robots Via Competition,"Kangyao Huang, Di Guo, Xinyu Zhang, Xiangyang Ji, Huaping Liu","Tsinghua University,Beijing University of Posts and Telecommunications",Multi-Robot Systems I,"It is common for us to feel pressure in a competition environment, which arises from the desire to obtain success comparing with other individuals or opponents. Although we might get anxious under the pressure, it could also be a drive for us to stimulate our potentials to the best in order to keep up with others. Inspired by this, we propose a competitive learning framework which is able to help individual robot to acquire knowledge from the competition, fully stimulating its dynamics potential in the race. Specifically, the competition information among competitors is introduced as the additional auxiliary signal to learn advantaged actions. We further build a Multiagent-Race environment, and extensive experiments are conducted, demonstrating that robots trained in competitive environments outperform ones that are trained with SoTA algorithms in single robot environment."
Multi-Agent Visual Coordination Using Optical Wireless Communication,"Haruyuki Nakagawa, Asako Kanezaki",Tokyo Institute of Technology,Multi-Robot Systems I,"Communication is a key element in applying multi-agent reinforcement learning to a wide range of real-world scenarios. We focus on optical wireless communication (OWC), which is a practical solution to be used in various real situations where radio communication is not available, such as underwater or in a lot of radio noise environment. OWC is a method of communicating only with other agents in visual range using light, unlike radio wave like communication which is mostly assumed in existing research on multi-agent reinforcement learning. Due to limited communication, when OWC is used, overall performance is generally degraded from the case with full communication. In this paper, we propose a reinforcement learning method that learns visual coordination behavior using OWC. Our proposed visually cooperative behavior enables agents equipped with limited field of view (FOV) cameras to efficiently comprehend and imagine their surrounding environment through cooperative communication. Experimental results in simulation demonstrated that, using the proposed visual coordination method, multi-agents using OWC with general FOV show comparable performance to those with radio wave like full communication. Additionally, it has been demonstrated that this method can improve performance in various multi-agent reinforcement learning algorithms. We also implement OWC devices on real mobile robots and demonstrated the proposed multi-agent operation."
Multi-Modal 3D Human Tracking for Robots in Complex Environment with Siamese Point-Video Transformer,"Shuo Xin, Zhen Zhang, Mengmeng Wang, Xiaojun Hou, Yaowei Guo, Xiao Kang, Liang Liu, Yong Liu","Zhejiang University,China North Vehicle Research Institute",Sensors and Audition,"Tracking a specific person in 3D scene is gaining momentum due to its numerous applications in robotics. Currently, most 3D trackers focus on driving scenarios with neglected jitter and uncomplicated surroundings, which results in their severe degeneration in complex environments, especially on jolting robot platforms (only 20-60% success rate). To improve the accuracy, a Point-Video-based Transformer Tracking model (PVTrack) is presented for robots. It is the first multi-modal 3D human tracking work that incorporates point clouds together with RGB videos to achieve information complementarity. Moreover, PVTrack proposes the Siamese Point-Video Transformer for feature aggregation to overcome dynamic environments, which captures more target-aware information through the hierarchical attention mechanism adaptively. Considering the violent shaking on robots and rugged terrains, a lateral Human-ware Proposal Network is designed together with an Anti-shake Proposal Compensation module. It alleviates the disturbance caused by complex scenes as well as the particularity of the robot platform. Experiments show that our method achieves state-of-the-art performance on both KITTI/Waymo datasets and a quadruped robot for various indoor and outdoor scenes."
Efficient Gesture Recognition on Spiking Convolutional Networks through Sensor Fusion of Event-Based and Depth Data,"Lea Steffen, Thomas Trapp, Arne Roennau, Rüdiger Dillmann","FZI Research Center for Information Technology, ,,,,, Karlsruhe,,FZI Research Center for Information Technology,FZI Forschungszentrum Informatik, Karlsruhe,FZI - Forschungszentrum Informatik - Karlsruhe",Sensors and Audition,"As intelligent systems become increasingly important in our daily lives, new ways of interaction are needed. Classical user interfaces pose issues for the physically impaired and are partially not practical or convenient. Gesture recognition is an alternative, but often not reactive enough when conventional cameras are used. This work proposes a Spiking Convolutional Neural Network, processing event- and depth data for gesture recognition. The network is simulated using the open-source neuromorphic computing framework LAVA for offline training and evaluation on an embedded system. For the evaluation three open source data sets are used. Since these do not represent the applied bi-modality, a new data set with synchronized event- and depth data was recorded. The results show the viability of temporal encoding on depth information and modality fusion, even on differently encoded data, to be beneficial to network performance and generalization capabilities."
Smoothly Connected Preemptive Impact Reduction and Contact Impedance Control,"Hikaru Arita, Hayato Nakamura, Takuto Fujiki, Kenji Tahara",Kyushu University,Sensors and Audition,"This study proposes novel control methods that lower impact force by preemptive movement and smoothly transition to conventional contact-based impedance control. These techniques are suggested for application in force control-based robots and position/velocity control-based robots. Strong impact forces have a negative influence on multiple robotic tasks. Recently, preemptive impact reduction techniques that expand conventional contact impedance control using proximity sensors have been examined. However, a seamless transition from impact reduction to contact impedance control has yet to be demonstrated. In contrast, our proposed methods solve this problem. The preemptive impact reduction feature can be added to an already-implemented impedance controller because the parameter design is divided into impact reduction and contact impedance control. There is no abrupt alteration in the contact force during the transition. Furthermore, although the preemptive impact reduction uses a crude optical proximity sensor, the influence of reflectance is minimized. Analyses and real-world experiments confirm these features, which are useful for many robots performing contact tasks."
Point Cloud-Based Control Barrier Function Regression for Safe and Efficient Vision-Based Control,"Massimiliano De Sa, Venkata Naga Prasanth Kotaru, Koushil Sreenath","University of California, Berkeley,University of California Berkeley",Sensors and Audition,"Control barrier functions have become an increasingly popular framework for safe real-time control. In this work, we present a computationally low-cost framework for synthesizing barrier functions over point cloud data for safe vision-based control. We take advantage of surface geometry to locally define and synthesize a quadratic CBF over a point cloud. This CBF is used in a CBF-QP for control and verified in simulation on quadrotors and in hardware on quadrotors and the TurtleBot3. This technique enables safe navigation through unstructured and dynamically changing environments and is shown to be significantly more efficient than current methods."
Stability Analysis of Plane-To-Plane Positioning by Proximity-Based Control,"John Thomas, Francois Chaumette","Inria Rennes,Inria center at University of Rennes",Sensors and Audition,"In this paper, we discuss the stability analysis of Plane-to-Plane positioning task when the task is designed in proximity sensor space. We utilize a multi-sensor arrangement of proximity sensors that forms a proximity array to obtain the necessary information in sensor space. For the task considered, we provide closed-form equations for the closed-loop system by obtaining the analytical form of pseudo-inverse for the interaction matrix involved. This further enables us to suggest a new control law producing a decoupled exponential decrease of the sensor errors in perfect conditions, while being more robust to estimation errors in the surface normal. By applying Gershgorinâ€™s theorem to the closed-form matrices, we are able to provide conditions for stability with respect to errors in extrinsic parameters and surface normal. Simulation results are provided to discuss the robustness of the task with respect to these modeling parameters."
An Image Acquisition Scheme for Visual Odometry Based on Image Bracketing and Online Attribute Control,"Shuyang Zhang, Jinhao He, Bohuan Xue, Wu Jin, Pengyu Yin, Jianhao Jiao, Ming Liu","The Hong Kong University of Science and Technology,The Hong Kong University of Science and Technology (Guangzhou),HKUST,UESTC,Nanyang Technological University,University College London,Hong Kong University of Science and Technology (Guangzhou)",Sensors and Audition,"Visual odometry system is challenged by complex illumination environments. Image quality and its consistency in the time domain directly determine feature detection and tracking performance, which further affect the robustness and accuracy of the entire system. In this paper, an image acquisition scheme with image bracketing patterns is proposed. Images with different exposure levels are continuously captured to explore the scene under varying illumination sufficiently. An attribute control method is designed to adjust image exposures within the brackets online. Gaussian process regression fits the relationship between image quality metric and exposure via image synthesis technique, and the optimal exposures for the next bracket are obtained directly without attempts to ensure a quick response. Experiments show our acquisition system's effectiveness and performance improvement for VO tasks in complex illumination scenes."
MagicTip: A Novel High-Resolution 3D Multi-Layer Grid-Based Tactile Sensor,"Wen Fan, Haoran Li, Dandan Zhang","University of Bristol,Imperial College London",Sensors and Audition,"Accurate robotic control over interactions with the environment is fundamentally grounded in understanding tactile contacts. In this paper, we introduce MagicTac, a novel high-resolution grid-based tactile sensor. This sensor employs a 3D multi-layer grid-based design, inspired by the Magic Cube structure. This structure can help increase the spatial resolution of MagicTac to perceive external interaction contacts. Moreover, the sensor is produced using the multi-material additive manufacturing technique, which simplifies the manufacturing process while ensuring repeatability of production. Compared to traditional vision-based tactile sensors, it offers the advantages of i) high spatial resolution, ii) significant affordability, and iii) fabrication-friendly construction that requires minimal assembly skills. We evaluated the proposed MagicTac in the tactile reconstruction task using the deformation field and optical flow. Results indicated that MagicTac could capture fine textures and is sensitive to dynamic contact information. Through the grid-based multi-material additive manufacturing technique, the affordability and productivity of MagicTac can be enhanced with a minimum manufacturing cost of Â£4.76 and a minimum manufacturing time of 24.6 minutes."
Microphone Pair Training for Robust Sound Source Localization with Diverse Array Configurations,"Inkyu An, Guoyuan An, Taeyoung Kim, Sung-Eui Yoon","ETRI,KAIST",Sensors and Audition,"We present a novel sound source localization method that leverages microphone pair training, designed to deliver robust performance in various real-world environments. Existing deep learning (DL)-based approaches face scalability issues when dealing with various types of microphone arrays. To address these issues, our approach has been structured into two training steps: the first step focuses on microphone pair training, while the second step is designed for array geometry-aware training. The first training step enables our model to learn from multiple datasets covering various real-world situations, allowing it to robustly estimate the time difference of arrival (TDoA). Our robust-TDoA model incorporates a Mel scale learnable filter bank (MLFB) and a hierarchical frequency-to-time attention network (HiFTA-net). This allows it to effectively learn from various situations in multiple datasets, including those involving simultaneous sources and various sound events. The second training step enables our approach to estimate the direction of arrival (DoA) of sound based on TDoA information computed by our robust-TDoA model, which begins with parameters acquired during the first training step. During this process, our approach can be trained to accommodate geometry information of the target microphone array, which can span diverse array types. As a result, our method demonstrates robust performance across two DoA estimation tasks using three different types of arrays."
Mobile Bot Rotation Using Sound Source Localization and Distant Speech Recognition,"Swapnil Sontakke, Pradyoth Hegde, Prashant Bannulmath, Deepak K T","Indian Institute of Information Technology Dharwad,INDIAN INSTITUTE OF INFORMATION TECHNOLOGY DHARWAD",Sensors and Audition,"In the last few years, mobile robots such as floor cleaners, assistive robots, and home telepresence have become an essential part of our day-to-day activities. In human-computer interaction, speech is the preferred way of communication, especially in indoor environments. This paper proposes a speech module to rotate the mobile robot. It has two components, namely, a distant automatic speech recognizer and a sound source localizer. To build distant speech recognizer, far-field speech data is collected at 1, 3, and 5-meter distances. The model performs well even at a 5-meter distance with a Word Error Rate of 40.38% and a Character Error Rate of 28.85%. The direction of arrival of the speech signal is computed from the 4-mic circular array microphone. The speech module is integrated with the Robot Operating System and physically demonstrated on Turtlebot3 Waffle Pi. It is observed that the speech recognizer and sound source localizer work well in the reverberant indoor environment with a small single-board computer."
Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes,"Alloy Das, Sanket Biswas, Umapada Pal, Josep Lladós","Indian Statistical Institute, Kolkata,Computer Vision Center, Universitat Autònoma de Barcelona",2D/3D Visual Perception,"When used in a real-world noisy environment, the capacity to generalize to multiple domains is essential for any autonomous scene text spotting system. However, existing state-of-the-art methods employ pretraining and fine-tuning strategies on natural scene datasets, which do not exploit the feature interaction across other complex domains. In this work, we explore and investigate the problem of domain-agnostic scene text spotting, i.e., training a model on multi-domain source data such that it can directly generalize to target domains rather than being specialized for a specific domain or scenario. In this regard, we present the community a text spotting validation benchmark called Under-Water Text (UWT) for noisy underwater scenes to establish an important case study. Moreover, we also design an efficient super-resolution based end-to-end transformer baseline called DA-TextSpotter which achieves comparable or superior performance over existing text spotting architectures for both regular and arbitrary-shaped scene text spotting benchmarks in terms of both accuracy and model efficiency. The dataset, code and pre-trained models have been released in our GitHub."
Masked Local-Global Representation Learning for 3D Point Cloud Domain Adaptation,"Bowei Xing, Xianghua Ying, Ruibin Wang",Peking University,2D/3D Visual Perception,"Point cloud is a popular and widely used geometric representation, which has attracted significant attention in 3D vision. However, the geometric variability of point cloud representations across different datasets can cause domain discrepancies, which hinder knowledge transfer and model generalization, resulting in degraded performance in target domain. In this paper, we present a novel approach to improve point cloud domain adaptation by employing masked representation learning in a self-supervised manner. Specifically, our method combines masked feature prediction and masked sample consistency to encode both local structure and global semantic information for learning invariant point cloud representation across domains. Moreover, to learn domain-specific representation and transfer knowledge from source to target, we propose prototype-calibrated self-training. By exploiting class-wise prototypes in the shared feature space, the soft pseudo labels can be adaptively denoised, which benefits the decision boundary learning in target domain. We conduct experiments on PointDA-10 and PointSegDA for 3D point cloud shape classification and semantic segmentation, respectively. The results demonstrate the effectiveness of our method and show that we can achieve the new state-of-the-art performance on point cloud domain adaptation."
Continuous Adaptation in Person Re-Identification for Robotic Assistance,"Federico Rollo, Andrea Zunino, Nikos Tsagarakis, Enrico Mingo Hoffman, Arash Ajoudani","Leonardo S.p.A,Leonardo,Istituto Italiano di Tecnologia,INRIA Nancy - Grand Est",2D/3D Visual Perception,"In scenarios of Human-Robot Interaction (HRI), it is often assumed that the robot should cooperate with the closest individual or that only one person is present. However, in real-life situations, such as shop floor operations, this assumption may not hold. Thus, it becomes necessary for a robot to recognize a specific target in a crowded environment. To address this problem, we propose a person re-identification module that uses continuous visual adaptation techniques. This module ensures that the robot can seamlessly cooperate with the appropriate individual despite its appearance changes or partial or total occlusions. We used both a laboratory environment and an HRI scenario where the robot followed a person to test our framework. During the test, the targets were asked to change their appearance and disappear from the camera's field of view to test the module's ability to handle challenging cases of occlusion and outfit variations. We compared our framework with a state-of-the-art Multi-Object Tracking (MOT) method, and the results showed that our module, shortly named CARPE-ID, accurately tracked each selected target throughout the experiments in all cases except for two cases. In contrast, the MOT had an average of 4 tracking errors for each video."
Spectral Geometric Verification: Re-Ranking Point Cloud Retrieval for Metric Localization,"Kavisha Vidanapathirana, Peyman Moghadam, Sridha Sridharan, Clinton Fookes","Queensland University of Technology,CSIRO",2D/3D Visual Perception,"In large-scale metric localization, an incorrect result during retrieval will lead to an incorrect pose estimate or loop closure. Re-ranking methods propose to take into account all the top retrieval candidates and re-order them to increase the likelihood of the top candidate being correct. However, state-of-the-art re-ranking methods are inefficient when re-ranking many potential candidates due to their need for resource intensive point cloud registration between the query and each candidate. In this work, we propose an efficient spectral method for geometric verification (named SpectralGV) that does not require registration. We demonstrate how the optimal inter-cluster score of the correspondence compatibility graph of two point clouds represents a robust fitness score measuring their spatial consistency. This score takes into account the subtle geometric differences between structurally similar point clouds and therefore can be used to identify the correct candidate among potential matches retrieved by global similarity search. SpectralGV is deterministic, robust to outlier correspondences, and can be computed in parallel for all potential candidates. We conduct extensive experiments on 5 large-scale datasets to demonstrate that SpectralGV outperforms other state-of-the-art re-ranking methods and show that it consistently improves the recall and pose estimation of 3 state-of-the-art metric localization architectures while having a negligible effect on their runtime."
Incorporating Scene Graphs into Pre-Trained Vision-Language Models for Multimodal Open-Vocabulary Action Recognition,"Chao Wei, Zhidong Deng",Tsinghua University,2D/3D Visual Perception,"This paper presents Action-SGFA, a novel action feature alignment approach to learn unified joint embeddings across four action modalities incorporating scene graph (SG) comprehension. A new training paradigm for Action-SGFA is also devised to improve pre-trained VL models using datasets with SG annotation. When learning from image-SG pairs, it captures structure-associated action knowledge for visual and textual encoders. SG supervision generates fine-grained captions based on various graph augmentations highlighting different compositional aspects of action scenes. Furthermore, our research reveals that all combinations of paired data are unnecessary to train such unified embeddings, and only image-paired data is sufficient to bind all action modalities together. Our Action-SGFA can leverage existing large VL models, enhancing their zero-shot capabilities of new modalities due to their natural pairings with images. The open-vocabulary zero-shot performance improves with the strength of the pre-trained VL model and the SG comprehension. We establish a new state-of-the-art in several zero-shot action recognition tasks across modalities, significantly surpassing the vanilla skeleton zero-shot method by 27.0% and 19.7% on NTU-60 and NTU-120, respectively. Additionally, in the context of RGB videos, we surpass the state-of-the-art method on Kinetics-400 by 2.1%."
LPS-Net: Lightweight Parameter-Shared Network for Point Cloud-Based Place Recognition,"Chengxin Liu, Guiyou Chen, Ran Song","Shandong University,ShanDong University",2D/3D Visual Perception,"With innovation in fields such as autonomous driving and augmented reality, point cloud-based place recognition has gained significant attention. Many methods try to address this problem by extracting and matching global descriptors in a database, but they often must balance the extraction of comprehensive contextual information and large model sizes. To overcome this challenge, we propose a lightweight parameter-shared network (LPS-Net), which includes multiple bidirectional perception units (BPUs) to extract multi-scale long-range contextual information and parameter-shared NetVLADs (PS-VLADs) to aggregate descriptors. A BPU includes a parameter-shared convolution module (SharedConv) that significantly compresses the model and enhances its ability to capture informative features. In PS-VLADs, we replace half the parameters used in the original NetVLAD with trainable scalars, which further reduces the model size, and theoretically prove their equivalence. Experimental results demonstrate that LPS-Net achieves state-of-the-art performance at the task of point cloud-based place recognition while maintaining a small model size. Code and supplementary materials can be found at https://github.com/Yavinr/LPS-Net."
Joint Response and Background Learning for UAV Visual Tracking,"Biao Wang, Wenling Li, Bin Zhang, Yang Liu","Beihang University,Beijing University of Posts and Telecommunications",2D/3D Visual Perception,"Correlation filter (CF)-based approaches have gained widespread attention in the field of unmanned aerial vehicle (UAV) visual tracking due to their light-weight characteristics. However, CFs are prone to generating low-quality response in challenging UAV scenarios, e.g., fast motion and background clutter. In this paper, in order to model the tracker more robustly, we first conduct an effective regularization analysis from the perspectives of response- and background-learning. Specifically, to address response degradation, we propose a module for learning temporal consistency and reversibility of response, supplemented by a novel background-aware module to enhance the ability to learn from negative samples. In addition, we propose a fast coarse-to-fine scale search strategy, which alleviates the challenges in estimating bounding boxes under non-uniform aspect ratios. We have developed two tracker versions, namely RBLT and DeepRBLT, based on the depth of the features. Comprehensive experiments on four UAV benchmarks and one generic benchmark have indicated the superiority of our trackers compared to other state-of-the-art trackers, with enough speed for real-time applications."
ZS6D: Zero-Shot 6D Object Pose Estimation Using Vision Transformers,"Philipp Ausserlechner, David Dylan Haberger, Stefan Thalhammer, Jean-baptiste Weibel, Markus Vincze","TU Wien,Vienna University of Technology",2D/3D Visual Perception,"As robotic systems increasingly encounter complex and unconstrained real-world scenarios, there is a demand to recognize diverse objects. The state-of-the-art 6D object pose estimation methods rely on object-specific training and therefore do not generalize to unseen objects. Recent novel object pose estimation methods are solving this issue using task-specific fine-tuned CNNs for deep template matching. This adaptation for pose estimation still requires expensive data rendering and training procedures. MegaPose for example is trained on a dataset consisting of two million images showing 20,000 different objects to reach such generalization capabilities. To overcome this shortcoming we introduce ZS6D, for zero-shot novel object 6D pose estimation. Visual descriptors, extracted using pre-trained Vision Transformers (ViT), are used for matching rendered templates against query images of objects and for establishing local correspondences. These local correspondences enable deriving geometric correspondences and are used for estimating the object's 6D pose with RANSAC-based Ptextit{n}P. This approach showcases that the image descriptors extracted by pre-trained ViTs are well-suited to achieve a notable improvement over two state-of-the-art novel object 6D pose estimation methods, without the need for task-specific fine-tuning. Experiments are performed on LMO, YCBV, and TLESS. In comparison to MegaPose, we improve the Average Recall on all three datasets and compared to OSOP we improve on two datasets. The code is available at https://github.com/PhilippAuss/ZS6D."
Fluxformer: Flow-Guided Duplex Attention Transformer Via Spatio-Temporal Clustering for Action Recognition,"Younggi Hong, Min Ju Kim, Isack Lee, Seok Bong Yoo",Chonnam National University,2D/3D Visual Perception,"Vision transformers have demonstrated impressive performance in various robotics and automation applications, such as classification automation and action recognition. However, the drawback of transformers is their quadratic increase in computing resources with larger inputs and dependence on considerable data for training. Most action recognition models using the transformer structure rely on a few frames from the original video to reduce computation, so temporal information is compromised by low frame rates. Spatial information is also compromised by reducing the number of embeddings as the transformer layer iterates. The letter proposes a robust model for action recognition that overcomes the limitations of most action recognition models with the transformer structure using the duplex attention function, flow-guided information, RGB information, and spatial support tokens. The proposed duplex attention mechanism leverages optical flow and RGB to address the lack of temporal information. The method employs spatial interest clustering to convert input data into tokens, improving the preservation of spatial information. Finally, meaningful action event frames are extracted by analyzing the flow and clustering to distinguish scenes. The experimental results reveal that the proposed model outperforms state-of-the-art methods in action recognition accuracy."
LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation,"Shuo Cheng, Danfei Xu","Gatech,Georgia Institute of Technology",Continual Learning,"To assist with everyday human activities, robots must solve complex long-horizon tasks and generalize to new settings. Recent deep reinforcement learning (RL) methods show promise in fully autonomous learning, but they struggle to reach long-term goals in large environments. On the other hand, Task and Motion Planning (TAMP) approaches excel at solving and generalizing across long-horizon tasks, thanks to their powerful state and action abstractions. But they assume predefined skill sets, which limits their real-world applications. In this work, we combine the benefits of these two paradigms and propose an integrated task planning and skill learning framework named LEAGUE (Learning and Abstraction with Guidance). LEAGUE leverages the symbolic interface of a task planner to guide RL- based skill learning and creates abstract state space to enable skill reuse. More importantly, LEAGUE learns manipulation skills in-situ of the task planning system, continuously growing its capability and the set of tasks that it can solve. We evaluate LEAGUE on four challenging simulated task domains and show that LEAGUE outperforms baselines by large margins. We also show that the learned skills can be reused to accelerate learning in new tasks domains and transfer to a physical robot platform."
Test-Time Adaptation in the Dynamic World with Compound Domain Knowledge Management,"Junha Song, Kwanyong Park, Inkyu Shin, Sanghyun Woo, Chaoning Zhang, In So Kweon",KAIST,Continual Learning,"Prior to the deployment of robotic systems, pre-training the deep-recognition models on all potential visual cases is infeasible in practice. Hence, test-time adaptation (TTA) allows the model to adapt itself to novel environments and improve its performance during test time (i.e., lifelong adaptation). Several works for TTA have shown promising adaptation performances in continuously changing environments. However, our investigation reveals that existing methods are vulnerable to dynamic distributional changes and often lead to overfitting of TTA models. To address this problem, this paper first presents a robust TTA framework with compound domain knowledge management. Our framework helps the TTA model to harvest the knowledge of multiple representative domains (i.e., compound domain) and conduct the TTA based on the compound domain knowledge. In addition, to prevent overfitting of the TTA model, we devise novel regularization which modulates the adaptation rates using domain-similarity between the source and the current target domain. With the synergy of the proposed framework and regularization, we achieve consistent performance improvements in diverse TTA scenarios, especially on dynamic domain shifts. We demonstrate the generality of proposals via extensive experiments including image classification on ImageNet-C and semantic segmentation on GTA5, C-driving, and corrupted Cityscapes datasets."
VERSE: Virtual-Gradient Aware Streaming Lifelong Learning with Anytime Inference,"Soumya Banerjee, Vinay Kumar Verma, Avideep Mukherjee, Deepak Gupta, Vinay Namboodiri, Piyush Rai","IIT Kanpur,AMAZON,University of Bath,University of Utah",Continual Learning,"Lifelong learning or continual learning is the problem of training an AI agent continuously while also preventing it from forgetting its previously acquired knowledge. Streaming lifelong learning is a challenging setting of lifelong learning with the goal of continuous learning in a dynamic non-stationary environment without forgetting. We introduce a novel approach to lifelong learning, which is streaming (observes each training example only once), requires a single pass over the data, can learn in a class-incremental manner, and can be evaluated on-the-fly (anytime inference). To accomplish these, we propose a novel emph{virtual gradients} based approach for continual representation learning which adapts to each new example while also generalizing well on past data to prevent catastrophic forgetting. Our approach also leverages an exponential-moving-average-based semantic memory to further enhance performance. Experiments on diverse datasets with temporally correlated observations demonstrate our method's efficacy and superior performance over existing methods."
Experience Consistency Distillation Continual Reinforcement Learning for Robotic Manipulation Tasks,"Chao Zhao, Jie Xu, Ru Peng, Xingyu Chen, Kuizhi Mei, Xuguang Lan",Xi'an Jiaotong University,Continual Learning,"Continual reinforcement learning, which aims to help robots acquire skills without catastrophic forgetting, obviating the need to re-learn all tasks from scratch. In order to enable lifelong acquisition of skills in robots, replay-based continual reinforcement learning has emerged as a promising research direction. These techniques replay data from previous tasks to mitigate forgetting when learning new skills. However, existing replay-based methods store poor representative experience, and the experience utilization of old tasks is inefficient. To address these issues, we propose a experience consistency distillation method for robot continual reinforcement learning to improve the data efficiency of the experience. Specifically, the experience of old tasks are distilled to obtain Markov Decision Process (MDP) data with high compression ratio and information content. To ensure consistent data distributions before and after distillation, we further utilize a FrÃ©chet Inception Distance (FID) loss as a regularization constraint. In order to improve experience utilization efficiency, the policy is then trained using both the distilled data and current task data, with policy distillation performed based on uncertainty metrics. Our method is validated in the continual reinforcement learning simulation platform and real scene with a UR5e robot arm. Experimental results indicate that our method achieves higher success and lower buffer size requirement compared to other methods."
Adapting to the â€œOpen Worldâ€: The Utility of Hybrid Hierarchical Reinforcement Learning and Symbolic Planning,"Pierrick Lorang, Helmut Horvath, Tobias Kietreiber, Patrik Zips, Clemens Heitzinger, Matthias Scheutz","AIT Austrian Institute of Technology GmbH - Tufts University,Technische Universität Wien, TUW,University of Applied Sciences St. Poelten,AIT Austrian Institute of Technology GmbH,TU Wien,Tufts University",Continual Learning,"Open-world robotic tasks such as autonomous driving pose significant challenges to robot control due to unknown and unpredictable events that disrupt task performance. Neural network-based reinforcement learning (RL) techniques (like DQN, PPO, SAC, etc.) struggle to adapt in large domains and suffer from catastrophic forgetting. Hybrid planning and RL approaches have shown some promise in handling environmental changes but lack efficiency in accommodation speed. To address this limitation, we propose an enhanced hybrid system with a nested hierarchical action abstraction that can utilize previously acquired skills to effectively tackle unexpected novelties. We show that it can adapt faster and generalize better compared to state-of-the-art RL and hybrid approaches, significantly improving robustness when multiple environmental changes occur at the same time."
Lifelong Robot Library Learning: Bootstrapping Composable and Generalizable Skills for Embodied Control with Language Models,"Georgios Tziafas, Hamidreza Kasaei",University of Groningen,Continual Learning,"Large Language Models (LLMs) have emerged as a new paradigm for embodied reasoning and control, most recently by generating robot policy code that utilizes a custom library of vision and control primitive skills. However, prior arts fix their skills library and steer the LLM with carefully hand-crafted prompt engineering, limiting the agent to a stationary range of addressable tasks. In this work, we introduce LRLL, an LLM-based lifelong learning agent that continuously grows the robot skill library to tackle manipulation tasks of ever-growing complexity. LRLL achieves this with four novel contributions : 1) a soft memory module that allows dynamic storage and retrieval of past experiences to serve as context, 2) a self-guided exploration policy that proposes new tasks in simulation, 3) a skill abstractor that distills recent experiences into new library skills, and 4) a lifelong learning algorithm for enabling human users to bootstrap new skills with minimal online interaction. LRLL continuously transfers knowledge from the memory to the library, building composable, general and interpretable policies, while bypassing gradient-based optimization, thus relieving the learner from catastrophic forgetting. Empirical evaluation in a simulated tabletop environment shows that LRLL outperforms end-to-end and vanilla LLM approaches in the lifelong setup, while learning skills that are transferable to the real world. Project material will become available at the webpage https://gtziafas.github.io/LRLL_project/"
Lifelong Robot Learning with Human Assisted Language Planners,"Meenal Parakh, Alisha Fong, Anthony Simeonov, Tao Chen, Abhishek Gupta, Pulkit Agrawal","Princeton University,Massachusetts Institute of Technology,University of Washington,MIT",Continual Learning,"Large Language Models (LLMs) have been shown to act like planners that can decompose high-level instructions into a sequence of executable instructions. However, current LLM-based planners are only able to operate with a fixed set of skills. We overcome this critical limitation and present a method for using LLM-based planners to query new skills and teach robots these skills in a data and time-efficient manner for rigid object manipulation. Our system can re-use newly acquired skills for future tasks, demonstrating the potential of open world and lifelong learning. We evaluate the proposed framework on multiple tasks in simulation and the real world. Videos are available at: url{https://sites.google.com/view/halp-submission}."
Probabilistic Spiking Neural Network for Robotic Tactile Continual Learning,"Senlin Fang, Yi Wen Liu, Chengliang Liu, Jingnan Wang, Yuanzhe Su, Yupo Zhang, Hoiio Kong, Zhengkun Yi, Xinyu Wu","Shenzhen Institute of Advanced Technology,Shenzhen Institute of Advanced Technology,University of Chinese ,Shenzhen Institute of Advanced Technology, Chinese Academy of Sc,University of Chinese Academy of Sciences,Southern University of Science and Technology,City University of Macau,CAS",Continual Learning,"The sense of touch is essential for robots to perform various daily tasks. Artificial Neural Networks have shown significant promise in advancing robotic tactile learning. However, due to the changing of tactile data distribution as robots encounter new tasks, ANN-based robotic tactile learning suffers from catastrophic forgetting. To solve this problem, we introduce a novel continual learning (CL) framework called the Probabilistic Spiking Neural Network with Variational Continual Learning (PSNN-VCL). In this framework, PSNN introduces uncertainty during spike emission and can apply fast Variational Inference by optimizing the uncertainty through backpropagation, which significantly reduces the required model parameters for VCL. We establish a robotic tactile CL benchmark using publicly available datasets to evaluate our method. Experimental results demonstrated that, compared to other CL methods, PSNN-VCL not only achieves superior performance in terms of widely used CL metrics but also achieves at least a 50% reduction in model parameters on the robotic tactile CL benchmark."
LOTUS: Continual Imitation Learning for Robot Manipulation through Unsupervised Skill Discovery,"Weikang Wan, Yifeng Zhu, Rutav Shah, Yuke Zhu","Peking University,The University of Texas at Austin",Continual Learning,"We introduce LOTUS, a continual imitation learning algorithm that empowers a physical robot to continuously and efficiently learn to solve new manipulation tasks throughout its lifespan. The core idea behind LOTUS is constructing an ever-growing skill library from a sequence of new tasks with a small number of corresponding task demonstrations. LOTUS starts with a continual skill discovery process using an open-vocabulary vision model, which extracts skills as recurring patterns presented in unstructured demonstrations. Continual skill discovery updates existing skills to avoid catastrophic forgetting of previous tasks and adds new skills to exhibit novel behaviors. LOTUS trains a meta-controller that flexibly composes various skills to tackle vision-based manipulation tasks in the lifelong learning process. Our comprehensive experiments show that LOTUS outperforms state-of-the-art baselines by over 11% in average success rates, showing its superior knowledge transfer ability compared to prior methods. More experimental videos and results can be found on the project website: https://ut-austin-rpl.github.io/Lotus/"
Synthesize Efficient Safety Certificates for Learning-Based Safe Control Using Magnitude Regularization,"Haotian Zheng, Haitong Ma, Sifa Zheng, Shengbo Li, Jianqiang Wang","Tsinghua University,Harvard University",Learning,"Safety certificates based on energy functions can provide demonstrable safety for complex robotic systems. However, all recent studies on learning-based energy function synthesis only consider the feasibility of the control policy, which might cause over-conservativeness and even fail to achieve the control goal. To solve the problem of over-conservative controllers, we proposed the magnitude regularization technique to improve the controller performance of safe controllers by reducing the conservativeness inside the energy function, while keeping the promising provable safety guarantees. Specifically, we quantify the conservativeness by the magnitude of the energy function, and we reduce the conservativeness by adding a magnitude regularization term to the synthesis loss. We propose an algorithm using reinforcement learning (RL) for synthesis to unify the learning process of safe controllers and energy functions. We conducted simulation experiments on Safety Gym and real-robot experiments using small quadrotors. Simulation results show that the proposed algorithm does reduce the conservativeness of the energy function and outperforms baselines in terms of controller performance while maintaining safety. Real-robot experiments have shown that the proposed algorithm indeed reduce conservativeness on the small quadrotors."
"On the Optimality, Stability, and Feasibility of Control Barrier Functions: An Adaptive Learning-Based Approach","Alaa Eddine Chriat, Chuangchuang Sun",Mississippi State University,Learning,"Safety has been a critical issue for the deployment of learning-based approaches in real-world applications. To address this issue, control barrier function (CBF) and its variants have attracted extensive attention for safety-critical control. However, due to the myopic one-step nature of CBF and the lack of principled methods to design the class-$mathcal{K}$ functions, there are still fundamental limitations of current CBFs: optimality, stability, and feasibility. In this paper, we proposed a novel and unified approach to address these limitations with Adaptive Multi-step Control Barrier Function (AM-CBF), where we parameterize the class-K function by a neural network and train it together with the reinforcement learning policy. Moreover, to mitigate the myopic nature, we propose a novel multi-step training and single-step execution paradigm to make CBF farsighted while the execution remains solving a single-step convex quadratic program. Our method is evaluated on the first and second-order systems in various scenarios, where our approach outperforms the conventional CBF both qualitatively and quantitatively."
Learning Failure Prevention Skills for Safe Robot Manipulation,"Abdullah Cihan Ak, Eren Erdal Aksoy, Sanem Sariel","Istanbul Technical University,Halmstad University",Learning,"Robots are more capable of achieving manipulation tasks for everyday activities than before. But the safety of manipulation skills that robots employ is still an open problem. Considering all possible failures during skill learning increases the complexity of the process and restrains learning an optimal policy. Nonetheless, safety-focused modularity in the acquisition of skills has not been adequately addressed in previous works. For that purpose, we reformulate skills as base and failure prevention skills, where base skills aim at completing tasks and failure prevention skills aim at reducing the risk of failures to occur. Then, we propose a modular and hierarchical method for safe robot manipulation by augmenting base skills by learning failure prevention skills with reinforcement learning and forming a skill library to address different safety risks. Furthermore, a skill selection policy that considers estimated risks is used for the robot to select the best control policy for safe manipulation. Our experiments show that the proposed method achieves the given goal while ensuring safety by preventing failures. We also show that with the proposed method, skill learning is feasible and our safe manipulation tools can be transferred to the real environment."
GG-LLM: Geometrically Grounding Large Language Models for Zero-Shot Human Activity Forecasting in Human-Aware Task Planning,"Moritz A. Graule, Volkan Isler","Harvard University,University of Minnesota",Learning,"A robot in a human-centric environment needs to account for the humanâ€™s intent and future motion in its task and motion planning to ensure safe and effective operation. This requires symbolic reasoning about probable future actions and the ability to tie these actions to specific locations in the physical environment. While one can train behavioral models capable of predicting human motion from past activities, this approach requires large amounts of data to achieve acceptable long-horizon predictions. More importantly, the resulting models are constrained to specific data formats and modalities. Moreover, connecting predictions from such models to the environment at hand to ensure the applicability of these predictions is an unsolved problem. We present a system that utilizes a Large Language Model (LLM) to infer a humanâ€™s next actions from a range of modalities without fine-tuning. A novel aspect of our system that is critical to robotics applications is that it links the predicted actions to specific locations in a semantic map of the environment. Our method leverages the fact that LLMs, trained on a vast corpus of text describing typical human behaviors, encode substantial world knowledge, including probable sequences of human actions and activities. We demonstrate how these localized activity predictions can be incorporated in a human-aware task planner for an assistive robot to reduce the occurrences of undesirable human-robot interactions by 29.2% on average."
Modality Attention for Prediction-Based Robot Motion Generation: Improving Interpretability and Robustness of Using Multi-Modality,"Hideyuki Ichiwara, Hiroshi Ito, Kenjiro Yamamoto, Hiroki Mori, Tetsuya Ogata","Hitachi, Ltd. / Waseda University,Hitachi, Ltd.,Waseda University",Learning,"We developed a modality attention motion generation model on the basis of multi-modality prediction. This model provides interpretability about modality usage and demonstrates robustness against disturbances. We used a hierarchical model consisting of low-level recurrent neural networks (RNNs) for processing each modality individually and a high-level RNN that integrates the multi-modality. This integration is achieved by efficiently gating multi-modality and inputting it to the high-level RNN. We verified the interpretability and robustness of the task of inserting a furniture part, which consists of the ``approach"" phase to bring the wooden dowel closer to the hole and the ``insertion"" phase. While the proposed model achieves the same task success rate as the conventional model, it clarifies that it refers to vision during ``approach"" and force during ``insertion,"" providing interpretability regarding modality use. Furthermore, in contrast to the non-modality attention model, whose task success rate drops significantly under disturbance, the proposed model enhances robustness against disturbances to modalities it does not direct attention during the task, resulting in a consistently high success rate (â‰’90%)."
Adaptive Whole-Body Robotic Tool-Use Learning on Low-Rigidity Plastic-Made Humanoids Using Vision and Tactile Sensors,"Kento Kawaharazuka, Kei Okada, Masayuki Inaba",The University of Tokyo,Learning,"Various robots have been developed so far; however, we face challenges in modeling the low-rigidity bodies of some robots. In particular, the deflection of the body changes during tool-use due to object grasping, resulting in significant shifts in the tool-tip position and the body's center of gravity. Moreover, this deflection varies depending on the weight and length of the tool, making these models exceptionally complex. However, there is currently no control or learning method that takes all of these effects into account. In this study, we propose a method for constructing a neural network that describes the mutual relationship among joint angle, visual information, and tactile information from the feet. We aim to train this network using the actual robot data and utilize it for tool-tip control. Additionally, we employ Parametric Bias to capture changes in this mutual relationship caused by variations in the weight and length of tools, enabling us to understand the characteristics of the grasped tool from the current sensor information. We apply this approach to the whole-body tool-use on KXR, a low-rigidity plastic-made humanoid robot, to validate its effectiveness."
Generating and Transferring Priors for Causal Bayesian Network Parameter Estimation in Robotic Tasks,"Maximilian Diehl, Karinne Ramirez-Amaro",Chalmers University of Technology,Learning,"Robots acting in human environments will often face new situations and can benefit from transferring prior experience. Priors could enable robots to handle new tasks zero-shot and help prevent failures, which can be particularly costly in real robot applications. Due to their interpretable nature, causal Bayesian Networks (CBN) are popular for modeling cause-effect relations between semantically meaningful environment features and their effects on action success. While the CBN structure is often intuitively transferable to a new context, its probability distribution might change, requiring data-intensive relearning. In this work, we propose three strategies that utilize semantic similarity and relatedness between the variables of two CBNs to generate and transfer informed CBN distribution priors. We evaluate the parameter prior accuracy in five different transfer scenarios, including sim-2-real, transferring parameters to more complex tasks with a larger number of parameters and even between two different tasks, which is particularly challenging. We show that the priors lead to better distribution estimates, particularly under a limited amount of new experiments, and improve the robotâ€™s ability to predict and prevent action failures by up to 50%."
Training Diverse High-Dimensional Controllers by Scaling Covariance Matrix Adaptation MAP-Annealing,"Bryon Tjanaka, Matthew Fontaine, David H. Lee, Aniruddha Kalkar, Stefanos Nikolaidis","University of Southern California,UNIVERSITY OF SOUTHERN CALIFORNIA",Learning,"Pre-training a diverse set of neural network controllers in simulation has enabled robots to adapt online to damage in robot locomotion tasks. However, finding diverse, high-performing controllers requires expensive network training and extensive tuning of a large number of hyperparameters. On the other hand, Covariance Matrix Adaptation MAP-Annealing (CMA-MAE), an evolution strategies (ES)-based quality diversity algorithm, does not have these limitations and has achieved state-of-the-art performance on standard QD benchmarks. However, CMA-MAE cannot scale to modern neural network controllers due to its quadratic complexity. We leverage efficient approximation methods in ES to propose three new CMA-MAE variants that scale to high dimensions. Our experiments show that the variants outperform ES-based baselines in benchmark robotic locomotion tasks, while being comparable with or exceeding state-of-the-art deep reinforcement learning-based quality diversity algorithms."
Robotic Constrained Imitation Learning for the Peg Transfer Task in Fundamentals of Laparoscopic Surgery,"Kento Kawaharazuka, Kei Okada, Masayuki Inaba",The University of Tokyo,Learning,"In this study, we present an implementation strategy for a robot that performs peg transfer tasks in Fundamentals of Laparoscopic Surgery (FLS) via imitation learning, aimed at the development of an autonomous robot for laparoscopic surgery. Robotic laparoscopic surgery presents two main challenges: (1) the need to manipulate forceps using ports established on the body surface as fulcrums, and (2) difficulty in perceiving depth information when working with a monocular camera that displays its images on a monitor. Especially, regarding issue (2), most prior research has assumed the availability of depth images or models of a target to be operated on. Therefore, in this study, we achieve more accurate imitation learning with only monocular images by extracting motion constraints from one exemplary motion of skilled operators, collecting data based on these constraints, and conducting imitation learning based on the collected data. We implemented an overall system using two Franka Emika Panda Robot Arms and validated its effectiveness."
Mobile Robot Oriented Large-Scale Indoor Dataset for Dynamic Scene Understanding,"Ivan Tang, Cong Tai, Fang-xing Chen, Wanting Zhang, Tao Zhang, Xueping Liu, Yong-Jin Liu, Long Zeng","Tsinghua University,Tsinghua Shenzhen International Graduate School,Pudu Technology Ltd.",Datasets for Robot Learning,"Most existing robotic datasets capture static scene data and thus are limited in evaluating robots' dynamic performance. To address this, we present a mobile robot oriented large-scale indoor dataset, denoted as THUD (Tsinghua University Dynamic) robotic dataset, for training and evaluating their dynamic scene understanding algorithms. Specifically, the THUD dataset construction is first detailed, including organization, acquisition, and annotation methods. It comprises both real-world and synthetic data, collected with a real robot platform and a physical simulation platform, respectively. Our current dataset includes 13 larges-scale dynamic scenarios, 90K image frames, 20M 2D/3D bounding boxes of static and dynamic objects, camera poses, and IMU. The dataset is still continuously expanding. Then, the performance of mainstream indoor scene understanding tasks, e.g. 3D object detection, semantic segmentation, and robot relocalization, is evaluated on our THUD dataset. These experiments reveal serious challenges for some robot scene understanding tasks in dynamic scenes. By sharing this dataset, we aim to foster and iterate new mobile robot algorithms quickly for robot actual working dynamic environment, i.e. complex crowded dynamic scenes."
InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions,"Kushal Kedia, Atiksh Bhardwaj, Prithwish Dan, Sanjiban Choudhury",Cornell University,Datasets for Robot Learning,"In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collaborative human-robot manipulation tasks and show that our conditional model improves over various marginal baselines. We also introduce new techniques to tele-operate a 7-DoF robot arm and collect a diverse range of human-robot collaborative manipulation data which we open-source. We release our code and datasets at https://portal-cornell.github.io/interact/."
Towards Learning-Based Planning: The nuPlan Benchmark for Real-World Autonomous Driving,"Napat Karnchanachari, Dimitrios Panagiotis Geromichalos, Kok Seang Tan, Nanxiang Li, Christopher Eriksen, Shakiba Yaghoubi, Noushin Mehdipour, Gianmarco Bernasconi, Whye Kit Fong, Yiluan Guo, Holger Caesar","Motional,Motional AD,Delft University of Technology",Datasets for Robot Learning,"Machine Learning (ML) has replaced traditional handcrafted methods for perception and prediction in autonomous vehicles. Yet for the equally important planning task, the adoption of ML-based techniques is slow. We present nuPlan, the worldâ€™s first real-world autonomous driving dataset, and benchmark. The benchmark is designed to test the ability of ML-based planners to handle diverse driving situations and to make safe and efficient decisions. To that end, we introduce a new large-scale dataset that consists of 1282 hours of diverse driving scenarios from 4 cities (Las Vegas, Boston, Pittsburgh, and Singapore) and includes high-quality auto-labeled object tracks and traffic light data. We exhaustively mine and taxonomize common & rare driving scenarios which are used during evaluation to get fine-grained insights into the performance and characteristics of a planner. Beyond the dataset, we provide a simulation and evaluation framework that enables a plannerâ€™s actions to be simulated in closed-loop to account for interactions with other traffic participants. We present a detailed analysis of numerous baselines and investigate gaps between ML-based and traditional methods. Find the nuPlan dataset and code at nuplan.org."
"TBD Pedestrian Data Collection: Towards Rich, Portable, and Large-Scale Natural Pedestrian Data","Allan Wang, Daisuke Sato, Yasser Corzo, Sonya Simkin, Abhijat Biswas, Aaron Steinfeld",Carnegie Mellon University,Datasets for Robot Learning,"Social navigation and pedestrian behavior research has shifted towards machine learning-based methods and converged on the topic of modeling inter-pedestrian interactions and pedestrian-robot interactions. For this, large-scale datasets that contain rich information are needed. We describe a portable data collection system, coupled with a semi-autonomous labeling pipeline. As part of the pipeline, we designed a label correction web application that facilitates human verification of automated pedestrian tracking outcomes. Our system enables large-scale data collection in diverse environments and fast trajectory label production. Compared with existing pedestrian data collection methods, our system contains three components: a combination of top-down and ego-centric views, natural human behavior in the presence of a socially appropriate ""robot"", and human-verified labels grounded in the metric space. To the best of our knowledge, no prior data collection system has a combination of all three components. We further introduce our ever-expanding dataset from the ongoing data collection effort -- the TBD Pedestrian Dataset and show that our collected data is larger in scale, contains richer information when compared to prior datasets with human-verified labels, and supports new research opportunities."
RoboVQA: Multimodal Long-Horizon Reasoning for Robotics,"Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-arnold, Sharath Maddineni, Nikhil J Joshi, Peter Florence, Wei Han, Baruch Robert, Yao Lu, Suvir Mirchandani, Peng Xu, Pannag Sanketi, Karol Hausman, Izhak Shafran, Brian Ichter, Yuan Cao","Google,Google Inc,Google LLC,MIT,Google.com,Google Brain",Datasets for Robot Learning,"We present a scalable, bottom-up and intrinsically diverse data collection scheme that can be used for high-level reasoning with long and medium horizons and that has 2.2x higher throughput compared to traditional narrow top-down step-by-step collection. We collect realistic data by performing any user requests within the entirety of 3 office buildings and using multiple embodiments (robot, human, human with grasping tool). With this data, we show that models trained on all embodiments perform better than ones trained on the robot data only, even when evaluated solely on robot episodes. We explore the economics of collection costs and find that for a fixed budget it is beneficial to take advantage of the cheaper human collection along with robot collection. We release a large and highly diverse (29,520 unique instructions) dataset dubbed RoboVQA containing 829,502 (video, text) pairs for robotics-focused visual question answering. We also demonstrate how evaluating real robot experiments with an intervention mechanism enables performing tasks to completion, making it deployable with human oversight even if imperfect while also providing a single performance metric. We demonstrate a single video-conditioned model named RoboVQA-VideoCoCa trained on our dataset that is capable of performing a variety of grounded high-level reasoning tasks in broad realistic settings with a cognitive intervention rate 46% lower than the zero-shot state of the art visual language model (VLM) baseline and is able to guide real robots through long-horizon tasks. The performance gap with zero-shot state-of-the-art models indicates that a lot of grounded data remains to be collected for real-world deployment, emphasizing the critical need for scalable data collection approaches. Finally, we show that video VLMs significantly outperform single-image VLMs with an average error rate reduction of 19% across all VQA tasks. Dataand videos are available at https://robovqa.github.io"
CC-SGG: Corner Case Scenario Generation Using Learned Scene Graphs,"George Drayson, Efimia Panagiotaki, Daniel Omeiza, Lars Kunze","University College London,University of Oxford",Datasets for Robot Learning,"Corner case scenarios are an essential tool for testing and validating the safety of autonomous vehicles (AVs). As these scenarios are often insufficiently present in naturalistic driving datasets, augmenting the data with synthetic corner cases greatly enhances the safe operation of AVs in unique situations. However, the generation of synthetic, yet realistic, corner cases poses a significant challenge. In this work, we introduce a novel approach based on Heterogeneous Graph Neural Networks (HGNNs) to transform regular driving scenarios into corner cases. To achieve this, we first generate concise representations of regular driving scenes as scene graphs, minimally manipulating their structure and properties. Our model then learns to perturb those graphs to generate corner cases using attention and triple embeddings. The input and perturbed graphs are then imported back into simulation to generate corner case scenarios. Our model successfully learned to produce corner cases from input scene graphs, achieving 89.9% prediction accuracy on our testing dataset. We further validate the generated scenarios on baseline autonomous driving methods, demonstrating our modelâ€™s ability to effectively create critical situations for the baselines."
RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot,"Hao-shu Fang, Hongjie Fang, Zhenyu Tang, Jirong Liu, Chenxi Wang, Junbo Wang, Haoyi Zhu, Cewu Lu","Shanghai Jiao Tong University,Shanghai Jiaotong University,University of Science and Technology of China,ShangHai Jiao Tong University",Datasets for Robot Learning,"A key challenge for robotic manipulation in open domains is how to acquire diverse and generalizable skills for robots. Recent progress in one-shot imitation learning and robotic foundation models have shown promise in transferring trained policies to new tasks based on demonstrations. This feature is attractive for enabling robots to acquire new skills and improve their manipulative ability. However, due to limitations in the training dataset, the current focus of the community has mainly been on simple cases, such as push or pick-place tasks, relying solely on visual guidance. In reality, there are many complex skills, some of which may even require both visual and tactile perception to solve. This paper aims to unlock the potential for an agent to generalize to hundreds of real-world skills with multi-modal perception. To achieve this, we have collected a dataset comprising over 110,000 contact-rich robot manipulation sequences across diverse skills, contexts, robots, and camera viewpoints, all collected in the real world. Each sequence in the dataset includes visual, force, audio, and action information. Moreover, we also provide a corresponding human demonstration video and a language description for each robot sequence. We have invested significant efforts in calibrating all the sensors and ensuring a high-quality dataset. The dataset is made publicly available on our website: https://rh20t.github.io."
SACSoN: Scalable Autonomous Control for Social Navigation,"Noriaki Hirose, Dhruv Shah, Ajay Sridhar, Sergey Levine","UC Berkeley / TOYOTA Motor North America,University of California, Berkeley,UC Berkeley",Datasets for Robot Learning,"Machine learning provides a powerful tool for building socially compliant robotic systems that go beyond simple predictive models of human behavior. By observing and understanding human interactions from past experiences, learning can enable effective social navigation behaviors directly from data. In this paper, our goal is to develop methods for training policies for socially unobtrusive behavior, such that robots can navigate among humans in ways that don't disturb human behavior in visual navigation using only onboard RGB observations. We introduce a definition for such behavior based on the counterfactual perturbation of the human: if the robot had not intruded into the space, would the human have acted in the same way? By minimizing this counterfactual perturbation, we can induce robots to behave in ways that do not alter the natural behavior of humans in the shared space. Instantiating this principle requires training policies to minimize their effect on human behavior, and this in turn requires data that allows us to model the behavior of humans in the presence of robots. Therefore, our approach is based on two key contributions. First, we collect a large dataset where an indoor mobile robot interacts with human bystanders. Second, we utilize this dataset to train policies that minimize counterfactual perturbation. We provide supplementary videos and make publicly available the visual navigation dataset on our project page."
Lightweight Untethered Soft Robotic Fish,"Xiangxing Wang, Xuan Pei, Xinyang Wang, Taogang Hou","Beijing Jiaotong University, School of Electronics and Informati,Beijing Jiaotong University,Beihang Universityï¼ŒSchool of Mechanical Engineering and A",Soft Robot Materials and Design I,"Aquatic organisms, due to soft body structure and high agility, have inspired many biomimetic robots. However, considering the issues of insulation and waterproofing, as well as the driving module of soft materials, their control systems are usually larger and heavier. Therefore, small underwater robots often tethered, i.e., it cannot integrate energy and control systems onto the body, which greatly limited in its working range and activity mode. This paper presents a small untethered bionic manta ray. The robotic fish is driven by dielectric elastomer actuators (DEA), which controls the double wing structure on both sides by the central muscle part to simulate the process of the manta rayâ€™s lateral fins fanning to propel itself forward. And the flexible printed circuit board (FPC) constitutes the body of the fish and is also an independent energy control system. The electronic components are evenly distributed on the double-wing structure of the robotic fish to realize the integration of the energy control system. This circuit system can be powered by a small lithium battery and output a periodic voltage to drive the motion of the robotic fish. The masses of our tethered and untethered fish are 1.9g and 5.1g respectively. The swimming speed of these two types of fish can reach 42.5mm/s and 17.0 mm/s. And this design principle can be extended to the research and design of various flexible devices and soft robots."
Optimal Design of Flexible-Link Mechanisms with Desired Load-Displacement Profiles,"Guirec Maloisel, Espen Knoop, Christian Schumacher, Bernhard Thomaszewski, Moritz Bächer, Stelian Coros","Disney Research,The Walt Disney Company,Université de Montréal,ETH Zurich",Soft Robot Materials and Design I,"Robot mechanisms that exploit compliance can perform complex tasks under uncertainty using simple control strategies, but it remains difficult to design mechanisms with a desired embodied intelligence. In this article, we propose an automated design technique that optimizes the desired load-displacement behavior of planar flexible-link mechanisms. To do so, we replace a subset of rigid with flexible links in an existing mechanism, and optimize their rest shape. We demonstrate the efficacy of our approach on a set of examples, including two fabricated prototypes, illustrating applications for grasping and locomotion tasks."
"A Scalable, Light-Controlled, Individually Addressable, Non-Metal Actuator Array","Sophie Paul, Matthew Devlin, Elliot Hawkes","University of California, Santa Barbara",Soft Robot Materials and Design I,"Research in the area of photo-actuation is growing rapidly, yet there are few examples of photo-actuators with practical use cases. One potential application is for the control of intelligent electromagnetic surfaces, or two-dimensional ar- rays that could shape and control an incident electromagnetic field in ideally any manner. A promising concept to realize such a surface leverages signal refraction via antenna edges, but requires non-metal actuation, large antenna rotations, and high antenna angular accuracy for long periods of time. Here, we present a nonmetal, light-controlled, multi-position inchworm actuator array that can rotate an antenna 88 degrees in incre- mental steps of less than 3.4 degrees with zero-power shape- persistence. The design is modular and rapidly manufacturable via a layered laser-cutting technique, such that the actuator can be tiled into an array to control the rotation of many antennas. We control the array with a single focused IR light that rasters across the actuators to precisely control all antenna positions. We characterize the response time, accuracy, and repeatability of a single actuator, and demonstrate the array achieving diverse antenna configurations. This work advances the precision and scalability of photothermal actuation not only for use in intelligent electromagnetic surfaces but for any application benefitting from light-controlled actuation."
"A Passively Bendable, Compliant Tactile Palm with RObotic Modular Endoskeleton Optical (ROMEO) Fingers","Sandra Q. Liu, Edward Adelson","Massachusetts Institute of Technology,MIT",Soft Robot Materials and Design I,"Many robotic hands currently rely on extremely dexterous robotic fingers and a thumb joint to envelop themselves around an object. Few hands focus on the palm even though human hands greatly benefit from their central fold and soft surface. As such, we develop a novel structurally compliant soft palm, which enables more surface area contact for the objects that are pressed into it. Moreover, this design, along with the development of a new low-cost, flexible illumination system, is able to incorporate a high-resolution tactile sensing system inspired by the GelSight sensors. Concurrently, we design RObotic Modular Endoskeleton Optical (ROMEO) fingers, which are underactuated two-segment soft fingers that are able to house the new illumination system, and we integrate them into these various palm configurations. The resulting robotic hand is slightly bigger than a baseball and represents one of the first soft robotic hands with actuated fingers and a passively compliant palm, all of which have high-resolution tactile sensing. This design also potentially helps researchers discover and explore more soft-rigid tactile robotic hand designs with greater capabilities in the future."
Shape-Conformable Suction Cups with Controllable Adaptive Suction on Complex Surfaces,"Tianqi Yue, Hermes Gadelha, Jonathan Rossiter","University of Bristol,University of Bristol, UK",Soft Robot Materials and Design I,
A Phase-Change Emulsion Jamming Gripper for Manipulation of Micro-Scale Textured Surfaces,"Alexander Keller, Tianqi Yue, Qiukai Qi, Andrew Conn, Jonathan Rossiter",University of Bristol,Soft Robot Materials and Design I,"The inherent elasticity of soft materials can be used to create robotic grippers that deform and comply to a variety of irregular shapes. To date, several soft adaptive grasping strategies have been reported, however, most of them focus on adapting to the overall shape of the structure, while the adaptive grasping of small surface asperities is overlooked. In this paper, we propose a novel method to achieve adaptive grasping on surface asperities with a smart shape-memory silicone sponge. Heating above 60â—¦C makes the sponge soft and deformable to allow it to penetrate within surface asperities via a pressure normal to the surface. Cooling down below 60â—¦C makes the sponge â€œjamâ€ to retain its deformed shape. The interlocking force between the jammed sponge and the asperities, and the increased area of contact, allows for adaptive grasping on asperities down to 0.4 mm with an adhesive force of up to 27.7 N in a 40 Ã— 40 mm contacting area. We introduce the design, working principle, fabrication, and optimization of a robotic gripper based on this shape-memory silicone sponge. This sponge-jamming gripper shows great potential for developing next-generation robotic grippers for the manipulation of textured and discontinuous surfaces."
Design and Fabrication of String-Driven Origami Robots,"Peiwen Yang, Shuguang Li","Tsinghua University,Tsinghua University / MIT / Harvard University",Soft Robot Materials and Design I,"Origami designs and structures have been widely used in many fields, such as morphing structures, robotics, and metamaterials. However, the design and fabrication of origami structures rely on human experiences and skills, which are both time and labor-consuming. In this paper, we present a rapid design and fabrication method for string-driven origami structures and robots. We developed an origami design software to generate desired crease patterns based on analytical models and Evolution Strategies (ES). Additionally, the software can automatically produce 3D models of origami designs. We then used a dual-material 3D printer to fabricate those wrapping-based origami structures with the required mechanical properties. We utilized Twisted String Actuators (TSAs) to fold the target 3D structures from flat plates. To demonstrate the capability of these techniques, we built and tested an origami crawling robot and an origami robotic arm using 3D-printed origami structures driven by TSAs."
Design and Implementation of a Ferrofluid-Based Liquid Robot for Small-Scale Manipulation,"Fanxing Kong, Jie Zhao, Hegao Cai, Yanhe Zhu",Harbin Institute of Technology,Soft Robot Materials and Design I,"Magnetic manipulation of miniature soft or liquid robots capable of deformation has gained increasing attention and is demonstrating great potential in small-scale applications, such as drug delivery, minimal invasive surgery, and manipulation of delicate objects. In this study, we introduce a liquid robot composed of ferrofluid that shows promise for small-scale magnetic manipulation applications. The objective of this work is to achieve more flexible manipulation capabilities of the robot. To this end, we utilize a redundant magnetic actuation system composed of five electromagnets and implement 4 degrees of freedom (4-DOF) control of the liquid robot in planar space. Based on the planar 4-DOF control, the liquid robot is able to perform various actions and implement versatile manipulation tasks, such as transporting objects, separating or assembling miniature parts, and operating customized tools. Furthermore, we suggest an automatic transportation method to enhance manipulation precision. A series of experiments are conducted to validate the effectiveness of the proposed method and the robot's capacity to accomplish diversified manipulation tasks. The proposed liquid robot indicates flexibility and provides novel solutions for small-scale untethered manipulation."
Towards Optimal Design of Dielectric Elastomer Actuators Using a Graph Neural Network Encoder,"Yangfan Li, Jun LIU, Wenyu Liang, Zhuangjian Liu","Institute of High Performance Computing, A*Star,Institute of High Performance Computing,Institute for Infocomm Research, A*STAR,INSTITUTE OF HIGH PERFORMANCE COMPUTING",Soft Robot Materials and Design I,"Dielectric elastomer actuators (DEAs), a type of ""artificial muscles"", can generate significant deformations and offer speedy responses when exposed to voltage. Owing to their high electromechanical conversion efficiency and great flexibility, they have been extensively used in soft robot applications, such as soft grippers, walking robots, crawling robots, climbing robots, swimming robots, etc. Although previous research has explored the use of DEAs in soft robot locomotion, achieving optimal behavior is challenging due to the complexity of the constituent materials and the highly nonlinear nature of the problem. In this study, a simulation-based design optimization approach is proposed to address this challenge. The proposed approach involves developing a computational modeling framework that evaluates the electromechanical behavior of the DEA. A graph neural network (GNN) is employed as an encoder to extract the latent representation of the geometry in a low dimensional space, which is further used to construct a surrogate model for fast prediction of target responses. To achieve an optimal actuation capability under design constraints, a multi-objective optimization function is formulated to balance the actuation distance and the actuator size, where the Pareto front demonstrates the trade-off between the actuation distance and design constraint. Finally, three optimized designs are fabricated and tested, demonstrating a performance improvement of over 140% compared to an"
Learning-Based Reflection-Aware Virtual Point Removal for Large-Scale 3D Point Clouds,"Oggyu Lee, Kyungdon Joo, Jae-young Sim","Ulsan National Institute of Science and Technology,UNIST",Deep Learning for Visual Perception I,"3D point clouds are widely used for robot perception and navigation. LiDAR sensors can provide large scale 3D point clouds (LS3DPC) that guarantee a certain level of accuracy in common environments. However, they often generate virtual points as reflection artifacts associated with reflective surfaces like glass planes, which may degrade the performance of various robot applications. In this letter, we propose a novel learning-based framework to remove the virtual points from LS3DPC. We first project 3D point clouds onto 2D image domain to investigate the distribution of the LiDAR's echo pulses, which is then used as an input to the glass probability estimation network. Moreover, the 3D feature similarity estimation network exploits the deep features to compare the symmetry and geometric similarity between real and virtual points with respect to the estimated glass plane. We provide a dataset of LS3DPC with synthetically generated reflection artifacts to train the proposed network. Experimental results show that the proposed method achieves the better performance qualitatively and quantitatively compared with the existing state-of-the-art methods of 3D reflection removal."
Multi-Confidence Guided Source-Free Domain Adaption Method for Point Cloud Primitive Segmentation,"Shaohu Wang, Yuchuang Tong, Xiuqin Shang, Zhengtao Zhang","Institute of Automation, Chinese Academy of Sciences,The Institute of Automation of the Chinese Academy of Sciences,Institute of automation, Chinese academy of sciences",Deep Learning for Visual Perception I,"Point cloud primitive segmentation aims to segment the surface point cloud into various geometric types of primitives, which plays a vital role in robot operation and industrial automation. However, differences in object structures and shapes across industrial datasets create domain shift issues, compounded by privacy concerns preventing dataset sharing. To address these challenges, we propose a novel source-free domain adaptation method for point cloud primitive segmentation, which follows the popular pseudo-label based self-training framework. Unlike previous works using single-model uncertainty to refine pseudo labels, our method leverages multi-confidence, including transformation consistency, task confidence, and geometric saliency to provide more informative guidance. Specifically, the transformation consistency is first utilized to vote pseudo-labels and task confidences. Furthermore, to filter out high-confident noises and obtain more reliable pseudo-labels, we investigate the geometric curvature properties of primitives and propose a geometric saliency guided dynamic prototype matching and label graph aggregation strategies for pseudo-label reassignment with different task confidence. For this novel task, we construct several datasets and verify the effectiveness of the proposed methods through a series of experiments."
FF-LOGO: Cross-Modality Point Cloud Registration with Feature Filtering and Local to Global Optimization,"Nan Ma, Mohan Wang, Yiheng Han, Yong-Jin Liu","Beijing University of Technology, Beijing, China,Beijing University of Technology,Tsinghua University",Deep Learning for Visual Perception I,"Cross-modality point cloud registration is confronted with significant challenges due to inherent differences in modalities between sensors. To deal with this problem, we propose FF-LOGO: a cross-modality point cloud registration framework with Feature Filtering and LOcal-Global Optimization. The cross-modality feature correlation filtering module extracts geometric transformation-invariant features from cross-modality point clouds and achieves point selection by feature matching. We also introduce a cross-modality optimization process, including a local adaptive key region aggregation module and a global modality consistency fusion optimization module. Experimental results demonstrate that our two-stage optimization significantly improves the registration accuracy of the feature association and selection module. Our method achieves a substantial increase in recall rate compared to the current state-of-the-art methods on the 3DCSR dataset, improving from 40.59% to 75.74%. Our code will be available at https://github.com/wangmohan17/FFLOGO."
CAPT: Category-Level Articulation Estimation from a Single Point Cloud Using Transformer,"Lian Fu, Ryoichi Ishikawa, Yoshihiro Sato, Takeshi Oishi","The University of Tokyo,Kyoto University of Advanced Science",Deep Learning for Visual Perception I,"The ability to estimate joint parameters is essential for various applications in robotics and computer vision. In this paper, we propose CAPT: category-level articulation estimation from a point cloud using Transformer. CAPT uses an end-to-end transformer-based architecture for joint parameter and state estimation of articulated objects from a single point cloud. The proposed CAPT methods accurately estimate joint parameters and states for various articulated objects with high precision and robustness. The paper also introduces a motion loss approach, which improves articulation estimation performance by emphasizing the dynamic features of articulated objects. Additionally, the paper presents a double voting strategy to provide the framework with coarse-to-fine parameter estimation. Experimental results on several category datasets demonstrate that our methods outperform existing alternatives for articulation estimation. Our research provides a promising solution for applying Transformer-based architectures in articulated object analysis."
Energy-Based Detection of Adverse Weather Effects in LiDAR Data,"Aldi Piroli, Vinzenz Dallabetta, Johannes Kopp, Marc Walessa, Daniel Meissner, Klaus Dietmayer","Universität Ulm,BMW Group,Ulm University,University of Ulm",Deep Learning for Visual Perception I,"Autonomous vehicles rely on LiDAR sensors to perceive the environment. Adverse weather conditions like rain, snow, and fog negatively affect these sensors, reducing their reliability by introducing unwanted noise in the measurements. In this work, we tackle this problem by proposing a novel approach for detecting adverse weather effects in LiDAR data. We reformulate this problem as an outlier detection task and use an energy-based framework to detect outliers in point clouds. More specifically, our method learns to associate low energy scores with inlier points and high energy scores with outliers allowing for robust detection of adverse weather effects. In extensive experiments, we show that our method performs better in adverse weather detection and has higher robustness to unseen weather effects than previous state-of-the-art methods. Furthermore, we show how our method can be used to perform simultaneous outlier detection and semantic segmentation. Finally, to help expand the research field of LiDAR perception in adverse weather, we release the SemanticSpray dataset, which contains labeled vehicle spray data in highway-like scenarios. The dataset will be available upon acceptance (see supplementary material for sample)."
EdgePoint: Efficient Point Detection and Compact Description Via Distillation,"Haodi Yao, Ning Hao, Chen Xie, Fenghua He",Harbin Institute of Technology,Deep Learning for Visual Perception I,"Efficient interest point detection and description in images play a crucial role in many tasks such as multi-robot SLAM and collaborative localization. To facilitate fast detection and generate compact descriptions on edge devices, we introduce EdgePoint, a lightweight neural network. We design a new detection loss UnfoldSoftmax to improve inference speed. Futhermore, we propose Ortho-Alignment loss combined with LocalPCA compression to learn compact 32-dimensional descriptors. To enable efficient storage or communication, we also quantize the generated descriptors into integral values. We perform EdgePoint on various datasets, and show that it surpasses SuperPoint in performance while utilizing only 1% of the parameters and achieving up to more than 10 times faster inference speed. By applying descriptor quantization, the requirements for storage and communication can be reduced by up to 97% without performance decreasing."
Fast and Robust Point Cloud Registration with Tree-Based Transformer,"Guangyan Chen, Meiling Wang, Yi Yang, Li Yuan, Yufeng Yue","Beijing Institute of technology,Beijing Institute of Technology,Peking University",Deep Learning for Visual Perception I,"Point cloud registration is essential in computer vision and robotics. Recently, transformer-based methods have achieved advanced point cloud registration performance. However, the standard attention mechanism utilized in these methods considers many low-relevance points, and it has difficulty focusing its attention weights on sparse and meaningful points, leading to limited local structure modeling capabilities and quadratic computational complexity. To address these limitations, we present the Tree-based Transformer (TrT), which is able to extract abundant local and global features with linear computational complexity. Specifically, the TrT builds coarse-to-dense feature trees, and a novel Tree-based Attention (TrA) is proposed to guide the progressive convergence of the attended regions toward meaningful points and to structurize point clouds following tree structures. In each layer, the top S key points with the highest attention scores are selected, such that in the next layer, attention is evaluated only within the specified high-relevance regions, corresponding to the child points of these selected S points. Additionally, coarse features containing high-level semantic information are incorporated into the child points to guide the feature extraction process, facilitating local structure modeling and multiscale information integration. Consequently, TrA enables the model to focus on critical local structures and extract rich local information with linear computational complexity. Experiments demonstrate that our method achieves state-of-the-art performance on 3DMatch and KITTI benchmarks. The code for our method is publicly available at https://github.com/CGuangyan-BIT/TrT."
Uncertainty-Driven Exploration Strategies for Online Grasp Learning,"Yitian Shi, Philipp Schillinger, Miroslav Gabriel, Alexander Qualmann, Hanna Ziesche, Zohar Feldman, Ngo Anh Vien","Bosch Center for Artificial Intelligence,Robert Bosch GmbH, Corporate Sector Research and Advance Enginee,Bosch BCAI,Bosch,Bosch GmbH",Deep Learning in Grasping and Manipulation I,"Existing grasp prediction approaches are mostly based on offline learning, while, ignoring the exploratory grasp learning during online adaptation to new picking scenarios, i.e., objects that are unseen or out-of-domain (OOD), camera and bin settings, etc. In this paper, we present an uncertainty-based approach for online learning of grasp predictions for robotic bin picking. Specifically, the online learning algorithm with an effective exploration strategy can significantly improve its adaptation performance to unseen environment settings. To this end, we first propose to formulate online grasp learning as an RL problem that will allow us to adapt both grasp reward prediction and grasp poses. We propose various uncertainty estimation schemes based on emph{Bayesian uncertainty quantification} and emph{distributional ensembles}. We carry out evaluations on real-world bin picking scenes of varying difficulty. The objects in the bin have various challenging physical and perceptual characteristics that can be characterized by semi- or total transparency, and irregular or curved surfaces. The results of our experiments demonstrate a notable improvement of grasp performance in comparison to conventional online learning methods which incorporate only naive exploration strategies. Video: https://youtu.be/fPKOrjC2QrU"
Pseudo-Labeling and Contextual Curriculum Learning for Online Grasp Learning in Robotic Bin Picking,"Huy Le, Philipp Schillinger, Miroslav Gabriel, Alexander Qualmann, Ngo Anh Vien","TU Dormund,Bosch Center for Artificial Intelligence,Robert Bosch GmbH, Corporate Sector Research and Advance Enginee,Bosch GmbH",Deep Learning in Grasping and Manipulation I,"The prevailing grasp prediction methods predominantly rely on offline learning, overlooking the dynamic grasp learning that occurs during real-time adaptation to novel picking scenarios. These scenarios may involve previously unseen objects, variations in camera perspectives, and bin configurations, among other factors. In this paper, we introduce introduce a novel approach, SSL-ConvSAC, that combines semi-supervised learning and reinforcement learning for online grasp learning. By treating pixels with reward feedback as labeled data and others as unlabeled, it efficiently exploits unlabeled data to enhance learning. In addition, we address the imbalance between labeled and unlabeled data by proposing a contextual curriculum-based method. We ablate the proposed approach on real-world evaluation data and demonstrate promise for improving online grasp learning on bin picking tasks using a physical 7-DoF Franka Emika robot arm with a suction gripper."
PoseFusion: Multi-Scale Keypoint Correspondence for Monocular Camera-To-Robot Pose Estimation in Robotic Manipulation,"Xujun Han, Shaochen Wang, Xiucai Huang, Zhen Kan","University of Science and Technology of China,Chongqing University",Deep Learning in Grasping and Manipulation I,"Visual-based robot pose estimation is a fundamen- tal challenge, involving the determination of the cameraâ€™s pose with respect to a robot. Conventional methods for camera-to- robot pose calibration rely on fiducial markers to establish keypoint correspondences. However, these approaches exhibit significant variability in accuracy and robustness, particularly in 2D keypoint detection. In this work, we present an end- to-end pose estimation approach that achieves camera-to-robot calibration using monocular images and keypoint information. Our method employs a two-level nested U-shaped architecture, featuring a bottom-level residual U-block to extract richer contextual information from diverse receptive fields to enhance keypoint refinement. By incorporating the perspective-n-point (PnP) algorithm and leveraging 3D robot joint keypoints, we establish correspondence of 3D coordinate points between the robotâ€™s coordinate system and the cameraâ€™s coordinate system, facilitating accurate pose estimation. Experimental evaluations encompass real-world and synthetic datasets, demonstrating competitive results across three distinct robot manipulators."
Online Fault Detection in Manipulation Tasks Via Generative Models,"Michael Lanighan, Oscar Youngquist","TRACLabs, Inc,University of Massachusetts Amherst",Deep Learning in Grasping and Manipulation I,"This paper introduces a method, Generative Adversarial Networks for Detecting Erroneous Results (GANDER), leveraging Generative Adversarial Networks to provide online error detection in manipulation tasks for autonomous robot systems. GANDER relies on mapping input images of a trained task to a learned manifold that contains only positive task executions and outcomes. When reconstructed through this manifold, the input images from successful task executions will remain largely unchanged, while the images from a failed task will change significantly. Using this insight, GANDER enables inspection and task outcome verification capabilities using a large number of positive examples but only a small set of negative examples, thus increasing the applicability of autonomous robot systems. We detail the design of GANDER and provide results of a proof-of-concept system, establishing its efficacy in an autonomous inspection, maintenance, and repair task. GANDER produces favorable results compared to baseline approaches and is capable of correctly identifying off-nominal behavior with 91.65% accuracy in our test task. Ablation studies were also performed to quantify the amount of data ultimately needed for this approach to succeed."
One-Shot Learning for Task-Oriented Grasping,"Valerija Holomjova, Andrew Joe Starkey, Bruno Yun, Pascal Meissner","University of Aberdeen,Université Claude Bernard, Lyon ,,Wuerzburg-Schweinfurt Technical University of Applied Sciences",Deep Learning in Grasping and Manipulation I,"Task-oriented grasping models aim to predict a suitable grasp pose on an object to fulfill a task. These systems have limited generalization capabilities to new tasks, but have shown the ability to generalize to novel objects by recognizing affordances. This object generalization comes at the cost of being unable to recognize the object category being grasped, which could lead to unpredictable or risky behaviors. To overcome these generalization limitations, we contribute a novel system for task-oriented grasping called the One-shot Task-oriented Grasping (OS-TOG) framework. OS-TOG comprises four interchangeable neural networks that interact through dependable reasoning components, resulting in a single system that predicts multiple grasp candidates for a specific object and task from multi-object scenes. Embedded one-shot learning models leverage references within a database for OS-TOG to generalize to novel objects and tasks more efficiently than existing alternatives. Additionally, the paper presents suitable candidates for the frameworkâ€™s neural components, covering essential adjustments for their integration and evaluative comparisons to state-of-the-art. In physical experiments with novel objects, OS-TOG recognizes 69.4% of detected objects correctly and predicts suitable task-oriented grasps with 82.3% accuracy, having a physical grasp success rate of 82.3%."
Multi-Level Reasoning for Robotic Assembly: From Sequence Inference to Contact Selection,"Xinghao Zhu, Devesh Jha, Diego Romeres, Lingfeng Sun, Masayoshi Tomizuka, Anoop Cherian","University of California, Berkeley,Mitsubishi Electric Research Laboratories,Mitsubishi Electric research laboratories,University of California,Mitsubishi Electric Research Labs",Deep Learning in Grasping and Manipulation I,"Automating the assembly of objects from their parts is a complex problem with innumerable applications in manufacturing, maintenance, and recycling. Unlike existing research, which is limited to target segmentation, pose regression, or using fixed target blueprints, our work presents a holistic multi-level framework for part assembly planning consisting of part assembly sequence inference, part motion planning, and robot contact optimization. We present the Part Assembly Sequence Transformer (PAST) -- a sequence-to-sequence neural network -- to infer assembly sequences recursively from a target blueprint. We then use a motion planner and optimization to generate part movements and contacts. To train PAST, we introduce D4PAS: a large-scale Dataset for Part Assembly Sequences consisting of physically valid sequences for industrial objects. Experimental results show that our approach generalizes better than prior methods while needing significantly less computational time for inference."
Learning to Design 3D Printable Adaptations on Everyday Objects for Robot Manipulation,"Michelle Guo, Ziang Liu, Stephen Tian, Zhaoming Xie, Jiajun Wu, Karen Liu","Stanford University,Cornell University",Deep Learning in Grasping and Manipulation I,"Advancements in robot learning for object manipulation have shown promising results, yet certain everyday objects remain challenging for robots to effectively interact with. This discrepancy arises from the fact that human-designed objects are optimized for human use rather than robot manipulation. To address this gap, we propose a framework to automatically design 3D printable adaptations that can be attached to hard-to-use objects, thus improving ""robot ergonomics"". Our learning-based framework formulates the adaptation design and control as a dual Markov decision process and is able to improve robot-object interactions for various robot end effectors and objects. We further validate our designs in the real world with a Franka Panda robot. Please see the supplementary video and https://object-adaptation.github.io for additional visualizations."
Evaluating Robustness of Visual Representations for Object Assembly Task Requiring Spatio-Geometrical Reasoning,"Chahyon Ku, Carl Winge, Ryan Diaz, Wentao Yuan, Karthik Desingh","University of Minnesota,University of Minnesota - Twin Cities,University of Washington",Deep Learning in Grasping and Manipulation I,"This paper focuses on benchmarking the robustness of visual representations toward policy learning in the context of object assembly tasks, particularly the alignment and insertion of objects with geometrical extrusions, commonly known as a peg-in-hole task. The accuracy required to detect and orient the peg and the hole geometry in SE(3) space for successful assembly poses significant challenges. Addressing this, we employ a general framework in visuomotor policy learning that utilizes visual pretraining models as vision encoders. Our study investigates the robustness of this framework when applied to a dual-arm manipulation setup, specifically to the grasp variations. Our quantitative analysis shows that existing pretrained models fail to capture the essential visual features necessary for this task. However, a visual encoder trained from scratch consistently outperforms the frozen pretrained models. Moreover, we discuss rotation representations and associated loss functions that substantially improve policy learning. We present a novel task scenario designed to evaluate the progress in visuomotor policy learning, with a specific focus on improving the robustness of intricate assembly tasks that require both geometrical and spatial reasoning. Videos, data, and code for our simulator and evaluation are available at https://sites.google.com/view/geometric-peg-in-hole."
DeRi-Bot: Learning to Collaboratively Manipulate Rigid Objects Via Deformable Objects,"Zixing Wang, Ahmed H. Qureshi",Purdue University,Deep Learning in Grasping and Manipulation I,"Recent research efforts have yielded significant advancements in manipulating objects under homogeneous settings where the robot is required to either manipulate rigid or deformable (soft) objects. However, the manipulation under heterogeneous setups that involve both 1D deformable and rigid objects remains an unexplored area of research. Such setups are common in various scenarios that involve the transportation of heavy objects via ropes, e.g., on factory floors, at disaster sites, and in forestry. To address this challenge, we introduce DeRi-Bot, the first framework that enables the collaborative manipulation of rigid objects with deformable objects. Our framework comprises an Action Prediction Network (APN) and a Configuration Prediction Network (CPN) to model the complex pattern and stochasticity of soft-rigid body systems. We demonstrate the effectiveness of DeRi-Bot in moving rigid objects to a target position with ropes connected to robotic arms. Furthermore, DeRi-Bot is a distributive method that can accommodate an arbitrary number of robots or human partners without reconfiguration or retraining. We evaluate our framework in both simulated and real-world environments and show that it achieves promising results with strong generalization across different types of objects and multi-agent settings, including human-robot collaboration."
Unified Power and Admittance Adaptation for Safe and Effective Physical Interaction with Unmodelled Dynamic Environments,"Federico Benzi, Cristian Secchi","University of Modena and Reggio Emilia,Univ. of Modena & Reggio Emilia",Physical Human-Robot Interaction I,"When interacting with unmodelled dynamic systems, a robot controller should be capable of adapting online its behavior, in order to be robust to the changing environmental conditions. In the paradigm of passivity-based control, virtual energy tanks allow to perform such adaptations in a robustly stable way, by bounding the amount of energy allocated to the controller. Nevertheless, when the workspace is shared with human collaborators, additional limits have to be imposed to the power the system can exert, in order to guarantee the overall safety. These bounds are difficult to estimate a priori, might vary over time and can significantly affect task execution. In this letter, we tackle this problem by simultaneously adapting online the admittance and the power limits in the controller, ensuring both safety and task performance. Experimental results with a collaborative manipulator validate the presented framework."
Towards Robot to Human Skill Coaching: A ML-Powered IoT and HRI Platform for Martial Arts Training,"Katia Bourahmoune, Karlos Ishac, Marc Garry Carmichael","University of Technology Sydney,Centre for Autonomous Systems",Physical Human-Robot Interaction I,"Advances in human sensing and machine learning are paving the way for new applications of robotics in sports and fitness, making skill coaching smarter, easier and more accessible. Physical and social human robot interaction in particular has received special attention as a feedback mechanism for human performance augmentation. A core challenge in deploying robots that interact physically with humans in dynamic environments such as sports, relates to modeling human skills and designing appropriate interaction schemes. We present the first ML-based HRI platform for physical robot to human skill coaching in real-time in Martial Arts which can be extended to various sports. Our system comprises of the Sawyer robot, our specially developed IoT katana and a skill-training program for the Martial Art of Iaido. We built and deployed in real-time a ML-based Iaido strike recognition model trained on expert and beginner data, and achieved accuracies ranging between 94.8% and 99.97%. We assessed the system's effectiveness in coaching skills through robot interaction in a sparring experiment and a survey involving 12 participants practicing key Iaido techniques with guided training from Sawyer. Our results demonstrated improvement in all participants' Iaido strike skill after training with Sawyer, and they responded positively to robot-assisted skill coaching."
Towards Robo-Coach: Robot Interactive Stiffness/Position Adaptation for Human Strength and Conditioning Training,"Chenzui Li, Xi Wu, Tao Teng, Sylvain Calinon, Fei Chen","The Chinese University of Hong Kong,Istituto Italiano di Tecnologia & Università Cattolica del Sacro,Idiap Research Institute",Physical Human-Robot Interaction I,"Traditional strength and conditioning training relies on the utilization of free weights, such as weighted implements, to elicit external stimuli. However, this approach poses a significant challenge when attempting to modify or adjust the loads within a single training set. This paper introduces an innovative method for achieving adjustable loads during resistance training by leveraging physical Human-Robot Interaction (pHRI). The primary objective is to regulate targeted muscle activation through the use of Robo-Coach (robotic coach system). We first utilize a Task-Parameterized Gaussian Mixture Model (TP-GMM) to learn the motion of coach demonstration, which can be generalized for the trainees. The 3D path extracted from the generated trajectory is then projected onto a 2D plane with respect to the direction of the load. Furthermore, we propose a hybrid stiffness/position generator for online task execution. This generator determines the desired positions in the 2D plane according to the contact point displacements in the stimuli direction and, simultaneously, sets the desired stiffness based on the muscle activation feedback. Finally, the Robo-Coach is implemented with a variable impedance controller to achieve load-adjustable resistance training with the trainee. The biceps curl exercises were conducted and the results showed favorable performance, indicating the effectiveness of this approach."
Predictive and Robust Robot Assistance for Sequential Manipulation,"Theodoros Stouraitis, Michael Gienger","RoboPhren and Honda Research Institute Europe,Honda Research Institute Europe",Physical Human-Robot Interaction I,"This paper presents a novel concept to support physically impaired humans in daily object manipulation tasks with a robot. Given a userâ€™s manipulation sequence, we propose a predictive model that uniquely casts the userâ€™s sequential behavior as well as a robot support intervention into a hierarchical multi-objective optimization problem. A major contribution is the prediction formulation, which allows to consider several different future paths concurrently. The second contribution is the encoding of a general notion of constancy constraints, which allows to consider dependencies between consecutive or far apart keyframes (in time or space) of a sequential task. We perform numerical studies, simulations and robot experiments to analyse and evaluate the proposed method in several table top tasks where a robot supports impaired users by predicting their posture and proactively re-arranging objects."
Nonlinear Subsystem-Based Adaptive Impedance Control of Physical Human-Robot-Environment Interaction in Contact-Rich Tasks,"Mahdi Hejrati, Jouni Mattila",Tampere University,Physical Human-Robot Interaction I,"Haptic upper limb exoskeletons are robots that assist human operators during task execution while having the ability to render virtual or remote environments. Therefore, ensuring the stability of such robots in physical human-robot-environment interaction (pHREI) is crucial. Having a wide range of Z-width, which indicates the region of passively renderable impedance by a haptic display, is also important for rendering a broad range of virtual environments. To address these issues, this study designs subsystem-based adaptive impedance control to achieve a stable pHREI for a 7 degrees of freedom haptic exoskeleton. The presented controller decomposes the entire system into subsystems and designs controller at the subsystem level. The stability of controller in the presence of contact with the virtual environment and human arm force is proven by employing the concept of virtual stability. Additionally, the Z-width of 7-DoF haptic exoskeleton is illustrated using experimental data and improved by exploiting varying virtual mass element. Finally, experimental results are provided to demonstrate the performance of the proposed controller. The control results are also compared to state-of-the-art control methods, highlighting the excellence of the designed controller."
Model Predictive Control with Graph Dynamics for Garment Opening Insertion During Robot-Assisted Dressing,"Stelios Kotsovolis, Yiannis Demiris",Imperial College London,Physical Human-Robot Interaction I,"Robots have a great potential to help people with movement limitations in activities of daily living, such as dressing. A common problem in almost all dressing tasks is the insertion of a garmentâ€™s opening around a part of the human body. The rich contact environment and the deformations of the garment make the task a challenging problem for robots. In this paper, we propose a bi-manual control method for garment opening insertion during robot-assisted dressing. Specifically, we propose a model predictive controller that uses an Attention-based Relational Graph Convolutional Network (ARGCN) for modeling the dynamics of the opening in the presence of the body. We train the model entirely in simulation and validate our method in four real-world dressing scenarios of a medical training manikin. We show that our method generalizes well in the real-world opening insertion tasks achieving an overall success rate of 97.5%, even though the dynamics and the shapes vastly differ from the simulation setup."
Force Feedback-Based Gamification: Performance Validation of Squat Exergame Using Pneumatic Gel Muscles and Dynamic Difficulty Adjustment,"Priyanka Ramasamy, Gunarajulu Renganathan, Yuichi Kurita",Hiroshima University,Physical Human-Robot Interaction I,"Exergames have been considered an advanced approach for enhancing the physical activities of the elderly community. Advancement includes greater immersive and enjoyment factors, which compute performance validation and sustain engagement to activate them from sedentary lifestyles. As a result, an at-home-based game design combined with squat workouts is essential to enhance lower-limb performance. We designed a virtual reality (VR) based exergame with dynamic difficulty adjustment (DDA) conditions in which the speed of the moving objects and air pressure of the PGM vary with respect to the knee shakiness feature. This letter aims to estimate the muscle loading and unloading effects of exergaming sessions at the onset and during the squat phase of the squat cycle. We acquired surface electromyography (sEMG) of five major lower limb muscles for seven subjects to evaluate the significant reduction in muscle activity during conventional and exergame-based squat sessions. In addition, we assessed the knee indicators to identify the variation in motion performance. The subjects performed 120 squats per session, followed by a maximum voluntary contraction (MVC) task. No adverse events, such as fatigue and dizziness, were reported during the study. Our results show a higher significant p"
External Dynamics Dependent Human Gait Adaptation Using a Cable-Driven Exoskeleton,"S S Sanjeevi Nakka, Vineet Vashista",Indian Institute of Technology Gandhinagar,Physical Human-Robot Interaction I,"The emergence of exoskeleton technology has enabled new opportunities for gait rehabilitation, but effective methods to restore healthy gait patterns with exoskeletons are not yet clearly understood. Early research in robot-based gait rehabilitation offered little improvement over current standards of physical care and emphasized the need for a deeper understanding of the complex interaction between humans and the robot, i.e., physical human-robot interaction (pHRI). Studies reported varied lower limb responses for a similar intervention with different exoskeletons, implying that the exoskeleton's external dynamics affect musculoskeletal adaptation outcomes. Accordingly, the current study aims at showcasing the external dynamics dependent gait adaptation using a Cable-Driven Leg Exoskeleton (CDLE). A swing phase gait intervention using three different CDLE cable-routing configurations that impose varied dynamics at human anatomical joints is studied. Twenty-four healthy participants, eight for each CDLE configuration, were tested. Results showed varied gait adaptation among the three groups such that the subjects used either predominantly their hip joint, knee joint, or a combination of both joints implying selective joint strategy adaptations for different external dynamics conditions for the same intervention. The results of this study can provide insights into the optimal design of leg exoskeleton-based rehabilitation paradigms for effective gait rehabilitation."
Comparison of Rating Scale and Pairwise Comparison Methods for Measuring Human Co-Worker Subjective Impression of Robot During Physical Human-Robot Collaboration,"Qiao Wang, Ziqi Wang, Marc Garry Carmichael, Dikai Liu, Chin-teng Lin","University of Technology Sydney,Centre for Autonomous Systems,University of Technology, Sydney,UTS",Physical Human-Robot Interaction I,"The Rating Scale method has been long deemed the standard for measuring subjective perceptions. However, in the field of physical human-robot collaboration (pHRC), its aptness should be put under scrutiny due to inherent challenges such as response bias, between-subject variations, and the granularity nature. Individual variances can introduce significant bias in the rating scale results. A high granularity in the scale could overwhelm participants, leading to unclear and biased responses, while a low granularity may gloss over the fine nuances of human feelings. Additionally, thereâ€™s a notable risk of receiving careless responses, which compromise data reliability. Recognizing these challenges, this paper proposes the application of Pairwise Comparison (PC) in pHRC â€” an alternative survey technique that emphasizes direct comparisons between items on the defined criteria. By using the NASA Task Load Index (NASA-TLX) as a template, RS and PC questionnaires are designed and used in a series of pHRC experiments. Our preliminary findings suggest that PC is more precise and robust than the rating scale method. Compared to RS, PC fosters authentic participant interests in the experiment by intuitive question design and reducing the experimental duration. Besides, the accuracy and reliability of PC are also found to be consistent regardless of the variations in our experimental procedure design."
A Transtibial Prosthesis Using a Parallel Spring Mechanism,"Donggyu Jung, Shinsuk Park, Junho Choi","Korea Institute of Science and Technology,Korea University",Prosthetics and Exoskeletons I,"Prosthetic legs have been used to restore function in the lower limbs lost due to amputation. Early designs including prosthetic legs with a passive joint or without any joint as well as the Energy Storing and Releasing (ESR) feet have shown deficiency in push-off torque, which results in asymmetric gait pattern, slower walking speed, and higher cost of transportation. Although powered prosthetic legs address the aforementioned problems, they suffer from lower energy efficiency, higher volume and weight. In this paper, a powered transtibial prosthesis using a Parallel Elastic Actuator (PEA) is proposed in order to generate the joint torque needed for walking with a lower-powered actuator for lighter and more compact design. A non-linear spring mechanism is proposed to generate the spring torque as needed. The implemented prosthetic leg is evaluated with three intact subjects. The experimental results shows that smaller torque is required for the motor with the spring mechanism. Therefore, less electrical power is consumed when the spring mechanism is used, which implies a lower-powered actuator is sufficient to generate the joint torque needed for walking."
Deep Learning Based Acoustic Measurement Approach for Robotic Applications on Orthopedics,"Bangyu Lan, Momen Abayazid, Nico Verdonschot, Stefano Stramigioli, Kenan Niu","University of Twente,Orthopaedic Research Lab; RadboudUMC",Prosthetics and Exoskeletons I,"In Total Knee Replacement Arthroplasty (TKA), surgical robotics can provide image-guided navigation to fit implants with high precision. Its tracking approach highly relies on inserting bone pins into the bones tracked by the optical tracking system. This is normally done by invasive, radiative manners (implantable markers and CT scans), which introduce unnecessary trauma and prolong the preparation time for patients. To tackle this issue, ultrasound-based bone tracking could offer an alternative. In this study, we proposed a novel deep-learning structure to improve the accuracy of bone tracking by an A-mode ultrasound (US). We first obtained a set of ultrasound dataset from the cadaver experiment, where the ground truth locations of bones were calculated using bone pins. These data were used to train the proposed CasAtt-UNet to predict bone location automatically and robustly. The ground truth bone locations and those locations of US were recorded simultaneously. Therefore, we could label bone peaks in the raw US signals. As a result, our method achieved sub-millimeter precision across all eight bone areas with the only exception of one channel in the ankle. This method enables the robust measurement of lower extremity bone positions from 1D raw ultrasound signals. It shows great potential to apply A-mode ultrasound in orthopedic surgery from safe, convenient, and efficient perspectives."
EMG-Based Intention Detection Using Deep Learning for Shared Control in Upper-Limb Assistive Exoskeletons,"Paniz Sedighi, Xingyu Li, Mahdi Tavakoli",University of Alberta,Prosthetics and Exoskeletons I,"In the field of human-robot interaction, surface electromyography (sEMG) provides a valuable tool for measuring active muscular effort. While numerous studies have investigated real-time control of upper extremity exoskeletons based on user intention and task-specific movements, the prediction of body joint positions based on EMG features has remained largely unexplored. In this paper, we address this gap by proposing a novel approach that leverages Convolutional Neural Networks and Long-Short-Term Memory (CNN-LSTM) models to generate exoskeleton joint trajectories. Our methodology involves collecting data from three channels of EMG and three degrees-of-freedom (DoF) joint angles and enables us to position control a pneumatic cable-driven upper-limb exoskeleton, thereby assisting users in various tasks. Through extensive experimentation, our intention-based model demonstrates robust performance across different speeds and is capable of detecting variations in payload and electrode placement. The empirical results yielded from our study underscore the efficacy of our approach, particularly in reducing the EMG levels of the user during different tasks by providing exoskeleton assistance as needed."
Gait Phase Detection Based on LSTM-CRF for Stair Ambulation,"Haochen Wei, Kai Yu Tong, Michael Yu Wang, Chao Chen","Monash University,The Chinese University of Hong Kong,[email protected]",Prosthetics and Exoskeletons I,"It is essential to accurately identify gait phases when active exoskeleton devices assist with the lower limbs. This work focuses on IMU-based phase detection for stair ambulation. In order to enhance the detection sensitivity of phase transition, this work utilises the LSTM-CRF hybrid model. Four IMU sensors attached to the thighs and shanks on both legs were utilised to collect data during trials on ten healthy subjects for stair ascent and descent. The networkâ€™s performance is evaluated by F1-score, recall (true positive rate), and precision, which are 96.3% on average with a standard deviation (std) of 1.9%, 96.6% on average with an std of 1.6%, and 95.9% on average with an std of 2.7%, respectively."
Towards a Unified Approach for Continuously-Variable Impedance Control of Powered Prosthetic Legs Over Walking Speeds and Inclines,"Albert Lee, Curt A. Laubscher, T. Kevin Best, Robert D. Gregg",University of Michigan,Prosthetics and Exoskeletons I,"Research in powered prosthesis control has explored the use of impedance-based control algorithms due to their biomimetic capabilities and intuitive structure. Modern impedance controllers feature parameters that smoothly vary over gait phase and task according to a data-driven model. However, these recent efforts only use continuous impedance control during stance and instead utilize discrete transition logic to switch to kinematic control during swing, necessitating two separate models for the different parts of the stride. In contrast, this paper presents a controller that uses smooth impedance parameter trajectories throughout the gait, unifying the stance and swing periods under a single, continuous model. Furthermore, this paper proposes a basis model to represent intertask relationships in the impedance parametersâ€”a strategy that has previously been shown to improve model accuracy over classic linear interpolation methods. In the proposed controller, a weighted sum of Fourier series is used to model the impedance parameters of each joint as continuous functions of gait cycle progression and task. Fourier series coefficients are determined via convex optimization such that the controller best reproduces the joint torques and kinematics in a reference able-bodied dataset. Experiments with a powered knee-ankle prosthesis show that this simpler, unified model produces competitive results when compared to a more complex hybrid impedance-kinematic model over varying walking speeds and inclines."
"Design, Simulation and Kinematic Verification of a Multi-Loop Ankle-Foot Prosthetic Mechanism","Majun Song, Zhongyi Li, Weihai Chen, Hao Zheng, Sheng Guo, John Rasmussen, Shaoping Bai","Hangzhou Innovation Institute, Beihang University,Beijing Jiaotong University,Aalborg University",Prosthetics and Exoskeletons I,"Inspired by the bionic characteristics of ankle and calf skeletal muscles, a novel ankle-foot prosthesis (AFP) with variable stiffness mechanisms (VSMs) is proposed to assist transtibial amputees to restore ankle plantarflexion- dorsiflexion. The prosthesis is designed in the form of a spring-loaded three-loop linkage for function of continuous energy absorption-release in gait stance phase, which can facilitate ankle plantarflexion- dorsiflexion and keep human body move forward steadily. A compliant crank slider mechanism is also developed to power-assist AFP mechanism to improve the adaptive compliant contact between prosthesis and ground. In this paper, mechanics models of the ATP are developed to reveal the variable moment of the ankle joint, which is verified by human-machine simulation. An AFP prototype is built to validate the design experimentally. The results demonstrate that the AFP mechanism has the advantages of low power consumption, human-like joint moment profile. In particular, it is shown that the AFP mechanism with 54W power provided in toe-off phase can reduce the peak power of the motor by 24%."
Short Term After-Effects of Small Force Fields Applied by an Upper-Limb Exoskeleton on Inter-Joint Coordination,"Océane Dubois, Agnès Roby-Brami, Ross Parry, Nathanael Jarrassé","Sorbonne University,université Pierre et Marie Curie, Paris ,,Université Paris Nanterre,Sorbonne Université, ISIR UMR ,,,, CNRS",Prosthetics and Exoskeletons I,"Exoskeleton technologies have numerous potential applications, ranging from improving human motor skills to aiding individuals in their daily activities. While exoskeletons are increasingly viewed, for example, as promising tools in industrial ergonomics, the effect of using them on human motor control, particularly on inter-joint coordination, remains relatively uncharted. This paper investigates the effects of generic low-amplitude force fields applied by an exoskeleton on motor strategies in asymptomatic users. The force fields mimic common perturbations encountered in exoskeletons, such as residual friction, over/under-tuned assistance, or structural elasticity. Fifty-five participants performed reaching tasks while connected to an arm exoskeleton, experiencing one of five tested force fields. Their movements before and after exposure to the exoskeleton force field were compared. The study focuses both on spatial and temporal changes in coordination using specific metrics. The results reveal that even brief exposure to a low-amplitude force field, or to uncompensated residual friction and dynamic forces, applied at the joint level can alter the inter-joint coordination, while task performance remains unaffected. The tested force fields induced varying degrees of changes in joint contributions and synchronization. This study highlights the importance of monitoring coordination changes to fully understand the impact of exoskeletons on human motor control and thus enable safe and widespread adoption of those devices."
Real-Time Dexterous Prosthesis Hand Control by Decoding Neural Information Based on EMG Decomposition,"Zhenzhi Ying, Xianyu Zhang, Shihao Li, Koki Nakashima, Liming Shu, Naohiko Sugita","The University of Tokyo,Dalian University of Technology",Prosthetics and Exoskeletons I,"The vague interpretation of myoelectrical signals on the residual limb end makes restoring dexterous hand function in amputees still impossible. Understanding motor control between human motion intention and synaptic inputs to motor neurons also remains a significant challenge. The neural decoding methods of surface EMG signals remains challenging, which limit the application of robot hand in real life. Herein, we propose and substantiate a human-machine interface for motor control that introduces neural information of motor neurons in conjunction with the combination mechanism of muscle contraction. The interface firstly introduces a new concept of motor unit (MU) spike trains, which combines decoupling of the electrical activations on motor neuron axons with extraction of motion patterns from the discharge timings of the motor neuron pools. We realized a real-time implementation of the EMG decomposition algorithm on our developed prosthesis hand control system. The control scheme provides an accurate classification of intuitive hand motions, enabling the amputee to perform versatile finger movements of the prosthesis hand. The concept of motor neuron discharge timings was evaluated through experiments on one amputee participant and six able-bodied participants. The results show that the neuroprosthesis hand control scheme based on MU spike trains has the capacity of generating accurate and intuitive hand movements for amputees in a physical environment."
A Novel Back-Support Exoskeleton with a Differential Series Elastic Actuator for Lifting Assistance,"Shuo Ding, Francisco Anaya Reyes, Shounak Bhattacharya, Ashwin Narayan, Shuaishuai Han, Seyram Ofori, Haoyong Yu","National University of Singapore,National university of Singapore,Nanjing University of Science and Technology",Prosthetics and Exoskeletons I,"Compared to conventional back-support exoskeletons (BSEs) with two motors, BSEs driven by a single motor have the advantage of light weight. However, current single-motor BSEs have problems with accommodating asynchronous hip movements, achieving precise force control and efficient force transmission, and giving autonomy to users when walking. In this paper, we propose a novel BSE with a differential series elastic actuator (D-SEA) for lifting assistance. The unique differential working principle can accommodate the angular difference between the hip joints and provide the same assistive torque at both hip joints. The D-SEA achieves precise force control with a custom controller based on accurate spring deflection feedback, and drives the hip joints via an efficient cable-roller mechanism. Taking advantage of the active backdrivability of the D-SEA, we proposed an intelligent assistive strategy that automatically provides adequate support for lifting tasks and grants autonomy to users during walking. In the experiments, the BSE reduced the activation level of the back muscles by up to 40% during lifting, without increasing the muscle activation during walking."
The Un-Kidnappable Robot: Acoustic Localization of Sneaking People,"Mengyu Yang, Patrick Grady, Samarth Brahmbhatt, Arun Balajee Vasudevan, Charlie Kemp, James Hays","Georgia Institute of Technology,Intel Corporation,Carnegie Mellon University,Hello Robot Inc.,Georgia Institute of Technology, Argo AI",Multi-Modal Perception for HRI I,"How easy is it to sneak up on a robot? We examine whether we can detect where people are using only the incidental sounds they produce as they move, even when they try to be quiet. We collect a robotic dataset of high-quality 4-channel audio paired with 360 degree RGB data of people moving in different indoor settings. We train models that predict whether there is a moving person nearby and then their location. We implement our method on a robot in real time, demonstrating the ability for robots to navigate populated indoor spaces in a passive manner. For demonstration videos, see our project page: https://sites.google.com/view/unkidnappable-robot"
Predicting the Intention to Interact with a Service Robot: The Role of Gaze Cues,"Simone Arreghini, Gabriele Abbate, Alessandro Giusti, Antonio Paolillo","IDSIA USI-SUPSI,Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDS",Multi-Modal Perception for HRI I,"For a service robot, it is crucial to perceive as early as possible that an approaching person intends to interact: in this case, it can proactively enact friendly behaviors that lead to an improved user experience. We solve this perception task with a sequence-to-sequence classifier of a potential user intention to interact, which can be trained in a self-supervised way. Our main contribution is a study of the benefit of features representing the personâ€™s gaze in this context. Extensive experiments on a novel dataset show that the inclusion of gaze cues significantly improves the classifier performance (AUROC increases from 84.5 % to 91.2 %); the distance at which an accurate classification can be achieved improves from 2.4 m to 3.2 m. We also quantify the systemâ€™s ability to adapt to new environments without external supervision. Qualitative experiments show practical applications with a waiter robot."
Non-Verbal Cues on Robot-Group Persuasion,"Alexandra Esperança Gonçalves, Plinio Moreno, Jodi Forlizzi, Leonel Garcia Marques, Alexandre Bernardino","Instituto Superior Técnico, University of Lisbon,IST-ID,Carnegie Mellon University,Faculty of Psychology, University of Lisbon,IST - Técnico Lisboa",Multi-Modal Perception for HRI I,"When integrating robots into human daily life, persuasive power can be essential. However, there are often group dynamics which can complicate persuasion. This study focuses on how non-verbal cues, specifically gaze and hand gestures, affect the persuasiveness of a social robot. We have designed a protocol to include non-verbal cues in the social robot Vizzy (head and eye gaze, hand gestures) and test them in a series of experiments using the paradigm of the ""Desert Survival Challenge"". The goal of the robot is to persuade the participants of the game into changing their answers whilst avoiding negative feelings. It is hypothesized that the non-verbal cues will help avoid psychological reactance without diminishing compliance to the verbal requests issued by the robot. This phenomenon has been verified before for single person persuasion, but it is yet to be tested on groups. Thus, the goal of this project is to verify the effect of non-verbal cues in group persuasion by a robot and comparing it to single person persuasion. The results showed that the robot's gestures increased compliance by the group and the gaze behaviour decreased psychological reactance."
Naturalistic Robot-To-Human Bimanual Handover in Complex Environments through Multi-Sensor Fusion,"Salih Ertug Ovur, Yiannis Demiris",Imperial College London,Multi-Modal Perception for HRI I,"Robot-human object handover has been extensively studied in recent years for a wide range of applications. However, it is still far from being as natural as human-human handovers, largely due to the robotsâ€™ limited sensing capabilities. Previous approaches in the literature typically simplify the handover scenarios, including one or more of (a) conducting handovers at fixed locations, (b) not adapting to human preferences, or (c) only focusing on single-arm handover with small objects due to the sensor occlusions caused by large objects. To advance the state of the art toward a human-human level of handover fluency, this paper investigates a bimanual handover scenario in a naturalistic, complex setup. Specifically, we target robot-to-human box transfer while the human partner is on a ladder, and ensure that object is adaptively delivered based on human preferences. To address the occlusion problem that arises in a complex environment, we develop an onboard multi-sensor perception system for the bimanual robot, introduce a measurement confidence estimation technique, and propose an occlusion-resilient multi-sensor fusion technique by positioning visual perception sensors in distinct locations on the robot with different fields of view. Multi-sensor fusion approach achieved a handover success rate above 86.7% for all experiments by successfully combining the strengths of both fields of view of human pose tracking under partial occlusions without sacrificing handover duration."
Language and Sketching: An LLM-Driven Interactive Multimodal Multitask Robot Navigation Framework,"Weiqin Zu, Wenbin Song, Ruiqing Chen, Ze Guo, Fanglei Sun, Zheng Tian, Wei Pan, Jun Wang","ShanghaiTech University,Harbin Institute of Technology,ShangTech University,The University of Manchester,University College London",Multi-Modal Perception for HRI I,"The socially-aware navigation system has evolved to adeptly avoid various obstacles while performing multiple tasks, such as point-to-point navigation, human-following, and -guiding. However, a prominent gap persists: in Human-Robot Interaction (HRI), the procedure of communicating commands to robots demands intricate mathematical formulations. Furthermore, the transition between tasks does not quite possess the intuitive control and user-centric interactivity that one would desire. In this work, we propose an LLM-driven interactive multimodal multitask robot navigation framework, termed LIM2N, to solve the above new challenge in the navigation field. We achieve this by first introducing a multimodal interaction framework where language and hand-drawn inputs can serve as navigation constraints and control objectives. Next, a reinforcement learning agent is built to handle multiple tasks with the received information. Crucially, LIM2N creates smooth cooperation among the reasoning of multimodal input, multitask planning, and adaptation and processing of the intelligent sensing modules in the complicated system. Extensive experiments are conducted in both simulation and the real world demonstrating that LIM2N has superior user needs understanding, alongside an enhanced interactive experience."
Dual-Modal Tactile E-Skin: Enabling Bidirectional Human-Robot Interaction Via Integrated Tactile Perception and Feedback,"Shilong Mu, Runze Zhao, Zenan Zenanlin, Yan Huang, Shoujie Li, Chenchang Li, Xiao-Ping (Steven) Zhang, Wenbo Ding","Tsinghua University,Wuhan University,Tsinghua Shenzhen International Graduate School,Ryerson University",Multi-Modal Perception for HRI I,"To foster an immersive and natural human-robot interaction (HRI), the implementation of tactile perception and feedback becomes imperative, effectively bridging the conventional sensory gap. In this paper, we propose a dual-modal electronic skin (e-skin) that integrates magnetic tactile sensing and vibration feedback for enhanced HRI. The dual-modal tactile e-skin offers multi-functional tactile sensing and programmable haptic feedback, underpinned by a layered structure comprised of flexible magnetic films, soft silicone elastomer, a Hall sensor and actuator array, and a microcontroller unit. The e-skin captures the magnetic field changes caused by subtle deformations through Hall sensors, employing deep learning for accurate tactile perception. Simultaneously, the actuator array generates mechanical vibrations to facilitate haptic feedback, delivering diverse mechanical stimuli. Notably, the dual-modal e-skin is capable of transmitting tactile information bidirectionally, enabling object recognition and fine-weighing operations. This bidirectional tactile interaction framework will enhance the immersion and efficiency of interactions between humans and robots."
Close-Range Human Following Control on a Cane-Type Robot with Multi-Camera Fusion,"Haowen Liu, Fengxian Wu, Bin Zhong, Yijun Zhao, Jiatong Zhang, Wenxin Niu, Mingming Zhang","Southern University of Science and Technology,Tongji University,Harbin Institute of Technology",Multi-Modal Perception for HRI I,
CAMInterHand: Cooperative Attention for Multi-View Interactive Hand Pose and Mesh Reconstruction,"Guwen Han, Qi Ye, Anjun Chen, Jiming Chen","Zhejiang university,Zhejiang University",Multi-Modal Perception for HRI I,"Interactive hand mesh reconstruction from single-view images poses a significant challenge with the severe occlusion and depth ambiguity inherent in interactive hand gestures. Recent approaches that employ probabilistic models and token-pruned techniques have shown decent results in multi-view human body reconstruction. Nevertheless, these methods have not fully utilized multi-scale semantic information from multi-view images and are not applicable in scenarios involving severe occlusion during dual-hand interactions. Simultaneously, current single-view methods independently reconstruct the left and right hands, which are ineffective in enhancing the interaction between both hands. To address these challenges, we propose CAMInterHand, a cooperative attention-based method for multi-view interactive hand pose and mesh reconstruction. Specifically, CAMInterHand extracts local pyramid features and global vertex features from multi-scale feature maps of multi-view images, enabling the exploration of rich local semantic information and facilitating effective feature alignment. Furthermore, CAMInterHand employs the cooperative attention fusion module to fuse all features from multi-view images, enhancing interactions among vertices of dual hands within global and local contexts. We conduct extensive experiments on the large-scale multi-view dataset InterHand2.6M and CAMInterHand achieves a substantial performance improvement over existing methods for multi-view and single-view interactive hand reconstruction."
Action Segmentation Using 2D Skeleton Heatmaps and Multi-Modality Fusion,"Syed Waleed Hyder, Muhammad Usama, Anas Zafar, Muhammad Naufil, Fawad Javed Fateh, Andrey Konin, Zeeshan Zia, Quoc-huy Tran","Retrocausal,Retrocausal, Inc,Retrocausal Inc.,Retrocausal, Inc.",Multi-Modal Perception for HRI I,"This paper presents a 2D skeleton-based action segmentation method with applications in fine-grained human activity recognition. In contrast with state-of-the-art methods which directly take sequences of 3D skeleton coordinates as inputs and apply Graph Convolutional Networks (GCNs) for spatiotemporal feature learning, our main idea is to use sequences of 2D skeleton heatmaps as inputs and employ Temporal Convolutional Networks (TCNs) to extract spatiotemporal features. Despite lacking 3D information, our approach yields comparable/superior performances and better robustness against missing keypoints than previous methods on action segmentation datasets. Moreover, we improve the performances further by using both 2D skeleton heatmaps and RGB videos as inputs. To our best knowledge, this is the first work to utilize 2D skeleton heatmap inputs and the first work to explore 2D skeleton+RGB fusion for action segmentation."
ViTacTip: Design and Verification of a Novel Biomimetic Physical Vision-Tactile Fusion Sensor,"Wen Fan, Haoran Li, Weiyong Si, Shan Luo, Nathan Lepora, Dandan Zhang","University of Bristol,University of Essex,King's College London,Imperial College London",Force and Tactile Sensing I,"Tactile sensing is significant for robotics since it can obtain physical contact information during manipulation. To capture multimodal contact information within a compact framework, we designed a novel sensor called ViTacTip, which seamlessly integrates both tactile and visual perception capabilities into a single, integrated sensor unit. ViTacTip features a transparent skin to capture fine features of objects during contact, which can be known as the see-through-skin mechanism. In the meantime, the biomimetic tips embedded in ViTacTip can amplify touch motions during tactile perception. For comparative analysis, we also fabricated a ViTac sensor devoid of biomimetic tips, as well as a TacTip sensor with opaque skin. Furthermore, we develop a Generative Adversarial Network (GAN)-based approach for modality switching between different perception modes, effectively alternating the emphasis between vision and tactile perception modes. We conducted a performance evaluation of the proposed sensor across three distinct tasks: i) grating identification, ii) pose regression, iii) contact localization and force estimation. In the grating identification task, ViTacTip demonstrated an accuracy of 99.72%, surpassing TacTip, which achieved 94.60%. It also exhibited superior performance in both pose and force estimation tasks with the minimum error of 0.08mm and 0.03N, respectively, in contrast to ViTac's 0.12mm and 0.15N. Results indicate that ViTacTip outperforms single-modality sensors."
A Large-Area Tactile Sensor for Distributed Force Sensing Using Highly Sensitive Piezoresistive Sponge,"Wendong Zheng, Kun Liu, Di Guo, Wuqiang Yang, Jun Zhu, Huaping Liu","Tsinghua University,Institute of Semiconductors, Chinese Academy of Sciences,Beijing University of Posts and Telecommunications,The University of Manchester,Nanjing University of Information Science and Technology",Force and Tactile Sensing I,"Tactile sensing plays a critical role in enabling robots to interact safely with target objects in dynamic and unstructured environments. While various tactile sensors based on different sensing principles or different sensitive materials have been proposed, the development of flexible large-area tactile sensors for robots is still challenging. In this paper, a novel highly sensitive piezoresistive sponge based on multi-walled carbon nanotubes (MWCNTs) and polyurethane (PU) sponge is fabricated for pressure sensing. The sensing behavior of the piezoresistive sponge was experimentally evaluated, showing high sensitivity and fast response. Based on the piezoresistive sponge, a flexible large-area tactile sensor is designed for distributed force detection with electrical resistance tomography technology. The sensing performance of the sensor is validated by touch location, sensitivity analysis, real-time touch discrimination, and touch modality recognition. The experimental results indicate that the sensor performs well in detecting the position and force of contact in a large area. The sensor's performance shows promise in embodied tactile sensing and humanâ€“robot interaction."
A Neuromorphic System for the Real-Time Classification of Natural Textures,"George Brayshaw, Benjamin Ward-Cherrier, Martin Pearson","University of Bristol,Bristol Robotics Laboratory",Force and Tactile Sensing I,"Tactile exploration of surfaces is a key component of everyday life, allowing us to make complex inferences about our environments even when vision is occluded. The emergence of biomimetic neuromorphic hardware in recent years has furthered our ability to create biologically plausible sensing solutions. While these platforms continue to improve in regards to latency and power consumption, within recent literature on tactile texture classification there is an emphasis on accuracy at the expense of real-time processing. In order for these tactile sensing systems to find use outside of experimental laboratory environments, it is key to design systems capable of capturing and processing data in real-time. Within this paper we present a system for the real-time classification of texture using a neuromorphic tactile sensor, a spiking neural network and a novel decision making algorithm. Our real-time system achieves classification accuracies of 94% on a dataset of 11 natural textile textures. Furthermore our system is capable of identifying textures at human-level performance in as little as 84ms. Additionally, benchmarking our system across CPU, GPU and Loihi2 hardware platforms resulted in a 96% reduction in power consumption on the neuromorphic platform. This system out-performed previous work by the authors and the state of art, both in terms of accuracy and classification speed."
An Electromagnetism-Inspired Method for Estimating In-Grasp Torque from Visuotactile Sensors,"Yuni Fuchioka, Masashi Hamaya","University of British Columbia,OMRON SINIC X Corporation",Force and Tactile Sensing I,"Tactile sensing has become a popular sensing modality for robot manipulators, due to the promise of providing robots with the ability to measure the rich contact information that gets transmitted through its sense of touch. Among the diverse range of information accessible from tactile sensors, torques transmitted from the grasped object to the fingers through extrinsic environmental contact may be particularly important for tasks such as object insertion. However, tactile torque estimation has received relatively little attention when compared to other sensing modalities, such as force, texture, or slip identification. In this work, we introduce the notion of the Tactile Dipole Moment, which we use to estimate tilt torques from gel-based visuotactile sensors. This method does not rely on deep learning, sensor-specific mechanical, or optical modeling, and instead takes inspiration from electromechanics to analyze the vector field produced from 2D marker displacements. Despite the simplicity of our technique, we demonstrate its ability to provide accurate torque readings over two different tactile sensors and three object geometries, and highlight its practicality for the task of USB stick insertion with a compliant robot arm. These results suggest that simple analytical calculations based on dipole moments can sufficiently extract physical quantities from visuotactile sensors."
Sim-To-Real Model-Based and Model-Free Deep Reinforcement Learning for Tactile Pushing,"Max Yang, Yijiong Lin, Alex Church, John Lloyd, Dandan Zhang, David A. W. Barton, Nathan Lepora","University of Bristol,Cambrian,Imperial College London",Force and Tactile Sensing I,"Object pushing presents a key non-prehensile manipulation problem that is illustrative of more complex robotic manipulation tasks. While deep reinforcement learning (RL) methods have demonstrated impressive learning capabilities using visual input, a lack of tactile sensing limits their capability for fine and reliable control during manipulation. Here we propose a deep RL approach to object pushing using tactile sensing without visual input, namely tactile pushing. We present a goal-conditioned formulation that allows both model-free and model-based RL to obtain accurate policies for pushing an object to a goal. To achieve real-world performance, we adopt a sim-to-real approach. Our results demonstrate that it is possible to train on a single object and a limited sample of goals to produce precise and reliable policies that can generalize to a variety of unseen objects and pushing scenarios without domain randomization. We experiment with the trained agents in harsh pushing conditions, and show that with significantly more training samples, a model-free policy can outperform a model-based planner, generating shorter and more reliable pushing trajectories despite large disturbances. The simplicity of our training environment and effective real-world performance highlights the value of rich tactile information for fine manipulation."
Learning Contact for Haptic Feedback: Switching X-Lateral Teleoperators,"Nural Yilmaz, Ugur Tumerdem",Marmara University,Force and Tactile Sensing I,"In this paper, we propose X-lateral teleoperation: a novel hybrid unilateral-bilateral teleoperation framework. Bilateral teleoperation enables kinesthetic coupling between the operator and the remote environment with haptic feedback. However, in free motion, unlike unilateral teleoperators, bilateral teleoperators reflect undesirable operational forces to the operator. The proposed X-lateral teleoperation framework benefits from a learning-based contact detection algorithm which triggers switching from unilateral teleoperation in free motion to bilateral teleoperation in contact. We also present a neural network based two-class classification technique to detect contacts even with environments not seen in training. In experiments with linear motors and Phantom Omni devices, using sensorless force estimation, we show that the proposed method can decrease operational forces significantly over transparency-optimized bilateral architectures."
L3 F-TOUCH: A Wireless GelSight with Decoupled Tactile and Three-Axis Force Sensing,"Wanlin Li, Meng Wang, Jiarui Li, Yao Su, Devesh Jha, Xinyuan Qian, Kaspar Althoefer, Hangxin Liu","Beijing Institute for General Artificial Intelligence (BIGAI),Beijing Institute for General Artificial Intelligence,Peking University,Beijing Institute for General Artificial Intelligence （BIGAI）,Mitsubishi Electric Research Laboratories,University of Science and Technology Beijing,Queen Mary University of London",Force and Tactile Sensing I,"GelSight sensors that estimate contact geometry and force by reconstructing the deformation of their soft elastomer from images would yield poor force measurements when the elastomer deforms uniformly or reaches deformation saturation. Here we present L3 F-TOUCH sensor that considerably enhances the three-axis force sensing capability of typical GelSight sensors. Specifically, L3 F-TOUCH sensor comprises: (i) an elastomer structure resembling the classic GelSight sensor design for fine-grained contact geometry sensing; and (ii) a mechanically simple suspension structure to enable three-dimensional elastic displacement of the elastomer structure upon contact. Such displacement is tracked by detecting the displacement of an ARTag and is transformed to three-axis contact force via calibration. We further revamp the sensorâ€™s optical system by fixing the ARTag on the base and reflecting it to the same camera viewing the elastomer through a mirror. As a result, the tactile and force sensing modes can operate independently, but the entire L3 F-TOUCH remains Light-weight and Low-cost while facilitating a wireLess deployment. Evaluations and experiment results demonstrate that the proposed L3 F-TOUCH sensor compromises GelSightâ€™s limitation in force sensing and is more practical compared with equipping commercial three-axis force sensors. Thus, the L3 F-TOUCH could further empower existing Vision-based Tactile Sensors (VBTSs) in replication and deployment."
GelLink: A Compact Multi-Phalanx Finger with Vision-Based Tactile Sensing and Proprioception,"Yuxiang Ma, Alan Zhao, Edward Adelson","Massachusetts Institute of Technology,MIT",Force and Tactile Sensing I,"Compared to fully-actuated robotic end-effectors, underactuated ones are generally more adaptive, robust, and cost-effective. However, state estimation for underactuated hands is usually more challenging. Vision-based tactile sensors, like Gelsight, can mitigate this issue by providing high-resolution tactile sensing and accurate proprioceptive sensing. As such, we present GelLink, a compact, underactuated, linkage-driven robotic finger with low-cost, high-resolution vision-based tactile sensing and proprioceptive sensing capabilities. In order to reduce the amount of embedded hardware, i.e. the cameras and motors, we optimize the linkage transmission with a planar linkage mechanism simulator and develop a planar reflection simulator to simplify the tactile sensing hardware. As a result, GelLink only requires one motor to actuate the three phalanges, and one camera to capture tactile signals along the entire finger. Overall, GelLink is a compact robotic finger that shows adaptability and robustness when performing grasping tasks. The integration of vision-based tactile sensors can significantly enhance the capabilities of underactuated fingers and potentially broaden their future usage."
"RainbowSight: A Family of Generalizable, Curved, Camera-Based Tactile Sensors for Shape Reconstruction","Megha Tippur, Edward Adelson","Massachusetts Institute of Technology,MIT",Force and Tactile Sensing I,"Camera-based tactile sensors can provide high resolution positional and local geometry information for robotic manipulation. Curved and rounded fingers are often advantageous, but it can be difficult to derive illumination systems that work well within curved geometries. To address this issue, we introduce RainbowSight, a family of curved, compact, camera-based tactile sensors which use addressable RGB LEDs illuminated in a novel rainbow spectrum pattern. In addition to being able to scale the illumination scheme to different sensor sizes and shapes to fit on a variety of end effector configurations, the sensors can be easily manufactured and require minimal optical tuning to obtain high resolution depth reconstructions of an object deforming the sensorâ€™s soft elastomer surface. Additionally, we show the advantages of our new hardware design and improvements in calibration methods for accurate depth map generation when compared to alternative lighting methods commonly implemented in previous camera-based tactile sensors. With these advancements, we make the integration of tactile sensors more accessible to roboticists by allowing them the flexibility to easily customize, fabricate, and calibrate camera-based tactile sensors to best fit the needs of their robotic systems."
Walking-By-Logic: Signal Temporal Logic-Guided Model Predictive Control for Bipedal Locomotion Resilient to External Perturbations,"Zhaoyuan Gu, Rongming Guo, William Yates, Yipu Chen, Yuntian Zhao, Ye Zhao","Georgia Institute of Technology,Southern University of Science and Technology",Legged Robots I,"This study proposes a novel planning framework based on a model predictive control formulation that incorporates signal temporal logic (STL) specifications for task completion guarantee and robustness quantification. This marks the first-ever study to apply STL-guided trajectory optimization for bipedal locomotion push recovery, where the robot experiences unexpected disturbances. Existing recovery strategies often struggle with complex task logic reasoning and locomotion robustness evaluation, making them susceptible to failures due to inappropriate recovery strategies or insufficient robustness. To address this issue, the STL-guided framework generates optimal and safe recovery trajectories that simultaneously satisfy the task specification and maximize the locomotion robustness. Our framework outperforms a state-of-the-art locomotion controller in a high-fidelity dynamic simulation, especially in scenarios involving crossed-leg maneuvers. Furthermore, it demonstrates versatility in tasks such as locomotion on stepping stones, where the robot must select from a set of disjointed footholds to maneuver successfully."
Seamless Reaction Strategy for Bipedal Locomotion Exploiting Real-Time Nonlinear Model Predictive Control,"Jonghun Choe, Joon-Ha Kim, Seungwoo Hong, Jinoh Lee, Hae-Won Park","KAIST,Korea Advanced Institute of Science and Technology(KAIST),MIT (Massachusetts Institute of Technology),German Aerospace Center (DLR),Korea Advanced Institute of Science and Technology",Legged Robots I,"This paper presents a reactive locomotion method for bipedal robots enhancing robustness and external disturbance rejection performance by seamlessly rendering several walking strategies of the ankle, hip, and footstep adjustment. The Nonlinear Model Predictive Control (NMPC) is formulated to take into account nonlinear Divergent Component of Motion (DCM) error dynamics that predicts the future states of the robot in response to the walking strategies. This formulated NMPC enables the seamless application of these strategies improving push disturbance rejection performance. The proposed controller is validated in simulation and through an experiment on a bipedal robot platform, Gazelle, which confirms its effectiveness in real-time."
Synthesizing Robust Walking Gaits Via Discrete-Time Barrier Functions with Application to Multi-Contact Exoskeleton Locomotion,"Maegan Tucker, Kejun Li, Aaron Ames","Georgia Institute of Technology,California Institute of Technology,Caltech",Legged Robots I,"Successfully achieving bipedal locomotion remains challenging due to real-world factors such as model uncertainty, random disturbances, and imperfect state estimation. In this work, we propose a novel metric for locomotive robustness -- the estimated size of the hybrid forward invariant set associated with the step-to-step dynamics. Here, the forward invariant set can be loosely interpreted as the region of attraction for the discrete-time dynamics. We illustrate the use of this metric towards synthesizing nominal walking gaits using a simulation-in-the-loop learning approach. Further, we leverage discrete-time barrier functions and a sampling-based approach to approximate sets that are maximally forward invariant. Lastly, we experimentally demonstrate that this approach results in successful locomotion for both flat-foot walking and multi-contact walking on the Atalante lower-body exoskeleton."
"Efficient, Dynamic Locomotion through Step Placement with Straight Legs and Rolling Contacts","Stefan Fasano, James Paul Foster, Sylvain Bertrand, Christian Debuys, Robert Griffin","Florida Institute for Human & Machine Cognition,University of West Florida,Institute for Human and Machine Cognition,Texas A&M University,Institute for Human and Machine Cognition (IHMC)",Legged Robots I,"For humans, fast, efficient walking over flat ground represents the vast majority of locomotion that an individual experiences on a daily basis, and for an effective, real-world humanoid robot the same will likely be the case. In this work, we propose a locomotion controller for efficient walking over near-flat ground using a relatively simple, model-based controller that utilizes a novel combination of several interesting design features including an ALIP-based step adjustment strategy, stance leg length control as an alternative to center of mass height control, and rolling contact for heel-to-toe motion of the stance foot. We then present the results of this controller on our robot Nadia, both in simulation and on hardware. These results include validation of this controllerâ€™s ability to perform fast, reliable forward walking at 0.75 m/s along with backwards walking, side-stepping, turning in place, and push recovery. We also present an efficiency comparison between the proposed control strategy and our baseline walking controller over three steady-state walking speeds. Lastly, we demonstrate some of the benefits of utilizing rolling contact in the stance foot, specifically the reduction of necessary positive and negative work throughout the stride."
"Unified Motion Planner for Walking, Running, and Jumping Using the Three-Dimensional Divergent Component of Motion","George Mesesan, Robert Schuller, Johannes Englsberger, Christian Ott, Alin Albu-Schäffer","German Aerospace Center (DLR),DLR (German Aerospace Center),TU Wien,DLR - German Aerospace Center",Legged Robots I,"Running and jumping are locomotion modes that allow legged robots to rapidly traverse great distances and overcome difficult terrain. In this article, we show that the 3-D Divergent Component of Motion (3D-DCM) framework, which was successfully used for generating walking trajectories in previous works, retains its validity and coherence during flight phases, and, therefore, can be used for planning running and jumping motions. We propose a highly efficient motion planner that generates stable center-of-mass (CoM) trajectories for running and jumping with arbitrary contact sequences and time parametrizations. The proposed planner constructs the complete motion plan as a sequence of motion phases that can be of different types: stance, flight, transition phases, etc. We introduce a unified formulation of the CoM and DCM waypoints at the start and end of each motion phase, which makes the framework extensible and enables the efficient waypoint computation in matrix and algorithmic form. The feasibility of the generated reference trajectories is demonstrated by extensive whole-body simulations with the humanoid robot TORO."
Data-Driven Latent Space Representation for Robust Bipedal Locomotion Learning,"Guillermo Castillo, Bowen Weng, Wei Zhang, Ayonga Hereid","The Ohio State University,Iowa State University,Southern University of Science and Technology,Ohio State University",Legged Robots I,"This paper presents a novel framework for learning robust bipedal walking by combining a data-driven state representation with a Reinforcement Learning (RL) based locomotion policy. The framework utilizes an autoencoder to learn a low-dimensional latent space that captures the complex dynamics of bipedal locomotion from existing locomotion data. This reduced dimensional state representation is then used as states for training a robust RL-based gait policy, eliminating the need for heuristic state selections or the use of template models for gait planning. The results demonstrate that the learned latent variables are disentangled and directly correspond to different gaits or speeds, such as moving forward, backward, or walking in place. Compared to traditional template model-based approaches, our framework exhibits superior performance and robustness in simulation. The trained policy effectively tracks a wide range of walking speeds and demonstrates good generalization capabilities to unseen scenarios."
"LIKO: LiDAR, Inertial, and Kinematic Odometry for Bipedal Robots","Qingrui Zhao, Mingyuan Li, Yongliang Shi, Xuechao Chen, Zhangguo Yu, Lianqiang Han, Zhenyuan Fu, Jintao Zhang, Chao Li, Yuanxi Zhang, Qiang Huang","Beijing Institute of Technology,Tsinghua University,Beijing Insititute of Technology,Beijing institute of technology,School of Mechatronic Engineering, Beijing Institute of Technolo",Legged Robots I,"High-frequency and accurate state estimation is crucial for biped robots. This paper presents a tightly-coupled LiDAR-Inertial-Kinematic Odometry (LIKO) for biped robot state estimation based on an iterated extended Kalman filter. Beyond state estimation, the foot contact position is also modeled and estimated. This allows for both position and velocity updates from kinematic measurement. Additionally, the use of kinematic measurement results in an increased output state frequency of about 1kHz. This ensures temporal continuity of the estimated state and makes it practical for control purposes of biped robots. We also announce a biped robot dataset consisting of LiDAR, inertial measurement unit (IMU), joint encoders, force/torque (F/T) sensors, and motion capture ground truth to evaluate the proposed method. The dataset is collected during robot locomotion, and our approach reached the best quantitative result among other LIO-based methods and biped robot state estimation algorithms. The dataset and source code will be available at https://github.com/Mr-Zqr/LIKO."
Barry: A High-Payload and Agile Quadruped Robot,"Giorgio Valsecchi, Nikita Rudin, Lennart Nachtigall, Konrad Mayer, Fabian Tischhauser, Marco Hutter","Robotic System Lab, ETH,ETH Zurich, NVIDIA,ETH Zurich",Legged Robots I,"This paper introduces Barry, a dynamically balancing quadruped robot optimized for high payload capabilities and efficiency. It presents a new high-torque and low-inertia leg design, which includes custom-built high-efficiency actuators and transparent, sensorless transmissions. The robotâ€™s reinforcement learning-based controller is trained to fully leverage the new hardware capabilities to balance and steer the robot. The newly developed controller can manage the non-linearities introduced by the new leg design and handle unmodeled payloads up to 90kg while operating at high efficiency. The approachâ€™s efficacy is demonstrated by a high payload-to-weight ratio verified with multiple tests, with a maximum ratio of 2 on flat terrain. Experiments also demonstrate Barryâ€™s power consumption and cost of transport, which converge to a value of 0.7 at 1.4m/s, regardless of the payload mass."
A Robotic Manipulator Using Dual-Motor Joints: Prototype Design and Anti-Backlash Control,"Jiqian Xu, Huaizhen Wang, Qiankun Zhao, Yue Gao, Yingcai Wan, Lijin Fang","Northeastern University,Inspur group",Motion Control I,"This letter focuses on the design and control of a novel seven-degree-of-freedom (7-DOF) robotic manipulator (D-Arm) to address the issue of backlash nonlinearity coupling unknown disturbance through the dual-motor anti-backlash control technology. Specifically, the first three axes of the D-Arm near the base are implemented as dual-motor joints (DMJs), while the remaining four axes are single-motor joints (SMJs), which achieve a more comprehensive performance. For DMJs, which are over-actuated systems, we first discover an internal disturbance phenomenon named servo-conflict and consider it in the controller design. To mitigate the adverse effects of backlash coupling disturbance, an admittance control-based position compensator is proposed. Then, after the backlash elimination, a dual-motor linear active disturbance rejection controller is effectively developed for load tracking task of the DMJ. In the presence of unknown backlash and disturbance, the proposed strategy can improve both transient and steady-state position tracking response, reduce energy consumption without requiring any backlash model information. The effectiveness and simplicity of the developed control strategy are verified through comparative experiments on the D-Arm."
CoNi-MPC: Cooperative Non-Inertial Frame Based Model Predictive Control,"Baozhe Zhang, Xinwei Chen, Zhehan Li, Giovanni Beltrame, Chao Xu, Fei Gao, Yanjun Cao","The Chinese University of Hong Kong, Shenzhen,Zhejiang University,Ecole Polytechnique de Montreal,Zhejiang University, Huzhou Institute of Zhejiang University",Motion Control I,
"Uniform Passive Fault-Tolerant Control of a Quadcopter with One, Two, or Three Rotor Failure","Chenxu Ke, Kai-yuan Cai, Quan Quan","Beihang University,Beijing University of Aeronautics and Astronautics",Motion Control I,"This study proposes a uniform passive fault-tolerant control (FTC) method for a quadcopter that does not rely on fault information subject to one, two adjacent, two opposite, or three rotor failure. The uniform control implies that the passive FTC is able to cover the condition from quadcopter fault-free to rotor failure without controller switching. To achieve the purpose of the passive FTC, the rotorsâ€™ fault is modeled as a lumped disturbance acting on the virtual control of the quadcopter system. The disturbance estimate is used directly for the passive FTC with rotor failure. At the same time, a modified controller structure is designed to achieve the passive FTC ability for two and three rotor failure. To avoid the control allocation switch from the fault-free control to the FTC, a dynamic control allocation is used. In addition, the closed-loop stability is analyzed under up to three rotor failure. To validate the proposed uniform passive FTC method, outdoor experiments are performed for the first time, which have demonstrated that the hovering quadcopter is able to recover from one rotor failure by the proposed controller and continue to fly even if two adj"
Geometric Slosh-Free Tracking for Robotic Manipulators,"Jon Arrizabalaga, Lukas Pries, Riddhiman Laha, Runkang Li, Sami Haddadin, Markus Ryll","Technical University of Munich (TUM),Technical University of Munich,Technical University Munich",Motion Control I,"This work focuses on the agile transportation of liquids with robotic manipulators. In contrast to existing methods that are either computationally heavy, system/container specific or dependant on a singularity-prone pendulum model, we present a real-time slosh-free tracking technique. This method solely requires the reference trajectory and the robot's kinematic constraints to output kinematically feasible joint space commands. The crucial element underlying this approach consists on mimicking the end-effector's motion through a virtual quadrotor, which is inherently slosh-free and differentially flat, thereby allowing us to calculate a slosh-free reference orientation. Through the utilization of a cascaded proportional-derivative (PD) controller, this slosh-free reference is transformed into task space acceleration commands, which, following the resolution of a Quadratic Program (QP) based on Resolved Acceleration Control (RAC), are translated into a feasible joint configuration. The validity of the proposed approach is demonstrated by simulated and real-world experiments on a 7 DoF Franka Emika Panda robot."
Development of a Four-Wheel Steering Scale Vehicle for Research and Education on Autonomous Vehicle Motion Control,"Christopher Rother, Zhaodong Zhou, Jun Chen","Oakland University,oakland university",Motion Control I,"Autonomous vehicle motion control development requires testing and evaluation at all stages of the process. The development phase involving the instrumentation and operation of a full-size vehicle can be especially costly. Scale vehicles have been developed in the literature to serve as a cost-effective transition from testing in a simulation environment to a physical system. However, the existing scale vehicle platforms do not support four-wheel steering and cannot isolate the performance of motion control algorithms from other modules such as perception and path planning. This paper closes this gap by proposing a new scale vehicle platform, called JetRacer-4WS, based on the open-source JetRacer autonomous vehicle with additional modifications to support four-wheel steering, model predictive control-based path following, and high-precision ultrasonic-based real-time positioning. The proposed JetRacer-4WS can be used as a low-cost platform for both research and education on motion control, path following, and vehicle dynamics. We describe the design of JetRacer-4WS, and experimentally demonstrate JetRacer-4WSâ€™ ability to perform controller auto-tuning and illustrate the advantage of four-wheel steering. We also show that JetRacer-4WS can be used as a validation platform for testing advanced control algorithms such as event-triggered model predictive control. The associated code is open-sourced and available at: https://github.com/jchenee2015/jetracer-4WS."
Online-Learning-Based Distributionally Robust Motion Control with Collision Avoidance for Mobile Robots,"Han Wang, Chao Ning, Longyan Li, Weidong Zhang","Shanghai Jiao Tong University,Shanghai JiaoTong University",Motion Control I,"Collision-free navigation is a critical issue in robotic systems as the environment is often dynamic and uncertain. This paper investigates a data-stream-driven motion control problem for mobile robots to avoid randomly moving obstacles when the probability distribution of the obstacleâ€™s movement is partially observable through data and can be even time varying. A data-stream-driven ambiguity set is firstly constructed from movement data by leveraging a Dirichlet process mixture model and is updated online using real-time data. Then we propose an Online-Learning-based Distributionally Robust Nonlinear Model Predictive Control (OL-DR-NMPC) approach for limiting the risk of collision through considering the worst-case distribution within the ambiguity set. To facilitate solving the OL-DR-NMPC problem, we reformulate it as a finite-dimensional nonlinear optimization problem. To cope with the bilinear matrix inequality constraints in the nonlinear problem, we develop a parabolic relaxation and a sequential algorithm, by which the problem is further transformed into polynomial-time solvable surrogates. The simulations using a quadrotor model are employed to demonstrate the effectiveness and advantages of the proposed method."
Prediction of Pose Errors Implied by External Forces Applied on Robots: Towards a Metric for the Control of Collaborative Robots,"Vincent Fortineau, Vincent Padois, David Daney","Inria, Talence, France,Inria Bordeaux,Inria centre at the university of Bordeaux, F-,,,,, Talence, Fra",Motion Control I,"The presented work tackles the question of quantifying the pose deviations of robots subject to external disturbance forces. While this question may not be central for large robots perfectly rejecting disturbances through high controller gains, it is an important factor when considering collaborative settings where smaller robots may be deviated from their task because of unmodeled physical interactions. This is all the more true with human-robot collaboration where human capacities may fluctuate over time and have to be compensated by a proper adaptation of the robot control. To move forward in this direction, this work first derives a deviation prediction methodology and exemplifies it using three largely employed control approaches. The proposed prediction method is then validated using simulated and real robot experiments both in single and multiple robots cases. The obtained results constitute a stepping stone towards a quantitative metric for robots adapting their behaviour to human motor fluctuation."
Iterative Learning Control for Deformable Open-Frame Cable-Driven Parallel Robots,"Wuichung Cheng, Ngo Foon Chan, Darwin Lau",The Chinese University of Hong Kong,Motion Control I,"This paper proposed an iterative learning control (ILC) scheme for deformable open-frame cable-driven parallel robots (D-CDPRs). In contrast to the straightforward inverse kinematics of the rigid frame cable-driven parallel robots (CDPRs), accurate modeling of the deformable frame poses challenges due to errors and uncertainties. To address these issues, the authors propose the use of ILC, a control strategy that modifies the control input over iterations based on previous results. ILC has been successfully applied to traditional cable robots, particularly in handling model uncertainty. The paper presents a novel ILC control scheme specifically designed for D-CDPRs, with a focus on reducing tracking errors over repetitive operations. Additionally, hardware experiments are conducted to validate the effectiveness and reliability of the proposed ILC approach. The results demonstrate the efficacy of ILC in mitigating tracking errors, even in scenarios where the dynamic model of the D-CDPRs is unknown."
A Study of Force-Free Control Framework for Industrial Manipulator Tasks Based on High-Pass Filter,"Guanwei He, Guodong Feng, Beichen Ding","Shenzhen Campus of Sun Yat-sen University,School of Information Science and Technology, Sun Yat-sen Univer,Sun Yat-sen University",Motion Control I,"Force-free control (FFC) allows for flexible manipulator motion in response to external forces, making it a vital component of human-robot interaction (HRI). Manual intervention may cause uneven forces on the manipulator or frequencies close to the natural frequency, and mechanical resonance can occur due to the inertia of the manipulator and adjustable equivalent stiffness of the controller. This paper proposes an FFC approach for industrial manipulators using a six-axis force/torque sensor (F/T sensor), implemented through a three-layer control architecture, consisting of motion control layer, Admittance Control layer and force decoupling layer. To mitigate the effects of mechanical resonance, a high-pass filter (HPF) is integrated with the F/T sensor and its impact is investigated. Experimental validation is conducted using both a simulation model and an industrial manipulator. Test results indicate that the proposed FFC architecture enables the manipulator not only to interact smoothly with external forces, but also to distinguish load forces at different frequencies and potentially address the issue of mechanical resonance between the manipulator and the applied load forces."
Uncertainty-Aware Contextual Visualization for Human Supervision of OCT-Guided Autonomous Robotic Subretinal Injection,"Michael Sommersperger, Shervin Dehghani, Philipp Matten, Hessam Roodaki, Nassir Navab","Technical University of Munich,TUM,Medical University of Vienna,Technische Universität München,TU Munich",Medical Robots I,"The injection of therapeutic agents into the subretinal space might allow improved treatment of age-related macular degeneration. Various robotic systems have been developed to achieve the required precision, and in combination with intraoperative Optical Coherence Tomography (iOCT) imaging, methods for autonomous robotic guidance have been proposed. In such systems, the robotâ€™s cognition is often governed by machine learning algorithms, such as convolutional neural networks (CNNs), which provide semantic scene information from iOCT images. Although the robot performs a surgical task autonomously, human supervision is critical to monitor the robotâ€™s execution and, if necessary, stop the robot or take control to avoid trauma to the patient. In this paper, we propose a novel visualization concept for improved human supervision of autonomous robotic subretinal injection that integrates uncertainty information of the data provided to the robot. We design a focus and context visualization that renders an automatically identified instrument-aligned B-scan in the context of the 3D OCT volume. Our visualization is enriched by augmenting the uncertainty information on the instrument-aligned B-scan. To dynamically model task-specific uncertainty, we introduce a weighting scheme to assign an importance factor to each pair of classes, controlling the impact of their confusion on the overall uncertainty. We demonstrate our visualization concept on iOCT volumes acquired at different stages during subretinal injection on ex-vivo porcine eyes. We show that our processing pipeline achieves sufficient update rates for surgical display and discuss the impact of our visualization concept on the acceptance of robotic task autonomy for subretinal injection procedures."
A Track-Based Colon Endoscopic Robot with Depth Perception Stereo Cameras for Haustral Fold Detection During Colonic Navigation,"Shujing He, Yujie Zhang, Baoyi Huang, Jie Lin, Chaoyang Shi, Chengzhi Hu","Southern University of Science and Technology,Guangzhou university of Chinese medicine,Tianjin University",Medical Robots I,"Colon endoscopic robots represent a promising screening modality for the visualization of colon cancers with high sensitivity. However, current colonoscopy robots are often characterized by intricate and bulky mechanical structures, which pose practical challenges when moving through the complex and narrow environment of the colon. Moreover, these robots are typically equipped with a single camera, limiting their ability to accurately estimate the depth of haustral folds in colon, which is of great importance for the active colonic navigation of the robots. To address these challenges, we develop a track-based stereoscopic endoscopic robot (TSER) which is equipped with four tracks positioned at the corners of its body. This innovative design maximizes the contact between the tracks and the colon wall, enhancing maneuverability. The tracks are constructed from de-molded polydimethylsiloxane (PDMS) and incorporate micro-patterns on their outer surfaces. We have proposed a straightforward strategy for detecting haustral folds using TSER's stereo camera, which allows for precise identification of their position and depth. The TSER achieves an average motion speed of 9.8 mm/s in a bellows tube that contains silicone oil and a speed of 5.2 mm/s in an ex-vivo porcine intestinal segment. Impressively, the TSER boasts an 88.11% accuracy rate in haustral fold depth estimation, surpassing the performance of existing geometric shape fitting methods. These results demonstrate that the TSER holds great potential for effective and efficient movement and inspection within the colon, offering a promising solution for improved colon cancer screening."
Caveats on the First-Generation Da Vinci Research Kit: Latent Technical Constraints and Essential Calibrations,"Zejian Cui, João Cartucho, Stamatia Giannarou, Ferdinando Rodriguez Y Baena","Imperial College London,Imperial College, London, UK",Medical Robots I,"Telesurgical robotic systems provide a well established form of assistance in the operating theater, with evidence of growing uptake in recent years. Until now, the da Vinci surgical system (Intuitive Surgical Inc, Sunnyvale, California) has been the most widely adopted robot of this kind, with more than 6,700 systems in current clinical use worldwide. To accelerate research on robotic-assisted surgery, the retired first-generation da Vinci robots have been redeployed for research use as ""da Vinci Research Kits"" (dVRKs), which have been distributed to research institutions around the world to support both training and research in the sector. In the past ten years, a great amount of research on the dVRK has been carried out across a vast range of research topics. During this extensive and distributed process, common technical issues have been identified that are buried deep within the dVRK research and development architecture, and were found to be common among dVRK user feedback, regardless of the breadth and disparity of research directions identified. This paper gathers and analyzes the most significant of these, with a focus on the technical constraints of the first-generation dVRK, which both existing and prospective users should be aware of before embarking onto dVRK-related research. The hope is that this review will aid users in identifying and addressing common limitations of the systems promptly, thus helping to accelerate progress in the field."
A Passive Variable Impedance Control Strategy with Viscoelastic Parameters Estimation of Soft Tissues for Safe Ultrasonography,"Luca Beber, Edoardo Lamon, Davide Nardi, Daniele Fontanelli, Matteo Saveriano, Luigi Palopoli",University of Trento,Medical Robots I,"In the context of telehealth, robotic approaches have proven a valuable solution to in-person visits in remote areas, with decreased costs for patients and infection risks. In particular, in ultrasonography, robots have the potential to reproduce the skills required to acquire high-quality images while reducing the sonographer's physical efforts. In this paper, we address the control of the interaction of the probe with the patient's body, a critical aspect of ensuring safe and effective ultrasonography. We introduce a novel approach based on variable impedance control, allowing the real-time optimisation of compliant controller parameters during ultrasound procedures. This optimisation is formulated as a quadratic programming problem and incorporates physical constraints derived from viscoelastic parameter estimations. Safety and passivity constraints, including an energy tank, are also integrated to minimise potential risks during human-robot interaction. The proposed method's efficacy is demonstrated through experiments on a patient's dummy torso, highlighting its potential for achieving safe behaviour and accurate force control during ultrasound procedures, even in cases of contact loss."
"A Robotic System for Transanal Endoscopic Microsurgery: Design, Dexterity Optimization and Prototyping","Jichen Li, Shuxin Wang, Zhiqiang Zhang, Chaoyang Shi","Tianjin University,University of Leeds",Medical Robots I,"The paper introduces a novel robotic system for transanal endoscopic microsurgery (TEM) with a master-slave operated configuration. This slave manipulator features a modular distal continuum section, comprising two 7-DoF surgical instruments and a 5-DoF endoscopic arm designed to enhance hand-eye coordination and instrument triangulation in narrow and shallow rectal spaces. Key innovations include the hybrid coaxial continuum unit (HCCU) for improved bending characteristics and structural stiffness, and a design optimization for dexterity under anatomical constraints. Experimental validations demonstrate the system's precision and capability in simulated surgical tasks, highlighting its potential for advanced TEM applications with improved operational dexterity and reduced view obstruction."
Implicit Neural Representations for Breathing-Compensated Volume Reconstruction in Robotic Ultrasound,"Yordanka Velikova, Mohammad Farid Azampour, Walter Simson, Marco Esposito, Nassir Navab","TU Munich,Technical Univeristy of Munich,Technical University Munich,ImFusion GmbH",Medical Robots I,"Ultrasound (US) imaging is widely used in diagnosing and staging abdominal diseases due to its lack of non-ionizing radiation and prevalent availability. However, significant inter-operator variability and inconsistent image acquisition hinder the widespread adoption of broader screening programs. Robotic ultrasound systems have emerged as a promising solution, offering standardized acquisition protocols and the possibility of automated acquisition. Additionally, robotic ultrasound systems enable access to 3D data via robotic tracking and incoherent compounding of ultrasound frames, which results in improved interpretation and disease diagnosis. However, the interpretability of 3D ultrasound reconstruction of abdominal images can be affected by the patient's breathing motion. This study introduces a method to compensate for breathing motion in 3D ultrasound compounding by leveraging implicit neural representations. Our approach employs a robotic ultrasound system for automated screenings. To demonstrate the method's effectiveness, we evaluate our proposed method for the diagnosis and monitoring of abdominal aorta aneurysms as a representative use case. Our experiments demonstrate that our proposed pipeline facilitates robust automated robotic acquisition, mitigating artifacts from breathing motion, and yields smoother 3D reconstructions for enhanced screening and medical diagnosis."
Shadow-Based 3D Pose Estimation of Intraocular Instrument Using Only 2D Images,"Junjie Yang, Zhihao Zhao, Mathias Maier, Kai Huang, Nassir Navab, M. Ali Nasseri","TUM,Technische Universität München,Klinikum rechts der Isar der TU München,Sun Yat-sen University,TU Munich,Technische Universitaet Muenchen",Medical Robots I,"In ophthalmic surgeries, such as vitreoretinal operations, surgeons rely on imaging systems, primarily microscopes, for real-time instrument monitoring and motion planning. However, novice surgeons struggle to estimate instrument positions from 2D microscope frames, necessitating extensive trial-and-error experience with the background that additional imaging modalities such as iOCT remain inaccessible in most operating rooms. Targeting intraocular assessment within the current surgical setup, this paper presents an image-based pose estimation method to obtain real-time instrument positions in a standard 12mm-radius spherical eyeball model, achieved by linking floating instruments with on-the-retinal objects based on the intraocular shadowing principle. We validate this pose-estimation method in a Unity simulator and verify its depth estimation capability using a specially designed eyeball phantom. Both simulator and phantom experiments demonstrate an average needle-tip estimation error within [1.0, 2.0] mm using only 2D microscope frames."
Skeleton Graph-Based Ultrasound-CT Non-Rigid Registration,"Zhongliang Jiang, Xuesong Li, Chenyu Zhang, Yuan Bi, Walter Stechele, Nassir Navab","Technical University of Munich,TUM,TU Munich",Medical Robots I,"Autonomous ultrasound (US) scanning has attracted increased attention, and it has been seen as a potential solution to overcome the limitations of conventional US examinations, such as inter-operator variations. However, it is still challenging to autonomously and accurately transfer a planned scan trajectory on a generic atlas to the current setup for different patients, particularly for thorax applications with limited acoustic windows. To address this challenge, we proposed a skeleton graph-based non-rigid registration to adapt patient-specific properties using subcutaneous bone surface features rather than the skin surface. To this end, the self-organization mapping is successively used twice to unify the input point cloud and extract the key points, respectively. Afterward, the minimal spanning tree is employed to generate a tree graph to connect all extracted key points. To appropriately characterize the rib cartilage outline to match the source and target point cloud, the path extracted from the tree graph is optimized by maximally maintaining continuity throughout each rib. To validate the proposed approach, we manually extract the US cartilage point cloud from one volunteer and seven CT cartilage point clouds from different patients. The results demonstrate that the proposed graph-based registration is more effective and robust in adapting to the inter-patient variations than the ICP (distance error meanÂ±SD: 5.0Â±1.9mm vs 8.6Â±6.7mm on seven CTs)."
Automated Image Acquisition of Parasternal Long-Axis View with Robotic Echocardiography,"Yuuki Shida, Souto Kumagai, Ryosuke Tsumura, Hiroyasu Iwata","Waseda University,National Institute of Advanced Industrial Science and Technology",Medical Robots I,"This study proposes a method for finding a parasternal long-axis view in echocardiography autonomously with a robotic ultrasound (US) system. In obtaining this view, it is necessary to avoid the ribs and lungs because they reduce the clarity of US image. Meanwhile, the anatomical position and size of the heart, lungs, and ribs differ between individuals, which makes it difficult to find the optimal position of the US probe. Our proposed system is comprised of the following three processes. First, an exhaustive scan of the chest wall region is performed. The position of the probe that allows the mitral valve to be centrally positioned is estimated based on this scan. Second, the probe is rotated once in the yaw direction while being fixed in that position. The yaw angle is estimated at a point parallel to the left ventricular longitudinal axis in the acquired images. Finally, the pitch angle of the probe is estimated so that the probe avoids the connection between the mitral valve and the papillary muscle and chordae. To validate the proposed method, we performed human trials with five healthy subjects and measured the detection rate of observation points used to evaluate the image quality of parasternal long-axis view. The result showed that the median detection rate of the observation points was 63.3 Â± 5.3%, which implies that the proposed method is valid."
Sim-Suction: Learning a Suction Grasp Policy for Cluttered Environments Using a Synthetic Benchmark,"Juncheng Li, Dave Cappelleri",Purdue University,Mobile Manipulation,"This paper presents Sim-Suction, a robust suction grasp policy for mobile manipulation platforms with dynamic camera viewpoints, designed to pick up unknown objects from cluttered environments. We address the lack of large-scale, accurately-annotated suction grasp datasets by proposing a benchmark synthetic dataset, Sim-Suction-Dataset. It comprises 500 cluttered environments with 3.2 million annotated suction grasp poses. The dataset generation process combines analytical models and dynamic physical simulations to create fast and accurate suction grasp pose annotations. We introduce Sim-Suction-Pointnet to generate robust 6D suction grasp poses by learning point-wise affordances from the Sim-Suction-Dataset, leveraging the synergy of zero-shot text-to-segmentation. Real-world experiments for picking up all objects demonstrate that Sim-Suction-Pointnet achieves success rates of 96.76%, 94.23%, and 92.39% on cluttered level 1 objects, cluttered level 2 objects, and cluttered mixed objects, respectively. The Sim-Suction policies outperform state-of-the-art benchmarks tested by approximately 21% in cluttered mixed scenes."
Robot Task Planning under Local Observability,"Max Merlin, Shane Parr, Neev Parikh, Sergio Orozco, Vedant Gupta, Eric Rosen, George Konidaris","Brown University,University of Massachusetts Amherst",Mobile Manipulation,"Real-world robot task planning is intractable in part due to partial observability. A common approach to reducing complexity is introducing additional structure into the decision process, such as mixed-observability, factored states, or temporally-extended actions. We propose the locally observable Markov decision process, a novel formulation that models task-level planning where uncertainty pertains to object- level attributes and where a robot has subroutines for seeking and accurately observing objects. This models sensors that are range-limited and line-of-sightâ€”objects occluded or outside sensor range are unobserved, but the attributes of objects that fall within sensor view can be resolved via repeated observation. Our model results in a three-stage planning process: first, the robot plans using only observed objects; if that fails, it generates a target object that, if observed, could result in a feasible plan; finally, it attempts to locate and observe the target, replanning after each newly observed object. By combining LOMDPs with off-the-shelf Markov planners, we outperform state-of-the-art-solvers for both object-oriented POMDP and MDP analogues with the same task specification. We then apply the formulation to successfully solve a task on a mobile robot."
Real-Time Whole-Body Motion Planning for Mobile Manipulators Using Environment-Adaptive Search and Spatial-Temporal Optimization,"Chengkai Wu, Ruilin Wang, Mianzhi Song, Fei Gao, Jie Mei, Boyu Zhou","Harbin Institute of Technology, Shenzhen,Sun Yat-sen University,SUN YAT-SEN UNIVERSITY,Zhejiang University,City University of Hong Kong",Mobile Manipulation,"Mobile manipulators have recently gained significant attention in the robotics community due to their superior potential in industrial and service applications. However, the high degree of freedom associated with mobile manipulators poses challenges in achieving real-time whole-body motion planning. To bridge the gap, this paper presents a motion planning method capable of generating high-quality, safe, agile and feasible trajectories for mobile manipulators in real time. First, we present a novel environment-adaptive path searching method, which can generate paths in real-time in various environments by adaptively adjusting searching dimension based on environment complexity. Additionally, we propose a real-time spatial-temporal trajectory optimization method that takes into account the whole-body safety, agility and dynamic feasibility of mobile manipulators. Moreover, task constraints are applied to ensure that the trajectory can fulfill specific task requirements. Simulation and real-world experiments demonstrate that our method is capable of generating whole-body trajectories in real-time in challenging environments. We will release our code to benefit the community."
Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation,"Fabian Schmalstieg, Daniel Honerkamp, Tim Welschehold, Abhinav Valada","University of Freiburg,Albert Ludwigs Universität Freiburg,Albert-Ludwigs-Universität Freiburg",Mobile Manipulation,"Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real world that demonstrate that, with accurate perception, the decision making of HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subpolicies, failures in their execution, and different robot kinematics. These capabilities open the door to a wide range of downstream tasks across embodied AI and real-world use cases."
Keep It Upright: Model Predictive Control for Nonprehensile Object Transportation with Obstacle Avoidance on a Mobile Manipulator,"Adam Heins, Angela P. Schoellig","University of Toronto,TU Munich",Mobile Manipulation,"We consider a nonprehensile manipulation task in which a mobile manipulator must balance objects on its end effector without grasping them---known as the waiter's problem---and move to a desired location while avoiding static and dynamic obstacles. In contrast to existing approaches, our focus is on fast online planning in response to new and changing environments. Our main contribution is a whole-body constrained model predictive controller (MPC) for a mobile manipulator that balances objects and avoids collisions. Furthermore, we propose planning using the minimum statically-feasible friction coefficients, which provides robustness to frictional uncertainty and other force disturbances while also substantially reducing the compute time required to update the MPC policy. Simulations and hardware experiments on a velocity-controlled mobile manipulator with up to seven balanced objects, stacked objects, and various obstacles show that our approach can handle a variety of conditions that have not been previously demonstrated, with end effector speeds and accelerations up to 2.0 m/s and 7.9 m/s^2, respectively. Notably, we demonstrate a projectile avoidance task in which the robot avoids a thrown ball while balancing a tall bottle."
Gaussian Mixture Likelihood-Based Adaptive MPC for Interactive Mobile Manipulators,"Dimitrios Rakovitis, Dennis Mronga","DFKI,University of Bremen, German Research Center for Artificial Inte",Mobile Manipulation,"Mobile robots are nowadays frequently used for interaction tasks in the real world, e.g. for opening doors or for pick-and-place tasks. When used in real-world environments, adapting the robot controllers to uncertain contact dynamics is a significant challenge. Adaptive Model Predictive Control (AMPC) is an approach for controlling robot motions while adapting to uncertain or changing dynamics. However, most of the existing AMPC approaches used in mobile manipulation require either expert tuning or extensive training, making it very difficult to introduce novel or diverse tasks. In addition, the adjustment of several, independent environment parameters is usually not considered in the AMPC formulation. In this work, we introduce a hierarchical approach that uses Gaussian Mixture Models (GMMs) and Gaussian Mixture Regression (GMR) to predict the dynamic model parameters of MPC based on proprioceptive measurements and perform tasks with multiple unknown environmental parameters. The approach is evaluated in simulation and in real experiments on a mobile manipulator and compared to several baseline methods. It is shown that it outperforms standard MPC and an existing AMPC approach on several tasks such as carrying, pushing, and door opening."
GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning Based on Online Grasping Pose Fusion,"Jiazhao Zhang, Gireesh Nandiraju, Jaylon Wang, Xiaomeng Fang, Chaoyi Xu, Weiguang Chen, Liu Dai, He Wang","Peking University,IIIT Hyderabad,University of California Santa Cruz,Beijing Academy of Artificial Intelligence,BAAI,Beijing University of Posts and Telecommunications,Tongji University",Mobile Manipulation,"Mobile manipulation constitutes a fundamental task for robotic assistants and garners significant attention within the robotics community. A critical challenge inherent in mobile manipulation is the effective observation of the target while approaching it for grasping. In this work, we propose a graspability-aware mobile manipulation approach powered by an online grasping pose fusion framework that enables a temporally consistent grasping observation. Specifically, the predicted grasping poses are online organized to eliminate the redundant, outlier grasping poses, which can be encoded as a grasping pose observation state for reinforcement learning. Moreover, on-the-fly fusing the grasping poses enables a direct assessment of graspability, encompassing both the quantity and quality of grasping poses. This assessment can subsequently serve as an observe-to-grasp reward, motivating the agent to prioritize actions that yield detailed observations while approaching the target object for grasping. Through extensive experiments conducted on the Habitat and Isaac Gym simulators, we find that our method attains a good balance between observation and manipulation, yielding high performance under various grasping metrics. Furthermore, we discover that the incorporation of temporal information from grasping poses aids in mitigating the sim-to-real gap, leading to robust performance in challenging real-world experiments."
Dynamic Interaction Control in Legged Mobile Manipulators: A Decoupled Approach,"Qikai Li, Qinchen Meng, Yuxing Qin, Jiawei Chen, Xilun Ding, Kun Xu","Beihang University,Beihang university,Beijing Univerisity of Aeronautics and Astronautics,Beijing University",Mobile Manipulation,"Legged mobile manipulators are receiving much more attention. Mobile platforms can infinitely expand the workspace of robotic arms, providing more possibilities for robot application scenarios. Compared with wheeled mobile manipulators, legged mobile manipulators have higher requirements for cooperative control of legged robots and robotic arms. This work decouples the control of the robotic arm and the legged robot. On the legged robot side, we explicitly estimate the wrench exerted by the robotic arm on the base and bring it into the legged robot's dynamics, and then use a nonlinear model predictive controller (NMPC) to control the legged robot. On the robotics arm side, we adopt an impedance controller to realize the end-effector's force control, and the introduction of impedance control has improved the safety and interactivity of legged mobile manipulators. We conducted experiments on physical robot to compare the differences between decoupled control and independent control, and the results show that the stability and robustness of robot systems have improved using decoupled control."
Active-Perceptive Motion Generation for Mobile Manipulation,"Snehal Jauhri, Sophie C. Lueth, Georgia Chalvatzaki","TU Darmstadt,Technical University of Darmstadt, Stanford University,Technische Universität Darmstadt",Mobile Manipulation,"Mobile Manipulation (MoMa) systems incorporate the benefits of mobility and dexterity, due to the enlarged space in which they can move and interact with their environment. However, even when equipped with onboard sensors, e.g., an embodied camera, extracting task-relevant visual information in unstructured and cluttered environments, such as households, remains challenging. In this work, we introduce an active perception pipeline for mobile manipulators to generate motions that are informative toward manipulation tasks, such as grasping in unknown, cluttered scenes. Our proposed approach, ActPerMoMa, generates robot paths in a receding horizon fashion by sampling paths and computing path-wise utilities. These utilities trade-off maximizing the visual Information Gain (IG) for scene reconstruction and the task-oriented objective, e.g., grasp success, by maximizing grasp reachability. We show the efficacy of our method in simulated experiments with a dual-arm TIAGo++ MoMa robot performing mobile grasping in cluttered scenes with obstacles. We empirically analyze the contribution of various utilities and parameters, and compare against representative baselines both with and without active perception objectives. Finally, we demonstrate the transfer of our mobile grasping strategy to the real world, indicating a promising direction for active-perceptive MoMa."
Visual CPG-RL: Learning Central Pattern Generators for Visually-Guided Quadruped Locomotion,"Guillaume Bellegarda, Milad Shafiee, Auke Ijspeert",EPFL,Bioinspired Locomotion,"We present a framework for learning visually-guided quadruped locomotion by integrating exteroceptive sensing and central pattern generators (CPGs), i.e. systems of coupled oscillators, into the deep reinforcement learning (DRL) framework. Through both exteroceptive and proprioceptive sensing, the agent learns to coordinate rhythmic behavior among different oscillators to track velocity commands, while at the same time override these commands to avoid collisions with the environment. We investigate several open robotics and neuroscience questions: 1) What is the role of explicit interoscillator couplings between oscillators, and can such coupling improve sim-to-real transfer for navigation robustness? 2) What are the effects of using a memory-enabled vs. a memory-free policy network with respect to robustness, energy-efficiency, and tracking performance in sim-to-real navigation tasks? 3) How do animals manage to tolerate high sensorimotor delays, yet still produce smooth and robust gaits? To answer these questions, we train our perceptive locomotion policies in simulation and perform sim-to-real transfers to the Unitree Go1 quadruped, where we observe robust navigation in a variety of scenarios. Our results show that the CPG, explicit interoscillator couplings, and memory-enabled policy representations are all beneficial for energy efficiency, robustness to noise and sensory delays of 90 ms, and tracking performance for successful sim-to-real transfer for navigation tasks."
Form Closure for Fully Actuated and Robust Obstacle-Aided Locomotion in Snake Robots,"Jostein Løwer, Irja Gravdahl, Damiano Varagnolo, Øyvind Stavdahl","Norwegian University of Science and Technology,Norwegian University of Science and Technology (NTNU)",Bioinspired Locomotion,"In this paper we adapt the theory of form closure to define the form closed region, i.e., the subset of a snake robot's configuration space for which the constraints imposed by the obstacles in its environment render the system fully actuated. We show that the identification of form closed configurations is numerically feasible, and introduce the relaxed condition of form boundedness to achieve robustness in the presence of model uncertainties. We moreover show an example application where the concept of form closed region is used to produce predictable constrained motion in a cluttered environment using lateral undulation."
Self-Righting Shell for Robotic Hexapod,"Katelyn King, Shai Revzen",University of Michigan,Bioinspired Locomotion,"Decimeter scale robots in human environments are small relative to obstacles they encounter, making them prone to flipping over and needing to self-right. We present a multi-faceted shell that by its geometry alone enables the hexapedal robot MediumANT to passively self-right without the need for additional sensory feedback. We designed the shell by specifying the cross-sectional geometry in the yz and xy planes such that the robot returns to an upright position by rolling around the longitudinal (x) axis, and then tweaked the design to reduce the number of faces. We then attached the shell to the robot by modifying some of its chassis structural plates to extend to and support the shell. We evaluated the effectiveness of the shell in two experimental scenarios: passive righting â€“ balancing the robot on each face of the shell before releasing the robot â€“ and an intentional fall â€“ walking the robot off a ledge at various approach angles. As intended by our design, the robot recovered the upright orientation from all starting faces in the passive righting test and righted itself and continued walking in all falling trials. This work presents an example of using biologically inspired simplicity to solve what would otherwise be a technically challenging problem."
Quadruped-Frog: Rapid Online Optimization of Continuous Quadruped Jumping,"Guillaume Bellegarda, Milad Shafiee, Merih Ekin Özberk, Auke Ijspeert","EPFL,École Polytechnique Fédérale De Lausanne",Bioinspired Locomotion,"Legged robots are becoming increasingly agile in exhibiting dynamic behaviors such as running and jumping. Usually, such behaviors are either optimized and engineered offline (i.e. the behavior is designed for before it is needed), either through model-based trajectory optimization, or through deep learning-based methods involving millions of timesteps of simulation interactions. Notably, such offline-designed locomotion controllers cannot perfectly model the true dynamics of the system, such as the motor dynamics. In contrast, in this paper, we consider a quadruped jumping task that we rapidly optimize online. We design foot force profiles parameterized by only a few parameters which we optimize for directly on hardware with Bayesian Optimization. The force profiles are tracked at the joint level, and added to Cartesian PD impedance control and Virtual Model Control to stabilize the jumping motions. After optimization, which takes only a handful of jumps, we show that this control architecture is capable of diverse and omnidirectional jumps including forward, lateral, and twist (turning) jumps, even on uneven terrain, enabling the Unitree Go1 quadruped to jump 0.5 m high, 0.5 m forward, and jump-turn over 2 rad."
AeroDima: Cheetah-Inspired Aerodynamic Tail Design for Rapid Maneuverability,"Daryn Bright, Stacey Leigh Shield, Amir Patel",University of Cape Town,Bioinspired Locomotion,"Scientists have long theorized that the cheetahâ€™s tail contributes to its impressive maneuvrability at high speeds by stabilizing its body. This has inspired the design of several agile robots, including Dima - a wheeled platform that used cheetah-inspired inertial tail swings to better execute rapid acceleration and turning motions. Subsequent research suggests that the effectiveness of the cheetahâ€™s tail might be enhanced by aerodynamic effects. In this paper, we introduce AeroDima: a follow-up to the original Dima design that uses aerodynamic drag on the tail as the primary mechanism for generating the stabilizing torque. The resulting sail-like tail is substantially lighter than the original, but still improves the performance of the platform, allowing it to enter turns at a higher speed without toppling. While the yaw rate of the robot was actually higher without the tail, the tail substantially reduced unwanted roll, confirming that this appendage increases maneuvrability by increasing stability, rather than by directly contributing to lateral acceleration."
Spined Torso Renders Advanced Mobility for Quadrupedal Locomotion,"Jichao Wang, Jinyu Cheng, Jiangtao Hu, Wei Gao, Shiwu Zhang",University of Science and Technology of China,Bioinspired Locomotion,"Animals possessing spinal columns often exhibit exceptional agility for highly dynamic locomotion. The spine grants the trunk with increased degrees of freedom, thereby endowing diverse postures. This paper presents the development of a robot STRAY for quadrupedal locomotion, featuring a four-degree-of-freedom spine design. Using trajectory based reinforcement learning techniques, STRAY is able to trot and bound dynamically using its spine. Simulation results reveal the positive roles of spinal movement, such as twisting, extension, retraction and rotation, in helping STRAY realize efficient locomotion. Preliminary results from experiments demonstrate that STRAY can achieve a trotting gait of approximately 0.6 m/s and a bounding gait of 0.7 m/s, with desired velocities of 0.8 m/s and 1.0 m/s, respectively. The results also indicate that reinforcement learning is a feasible way to investigate how the spine should be used in dynamic quadrupedal locomotion and achieve more possibilities in the future."
Pegasus: A Novel Bio-Inspired Quadruped Robot with Underactuated Wheeled-Legged Mechanism,"Yuzhen Pan, Rezwan Al Islam Khan, Chenyun Zhang, Zhang Anzheng, Huiliang Shang","Fudan University,Fudan university",Bioinspired Locomotion,"This paper presents the design and analysis of Pegasus, a quadrupedal wheeled robot grounded in biomimicry principles. Pegasus offers two distinct motion modes, including a wheeled motion and a hybrid wheeled-legged motion, enabling adaptability across various tasks and environmental conditions. The robot draws inspiration from the joint structures of quadruped animals and incorporates biomimetic features. At the robot's ankle joint, we imitate the articulation of a radius-ulna joint to enhance the wheeled motion's agility. Additionally, we establish comprehensive mathematical models for adaptive dynamics model, providing a robust theoretical foundation for subsequent motion planning and high-precision control. A novel telescopic vehicle mode is also proposed for complex wheel-leg hybrid motion, offering optimized solutions for intricate robot locomotion. Furthermore, we employ parallel underactuated MPC controllers for each leg at the control level, contributing to heightened motion precision and stability. Extensive validation through physical platform experiments highlights the effectiveness and feasibility of the proposed controllers, offering substantial support for real-world applications in robotics."
LeapRun: A Dynamic Soft Robot with Running and Jumping Capabilities,"Jiangfeng Lu, Jiaming Liang, Dekuan Zhu, Dongkai Wang, Ying Liu, Huimin Chen, Yunfei Bai, Haolong Zhang, Min Zhang","Tsinghua University,Tencent,Tsinghua University, Tsinghua Shenzhen International Graduate Sc,Tsinghua University, Shenzhen International Graduate School",Bioinspired Locomotion,"In the natural world, insects exhibit remarkable locomotion capabilities through a combination of running and jumping. However, replicating this versatile locomotion in a soft robot poses technical and design complexities. Here, we propose a dynamic soft robot named LeapRun that possesses agile locomotion and the ability to perform continuous jumping. To achieve this, a prototype soft robot (weight of 300 mg, size of 30 mm Ã— 15 mm Ã— 5 mm), composed of piezoelectric thin film, shape memory alloy, magnet-locking mechanism, and corresponding support structures, is fabricated. Experimental results demonstrate a maximum moving speed of 15 cm/s and a maximum jumping height of 8.7 cm. Continuous jumping of steps and crossing of complex rugged surfaces is realized. Besides, integrated with the power source, wireless communication module, and control module, the untethered operation is also presented, showcasing the potential for multiple applications in search and rescue, exploration, and monitoring."
Machine Learning-Driven Burrowing with a Snake-Like Robot,"Sean Even, Holden Gordon, Hoeseok Yang, Yasemin Ozkan-Aydin","University of Notre Dame,Santa Clara University",Bioinspired Locomotion,"Subterranean burrowing is inherently difficult for robots because of the high forces experienced as well as the high amount of uncertainty in this domain. Because of the difficulty in modeling forces in granular media, we propose the use of a novel machine-learning control strategy to obtain optimal techniques for vertical self-burrowing. In this paper, we realize a snake-like bio-inspired robot that is equipped with an IMU and two triple-axis magnetometers. Utilizing magnetic field strength as an analog for depth, a novel deep learning architecture was proposed based on sinusoidal and random data in order to obtain a more efficient strategy for vertical self-burrowing. This strategy was able to outperform many other standard burrowing techniques and was able to automatically reach targeted burrowing depths. We hope these results will serve as a proof of concept for how optimization can be used to unlock the secrets of navigating in the subterranean world more efficiently."
Towards Centimeter-Scale Underwater Mobile Robots: An Architecture for Capable ÂµAUVs,"Pascal Spino, Daniela Rus","Massachusetts Institute of Technology,MIT",Marine Robotics I,"Underwater robots are indispensable for aquatic exploration, yet their size and complexity often limit broader application. This research presents a pioneering micro autonomous underwater vehicle (ÂµAUV) design. This robot is distinguished by its utilization of mass-produced drone components, novel jet propulsion mechanisms, and multifunctional spherical shell. Its architecture is modular, appendage-free, and largely seal-free. Preliminary tests highlight its motion capabilities and set new benchmarks for centimeter-scale ÂµAUV advancements."
Untethered Bimodal Robotic Fish with Tunable Bistability,"Xu Chao, Imran Hameed, D. Navarro-Alarcon, Xingjian Jing","The Hong Kong Polytechnic University,City University of Hong Kong",Marine Robotics I,"In nature, fish are excellent swimmers due to their flexible and precise control of tail, which allows them to freely transform between the smooth flapping and the motion of rapid response so that they can move with dexterity. Here, inspired by the versatile motion abilities of fish, a novel robotic fish has been developed, featuring the capability of adaptable bistability. Through tuning the bistability, the robot can acquire two locomotion modes, namely monostable and bistable modes, and it can also swim at different energy barrier that needs to be overcome to realize the bistable motion. The theoretical models are derived to facilitate the control of the robot and the understanding of its nonlinear behavior. The impact of the tunable bistability on the swimming and turning performance is investigated through extensive experiments. The study effectively demonstrates the robotic fishâ€™s capability to swiftly and efficiently navigate through mode switches, enabled by its tunable bistability. This feature is essential for underwater robots to perform tasks in intricate environments."
Tendon-Driven Continuum Robot for Deep-Sea Application,"Cora Maria Sourkounis, Tom Kwasnitschka, Annika Raatz","Leibniz University Hannover,GEOMAR Helmholtz Centre for Ocean Research Kiel,Leibniz Universität Hannover",Marine Robotics I,"The extreme conditions of the deep sea require the use of large and expensive diving robots designed to withstand the high pressure in these depths. In order to reduce the costs for sediment sampling in the deep sea and thus facilitate the explorations of rare deep-sea ecosystems, the goal of this research is to design an alternative manipulator for deepsea suction sampling. Instead of relying on heavy hydraulic rigid manipulators that deep-sea diving robots are commonly equipped with, we introduce a new concept for a lightweight actuation system that can be used in combination with a traditional diving robot and a suction sampling system. The proposed concept consists of a series of rigid links connected by angled swivel joints. Each segment is actuated by tendons, which allows for continuous bending. The system can be adapted to various sizes of host systems, and the links and joints are printed in place, simplifying the manufacturing process."
WAVE: An Open-Source underWater Arm-Vehicle Emulator,"Marcus Rosette, Hannah Kolano, Chris Holm, Geoffrey Hollinger, Aaron Marburg, Madison Pickett, Joseph Davidson","Oregon State University,University of Washington",Marine Robotics I,"Underwater vehicle manipulator systems (UVMS) are increasingly popular platforms for performing subsea operations that require precision manipulation. While there is high demand for fully autonomous or even semi-autonomous systems, most UVMS still require human support teams. Developing new hardware and algorithms for autonomous underwater manipulation is challenging. Simulations do not capture the full complexity of the underwater environment, and deploying a UVMS at sea for testing/validation is resource intensive and expensive. In this paper, we present a physical testbed for underwater manipulation that bridges the gap between simulation and full field trials. The underWater ArmVehicle Emulator (WAVE) is a 10-degree of freedom system designed to replicate an inspection-class UVMS. WAVE includes an underwater perception sensor and has 2 operating modes: rigid or passive-mode. In passive-mode, the ROV body can pitch similar to how a dynamically-coupled underactuated UVMS without pitch control would rotate during manipulation tasks. To validate the overall design and passive pitch concept, we evaluated the testbed during underwater experiments in energetic conditions at a wave basin. To support continued research and development in underwater robotics, we make the design open-access and freely available to the community."
Nezha-F: Design and Analysis of a Foldable and Self-Deployable HAUV,"Yulin Bai, Yufei Jin, Chunhu Liu, Zheng Zeng, Lian Lian","Shanghai Jiao Tong University,Shanghai Jiaotong University",Marine Robotics I,"This paper introduces a small hybrid aerial underwater vehicle (HAUV), which we named Nezha-F, that can fly in the air, perform vertical profiling underwater, and vertically take off and land from both the water surface and ground. A foldable and self-deployable arm mechanism linked to and driven by a piston variable buoyancy system (PVBS) is proposed to reduce the excessive underwater drag caused by aerial structures. By having a compact size and successfully balanced aerial and underwater performance without adding excessive actuators, this design provides a feasible idea for the miniaturization of amphibious floats. The dynamic characteristics of the small PVBS are linear fitted, and modeled. The originally nonlinear actuator performance is linearized by the post-fitting mapping. Asymmetric dead zones of the actuator are removed by adding compensation to the algorithm. During a 10-day field test, the vehicle showed good aerial performance and underwater control effect. Several full mission cycle tests proved the vehicleâ€™s ability in semi-autonomous operation and robust domain crossing and verified the vehicleâ€™s endurance during each mission stage."
Snapp: An Agile Robotic Fish with 3-D Maneuverability for Open Water Swim,"Timothy Ju Kin Ng, Nan Chen, Fu Zhang","The University of Hong Kong,University of Hong Kong",Marine Robotics I,"Fish exhibit impressive locomotive performance and agility in complex underwater environments, using their undulating tails and pectoral fins for propulsion and maneuverability. Replicating these abilities in robotic fish is challenging; existing designs focus on either fast swimming or directional control at limited speeds, mainly within a confined environment. To address these limitations, we designed Snapp, an integrated robotic fish capable of swimming in open water with high speeds and full 3-dimensional maneuverability. A novel cyclic-differential method is layered on the mechanism. It integrates propulsion and yawsteering for fast course corrections. Two independent pectoral fins provide pitch and roll control. We evaluated Snapp in open water environments and demonstrated significant improvements in speed and maneuverability, achieving swimming speeds of 1.5 m/s (1.7 body lengths per second) and performing complexmaneuvers, such as a figure-8 and S-shape trajectory. Instantaneous yaw changes of 15â—¦ in 0.4 s, a minimum turn radius of 0.85 m, and maximum pitch and roll rates of 3.5 and 1 rd/s, respectively, were recorded. Our results suggest that Snappâ€™s swimming capabilities have excellent practical prospects for open seas and contribute significantly to developing agile robotic fishes."
A Novel Omnidirectional Swimming Robot with Articulated-Compliant Legs,"Yaohui Xu, Hanlin Li, Furui Yu, Qiyang Zuo, Fengran Xie, Xiang Xie, Kai He","Shenzhen Institute of Advanced Technology, Chinese Academy of Sc,Yanshan Universityand Shenzhen Institute of Advanced Technology,Shaanxi University of Science & Technology, Shenzhen Institute o,Shenzhen Polytechnic",Marine Robotics I,"Stability, adaptability and maneuverability are the most important performance indexes for the underwater biomimetic robot, especially when it comes to the narrow space operation. However, these aspects are contradictory sometimes. In this paper, we present an omnidirectional swimming robot inspired by the whirligig beetle, with the design goals of good stability, adaptability, and maneuverability. First, the design of the robot, which is featured with four novel articulated-compliant robotic legs, is given. Second, its hydrodynamic model is formulated by using Kirchhoffâ€™s equation as well as Lagrangian method, and the hydrodynamic force is calculated by the quasisteady flow model. Third, extensive experiments are carried out to examine its thrust generation and speed. It is found that the omnidirectional robot has a significant improvement compared with the conventional one with single passive joint in the leg. More specifically, its swimming speed is fast as 0.34 m/s at the frequency of 1.4 Hz, showing a 30.8% increase. Finally, multimodal swimming of the robot is demonstrated by configuring various locomotive pattern of the articulatedcompliant legs, such as swimming forward, retreating, lateral swimming to the left or right, zero-radius turning, no zeroradius turning. The passing and collision experiments present the robotâ€™s potential applications in narrow spaces. Overall, this omnidirectional swimming robot shows a great balance among stability, adaptability and maneuve"
Marine Sediment Sampling with an Underwater Legged Robot,"Anna Astolfi, Mrudul Chellapurath, Giacomo Picardi, Martina Capriotti, Kayla Mladinich, Cecilia Laschi, Sergio Stefanni, Marcello Calisti","Scuola Superiore Sant'Anna,Instituto de Ciencias del Mar (ICM)—Consejo Superior de Investig,University of Camerino,University of Connecticut,National University of Singapore,Stazione Zoologica Anton Dohrn,The University of Lincoln",Marine Robotics I,"We present a novel approach to marine sediment sampling, which makes use of a hexapedal robotic platform, namely SILVER2, provided with a sediment sampling system. This approach tackles the disadvantages of state-of-the-art methods for sediment sampling in terms of increased station-keeping capabilities, low disturbance of the substrate, and precise position control. The sediment sampling system has been designed according to user requirements for microplastics (MPs) analysis from sampled sediment, which include the sampling depth, the sampled volume per area, and the possibility to collect replicas without returning to the boat or to the shore. We also defined a protocol for sediment collection and extensively tested the system both in a tank and in field experiments in different spots along the Tyrrhenian coast, in Italy. Sediments collected throughout the tests have been analyzed, extracting information about the quantity and composition of MPs, in order to provide an overview of the complete procedure. This work represents an important step towards the use of legged robots in marine operations, and contributes to highlight the importance of multidisciplinary collaborations among roboticists and scientists to develop novel solutions and increase the sampling capabilities of end-users."
Terrain-Adaptive Locomotion Control for an Underwater Hexapod Robot: Sensing Leg-Terrain Interaction with Proprioceptive Sensors,"Lepeng Chen, Rongxin Cui, Weisheng Yan, Hui Xu, Shouxu Zhang, Haitao Yu",Northwestern Polytechnical University,Marine Robotics I,"Underwater hexapod robot, driven by six C-shaped legs and eight thrusters, has the potential to traverse diverse terrains with unknown deformable properties, which can lead to unknown leg-terrain interaction forces. However, using exteroceptive sensors such as cameras and sonars to recognize underwater terrain's deformable properties is hard. Here, we propose a method to perceive the interaction forces and feed them into a controller for determining thrust inputs. The key idea lies in using supervised learning to obtain the properties from reliable proprioceptive sensory data. First, we propose a new expression called Zero Moment Point (ZMP) bias that can indirectly represent the leg-terrain interaction force, removing the effects caused by gravity, buoyancy, and thrust. Second, we gather a walking cycle's discrete ZMP biases and then parameterize them as polynomials. Then, we use several previous walking cycles' parameterized biases to predict the current walking cycle's biases to generate the needed pitch and roll moments. Finally, we propose a terrain-adaptive locomotion control for the robot, which uses thrust to compensate for the interaction force."
Sequential Trajectory Optimization for Externally-Actuated Modular Manipulators with Joint Locking,"Jaeu Choe, Jeongseob Lee, Hyunsoo Yang, Hai-nguyen Nguyen, Dongjun Lee","Seoul National University,seoul national university,CNRS",Mechanics and Control I,"In this paper, we present a novel trajectory planning method for externally-actuated modular manipulators (EAMMs), consisting of multiple rotor-actuated links with joints that can be either locked or unlocked. This joint-locking feature allows effective balancing of the payload capacity and dexterity of the robot but significantly complicates the planning problem by introducing binary decision variables. To address this challenge, we leverage the problem's intrinsic structure, i.e., the payload at the end-effector being enhanced by merely locking its immediate connected links; this allows us to break down the complex planning problem into a series of manageable subproblems and solve them sequentially. Our approach significantly reduces the problem's complexity: in a serial n-link EAMM with m joint-lock mechanisms, where there could potentially be 2^m distinct configurational dynamics, we require solving only n+1 trajectory optimization problems for single rigid body dynamics sequentially, thereby rendering the problem tractable. We substantiate the efficacy of our method through various simulation and experimental studies, covering ground-free and ground-bound configurations as well as both motion-only and manipulation tasks."
Rapid Resistography with Passive Overhead-Perching Mechanism in an Unmanned Aerial System for Wood Structure Inspection,"Shawndy Michael Lee, Jingmin Liu, Jer Luen Chien, Wei Hien Ng, Milven Lim, Shaohui Foong","SINGAPORE UNIVERSITY OF TECHNOLOGY AND DESIGN,Singapore University of Technology & Design,Singapore University of Technology and Design",Mechanics and Control I,"This paper presents an aerial robotic platform for rapid remote elevated overhead-perching drill operations for wood health inspection. The platform features an innovative passive prismatic-gripper mechanism affixed to the aerial robotâ€™s top, facilitating overhead drilling. The primary aim is to enhance the safety and efficiency of elevated wood structure inspection using the resistography method, which involves drilling into wooden structures to identify internal voids. The research centers on two key enabling technologies: a gripper mechanism for secure attachment to target surfaces and a tethered drill configuration for drilling operations. The novel gripper mechanism enables drilling on large planar surfaces and even small beam-width structures. The paper concludes with discussions on design simulations and drill resistance experiments, highlighting the effectiveness of the proposed approach in detecting internal cavities within wooden structures."
Dual Quaternion Control of UAVs with Cable-Suspended Load,"Yuxia Yuan, Markus Ryll","Technical University of Munich,Technical University Munich",Mechanics and Control I,"Modeling the kinematics and dynamics of robotics systems with suspended loads using dual quaternions has not been explored so far. This paper introduces a new innovative control strategy using dual quaternions for UAVs with cable-suspended loads, focusing on the sling load lifting and tracking problems. By utilizing the mathematical efficiency and compactness of dual quaternions, a unified representation of the UAV and its suspended loadâ€™s dynamics and kinematics is achieved, facilitating the realization of load lifting and trajectory tracking. The simulation results have tested the proposed strategyâ€™s accuracy, efficiency, and robustness. This study makes a substantial contribution to present this novel control strategy that harnesses the benefits of dual quaternions for cargo UAVs. Our work also holds promise for inspiring future innovations in under-actuated systems control using dualquaternions."
"Design, Modeling and Control of a Top-Loading Fully-Actuated Cargo Transportation Multirotor","Wooyong Park, Xiangyu Wu, Dongjae Lee, Seung Jae Lee","Seoul National University of Science and Technology,University of California, Berkeley,Seoul National University",Mechanics and Control I,"Existing multirotor-based cargo transportation does not maintain a constant cargo attitude due to underactuation; however, fragile payloads may require a consistent posture. The conventional method is also cumbersome when loading cargo, and the size of the cargo to be loaded is limited. To overcome these issues, we propose a new fully-actuated multirotor unmanned aerial vehicle platform capable of translational motion while maintaining a constant attitude. Our newly developed platform has a cubic exterior and can freely place cargo at any point on the flat top surface. However, the center-of-mass (CoM) position changes when cargo is loaded, leading to undesired attitudinal motion due to unwanted torque generation. To address this problem, we introduce a new model-free center-of-mass position estimation method named as the MOCE (Model-free Online Center-of-mass Estimation) algorithm, which is inspired by the extremum-seeking control (ESC) technique. Experimental results are presented to validate the performance of the proposed estimation method, effectively estimating the CoM position and showing satisfactory constant-attitude flight performance."
Aerial Interaction with Tactile Sensing,"Xiaofeng Guo, Guanqi He, Mohammadreza Mousaei, Junyi Geng, Guanya Shi, Sebastian Scherer","Carnegie Mellon Univeristy,Carnegie Mellon University,Pennsylvania State University",Mechanics and Control I,"While the field of autonomous Uncrewed Aerial Vehicles (UAVs) has grown rapidly, most applications only focus on passive visual tasks. Aerial interaction aims to execute tasks involving physical interactions, which offers a way to assist humans in high-altitude and high-risk operations. Tactile sensors, being both cost-effective and lightweight, are capable of sensing contact information including force distribution, as well as recognizing local textures. In this paper, we pioneer the use of vision-based tactile sensors on fully actuated UAVs in dynamic aerial manipulation tasks. We introduce a pipeline utilizing tactile feedback for force tracking via a hybrid motion-force controller and a method for wall texture detection during aerial interactions. Our experiments demonstrate that our system can effectively replace or complement traditional force/torque (F/T) sensors. Compared with only using the F/T sensor, our approach offers two solutions: substitution with tactile sensing, achieving comparable flight performance, or integration of tactile sensing with F/T sensor feedback, leading to around 16% improvement in position tracking accuracy. Our algorithm achieves 93.4% accuracy in real-time texture recognition, which further escalates to 100% in post-contact analysis. To the best of our knowledge, this is the first work to incorporate a vision-based tactile sensor into aerial interaction tasks."
A Meter-Scale Ornithopter Capable of Jumping Take-Off,"Wei Yan, Genliang Chen, Zhuang Zhang, Hao Wang","Shanghai Jiaotong University, Shanghai, China,Shanghai Jiao Tong University,Westlake University",Mechanics and Control I,"Flapping wing air vehicles(FWAV) or ornithopters are bio-inspired aerial robots that mimic the flying principles of insects and birds. Autonomous take-off is an important capability for FWAV to enhance its performance and extend its working time, which is equipped by almost every kind of bird. As a common method of take-off for birds, jumping take-off has a great ability to adapt to different terrain and high energy efficiency compared with running and rotor-based take-off. Despite recent research, there is no FWAV capable of jumping take-off to this day. In this paper, we present a process to realize the jumping take-off of a meter-scale FWAV from flat ground. To lower the mechanical complexity, we eliminate the design of traditional robotic legs. Instead, we realize steady standing through a tripod-like structure that consists of two wings and a jumping mechanism. The flapping wing is directly driven by two independent servos. Three carbon fiber springs are employed to build a lightweight jumping module with high elastic energy. We build the dynamic model to analyze the aerodynamic effect during the jumping phase and realize a stable transition to flapping flight. This work lays the foundation for outdoor flight without human assistance."
Autonomous Aerial Perching and Unperching Using Omnidirectional Tiltrotor and Switching Controller,"Dongjae Lee, Sunwoo Hwang, Jeonghyun Byun, Seung Jae Lee, H. Jin Kim","Seoul National University,Seoul National University of Science and Technology",Mechanics and Control I,"Aerial unperching of multirotors has received little attention as opposed to perching that has been investigated to elongate operation time. This study presents a new aerial robot capable of both perching and unperching autonomously on/from a ferromagnetic surface during flight, and a switching controller to avoid rotor saturation and mitigate overshoot during transition between free-flight and perching. To enable stable perching and unperching maneuvers on/from a vertical surface, a lightweight (approximately 1 kg), fully actuated tiltrotor that can hover at 90-degree pitch angle is first developed. We design a perching/unperching module composed of a single servomotor and a magnet, which is then mounted on the tiltrotor. A switching controller including exclusive control modes for transitions between free-flight and perching is proposed. Lastly, we propose a simple yet effective strategy to ensure robust perching in the presence of measurement and control errors and avoid collisions with the perching site immediately after unperching. We validate the proposed framework in experiments where the tiltrotor successfully performs perching and unperching on/from a vertical surface during flight. We further show effectiveness of the proposed transition mode in the switching controller by ablation studies where large overshoot and even collision with a perching site occur. To the best of the authors' knowledge, this work presents the first autonomous aerial unperching framework using a fully actuated tiltrotor."
RotorTM: A Flexible Simulator for Aerial Transportation and Manipulation,"Guanrui Li, Liu Xinyang, Giuseppe Loianno",New York University,Mechanics and Control I,"Low-cost autonomous Micro Aerial Vehicles (MAVs) have great potential to help humans by simplifying and speeding up complex tasks, such as construction, package delivery, and search and rescue. These systems, which may consist of single or multiple vehicles, can be equipped with passive connection mechanisms such as rigid links or cables for transportation and manipulation tasks. However, these systems are inherently complex. They are often underactuated and evolve in nonlinear manifold configuration spaces. In addition, the complexity escalates for systems with cable-suspended load due to the hybrid dynamics that vary with the cables' tension conditions. This paper presents the first aerial transportation and manipulation simulator incorporating different payloads and passive connection mechanisms with full system dynamics, planning, and control algorithms. Furthermore, it includes a novel general model accounting for the transient hybrid dynamics for aerial systems with cable-suspended load to closely mimic real-world systems. Comparisons between simulations and real-world experiments with different vehicles' configurations show the fidelity of the simulator results with respect to real-world settings. The experiments also show the simulator's benefit for the rapid prototyping and transitioning of aerial transportation and manipulation systems to real-world deployment."
Simulation and Experimental Validation of an Autonomous Perching and Takeoff Method for a Multirotor UAV on Vertical Surfaces Using a Suction Cup,"Bruno Chapdelaine, Mathis Celce, Charles Vidal, Lionel Birglen, Bruno Monsarrat","National Research Council Canada,Polytechnique Montréal,Aerospace Research Centre, National Research Council Canada,Ecole Polytechnique de Montreal",Mechanics and Control I,This paper details the simulation and experimental validation of an autonomous perching and take-off method for a multirotor unmanned aerial vehicle (UAV) using a suction cup perching mechanism on vertical surfaces. The suction cup interaction with different surface types is characterized with experimental tests to accurately model the perching manoeuvre. The resulting model is used to develop a realistic hardware-in-the-loop (HIL) simulation of the perching and take-off manoeuvre of the UAV in Gazebo. A control method is developed to automate the perching and take-off manoeuvre. The method is tested in simulation and is experimentally validated with flight tests. Comparisons between simulation and experimental data demonstrate that the simulation is accurate and can be used to continue the development of autonomous perching methods.
Field-VIO: Stereo Visual-Inertial Odometry Based on Quantitative Windows in Agricultural Open Fields,"Jianjing Sun, Shuang Wu, Jun Dong, Junming He","Anhui University,Hefei Institutes of Physical Science, Chinese Academy of Science",Visual-Inertial SLAM,"In agricultural open fields, accurate autonomous localization of robots requires long-term data correlation to reduce cumulative error. Our article presents a Stereo Visual-Inertial Odometry (VIO) system based on ORB-SLAM3 to address the malfunction of the Loop Closure Detection (LCD) methods in this environment. In this method, we first propose a concept of quantitative windows to describe the robotâ€™s trajectory along the crop rows. We design a driving state quantification algorithm and accurately separate the quantitative windows between the crop rows. Our system constructs spatial constraints according to the parallelism between the quantitative windows. We apply an anomaly correction method to maintain the constructed parallel matching relationship and implement holistic pose correction for keyframes within abnormal quantitative windows. Our system demonstrated excellent performance over long distances in experiments on the Rosario dataset, verifying its effectiveness in reducing cumulative positioning error in agricultural open fields."
Online Calibration of a Single-Track Ground Vehicle Dynamics Model by Tight Fusion with Visual-Inertial Odometry,"Haolong Li, Joerg Stueckler",Max Planck Institute for Intelligent Systems,Visual-Inertial SLAM,"Wheeled mobile robots need the ability to estimate their motion and the effect of their control actions for navigation planning. In this paper, we present ST-VIO, a novel approach which tightly fuses a single-track dynamics model for wheeled ground vehicles with visual inertial odometry (VIO). Our method calibrates and adapts the dynamics model online to improve the accuracy of forward prediction conditioned on future control inputs. The single-track dynamics model approximates wheeled vehicle motion under specific control inputs on flat ground using ordinary differential equations. We use a singularity-free and differentiable variant of the single-track model to enable seamless integration as dynamics factor into VIO and to optimize the model parameters online together with the VIO state variables. We validate our method with real-world data in both indoor and outdoor environments with different terrain types and wheels. In experiments, we demonstrate that ST-VIO can not only adapt to wheel or ground changes and improve the accuracy of prediction under new control inputs, but can even improve tracking accuracy."
VI-HSO: Hybrid Sparse Monocular Visual-Inertial Odometry,"Wenzhe Yang, Yan Zhuang, Dongting Luo, Xuetao Zhang, Wei Wang, Hong Zhang","Dalian University of Technology,College of Control Science and Engineering, Dalian University of,SUSTech",Visual-Inertial SLAM,"In this letter, we present VI-HSO, a hybrid sparse monocular visual-inertial odometry system based on two innovative techniques called adaptive interframe alignment (AIA) and dynamic inverse distance filter (DIDF). Although the sparse image alignment algorithm appears efficient for calculating frame-to-frame motion, it tends to fail in case of significant intensity changes and motion blur. To overcome these limitations, we propose an adaptive interframe alignment method that allows for an adaptive selection between the original Lucas-Kanade (LK) method and the inverse compositional method when constructing photometric errors, along with the addition of inertial information in the process. This approach enables the tracking phase to utilize the full image and inertial information. During intense motion, the inverse distance of the new candidate point often fails to converge, leading to either scale drift or tracking failure. We present a dynamic inverse distance filter that can adjust the convergence range to update candidate points' inverse distance. This adjustment is based on the convergence ratio of the inverse distance of keyframes, which enables more convergent map points aiding in robust tracking in regions lacking texture and during rapid rotation. We evaluate the performance of VI-HSO on public datasets and real-world experiments, and our system outperforms state-of-the-art algorithms. The code is published at https://github.com/luodongting/VI-HSO."
Square-Root Inverse Filter-Based GNSS-Visual-Inertial Navigation,"Jun Hu, Xiaoming Lang, Feng Zhang, Yinian Mao, Guoquan Huang","Meituan Inc,Meituan,Meituan-Dianping Group,University of Delaware",Visual-Inertial SLAM,"While Global Navigation Satellite System (GNSS) is often used to provide global positioning if available, its intermittency and/or inaccuracy calls for fusion with other sensors. In this paper, we develop a novel GNSS-Visual-Inertial Navigation System (GVINS) that fuses visual, inertial, and raw GNSS measurements within the square-root inverse sliding window filtering (SRI-SWF) framework in a tightly coupled fashion, which thus is termed SRI-GVINS. In particular, for the first time, we deeply fuse the GNSS pseudorange, Doppler shift, single-differenced pseudorange, and double-differenced carrier phase measurements, along with the visual-inertial measurements. Inherited from the SRI-SWF, the proposed SRI-GVINS gains significant numerical stability and computational efficiency over the start-of-the-art methods. Additionally, we propose to use a filter to sequentially initialize the reference frame transformation till converges, rather than collecting measurements for batch optimization. We also perform online calibration of GNSS-IMU extrinsic parameters to mitigate the possible extrinsic parameter degradation. The proposed SRI-GVINS is extensively evaluated on our own collected UAV datasets and the results demonstrate that the proposed method is able to suppress VIO drift in real-time and also show the effectiveness of online GNSS-IMU extrinsic calibration. The experimental validation on the public datasets further reveals that the proposed SRI-GVINS outperforms the state-of-the-art methods in terms of both accuracy and efficiency."
Omnidirectional Dense SLAM for Back-To-Back Fisheye Cameras,"Weijian Xie, Guanyi Chu, Quanhao Qian, Yihao Yu, Shangjin Zhai, Danpeng Chen, Nan Wang, Hujun Bao, Guofeng Zhang","Zhejiang University, SenseTime Research,SenseTime,SenseTime Research,Zhejiang Sensetime Technology Development Co., Ltd,Sensetime Research,Zhejiang University, Sensetime Research and Tetras.AI,Sensetime,Zhejiang University",Visual-Inertial SLAM,"We propose a real-time visual-inertial dense SLAM system that utilizes the online data streams from back-to-back dual fisheye cameras setup, providing 360 degree coverage of the environment. Firstly, we employ a sliding-window-based front-end to estimate real-time poses from the binocular fisheye images and IMU data. Then, we implement a lightweight panoramic depth completion network based on multi-basis depth representation. The network takes panoramic images (obtained by stitching dual-fisheye images with extrinsic and intrinsic parameters) and sparse depths (generated by the front-end local tracking) as input and predicts multiple depth bases along with corresponding confidence as output. The final dense depth is the linear combination of the multiple depth bases. Thanks to the multi-basis depth representation, we can continuously optimize the 360 degree depth with the traditional optimizer to achieve higher global consistency in depth. We conducted experiments on both simulated and real-world datasets to evaluate our method. The results demonstrate that the proposed method outperforms SoTA methods in terms of depth prediction and 3D reconstruction. In addition, we develop a demo that can run on a mobile to demonstrate the real-time capabilities of our method."
Visual Inertial Odometry Using Focal Plane Binary Features (BIT-VIO),"Matthew Lisondra, Junseo Kim, Riku Murai, Kourosh Zareinia, Sajad Saeedi","Toronto Metropolitan University,Imperial College London,Ryerson University",Visual-Inertial SLAM,"Focal-Plane Sensor-Processor Arrays (FPSP)s are an emerging technology that can execute vision algorithms directly on the image sensor. Unlike conventional cameras, FPSPs perform computation on the image plane â€“ at individual pixels â€“ enabling high frame rate image processing while consuming low power, making them ideal for mobile robotics. FPSPs, such as the SCAMP-5, use parallel processing and are based on the Single Instruction Multiple Data (SIMD) paradigm. In this paper, we present BIT-VIO, the first Visual Inertial Odometry (VIO) which utilises SCAMP-5. BIT-VIO is a loosely-coupled iterated Extended Kalman Filter (iEKF) which fuses together the visual odometry running fast at 300 FPS with predictions from 400 Hz IMU measurements to provide accurate and smooth trajectories."
PL-EVIO: Robust Monocular Event-Based Visual Inertial Odometry with Point and Line Features,"Weipeng Guan, Peiyu Chen, Yuhan Xie, Peng Lu",The University of Hong Kong,Visual-Inertial SLAM,"Robust state estimation in challenge situations is still an unsolved problem, especially achieving onboard pose feedback control for aggressive motion. In this paper, we propose robust and real-time event-based visual-inertial odometry (VIO) that incorporates event, image, and inertial measurements. Our approach utilizes line-based event features to provide additional structure and constraint information in human-made scenes, while point-based event and image features complement each other through well-designed feature management. To achieve reliable state estimation, we tightly couple the point-based and line-based visual residuals from the event camera, the point-based visual residual from the standard camera, and the residual from IMU pre-integration using a keyframe-based graph optimization framework. Experiments in the public benchmark datasets show that our method can achieve superior performance compared with the state-of-the-art image-based or event-based VIO. Furthermore, we demonstrate the effectiveness of our pipeline through onboard closed-loop quadrotor aggressive flight and large-scale outdoor experiments. Videos of the evaluations can be found on our website: https://youtu.be/KnWZ4anBMK4."
JacobiGPU: GPU-Accelerated Numerical Differentiation for Loop Closure in Visual SLAM,"Dhruv Kumar, Shishir Gopinath, Karthik Dantu, Steve Ko","Simon Fraser University,University of Buffalo",Visual-Inertial SLAM,"In this paper, we introduce JacobiGPU, a technique that uses a GPU to improve the efficiency of loop closure in visual-inertial SLAM systems, particularly when approximating Jacobians using the Finite Difference Method (FDM). Traditional FDM techniques often face computational overhead due to repeated perturbations in pose graphs. We address this overhead with a novel methodology, leveraging strategic graph partitioning and an optimized approach to Jacobian approximation. By integrating JacobiGPU into ORB-SLAM3's g2o, we enhance the linearization process. Our evaluation, conducted on 12 sequences of varying lengths from the EuRoC and TUM-VI datasets, demonstrated a speedup of up to 4.23x in the linearization stage and an overall enhancement of up to 2.08x in the overall optimization process."
MAVIS: Multi-Camera Augmented Visual-Inertial SLAM Using SE2(3) Based Exact IMU Pre-Integration,"Yifu Wang, Yonhon Ng, Inkyu Sa, Alvaro Joaquin Parra Bustos, Cristian Rodriguez, Tao Jun Lin, Hongdong Li","Tencent,The University of Adelaide,Australian Institute for Machine Learning,Australian National University,Australian National university and NICTA",Visual-Inertial SLAM,"We present a novel optimization-based Visual-Inertial SLAM system designed for multiple partially overlapped camera systems, named MAVIS. Our framework fully exploits the benefits of wide field-of-view from multi-camera systems, and the metric scale measurements provided by an inertial measurement unit (IMU). We introduce an improved IMU pre-integration formulation based on the exponential function of the automorphism of SE_2(3), which can effectively enhance tracking performance under fast rotational motion and extended integration time. Furthermore, we extend conventional front-end tracking and back-end optimization module designed for monocular or stereo setup towards multi-camera systems, and introduce implementation details that contribute to the performance of our system in challenging scenarios. The practical validity of our approach is supported by our experiments on public datasets. Our MAVIS won the first place in all the vision-IMU tracks (single and multi-session SLAM) on Hilti SLAM Challenge 2023 with 1.7 times the score compared to the second place."
Salience-Guided Ground Factor for Robust Localization of Delivery Robots in Complex Urban Environments,"Jooyong Park, Jungwoo Lee, Euncheol Choi, Younggun Cho","Inha University,Inha university",Localization I,"In urban environments for delivery robots, particularly in areas such as campuses and towns, many custom features defy standard road semantic categorizations. Addressing this challenge, our paper introduces a method leveraging Salient Object Detection (SOD) to extract these unique features, employing them as pivotal factors for enhanced robot loop closure and localization. Traditional geometric feature-based localization is hampered by fluctuating illumination and appearance changes. Our preference for SOD over semantic segmentation sidesteps the intricacies of classifying a myriad of non-standardized urban features. To achieve consistent ground features, the Motion Compensate IPM (MC-IPM) technique is implemented, capitalizing on motion for distortion compensation and subsequently selecting the most pertinent salient ground features through moment computations. For thorough evaluation, we validated the saliency detection and localization performances to the real urban scenarios. Project page:https://sites.google.com/view/salient-ground- factor/home."
Block-Map-Based Localization in Large-Scale Environment,"Yixiao Feng, Zhou Jiang, Yongliang Shi, Yunlong Feng, Xiangyu Chen, Hao Zhao, Guyue Zhou","University of New South Wales,Beijing Institute of Technology,Tsinghua University,ShanghaiTech University,Liverpool John Moores University",Localization I,"Accurate localization is an essential technology for the flexible navigation of robots in large-scale environments. Both SLAM-based and map-based localization will increase the computing load due to the increase in map size, which will affect downstream tasks such as robot navigation and services. To this end, we propose a localization system based on Block Maps (BMs) to reduce the computational load caused by maintaining large-scale maps. Firstly, we introduce a method for generating block maps and the corresponding switching strategies, ensuring that the robot can estimate the state in large-scale environments by loading local map information. Secondly, global localization according to Branch-and-Bound Search (BBS) in the 3D map is introduced to provide the initial pose. Finally, a graph-based optimization method is adopted with a dynamic sliding window that determines what factors are being marginalized whether a robot is exposed to a BM or switching to another one, which maintains the accuracy and efficiency of pose tracking. Comparison experiments are performed on publicly available large-scale datasets. Results show that the proposed method can track the robot pose even though the map scale reaches more than 6 kilometers, while efficient and accurate localization is still guaranteed on NCLT and M2DGR."
Subsurface Feature-Based Ground Robot/Vehicle Localization Using a Ground Penetrating Radar,"Haifeng Li, Jiajun Guo, Dezhen Song","Civil Aviation University of China,Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)",Localization I,"Robot localization using subsurface features captured by Ground-Penetrating Radar (GPR) complements and improves robustness over existing common sensor modalities, as subsurface features are less sensitive to weather, season and surface scene changes. Here, we propose a novel subsurface feature-based localization method that uses only GPR measurements with a known subsurface map. An efficient feature descriptor, the dominant energy curve (DEC), is designed to identify different locations in cluttered conditions. Specifically, image processing techniques that involve background segmentation, energy point detection, and energy curve refinement are designed to extract DEC features from a 2D radargram. With DECs features obtained, a metric subsurface feature map is constructed. Finally, we perform robot localization by feature matching under a particle swarm optimization framework. We have implemented our method and tested it with the public CMU-GPR dataset. The results show that our algorithm improves accuracy and robustness with real-time performance for robot localization tasks. Specifically, the mean localization error is $0.503$ m for all cases."
Colmap-PCD: An Open-Source Tool for Fine Image-To-Point Cloud Registration,"Chunge Bai, Ruijie Fu, Xiang Gao","AgiBot Technology Co. Ltd,Carnegie Mellon University",Localization I,"State-of-the-art techniques for monocular camera reconstruction predominantly rely on the Structure from Motion (SfM) pipeline. However, such methods often yield reconstruction outcomes that lack crucial scale information, and over time, accumulation of images leads to inevitable drift issues. In contrast, mapping methods based on LiDAR scans are popular in large-scale urban scene reconstruction due to their precise distance measurements, a capability fundamentally absent in visual-based approaches. Researchers have made attempts to utilize concurrent LiDAR and camera measurements in pursuit of precise scaling and color details within mapping outcomes. However, the outcomes are subject to extrinsic calibration and time synchronization precision. In this paper, we propose a novel cost-effective reconstruction pipeline that utilizes a pre-established LiDAR map as a fixed constraint to effectively address the inherent scale challenges present in monocular camera reconstruction. To our knowledge, our method is the first to register images onto the point cloud map without requiring synchronous capture of camera and LiDAR data, granting us the flexibility to manage reconstruction detail levels across various areas of interest. To facilitate further research in this domain, we have released Colmap-PCD, an open-source tool leveraging the Colmap algorithm, that enables precise fine-scale registration of images to the point cloud map."
COIN-LIO: Complementary Intensity-Augmented LiDAR Inertial Odometry,"Patrick Pfreundschuh, Helen Oleynikova, Cesar D. Cadena Lerma, Roland Siegwart, Olov Andersson","ETH Zurich,KTH Royal Institute",Localization I,"We present COIN-LIO, a LiDAR Inertial Odometry pipeline that tightly couples information from LiDAR intensity with geometry-based point cloud registration. The focus of our work is to improve the robustness of LiDAR-inertial odometry in geometrically degenerate scenarios, like tunnels or flat fields. We project LiDAR intensity returns into an image, and present a novel image processing pipeline that produces filtered images with improved brightness consistency within the image as well as across different scenes. We effectively leverage intensity as an additional modality, using our new feature selection scheme that detects uninformative directions in the point cloud registration and explicitly selects patches with complementary image information. Photometric error minimization in the image patches is then fused with inertial measurements and point-to-plane registration in an iterated Extended Kalman Filter. The proposed approach improves accuracy and robustness on a public dataset. We additionally publish a new dataset, that captures five real-world environments in challenging, geometrically degenerate scenes. By using the additional photometric information, our approach shows drastically improved robustness against geometric degeneracy in environments where all compared baseline approaches fail."
MegaParticles: Range-Based 6-DoF Monte Carlo Localization with GPU-Accelerated Stein Particle Filter,"Kenji Koide, Shuji Oishi, Masashi Yokozuka, Atsuhiko Banno","National Institute of Advanced Industrial Science and Technology,Nat. Inst. of Advanced Industrial Science and Technology,National Instisute of Advanced Industrial Science and Technology",Localization I,"This paper presents a 6-DoF range-based Monte Carlo localization method with a GPU-accelerated Stein particle filter. To update a massive amount of particles, we propose a Gauss-Newton-based Stein variational gradient descent (SVGD) with iterative neighbor particle search. This method uses SVGD to collectively update particle states with gradient and neighborhood information, which provides efficient particle sampling. For an efficient neighbor particle search, it uses locality sensitive hashing and iteratively updates the neighbor list of each particle over time. The neighbor list is then used to propagate the posterior probabilities of particles over the neighbor particle graph. The proposed method is capable of evaluating one million particles in real-time on a single GPU and enables robust pose initialization and re-localization without an initial pose estimate. In experiments, the proposed method showed an extreme robustness to complete sensor occlusion (i.e., kidnapping), and enabled pinpoint sensor localization without any prior information."
Tightly Coupled Range Inertial Localization on a 3D Prior Map Based on Sliding Window Factor Graph Optimization,"Kenji Koide, Shuji Oishi, Masashi Yokozuka, Atsuhiko Banno","National Institute of Advanced Industrial Science and Technology,Nat. Inst. of Advanced Industrial Science and Technology,National Instisute of Advanced Industrial Science and Technology",Localization I,"This paper presents a range inertial localization algorithm for a 3D prior map. The proposed algorithm tightly couples scan-to-scan and scan-to-map point cloud registration factors along with IMU factors on a sliding window factor graph. The tight coupling of the scan-to-scan and scan-to-map registration factors enables a smooth fusion of sensor ego-motion estimation and map-based trajectory correction that results in robust tracking of the sensor pose under severe point cloud degeneration and defective regions in a map. We also propose an initial sensor state estimation algorithm that robustly estimates the gravity direction and IMU state and helps perform global localization in 3- or 4-DoF for system initialization without prior position information. Experimental results show that the proposed method outperforms existing state-of-the-art methods in extremely severe situations where the point cloud data becomes degenerate, there are momentary sensor interruptions, or the sensor moves along the map boundary or into unmapped regions."
SPOT: Point Cloud Based Stereo Visual Place Recognition for Similar and Opposing Viewpoints,"Spencer Carmichael, Rahul Agrawal, Ram Vasudevan, Katherine A. Skinner",University of Michigan,Localization I,"Recognizing places from an opposing viewpoint during a return trip is a common experience for human drivers. However, the analogous robotics capability, visual place recognition (VPR) with limited field of view cameras under 180 degree rotations, has proven to be challenging to achieve. To address this problem, this paper presents Same Place Opposing Trajectory (SPOT), a technique for opposing viewpoint VPR that relies exclusively on structure estimated through stereo visual odometry (VO). The method extends recent advances in lidar descriptors and utilizes a novel double (similar and opposing) distance matrix sequence matching method. We evaluate SPOT on a publicly available dataset with 6.7-7.6 km routes driven in similar and opposing directions under various lighting conditions. The proposed algorithm demonstrates remarkable improvement over the state-of-the-art, achieving up to 91.7% recall at 100% precision in opposing viewpoint cases, while requiring less storage than all baselines tested and running faster than all but one. Moreover, the proposed method assumes no a priori knowledge of whether the viewpoint is similar or opposing, and also demonstrates competitive performance in similar viewpoint cases."
An Onboard Framework for Staircases Modeling Based on Point Clouds,"Chun Qing, Rongxiang Zeng, Xuan Wu, Yongliang Shi, Gan Ma","Shenzhen Technology University,Tsinghua University",Localization and Navigation,"The detection of traversable regions on staircases and the physical modeling constitutes pivotal aspects of the mobility of legged robots. This paper presents an onboard framework tailored to the detection of traversable regions and the modeling of physical attributes of staircases by point cloud data. To mitigate the influence of illumination variations and the overfitting due to the dataset diversity, a series of data augmentations are introduced to enhance the training of the fundamental network. A curvature suppression cross- entropy(CSCE) loss is proposed to reduce the ambiguity of prediction on the boundary between traversable and non- traversable regions. Moreover, a measurement correction based on the pose estimation of stairs is introduced to calibrate the output of raw modeling that is influenced by tilted perspectives. Lastly, we collected a dataset pertaining to staircases and introduced new evaluation criteria. Through a series of rigorous experiments conducted on this dataset, we substantiate the superior accuracy and generalization capabilities of our proposed method. Codes, models, and datasets will be available at https://github.com/szturobotics/Stair-detection-and-modelli ng-project."
V-STRONG: Visual Self-Supervised Traversability Learning for Off-Road Navigation,"Sanghun Jung, Joonho Lee, Xiangyun Meng, Byron Boots, Alexander Lambert","University of Washington,Univesity of Washington",Localization and Navigation,"Reliable estimation of terrain traversability is critical for the successful deployment of autonomous systems in wild, outdoor environments. Given the lack of large-scale annotated datasets for off-road navigation, strictly-supervised learning approaches remain limited in their generalization ability. To this end, we introduce a novel, image-based self-supervised learning method for traversability prediction, leveraging a state-of-the-art vision foundation model for improved out-of-distribution performance. Our method employs contrastive representation learning using both human driving data and instance-based segmentation masks during training. We show that this simple, yet effective, technique drastically outperforms recent methods in predicting traversability for both on- and off-trail driving scenarios. We compare our method with recent baselines on both a common benchmark as well as our own datasets, covering a diverse range of outdoor environments and varied terrain types. We also demonstrate the compatibility of resulting costmap predictions with a model-predictive controller. Finally, we evaluate our approach on zero- and few-shot tasks, demonstrating unprecedented performance for generalization to new environments. Videos and additional material can be found here: https://sites.google.com/view/visual-traversability-learning."
Follow the Footprints: Self-Supervised Traversability Estimation for Off-Road Vehicle Navigation Based on Geometric and Visual Cues,"Yurim Jeon, E-in Son, Seung-Woo Seo",Seoul National University,Localization and Navigation,"In this study, we address the off-road traversability estimation problem, that predicts areas where a robot can navigate in off-road environments. An off-road environment is an unstructured environment comprising a combination of traversable and non-traversable spaces, which presents a challenge for estimating traversability. This study highlights three primary factors that affect a robot's traversability in an off-road environment: surface slope, semantic information, and robot platform. We present two strategies for estimating traversability, using a guide filter network (GFN) and footprint supervision module (FSM). The first strategy involves building a novel GFN using a newly designed guide filter layer. The GFN interprets the surface and semantic information from the input data and integrates them to extract features optimized for traversability estimation. The second strategy involves developing an FSM, which is a self-supervision module that utilizes the path traversed by the robot in pre-driving, also known as a footprint. This enables the prediction of traversability that reflects the characteristics of the robot platform. Based on these two strategies, the proposed method overcomes the limitations of existing methods, which require laborious human supervision and lack scalability. Extensive experiments in diverse conditions, including automobiles and unmanned ground vehicles, herbfields, woodlands, and farmlands, demonstrate that the proposed method is compatible for various robot platforms and adaptable to a range of terrains."
Learning to Predict Navigational Patterns from Partial Observations,"Robin Karlsson, Alexander Carballo, Francisco Lepe-salazar, Keisuke Fujii, Kento Ohtani, Kazuya Takeda","Nagoya University,Gifu University,Ludolab",Localization and Navigation,"Human beings cooperatively navigate rule-constrained environments by adhering to mutually known navigational patterns, which may be represented as directional pathways or road lanes. Inferring these navigational patterns from incompletely observed environments is required for intelligent mobile robots operating in unmapped locations. However, algorithmically defining these navigational patterns is nontrivial. This letter presents the first self-supervised learning (SSL) method for learning to infer navigational patterns in real-world environments from partial observations only. We explain how geometric data augmentation, predictive world modeling, and an information-theoretic regularizer enable our model to predict an unbiased local directional soft lane probability (DSLP) field in the limit of infinite data. We demonstrate how to infer global navigational patterns by fitting a maximum likelihood graph to the DSLP field. Experiments show that our SSL model outperforms two SOTA supervised lane graph prediction models on the nuScenes dataset. We propose our SSL method as a scalable and interpretable continual learning paradigm for navigation by perception."
TSCM: A Teacher-Student Model for Vision Place Recognition Using Cross-Metric Knowledge Distillation,"Yehui Shen, Mingmin Liu, Huimin Lu, Xieyuanli Chen","NorthEast University,SIASUN Robot & Automation CO.,Ltd,National University of Defense Technology",Localization and Navigation,"Visual place recognition (VPR) plays a pivotal role in autonomous exploration and navigation of mobile robots within complex outdoor environments. While cost-effective and easily deployed, camera sensors are sensitive to lighting and weather changes, and even slight image alterations can greatly affect VPR efficiency and precision. Existing methods overcome this by exploiting powerful yet large networks, leading to significant consumption of computational resources. In this paper, we propose a high-performance teacher and lightweight student distillation framework called TSCM. It exploits our devised cross-metric knowledge distillation to narrow the performance gap between the teacher and student models, maintaining superior performance while enabling minimal computational load during deployment. We conduct comprehensive evaluations on large-scale datasets, namely Pitts- burgh30k and Pittsburgh250k. Experimental results demonstrate the superiority of our method over baseline models in terms of recognition accuracy and model parameter efficiency. Moreover, our ablation studies show that the proposed knowledge distillation technique surpasses other counterparts. Implementation of our method has been released as open source at https://github.com/shenyehui/TSCM."
3D-BBS: Global Localization for 3D Point Cloud Scan Matching Using Branch-And-Bound Algorithm,"Koki Aoki, Kenji Koide, Shuji Oishi, Masashi Yokozuka, Atsuhiko Banno, Junichi Meguro","Meijo University,National Institute of Advanced Industrial Science and Technology,Nat. Inst. of Advanced Industrial Science and Technology,National Instisute of Advanced Industrial Science and Technology",Localization and Navigation,"This paper presents an accurate and fast 3D global localization method, 3D-BBS, that extends the existing branch-and-bound (BnB)-based 2D scan matching (BBS) algorithm. To reduce memory consumption, we utilize a sparse hash table for storing hierarchical 3D voxel maps. To improve the processing cost of BBS in 3D space, we propose an efficient roto-translational space branching. Furthermore, we devise a batched BnB algorithm to fully leverage GPU parallel processing. Through experiments in simulated and real environments, we demonstrated that the 3D-BBS enabled accurate global localization with only a 3D LiDAR scan roughly aligned in the gravity direction and a 3D pre-built map. This method required only 878 msec on average to perform global localization and outperformed state-of-the-art global registration methods in terms of accuracy and processing speed."
DynaInsRemover: A Real-Time Dynamic Instance-Aware Static 3D LiDAR Mapping Framework for Dynamic Environment,"Huanfeng Zhao, Meibao Yao, Xueming Xiao, Bo Zheng","Jilin University,Changchun University of Science and Technology,Shanghai Aerospace Control Technology Institute",Localization and Navigation,"Dynamic objects diversify the distribution of point cloud in the map, degrading the performance of the robotic downstream tasks. To address this problem, we present a novel real-time dynamic instance-aware static mapping framework called DynaInsRemover, which exploits the geometric discrepancies between instances to efficiently remove dynamic objects preserve more details of static map. It contains the Instance Occupancy Check module for initial dynamic instance proposal and the Instance Belief Update module for reverting false positives. We quantitatively evaluate our approach performance on SemanticKITTI and validate it in real-world environment. Experimental evaluations show that our method achieves very promising results in dynamic environments. The implementation of our method is available as open source at: https://github.com/Zhaohuanfeng/DynaInsRemover.git."
Learning Semantic-Agnostic and Spatial-Aware Representation for Generalizable Visual-Audio Navigation,"Hongcheng Wang, Yuxuan Wang, Zfw Fangwei Zhong, Aaron Mingdong Wu, Jianwei Zhang, Yizhou Wang, Hao Dong","Peking University,Peking Univesity,University of Hamburg",Localization and Navigation,"Visual-audio navigation (VAN) is attracting more and more attention from the robotic community due to its broad applications, e.g., household robots and rescue robots. In this task, an embodied agent must search for and navigate to the sound source with egocentric visual and audio observations. However, the existing methods are limited in two aspects: 1) poor generalization to unheard sound categories; 2) sample inef- ficient in training. Focusing on these two problems, we propose a brain-inspired plug-and-play method to learn a semantic- agnostic and spatial-aware representation for generalizable visual-audio navigation. We meticulously design two auxiliary tasks for respectively accelerating learning representations with the above-desired characteristics. With these two auxiliary tasks, the agent learns a spatially-correlated representation of visual and audio inputs that can be applied to work on environments with novel sounds and maps. Experiment results on realistic 3D scenes (Replica and Matterport3D) demonstrate that our method achieves better generalization performance when zero-shot transferred to scenes with unseen maps and unheard sound categories."
Efficient 3D Instance Mapping and Localization with Neural Fields,"George Tang, Krishna Murthy, Antonio Torralba",MIT,Localization and Navigation,"We tackle the problem of learning an implicit scene representation for 3D instance segmentation from a sequence of posed RGB images. Towards this, we introduce 3DIML, a novel framework that efficiently learns a label field that may be rendered from novel viewpoints to produce view-consistent instance segmentation masks. 3DIML significantly improves upon training and inference runtimes of existing implicit scene representation based methods. Opposed to prior art that optimizes a neural field in a self-supervised manner, requiring complicated training procedures and loss function design, 3DIML leverages a two-phase process. The first phase, InstanceMap, takes as input 2D segmentation masks of the image sequence generated by a frontend instance segmentation model, and associates corresponding masks across images to 3D labels. These almost view-consistent pseudolabel masks are then used in the second phase, InstanceLift, to supervise the training of a neural label field, which interpolates regions missed by InstanceMap and resolves ambiguities. Additionally, we introduce InstanceLoc, which enables near realtime localization of instance masks given a trained label field and an off-the-shelf image segmentation model by fusing outputs from both. We evaluate 3DIML on sequences from the Replica and ScanNet datasets and demonstrate 3DIMLâ€™s effectiveness under mild assumptions for the image sequences. We achieve a 14-24Ã— speedup over existing implicit scene representation methods with comparable quality, showcasing its potential to facilitate faster and more effective 3D scene understanding."
Grasp It Like a Pro 2.0: A Data-Driven Approach Exploiting Basic Shapes Decomposition and Human Data for Grasping Unknown Objects,"Alessandro Palleschi, Franco Angelini, Chiara Gabellieri, Do Won Park, Lucia Pallottino, Antonio Bicchi, Manolo Garabini","Floating Robotics AG,University of Pisa,University of Twente,Università di Pisa,Fondazione Istituto Italiano di Tecnologia",Grasping I,
Visual-Tactile Fusion for Transparent Object Grasping in Complex Backgrounds,"Shoujie Li, Haixin Yu, Wenbo Ding, Houde Liu, Linqi Ye, Chongkun Xia, Xueqian Wang, Xiao-Ping (Steven) Zhang","Tsinghua Shenzhen International Graduate School,Tsinghua University,Shenzhen Graduate School, Tsinghua University,Shanghai University,Center for Artificial Intelligence and Robotics, Graduate School,Ryerson University",Grasping I,"The grasping of transparent objects is challenging but of significance to robots. In this article, a visual-tactile fusion framework for transparent object grasping in complex backgrounds is proposed, which synergizes the advantages of vision and touch, and greatly improves the grasping efficiency of transparent objects. First, we propose a multi-scene synthetic grasping dataset named SimTrans12K together with a Gaussian-Mask annotation method. Next, based on the TaTa gripper, we propose a grasping network named transparent object grasping convolutional neural network (TGCNN) for grasping position detection,which shows good performance in both synthetic and real scenes. Inspired by human grasping, a tactile calibration method and a visual-tactile fusion classification method are designed, which improve the grasping success rate by 36.7% compared to direct grasping and the classification accuracy by 39.1%. Furthermore, the Tactile Height Sensing (THS) module and the Tactile Position Exploration (TPE) module are added to solve the problem of grasping transparent objects in irregular and visually undetectable scenes. Experimental results demonstrate the validity of the framework."
Variable Stiffness Soft Robotic Fingers Using Snap-Fit Kinematic Reconfiguration,"Jérôme Bastien, Lionel Birglen","Polytechnique Montréal,Ecole Polytechnique de Montreal",Grasping I,"Versatile and secure grasping in robotic systems remains a difficult challenge to address when objects possess a wide range of different properties (size, weight, friction coefficient, etc.). The human hand is often the primary source of inspiration for many technologies addressing this challenge and a notable feature of our hands is that they can vary their stiffness to match the requirements of the task, e.g. become stiffer or more compliant depending on specific requirements. Many robotic devices have been proposed in the literature mirroring this capability, either using an adjustable internal tension mechanism similar to what happens with human tendons or another physical phenomenon yielding the same effect. This paper proposes a new type of soft robotic fingers using a novel method to produce a variable stiffness achieved by modifying the kinematic structure of the fingers using snap-fit joints, a very simple alternative to most variable stiffness mechanisms. The resulting modification of the geometry and kinematics of the fingers, including their number of degrees of freedom, allows to greatly alter the intrinsic stiffness of the grasp produced by these fingers. A notable feature of the proposed new design is that one pair of fingers can be used to switch the stiffness of another pair if a dual arm robot is used."
Grasp Transfer Based on Self-Aligning Implicit Representations of Local Surfaces,"Ahmet Tekden, Marc Peter Deisenroth, Yasemin Bekiroglu","Chalmers University of Technology,University College London,Chalmers University of Technology, University College London",Grasping I,"Objects we interact with and manipulate often share similar parts, e.g. handles, that allow us to transfer our actions flexibly due to their shared functionality. This work addresses the problem of transferring grasp experience or demonstration to a novel object that shares shape similarities with objects a robot has previously experienced. Existing approaches to solving this problem are typically restricted to a specific object category or a parametric shape. Our approach, however, can transfer grasps associated with implicit models of local shapes shared across object categories. Specifically, we employ a single expert grasp demonstration during training to learn a local implicit surface representation model. At inference time, this model can be utilized to transfer grasps to novel objects by identifying the most similar-looking surfaces to the one on which the expert grasp is demonstrated. Our model is trained entirely in simulation and is evaluated on simulated and real-world objects that are not seen during training. Simulation results show that our method acquires better spatial precision and grasp accuracy compared to the baselines. Moreover, our method can successfully perform grasp transfer to unseen object categories, as shown in both simulation and real-world experiments."
Amortized Inference for Efficient Grasp Model Adaptation,"Michael Noseworthy, Seiji Shaw, Chad C. Kessens, Nicholas Roy","Massachusetts Institute of Technology,United States Army Research Laboratory",Grasping I,"In robotic applications such as bin-picking or block-stacking, learned predictive models have been developed for manipulation of objects with varying but known dynamic properties (e.g., mass distributions and friction coefficients). When a robot encounters a new object, these properties are often difficult to observe and must be inferred through interaction, which can be expensive in both inference time and number of interactions. We propose an encoder/decoder action-feasibility model to efficiently adapt to new objects by estimating their unobserved properties through interaction. The encoder predicts a distribution over the unobserved parameters while the decoder predicts action feasibility, which can be used in an uncertainty-aware planner. An explicit representation of uncertainty in the encoder enables information-gathering heuristics to minimize adaptation interactions. The amortized distributions are efficient to compute and perform comparably to particle-based distributions in a grasping domain. Finally, we deploy our method on a Panda robot to grasp heavy objects."
Learning Realistic and Reasonable Grasps for Anthropomorphic Hand in Cluttered Scenes,"Haonan Duan, Yiming Li, Daheng Li, Wei Wei, Yayu Huang, Peng Wang","Institute of Automation, Chinese Academy of Sciences,Idiap Research Institute, École Polytechnique Fédérale de Lausan,University of Chinese Academy of Sciences,Chinese Acdamy of Sciences",Grasping I,"Grasping is one of the most fundamental skills for humans to interact with objects. However, it remains a challenging problem for anthropomorphic hands, due to the lack of object affordance understanding and high-dimensional grasp planning. In this work, we propose an anthropomorphic hand grasping framework to learn realistic and reasonable grasps in cluttered scenes, which tackles the problem in three items: 1) graspable point segmentation; 2) hand grasp generation and 3) grasp optimization. Specifically, our method generates high-quality hand grasps efficiently without complete object models by learning graspable points, associated grasp configurations from observed point cloud in a parallel manner and optimizing predicted grasps based on hand-object contacts. Simulation experiments show that our model generates physical plausible grasps for the anthropomorphic hand effectively with over 70% success rate. Real-world experiments demonstrate that the model trained in simulation performs satisfactorily in real-world scenarios for unseen objects."
FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single Annotated Example Object,"Hanzhi Chen, Binbin Xu, Stefan Leutenegger","Technical University of Munich (TUM),University of Toronto,Technical University of Munich",Grasping I,"We present FuncGrasp, a framework that can infer dense yet reliable grasp configurations for unseen objects using one annotated object and single-view RGB-D observation via categorical priors. Unlike previous works that only transfer a set of grasp poses, FuncGrasp aims to transfer infinite configurations parameterized by an object-centric continuous grasp function across varying instances. To ease the transfer process, we propose Neural Surface Grasping Fields (NSGF), an effective neural representation defined on the surface to densely encode grasp configurations. Further, we exploit function-to-function transfer using sphere primitives to establish semantically meaningful categorical correspondences, which are learned in an unsupervised fashion without any expert knowledge. We showcase the effectiveness through extensive experiments in both simulators and the real world. Remarkably, our framework significantly outperforms several strong baseline methods in terms of density and reliability for generated grasps."
Physical and Digital Adversarial Attacks on Grasp Quality Networks,"Naif Alharthi, Martim Brandao",King's College London,Grasping I,"Grasp Quality Networks are important components of grasping-capable autonomous robots, as they allow them to evaluate grasp candidates and select the one with highest chance of success. The widespread use of pick-and-place robots and Grasp Quality Networks raises the question of whether such systems are vulnerable to adversarial attacks, as that could lead to large economic damage. In this paper we propose two kinds of attacks on Grasp Quality Networks, one assuming physical access to the workspace (to place or attach a new object) and another assuming digital access to the camera software (to inject a pixel-intensity change on a single pixel). We then use evolutionary optimization to obtain attacks that simultaneously minimize the noticeability of the attacks and the chance that selected grasps are successful. Our experiments show that both kinds of attack lead to drastic drops in algorithm performance, thus making them important attacks to consider in the cybersecurity of grasping robots. Source code can be found at https://github.com/Naif-W-Alharthi/Physical-and-Digital-Attacks-on-Grasping-Networks"
Scaling Object-Centric Robotic Manipulation with Multimodal Object Identification,"Chaitanya Mitash, Mostafa Hussein, Jeroen Vanbaar, Vikedo Terhuja, Kapil Katyal","Amazon Robotics,Unvirisity of New Hampshire,MERL,Johns Hopkins University",Grasping I,"Robotic manipulation is a key enabler for automation in the fulfillment logistics sector. Such robotic systems require perception and manipulation capabilities to handle a wide variety of objects. Existing systems either operate on a closed set of objects or perform object-agnostic manipulation which lacks the capability for deliberate and reliable manipulation at scale. Object identification (ID) unlocks the ability for large-scale, object-centric manipulation by mapping object segments to one of the previously seen objects from a database. Nevertheless, it is often limited by the availability of reference data or coverage for objects in a database. In this work, we propose to perform object identification with multiple reference databases, including images and text references, each with a different coverage and matching challenge. We propose a training strategy that tackles the challenges of learning domain- invariant image embeddings, image-text matching and fusing predictions from different sources. We perform experiments over a recent benchmark with over 190K+ unique objects, extend the dataset with the additional reference sources and propose an evaluation strategy that simulates coverage for different reference sources. Model trained with the proposed learning pipeline shows robust performance over a range of simulation experiments."
Single-Motor Robotic Gripper with Three Functional Modes for Grasping in Confined Spaces,"Toshihiro Nishimura, Tetsuyou Watanabe",Kanazawa University,Grippers and Other End-Effectors I,"This study proposes a novel robotic gripper driven by a single motor. The main task is to pick up objects in confined spaces. For this purpose, the developed gripper has three operating modes: grasping, finger-bending, and pull-in modes. Using these three modes, the developed gripper can rotate and translate a grasped object, i.e., can perform in-hand manipulation. This in-hand manipulation is effective for grasping in extremely confined spaces, such as the inside of a box in a shelf, to avoid interference between the grasped object and obstacles. To achieve the three modes using a single motor, the developed gripper is equipped with two novel self-motion switching mechanisms. These mechanisms switch their motions automatically when the motion being generated is prevented. An analysis of the mechanism and control methodology used to achieve the desired behavior are presented. Furthermore, the validity of the analysis and methodology are experimentally demonstrated. The gripper performance is also evaluated through the grasping tests."
A Force-Controlled Gripper Capable of Measuring Mechanical Properties of an Object,"Yi-shian Tsai, Pin-chun Yeh, Chun-hung Huang, I-cheng Hsueh, Chao-Chieh Lan","National Cheng Kung University Mechanical Engineering Department,National Cheng Kung University",Grippers and Other End-Effectors I,"Various sensorized grippers have been developed to handle delicate objects safely. These grippers have sensors mounted on their fingersâ€™ surface that provide direct force measurements. However, multiple sensors are often required on one finger, leading to significant sensor placement and wire routing complexity. Finger-based sensors are limited to sensing external gripping force, and fingers cannot be easily replaced to meet the requirements of objects with specific geometries. To overcome the complexity and limitations of finger surface sensors, this paper proposes a force-controlled two-fingered gripper that relies on the deformation sensing of elastic elements in the drivetrain to obtain finger force. By using a minimum number of optical encoders placed in the drivetrain, accurate position and force sensing can be achieved at any location of each finger. When gripping an object, the size and stiffness of the object can thus be accurately measured. Simulation and experimental results demonstrate the proposed gripperâ€™s merits. We expect this new gripper to provide a more competitive solution for robots that need to manipulate objects and check their mechanical qualities at the same time."
Optimal Design of a Highly Self-Adaptive Gripper with Multi-Phalange Compliant Fingers for Grasping Irregularly Shaped Objects,"Chih-Hsing Liu, Sy-yeu Yang, Yi-chieh Shih","National Cheng Kung University,NCKU",Grippers and Other End-Effectors I,"The development of a robotic gripper for handling objects of various sizes, shapes, weights, and degrees of hardness is a challenging problem in the field of robotics. In order to design a highly self-adaptive gripper capable of conforming to a wide range of objects, this article presents an innovative topology-optimized design of a compliant finger consisting of several multi-material phalanges connected by flexure hinges. The prototype was produced by means of a metamaterial approach, which utilizes 3D-printed infill structures (periodic cells) with different infill densities to represent regions with different equivalent mechanical properties. Adaptability tests were conducted to demonstrate the effectiveness of the proposed design in grasping circular, rectangular, trapezoidal, and concave objects. The results were compared with those of the fingers with single infill densities and a commercially available Festo MultiChoiceGripper, which features a Fin Ray structure. The total contact length between the fingers and the grasped object was used as a measure of the grippersâ€™ adaptability. The test results demonstrate that this novel self-adaptive gripper is comparatively highly adaptable for grasping irregularly shaped objects and is able to carry a maximum payload of 6.76 kg."
Accelerating Robotic Picking of Rigid Objects with a Compliant Pneumatic Gripper and an Impact-Aware Trajectory Plan,"Frederik Ostyn, Bram Vanderborght, Guillaume Crevecoeur","Ghent University,VUB",Grippers and Other End-Effectors I,"Industrial robots are capable of moving at high speed. Each time they come into contact with their environment, e.g. to pick up an object, they decelerate to a near standstill. A solution involving a compliant pneumatic gripper and adapted trajectory plan is presented to initiate contact at a higher speed while remaining within hardware limits. By adding overload clutches in either the robot arm or gripper, tolerance to errors is provided. The key parameters such as gripper compliance and maximum allowed initial impact velocity are identified. Results show that by properly optimizing these parameters, robot picking of rigid objects can be accelerated. The complete high-speed picking solution is experimentally verified. A time reduction of 16% was obtained when making contact at 0.65 m/s."
Vertical Vibratory Transport of Grasped Parts Using Impacts,"Connor Yako, Jerome Nowak, Shenli Yuan, Kenneth Salisbury","Stanford University,SRI International",Grippers and Other End-Effectors I,"In this paper, we use impact-induced acceleration in conjunction with periodic stick-slip to successfully and quickly transport parts vertically against gravity. We show analytically that vertical vibratory transport is more difficult than its horizontal counterpart, and provide guidelines for achieving optimal vertical vibratory transport of a part. Namely, such a system must be capable of quickly realizing high accelerations, as well as supply normal forces at least several times that required for static equilibrium. We also show that for a given maximum acceleration, there is an optimal normal force for transport. To test our analytical guidelines, we built a vibrating surface using flexures and a voice coil actuator that can accelerate a magnetic ram into various surfaces to generate impacts. The surface was used to transport a part against gravity. Experimentally obtained motion tracking data confirmed the theoretical model. A series of grasping tests with a vibrating-surface equipped parallel jaw gripper confirmed the design guidelines."
Bionic Soft Fingers with Hybrid Variable Stiffness Mechanisms for Multimode Grasping,"Xiangbo Wang, Tianran Zhang, Hongze Yu, Zhenwei Wen, Lide Fang, Huaping Liu, Fuchun Sun, Tang Lixue, Bin Fang","College of Quality and Technology Supervising, Hebei University,,Beihang University,Beijing University of Posts and Telecommunications,Beijing University of Aeronautics and Astronautic,Hebei University,Tsinghua Univ.,Tsinghua University,Capital medical university,Beijing University of Posts and Telecommunications / Tsinghua Un",Grippers and Other End-Effectors I,"This paper presents a novel Bionic Soft Finger (BSF) that aims to overcome the limitations of conventional rigid manipulators in terms of adaptability and safety, as well as the challenges faced by soft hands regarding carrying capacity and stability. The BSF design uses a hybrid variable stiffness mechanism combining memory alloy actuators with particle jamming to achieve the desired bending angle and actuator stiffness. Our innovative approach utilizes a bionic finger design that incorporates a memory alloy skeleton and a water-cooled recirculation system, leading to a substantial reduction in the time required for each operation. Through the integration of particle jamming, we have enhanced the overall stiffness and performance of the manipulator, enabling load capacities of up to 3N per finger and more than twice the stiffness of a normal condition. Additionally, our design enables multimode grasping and incorporates a liquid metal strain sensor (METT) for real-time monitoring of finger bending angles. Comparative analyses demonstrate that our design exhibits superior stiffness and enables five-mode grasping in comparison to pneumatic actuators. We believe that bionic soft fingers present a promising solution for enhancing adaptability, safety, and performance in human-robot interaction applications."
Design and Fabrication of a Novel Miniature Magnetic Gripper,"Mengde Li, Fuqiang Zhao, Xiangli Li, Mingchang Li, Sheng Liu, Miao Li","The Institute of Technological Sciences, Wuhan University, Hubei,School of Power and Mechanical Engineering, Wuhan University, Hu,WuHan University,Wuhan University",Grippers and Other End-Effectors I,"Small-scale robots hold significant promise in the field of minimally invasive surgery (MIS). In this paper, we present a miniature magnetic gripper and develop a datadriven kinematic model. The gripper comprises four fingers, wherein each finger has a maximum size not exceeding 3mm, 4mm and 5.5mm in three dimensions. By integrating permanent magnets and elastic ropes as internal actuation elements into the fingers, the gripper is equipped with the capability to open-close under an external magnetic field, facilitating the manipulation of small objects in confined spaces. Modeling and analysis of the magnetic gripper are undertaken, wherein the relationship between the open angle and the external magnetic field is established. The average error between the experimentally observed open angles and the model-predicted values is 2.31â—¦. Subsequent experiments demonstrated the necessity of the magnetic gripper model for precise manipulation, verified its excellent sensitivity to magnetic fields, and demonstrated its potential for future applications in MIS."
Design of Highly Repeatable and Multi-Functional Grippers for Precision Handling with Articulated Robots,"Philip Gümbel, Klaus Dröder","Technische Universität Braunschweig, Insitute for Machine Tools ,Technische Universität Braunschweig",Grippers and Other End-Effectors I,"This paper presents a novel approach to designing, a low-cost gripper that is highly repeatable and functionally integrated. The gripper is optimized to compensate for gripping errors with particular consideration to potential challenges of articulated robots. The primary design goal is to achieve maximum repeatability during the gripping and releasing stages of a pick-and-place process for a chip-like silicon die. The design is centered around a custom printed circuit board integrates functionality for vision-based error compensation, vacuum level monitoring, part contact detection, and detection of abnormal vibrations. We detail our design requirements and specific design choices for the mechanical and electronic design and provide qualitative and quantitative experimental validation of the achieved repeatability and the integrated functions."
Generalized Partially Destructive Disassembly Planning for Robotic Disassembly,"Malte Hansjosten, Jan Baumgärtner, Jürgen Fleischer","Karlsruhe Institute of Technology (KIT),Karlsruhe Institute of Technology",Grippers and Other End-Effectors I,"While robotic assembly is a well researched topic, recycling and disassembly of products are also becoming ever more important as we transition to a more sustainable economy. In disassembly, we are typically only interested in a subset of product parts, which opens the possibility of using destructive processes such as tearing, cutting, or milling to speed up the disassembly. Currently, such destructive actions are only included as predefined case-specific actions such as milling away a screw head. By contrast, this paper presents a generalized approach to destructive disassembly planning that can automatically derive destructive disassembly actions from a symbolic representation of the disassembly state. Viable destructive actions are identified and verified only based on the underlying geometric model, circumventing the need for their explicit definition. We showcase the performance of this system both virtually on several test parts and physically by destructively and non-destructively disassembling a model of an electric motor using a robot manipulator with a multitool end effector."
IFFNeRF: Initialisation Free and Fast 6DoF Pose Estimation from a Single Image and a NeRF Model,"Matteo Bortolon, Theodore Tsesmelis, Stuart James, Fabio Poiesi, Alessio Del Bue","Istituto Italiano di Tecnologia; Fondazione Bruno Kessler; Unive,Istituto Italiano di Tecnologia,Durham University,Fondazione Bruno Kessler",Object Detection and Pose Estimation,"We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera pose of a given image, building on the Neural Radiance Fields (NeRF) formulation. IFFNeRF is specifically designed to operate in real-time and eliminates the need for an initial pose guess that is proximate to the sought solution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface points from within the NeRF model. From these sampled points, we cast rays and deduce the color for each ray through pixel-level view synthesis. The camera pose can then be estimated as the solution to a Least Squares problem by selecting correspondences between the query image and the resulting bundle. We facilitate this process through a learned attention mechanism, bridging the query image embedding with the embedding of parameterized rays, thereby matching rays pertinent to the image. Through synthetic and real evaluation settings, we show that our method can improve the angular and translation error accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing at 34fps on consumer hardware and not requiring the initial pose guess."
VeloVox: A Low-Cost and Accurate 4D Object Detector with Single-Frame Point Cloud of Livox LiDAR,"Tao Ma, Zhiwei Zheng, Hongbin Zhou, Xinyu Cai, Xuemeng Yang, Yikang Li, Botian Shi, Hongsheng Li","Shanghai AI Laboratory,UC Berkeley,Shanghai AI Lab,Shanghai Artificial Intelligence Laboratory,IDG Capital,Chinese University of Hong Kong",Object Detection and Pose Estimation,"Combining motion prediction in LiDAR-based 3D object detection is an effective method for improving overall accuracy, especially the downstream autonomous driving tasks. The recent development of low-cost LiDARs (e.g. Livox LiDAR) enables us to explore such 4D perception systems with a lower budget and higher performance. In this paper, we propose a 4D object detector, VeloVox, to establish accurate object detection and velocity estimation with a single-frame point cloud of Livox LiDAR. Based on the non-repetitive scanning pattern and point-level temporal nature, we propose a two-stage module to enhance the spatial-temporal point feature interaction along the time dimension. The aggregated feature also benefits a more accurate proposal refinement. To demonstrate the performance, comparison of VeloVox with several SOTA detector based baselines is evaluated on our in-house dataset and synthesized dataset built under Carla simulation. Code will be released at https://github.com/PJLab-ADG/VeloVox."
MTRadSSD: A Multi-Task Single-Stage Detector for Object Detection and Free Space Analysis in Radar Point Clouds,"Yinbao Li, Songshan Yu, Wang Dongfeng, Jingen Jiao","Jiaxing Joospeed Electronics Technology Co. Ltd,Jiaxing Jospeed Electronics Technology Co. Ltd",Object Detection and Pose Estimation,"Environmental perception tasks such as object detection and free space detection based on 3+1D radar severely suffer from the disorder and sparsity of point cloud. To tackle this problem, we propose a novel Multi-Task Radar-based Single Stage Detector, termed MTRadSSD, where we adopt instance-aware sampling strategies to discover multi-class road users and propose an occupancy map tool based on kernel density estimation (KDE) to make predictions in birdâ€™s eye view (BEV). The denoised occupancy map also plays key role in generating polygon represented free space in the scene. As a result, our elaborated sampling strategies effectively retained useful semantic information and narrowed the difference of detection performance across object categories. Meanwhile, our MTRadSSD outperforms those state-of-the-art approaches in terms of real-time requirement and detection accuracy. In detail, the proposed method achieves an satisfactory speed of Ëœ16.7 ms per frame in experiments on the public radar point cloud dataset View-of-Delft (VOD). With IoU thresholds 0.5/0.25/0.25 the average prediction precision (AP) of easy-level objects (cars, pedestrians and cyclists) reaches at competitive 52.2%, 61.1%, 86.3%, respectively, while mean IoU of free space is 87.8%. Especially, the occupancy map also makes difference in improving prediction precision of object orientation dramatically to averaged 64.0%."
Toward Accurate Camera-Based 3D Object Detection Via Cascade Depth Estimation and Calibration,"Chaoqun Wang, Yiran Qin, Zijian Kang, Ningning Ma, Ruimao Zhang","The Chinese University of Hong Kong, Shenzhen,CUHKsz,NIO. Inc,NIO,The Chinese University of Hong Kong (Shenzhen)",Object Detection and Pose Estimation,"Recent camera-based 3D object detection is limited by the precision of transforming from image to 3D feature spaces, as well as the accuracy of object localization within the 3D space. This paper aims to address such a fundamental problem of camera-based 3D object detection: How to effectively learn depth information for accurate feature lifting and object localization. Different from previous methods which directly predict depth distributions by using a supervised estimation model, we propose a cascade framework consisting of two depth-aware learning paradigms. First, a depth estimation (DE) scheme leverages relative depth information to realize the effective feature lifting from 2D to 3D spaces. Furthermore, a depth calibration (DC) scheme introduces depth reconstruction to further adjust the 3D object localization perturbation along the depth axis. In practice, the DE is explicitly realized by using both the absolute and relative depth optimization loss to promote the precision of depth prediction, while the capability of DC is implicitly embedded into the detection Transformer through a depth denoising mechanism in the training phase. The entire model training is accomplished through an end-to-end manner. We propose a baseline detector and evaluate the effectiveness of our proposal with +2.2%/+2.7% NDS/mAP improvements on NuScenes benchmark, and gain a comparable performance with 55.9%/45.7% NDS/mAP. Furthermore, we conduct extensive experiments to demonstrate its generality based on various detectors with about +2% NDS improvements."
DA-RAW: Domain Adaptive Object Detection for Real-World Adverse Weather Conditions,"Minsik Jeon, Junwon Seo, Jihong Min",Agency for Defense Development,Object Detection and Pose Estimation,"Despite the success of deep learning-based object detection methods in recent years, it is still challenging to make the object detector reliable in adverse weather conditions such as rain and snow. For the robust performance of object detectors, unsupervised domain adaptation has been utilized to adapt the detection network trained on clear weather images to adverse weather images. While previous methods do not explicitly address weather corruption during adaptation, the domain gap between clear and adverse weather can be decomposed into two factors with distinct characteristics: a style gap and a weather gap. In this paper, we present an unsupervised domain adaptation framework for object detection that can more effectively adapt to real-world environments with adverse weather conditions by addressing these two gaps separately. Our method resolves the style gap by concentrating on style-related information of high-level features using an attention module. Using self-supervised contrastive learning, our framework then reduces the weather gap and acquires instance features that are robust to weather corruption. Extensive experiments demonstrate that our method outperforms other methods for object detection in adverse weather conditions."
SyMFM6D: Symmetry-Aware Multi-Directional Fusion for Multi-View 6D Object Pose Estimation,"Fabian Duffhauss, Sebastian Koch, Hanna Ziesche, Ngo Anh Vien, Gerhard Neumann","Bosch Center for Artificial Intelligence,Ulm University, Robert Bosch GmbH,Bosch BCAI,Bosch GmbH,Karlsruhe Institute of Technology",Object Detection and Pose Estimation,"Detecting objects and estimating their 6D poses is essential for automated systems to interact safely with the environment. Most 6D pose estimators, however, rely on a single camera frame and suffer from occlusions and ambiguities due to object symmetries. We overcome this issue by presenting a novel symmetry-aware multi-view 6D pose estimator called SyMFM6D. Our approach fuses the RGB-D frames from multiple perspectives in a deep multi-directional fusion network and predicts predefined keypoints for all objects in the scene simultaneously. Based on the keypoints and an instance semantic segmentation, we efficiently compute the 6D poses by least-squares fitting. To address the ambiguity issues for symmetric objects, we propose a novel training procedure for symmetry-aware keypoint detection including a new objective function. Our SyMFM6D network significantly outperforms the state-of-the-art in both single-view and multi-view 6D pose estimation. We furthermore show the effectiveness of our symmetry-aware training procedure and demonstrate that our approach is robust towards inaccurate camera calibration and dynamic camera setups."
Mutual Information-Calibrated Conformal Feature Fusion for Uncertainty-Aware Multimodal 3D Object Detection at the Edge,"Alex Christopher Stutts, Danilo Erricolo, Sathya Ravi, Theja Tulabandhula, Amit Ranjan Trivedi","University of Illinois Chicago,University of Illinois at Chicago,University of Illinois at Chicago (UIC), Chicago, USA",Object Detection and Pose Estimation,"In the expanding landscape of AI-enabled robotics, robust quantification of predictive uncertainties is of great importance. Three-dimensional (3D) object detection, a critical robotics operation, has seen significant advancements; however, the majority of current works focus only on accuracy and ignore uncertainty quantification. Addressing this gap, our novel study integrates the principles of conformal inference (CI) with information theoretic measures to perform lightweight, Monte Carlo-free uncertainty estimation within a multimodal framework. Through a multivariate Gaussian product of the latent variables in a Variational Autoencoder (VAE), features from RGB camera and LiDAR sensor data are fused to improve the prediction accuracy. Normalized mutual information (NMI) is leveraged as a modulator for calibrating uncertainty bounds derived from CI based on a weighted loss function. Our simulation results show an inverse correlation between inherent predictive uncertainty and NMI throughout the model's training. The framework demonstrates comparable or better performance in KITTI 3D object detection benchmarks to similar methods that are not uncertainty-aware, making it suitable for real-time edge robotics."
RGB-Based Category-Level Object Pose Estimation Via Decoupled Metric Scale Recovery,"Jiaxin Wei, Xibin Song, Weizhe Liu, Laurent Kneip, Hongdong Li, Pan Ji","Technical University of Munich,Baidu,Tencent,ShanghaiTech University,Australian National university and NICTA",Object Detection and Pose Estimation,"While showing promising results, recent RGB-D camera-based category-level object pose estimation methods have restricted applications due to the heavy reliance on depth sensors. RGB-only methods provide an alternative to this problem yet suffer from inherent scale ambiguity stemming from monocular observations. In this paper, we propose a novel pipeline that decouples the 6D pose and size estimation to mitigate the influence of imperfect scales on rigid transformations. Specifically, we leverage a pre-trained monocular estimator to extract local geometric information, mainly facilitating the search for inlier 2D-3D correspondence. Meanwhile, a separate branch is designed to directly recover the metric scale of the object based on category-level statistics. Finally, we advocate using the RANSAC-PnP algorithm to robustly solve for 6D object pose. Extensive experiments have been conducted on both synthetic and real datasets, demonstrating the superior performance of our method over previous state-of-the-art RGB-based approaches, especially in terms of rotation accuracy. Code: https://github.com/goldoak/DMSR."
Implicit Coarse-To-Fine 3D Perception for Category-Level Object Pose Estimation from Monocular RGB Image,"Jia Li, Li Jin, Xibin Song, Yeheng Chen, Nan Li, Xueying Xueying Qin","Shandong University,Shandong university,Baidu,Zhejiang Lab",Object Detection and Pose Estimation,"Category-level object pose estimation demonstrates robust generalization capabilities that benefit robotics applications. However, exclusive reliance on RGB images without leveraging any 3D information introduces ambiguity in the translation and size of objects, leading to suboptimal performance. In this paper, we propose a framework for category-level pose estimation from a single RGB image in an end-to-end manner, i.e., Feature Auxiliary Perception Network (FAP-Net). To address inaccurate pose estimation caused by the inherent ambiguity of RGB images, we design a coarse-to-fine approach that first harnesses geometry supervision to facilitate coarse 3D feature perception and subsequently refines the features based on pose and size constraints. Experimental results on REAL275 and CAMERA25 demonstrate that FAP-Net achieves significant improvements (14.7% on 10Â°10cm and 11.4% on IoU50 on the real-scene REAL275 dataset) over the state-of-the-art and real-time inference (42 FPS)."
Vision-Language Interpreter for Robot Task Planning,"Keisuke Shirai, Cristian Beltran-Hernandez, Masashi Hamaya, Atsushi Hashimoto, Shohei Tanaka, Kento Kawaharazuka, Kazutoshi Tanaka, Yoshitaka Ushiku, Shinsuke Mori","Kyoto University,OMRON SINIC X,OMRON SINIC X Corporation,The University of Tokyo,OMRON SINIC X Corpolation",AI-Based Methods,"Large language models (LLMs) are accelerating the development of language-guided robot planners. Meanwhile, symbolic planners offer the advantage of interpretability. This paper proposes a new task that bridges these two trends, namely, multimodal planning problem specification. The aim is to generate a problem description (PD), a machine-readable file used by the planners to find a plan. By generating PDs from language instruction and scene observation, we can drive symbolic planners in a language-guided framework. We propose a Vision-Language Interpreter (ViLaIn), a new framework that generates PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine generated PDs via error message feedback from the symbolic planner. Our aim is to answer the question: How accurately can ViLaIn and the symbolic planner generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset called the problem description generation (ProDG) dataset. The framework is evaluated with four new evaluation metrics. Experimental results show that ViLaIn can generate syntactically correct problems with more than 99% accuracy and valid plans with more than 58% accuracy."
Trust-Region Neural Moving Horizon Estimation for Robots,"Bingheng Wang, Xuyang Chen, Lin Zhao",National University of Singapore,AI-Based Methods,"Accurate disturbance estimation is essential for safe robot operations. The recently proposed neural moving horizon estimation (NeuroMHE), which uses a portable neural network to model the MHE's weightings, has shown promise in further pushing the accuracy and efficiency boundary. Currently, NeuroMHE is trained through gradient descent, with its gradient computed recursively using a Kalman filter. This paper proposes a trust-region policy optimization method for training NeuroMHE. We achieve this by providing the second-order derivatives of MHE, referred to as the MHE Hessian. Remarkably, we show that many of the intermediate results used to obtain the gradient, especially the Kalman filter, can be efficiently reused to compute the MHE Hessian. This offers linear computational complexity with respect to the MHE horizon. As a case study, we evaluate the proposed trust region NeuroMHE on real quadrotor flight data for disturbance estimation. Our approach demonstrates highly efficient training in under 5 min using only 100 data points. It outperforms a state-of-the-art neural estimator by up to 68.1% in force estimation accuracy, utilizing only 1.4% of its network parameters. Furthermore, our method showcases enhanced robustness to network initialization compared to the gradient descent counterpart."
Multi-Category Decomposition Editing Network for the Accurate Visual Inspection of Texture Defects,"He Zhu, Li Junyi, Hua Yang, Jiankui Chen, Zhouping Yin","Hust,Huazhong University of Science and Technology,Huazhong university of science and technology,Professor, School of Mechanical Scienceand Engineering,Huazhong ",AI-Based Methods,"Spotting blemished areas automatically on a textured surface is a particular challenge, as both nominal and defective surface samples are inconsistent in large-scale industrial manufacturing. The most efficient solution uses the memory bank extracted from the nominal samples to detect outliers. We approach our strategy, the multi-category decomposition editing network (MCDEN), from a similar viewpoint. Notably, we do not use defect-free samples. Instead, we use virtual results to construct a defect library. MCDEN decomposes abnormalities to basic elements from the library while editing outlier features to reconstruct the texture normality, offering a rational segmentation map through decomposition and reconstruction. Based on the strategy, MCDEN is more interpretable than most neural network methods since interpretability is particularly important in industry to ensure stability. Experiments on texture surface samples from the MVTAD dataset confirm the efficacy of MCDEN with a pixel-level AUC score of 96.6%. In other experiments collected from semi-manufactured inkjet printing OLED panels, MCDEN demonstrates competitive results with a 99.2% detection rate and rapid real-time detection capability."
Kinematic-Aware Prompting for Generalizable Articulated Object Manipulation with LLMs,"Wenke Xia, Dong Wang, Xincheng Pang, Zhigang Wang, Bin Zhao, Di Hu, Xuelong Li","Renmin University of China,Shanghai Artificial Intelligence Laboratory,Shanghai AI Laboratory,Northwestern Polytechnical University",AI-Based Methods,"Generalizable articulated object manipulation is essential for home-assistant robots. Recent efforts focus on imitation learning from demonstrations or reinforcement learning in simulation, however, due to the prohibitive costs of real-world data collection and precise object simulation, it still remains challenging for these works to achieve broad adaptability across diverse articulated objects. Recently, many works have tried to utilize the strong in-context learning ability of Large Language Models (LLMs) to achieve generalizable robotic manipulation, but most of these researches focus on high-level task planning, sidelining low-level robotic control. In this work, building on the idea that the kinematic structure of the object determines how we can manipulate it, we propose a kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supporting various object manipulation. To effectively prompt LLMs with the kinematic structure of different objects, we design a unified kinematic knowledge parser, which represents various articulated objects as a unified textual description containing kinematic joints and contact location. Building upon this unified description, a kinematic-aware planner model is proposed to generate precise 3D manipulation waypoints via a designed kinematic-aware chain-of-thoughts prompting method. Our evaluation spanned 48 instances across 16 distinct categories, revealing that our framework not only outperforms traditional methods on 8 seen categories but also shows a powerful zero-shot capability for 8 unseen articulated object categories with only 17 demonstrations. Moreover, the real-world experiments on 7 different object categories prove our framework's adaptability in practical scenarios. Code is released at href{https://github.com/GeWu-Lab/LLM_articulated_object_manipulation}{https://github.com/GeWu-Lab/LLM_articulated_object_manipulation}."
ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning,"Zhehua Zhou, Jiayang Song, Kunpeng Yao, Zhan Shu, Lei Ma","University of Alberta,Massachusetts Institute of Technology,The University of Tokyo & University of Alberta",AI-Based Methods,"Motivated by the substantial achievements of Large Language Models (LLMs) in the field of natural language processing, recent research has commenced investigations into the application of LLMs for complex, long-horizon sequential task planning challenges in robotics. LLMs are advantageous in offering the potential to enhance the generalizability as task-agnostic planners and facilitate flexible interaction between human instructors and planning systems. However, task plans generated by LLMs often lack feasibility and correctness. To address this challenge, we introduce ISR-LLM, a novel framework that improves LLM-based planning through an iterative self-refinement process. The framework operates through three sequential steps: preprocessing, planning, and iterative self-refinement. During preprocessing, an LLM translator is employed to convert natural language input into a Planning Domain Definition Language (PDDL) formulation. In the planning phase, an LLM planner formulates an initial plan, which is then assessed and refined in the iterative self-refinement step by a validator. We examine the performance of ISR-LLM across three distinct planning domains. Our experimental results show that ISR-LLM is able to achieve markedly higher success rates in sequential task planning compared to state-of-the-art LLM-based planners. Moreover, it also preserves the broad applicability and generalizability of working with natural language instructions."
Efficient Hybrid Neuromorphic-Bayesian Model for Olfaction Sensing: Detection and Classification,"Rizwana Kausar, Fakhreddine Zayer, Jaime Viegas, Jorge Dias","Khalifa University,khalifa University",AI-Based Methods,"Abstractâ€”Olfaction sensing in autonomous robotics faces challenges in dynamic operations, energy efficiency, and edge processing. It necessitates a machine learning algorithm capable of managing real-world odor interference, ensuring resource efficiency for mobile robotics, and accurately estimating gas features for critical tasks such as odor mapping, localization, and alarm generation. This paper introduces a hybrid approach that exploits neuromorphic computing in combination with probabilistic inference to address these demanding requirements. Our approach implements a combination of a convolutional spiking neural network for feature extraction and a Bayesian spiking neural network for odor detection and identification. The developed algorithm is rigorously tested on a dataset for sensor drift compensation for robustness evaluation. Additionally, for efficiency evaluation, we compare the energy consumption of our model with a non-spiking machine learning algorithm under identical dataset and operating conditions. Our approach demonstrates superior efficiency alongside comparable accuracy outcomes."
Disentangled Neural Relational Inference for Interpretable Motion Prediction,"Victoria Magdalena Dax, Jiachen Li, Enna Sachdeva, Nakul Agarwal, Mykel Kochenderfer","Stanford University,University of California, Riverside,Honda Research Institute,IIIT Hyderabad",AI-Based Methods,"Effective interaction modeling and behavior prediction of dynamic agents play a significant role in interactive motion planning for autonomous robots. Although existing methods have improved prediction accuracy, few research efforts have been devoted to enhancing prediction modelsâ€™ interpretability and out-of-distribution (OOD) generalizability. This work addresses these two challenging aspects by designing a variational auto-encoder framework that integrates graph-based representations and time-sequence models to efficiently capture spatio-temporal relations between interactive agents and predict their dynamics. Our model infers dynamic interaction graphs in a latent space augmented with interpretable edge features that characterize the interactions. Moreover, we aim to enhance model interpretability and performance in OOD scenarios by disentangling the latent space of edge features, thereby strengthening model versatility and robustness. We validate our approach through extensive experiments on both simulated and real-world datasets. The results show superior performance compared to existing methods in modeling spatio-temporal relations, motion prediction, and identifying time-invariant latent features."
DeFlow: Decoder of Scene Flow Network in Autonomous Driving,"Qingwen Zhang, Yi Yang, Heng Fang, Ruoyu Geng, Patric Jensfelt","KTH Royal Institute of Technology,HONG KONG UNIVERSITY OF SCIENCE AND TECHNOLOGY,KTH - Royal Institute of Technology",AI-Based Methods,"Scene flow estimation determines a scene's 3D motion field, by predicting the motion of points in the scene, especially for aiding tasks in autonomous driving. Many networks with large-scale point clouds as input use voxelization to create a pseudo-image for real-time running. However, the voxelization process often results in the loss of point-specific features. This gives rise to a challenge in recovering those features for scene flow tasks. Our paper introduces DeFlow which enables a transition from voxel-based features to point features using Gated Recurrent Unit (GRU) refinement. To further enhance scene flow estimation performance, we formulate a novel loss function that accounts for the data imbalance between static and dynamic points. Evaluations on the Argoverse 2 scene flow task reveal that DeFlow achieves state-of-the-art results on large-scale point cloud data, demonstrating that our network has better performance and efficiency compared to others. The code is available at https://github.com/KTH-RPL/deflow."
Subequivariant Reinforcement Learning Framework for Coordinated Motion Control,"Haoyu Wang, Xiaoyu Tan, Xihe Qiu, Chao Qu","Shanghai University of Engineering Science,National University of Singapore,Inftech",AI-Based Methods,"Effective coordination is crucial for motion control with reinforcement learning, especially as the complexity of agents and their motions increases. However, many existing methods struggle to account for the intricate dependencies between joints. We introduce CoordiGraph, a novel architecture that leverages subequivariant principles from physics to enhance coordination of motion control with reinforcement learning. This method embeds the principles of equivariance as inherent patterns in the learning process under gravity influence, which aids in modeling the nuanced relationships between joints vital for motion control. Through extensive experimentation with sophisticated agents in diverse environments, we highlight the merits of our approach. Compared to current leading methods, CoordiGraph notably enhances generalization and sample efficiency."
A Large-Scale Suction-Based Climbing Parallel Robot for Wall Painting Application,"Abdur Rosyid, Bashar El-khasawneh",Khalifa University,Industrial Robotics and Automation,"This paper presents a large-scale climbing robot that employs a parallel mechanism with three translational degrees of freedom as its locomotion method. Using a robot frame having a triangular pyramid shape, the robot provides a good stability during the locomotion and task execution. Three suction cups, called the perimeter cups, are attached to the vertices of the robotâ€™s pyramid base, whereas three other suction cups called the middle cups, are attached to the end-effector of the parallel mechanism. The climbing motion is made by attaching and releasing the perimeter and middle cups one after another. The synchronization between the parallel mechanismâ€™s motion and the suction cups during locomotion, as well as the improved gait trajectory, was established to ensure successful climbing. The control scheme of the robot integrates the servo control, the suction control, and the application control in a modular fashion. The successful climbing of the robot proves the scalability of the proposed climbing robot using active suction cups with an optimized design. Finally, a painting application was presented to demonstrate the robotâ€™s capability to perform a wall painting task."
A Collision-Aware Cable Grasping Method in Cluttered Environment,"Lei Zhang, Kaixin Bai, Qiang Li, Zhaopeng Chen, Jianwei Zhang","University of Hamburg,Shenzhen Technology University",Industrial Robotics and Automation,"We introduce a Cable Grasping-Convolutional Neural Network (CG-CNN) designed to facilitate robust cable grasping in cluttered environments. Utilizing physics simulations, we generate an extensive dataset that mimics the intricacies of cable grasping, factoring in potential collisions between cables and robotic grippers. We employ the Approximate Convex Decomposition technique to dissect the non-convex cable model, with grasp quality autonomously labeled based on simulated grasping attempts. The CG-CNN is refined using this simulated dataset and enhanced through domain randomization techniques. Subsequently, the trained model predicts grasp quality, guiding the optimal grasp pose to the robot's controller for execution. Grasping efficacy is assessed across both synthetic and real-world settings. Given our model's implicit collision sensitivity, we achieved commendable success rates of 92.3% for known cables and 88.4% for unknown cables, surpassing contemporary state-of-the-art approaches. Supplementary materials can be found at https://leizhang-public.github.io/cg-cnn/."
A Simple Computationally Efficient Path ILC for Industrial Robotic Manipulators,"Michael Schwegel, Andreas Kugi",TU Wien,Industrial Robotics and Automation,"In this paper, a numerically efficient flexible control scheme for the absolute accuracy of industrial robots is presented and experimentally validated. A model-based controller that leverages all typically available parameters is combined with an online path iterative learning controller (ILC). The ILC law is employed to compensate for the unknown residual error dynamics caused by elastic and transmission effects. The proposed approach combines several benefits, including the possibility of a continuous execution of trials, a straightforward generalization of the learned data to different execution speeds, and learning from partial trials. The experimental validations on a 6-axis industrial robot with a laser tracker absolute measurement system show a $95%$ improvement in absolute accuracy after two trials. When the laser tracker is removed, the learned feedforward controller can sustain the accuracy achieved even without trial-by-trial learning."
RoboGrind: Intuitive and Interactive Surface Treatment with Industrial Robots,"Benjamin Alt, Florian StoÌˆckl, Silvan Müller, Christopher Braun, Julian Raible, Saad Alhasan, Oliver Rettig, Lukas Daniel Ringle, Darko Katic, Rainer Jäkel, Michael Beetz, Marcus Strand, Marco F. Huber","ArtiMinds Robotics,DHBW Karlsruhe,Baden-Württemberg Cooperative State University,University of Stuttgart, Institute of Industrial Manufacturing a,University of Stuttgart,DHBW - Karlsruhe,Karlsruhe Institute for Technology (KIT),Karlsruhe Institute of Technology,University of Bremen,,Baden-Wuerttemberg Cooperative State University Karlsruhe",Industrial Robotics and Automation,"Surface treatment tasks such as grinding, sanding or polishing are a vital step of the value chain in many industries, but are notoriously challenging to automate. We present RoboGrind, an integrated system for the intuitive, interactive automation of surface treatment tasks with industrial robots. It combines a sophisticated 3D perception pipeline for surface scanning and automatic defect identification, an interactive voice controlled wizard system for the AI-assisted bootstrapping and parameterization of robot programs, and an automatic planning and execution pipeline for force-controlled robotic surface treatment. RoboGrind is evaluated both under laboratory and real-world conditions in the context of refabricating fiberglass wind turbine blades."
An LLM-Driven Framework for Multiple-Vehicle Dispatching and Navigation in Smart City Landscapes,"Ruiqing Chen, Wenbin Song, Weiqin Zu, Zixin Dong, Ze Guo, Fanglei Sun, Zheng Tian, Jun Wang","ShanghaiTech University,Shanghaitech University,Harbin Institute of Technology,ShangTech University,University College London",Industrial Robotics and Automation,"In the context of smart cities, autonomous vehicles, such as unmanned delivery vehicles and taxis are gradually gaining acceptance. However, their application scenarios remain significantly fragmented. Typically, an Autonomous Multi-Functional Vehicle (AMFV) is not engaged in other scenarios when idle in a specific one. Currently, a unified system capable of coordinating and using these resources efficiently is lacking. Moreover, there is an absence of an advanced navigation algorithm for facilitating coordinated navigation among Heterogeneous Vehicles (HVs). To address these issues, we propose the LLM-driven Multi-vehicle Dispatching and navigation (LiMeda) framework. It comprises an LLM-driven scheduling module that facilitates efficient allocation considering task scenarios and vehicle information, which addresses the issue of incompatible vehicle resources across various smart city scenarios. And the other is a navigation module, founded on the Heterogeneous Agent Reinforcement Learning (HARL) framework we previously proposed, which can effectively perform cooperative navigation tasks among heterogeneous agents, assisting the cooperative task completion by HVs in a smart city. Experimental results show our method outperforms both traditional scheduling algorithms and Reinforcement Learning navigation algorithms in metric terms. Additionally, it shows remarkable scalability and generalization under varying city scales, vehicle numbers, and task numbers."
SCRNet: A Retinex Structure-Based Low-Light Enhancement Model Guided by Spatial Consistency,"Miao Zhang, Yiqing Shen, Shenghui Zhong, Guofeng Pan, Shuai Lu","Tsinghua Shenzhen International Graduate School,Johns Hopkins University,Zhongfa Aviation Instituteï¼ŒBeihang University,Shenzhen Yijiahe Technologies",Industrial Robotics and Automation,"Images captured by robotics under low-light conditions are often plagued by several challenges, including diminished contrast, increased noise, loss of fine details, and unnatural color reproduction. These factors can significantly hinder the performance of computer vision tasks such as object detection and image segmentation. As a result, improving the quality of low-light images is of paramount importance for practical applications in the computer vision domain. To effectively address these challenges, we present a novel low-light image enhancement model, termed Spatial Consistency Retinex Network (SCRNet), which leverages the Retinex-based structure and is guided by the principle of spatial consistency. Specifically, our proposed model incorporates three levels of consistency: channel level, semantic level, and texture level, inspired by the principle of spatial consistency. These levels of consistency enable our model to adaptively enhance image features, ensuring more accurate and visually pleasing results. Extensive experimental evaluations on various low-light image datasets demonstrate that our proposed SCRNet outshines existing state-of-the-art methods, highlighting the potential of SCRNet as an effective solution for enhancing low-light images."
Autonomous Field-Of-View Adjustment Using Adaptive Kinematic Constrained Control with Robot-Held Microscopic Camera Feedback,"Hung-ching Lin, Murilo Marinho, Kanako Harada","University of Tokyo,The University of Manchester,The University of Tokyo",Industrial Robotics and Automation,"Robotic systems for manipulation in millimeter scale often use a camera with high magnification for visual feedback of the target region. However, the limited field-of-view (FoV) of the microscopic camera necessitates camera motion to capture a broader workspace environment. In this work, we propose an autonomous robotic control method to constrain a robot-held camera within a designated FoV. Furthermore, we model the camera extrinsics as part of the kinematic model and use camera measurements coupled with a U-Net based tool tracking to adapt the complete robotic model during task execution. As a proof-of-concept demonstration, the proposed framework was evaluated in a bi-manual setup, where the microscopic camera was controlled to view a tool moving in a pre-defined trajectory. The proposed method allowed the camera to stay 94.1% of the time within the real FoV, compared to 54.4% without the proposed adaptive control."
RoSSO: A High-Performance Python Package for Robotic Surveillance Strategy Optimization Using JAX,"Yohan John, Connor Hughes, Gilberto Diaz-garcia, Jason Marden, Francesco Bullo","UC Santa Barbara,University of California, Santa Barbara,University of Colorado at Boulder,UCSB",Industrial Robotics and Automation,"To enable the computation of effective randomized patrol routes for single- or multi-robot teams, we present RoSSO, a Python package designed for solving Markov chain optimization problems. We exploit machine-learning techniques such as reverse-mode automatic differentiation and constraint parametrization to achieve superior efficiency compared to general-purpose nonlinear programming solvers. Additionally, we supplement a game-theoretic stochastic surveillance formulation in the literature with a novel greedy algorithm and multi-robot extension. We close with numerical results for a police district in downtown San Francisco that demonstrate RoSSO's capabilities on our new formulations and the prior work."
Semi-Autonomous Surface-Tracking Tasks Using Omnidirectional Mobile Manipulators,"Carlos Suarez Zapico, Y. R. Petillot, Mustafa Suphi Erden","Edinburgh Centre for Robotics,Heriot-Watt University",Industrial Robotics and Automation,"Despite the potential of mobile manipulators and applications where robots require a force-controlled physical interaction with the environment, the majority of robot automation nowadays is still based on fixed manipulators for free-motion tasks (e.g. welding, pick and place, or painting). In this work, we propose a control solution for omnidirectional mobile manipulators in force-tracking tasks, interacting with unknown surface geometries and with a human teleoperator in the control loop. Keeping a teleoperator in the loop makes the system widely applicable to unstructured environments. A human can take care, with little effort, of the mobile base navigation, self-collisions, and collisions with the environment, as well as selecting the area of the asset surface to process. The teleoperator interfaces with the robot platform by commanding motion in the mobile base in order to increase the workspace and maneuverability of the arm. The operator can also command the movement of the end-effector, sliding on the surface geometry to process a specific area. Alternatively, he can let the controller execute a parametric trajectory (spiral or raster) for an autonomous area coverage and meanwhile command the base in order to keep the arm in configurations with good dexterity. The autonomous controller, on the other hand, takes responsibility for following the unknown contour on the manipulated surface by only taking observations from a force/torque sensor attached to the arm's wrist, exerting a prescribed force, and handling the motion control in the base and the arm so that both can follow their respective task requests. Overall, we have developed a user-friendly control scheme, where an operator with little training and using a joystick, can guide the robot system to perform a physically interactive task on the surface of an asset."
Towards Optimal Lane-Changing Coordination of CAVs in Multi-Lane Mixed Traffic Scenarios,"Yan Ding, Yijun Mao, Chongshan Jiao, Pengju Ren",Xi'an Jiaotong University,Intelligent Transportation Systems I,"Lane changing is a fundamental but challenging operation for moving vehicles. Connected and Automated Vehicles(CAVs) enable autonomous vehicles to cooperate with each other to accomplish the lane changing tasks, profiting from their communication ability. However, dispatching CAVs in mixed traffic remains difficult due to the stochastic behaviors and uncertain intentions of Human-Driven Vehicles(HDVs). To tackle this issue, this paper devises a coordination approach based on Conflict-Based Search(CBS) theory. Firstly, HDVs are accurately modeled as constraints to enable usage of CBS in the mixed traffic. Additionally, virtual goals are introduced to search CAVsâ€™ priority and outlets along with path finding. Furthermore, we optimize the performance of CBS in dense traffic by defining the concept of following vehicles. Experiments show that performance is improved by utilizing new conflict prioritizing rules and a heuristic value calculation method that derived from following vehicles. Finally, we introduce grouping vehicles to extend the proposed method for solving extremely dense and large instances at a scale of more than one hundred without significant loss in efficiency."
Reducing Non-IID Effects in Federated Autonomous Driving with Contrastive Divergence Loss,"Tuong Do, Binh Nguyen, Quang Tran, Hien Nguyen, Erman Tjiputra, Te-chuan Chiu, Anh Nguyen","AIOZ,National Tsing Hua University,University of Liverpool",Intelligent Transportation Systems I,"Federated learning has been widely applied in autonomous driving since it enables training a learning model among vehicles without sharing users' data. However, data from autonomous vehicles usually suffer from the non-independent-and-identically-distributed (non-IID) problem, which may cause negative effects on the convergence of the learning process. In this paper, we propose a new contrastive divergence loss to address the non-IID problem in autonomous driving by reducing the impact of divergence factors from transmitted models during the local learning process of each silo. We also analyze the effects of contrastive divergence in various autonomous driving scenarios, under multiple network infrastructures, and with different centralized/distributed learning schemes. Our intensive experiments on three datasets demonstrate that our proposed contrastive divergence loss significantly improves the performance over current state-of-the-art approaches."
ODD-Based Query-Time Scenario Mutation Framework for Autonomous Driving Scenario Databases,"Yun Tang, Dhanush Raj, Xingyu Zhao, Xizhe Zhang, Antonio Anastasio Bruto Da Costa, Siddartha Khastgir, Paul Jennings","University of Warwick,WMG, University of Warwick, UK",Intelligent Transportation Systems I,"Large-scale scenario databases may contain hundreds of thousands of scenarios for the verification and validation (V&V) of autonomous vehicles (AV). Scenarios in the database are often labelled with semantic Operational Design Domain (ODD) tags (e.g., WeatherRainy, RoadTypeHighway and ActorTypeTruck) to be queried via exact tag matching. Such a scenario database design has two major limitations, i.e. combinatorial scenario generation inevitably leads to many redundant scenarios, and each ODD query matches only a small number of scenarios in the database (0.2% in our case study), rendering most of the database wealth wasted. We propose a novel scenario database design and the first ODD-based query-time scenario mutation framework to address the limitations. Our case study results show that the proposed framework has the potential to fully utilize all the database scenarios at query time while eliminating scenario redundancy in the database (in our case study, given the same ODD query, the number of final matched scenarios increased by 36 times, diversity increased by 99 times, and scenario database utilization rate increased from 0.2% to 36%)."
Cooperation for Scalable Supervision of Autonomy in Mixed Traffic,"Cameron Hickert, Sirui Li, Cathy Wu","Massachusetts Institute of Technology,MIT",Intelligent Transportation Systems I,"Advances in autonomy offer the potential for dramatic positive outcomes in a number of domains, yet enabling their safe deployment remains an open problem. This workâ€™s motivating question is: In safety-critical settings, can we avoid the need to have one human supervise one machine at all times? The work formalizes this scalable supervision problem by considering remotely located human supervisors and investigating how autonomous agents can cooperate to achieve safety. This article focuses on the safety-critical context of autonomous vehicles (AVs) merging into traffic consist- ing of a mixture of AVs and human drivers. The analysis establishes high reliability upper bounds on human supervision requirements. It further shows that AV cooperation can improve supervision reliability by orders of magnitude and counterintuitively requires fewer supervisors (per AV) as more AVs are adopted. These analytical results leverage queuing-theoretic analysis, order statistics, and a conservative, reachability-based approach. A key takeaway is the potential value of cooperation in enabling the deployment of autonomy at scale. While this work focuses on AVs, the scalable supervision framework may be of independent interest to a broader array of autonomous control challenges."
Hierarchical Learned Risk-Aware Planning Framework for Human Driving Modeling,"Nathan Ludlow, Yiwei Lyu, John Dolan","Brigham Young University,Carnegie Mellon University",Intelligent Transportation Systems I,"This paper presents a novel approach to modeling human driving behavior, designed for use in evaluating autonomous vehicle control systems in a simulation environments. Our methodology leverages a hierarchical forward-looking, risk-aware estimation framework with learned parameters to generate human-like driving trajectories, accommodating multiple driver levels determined by model parameters. This approach is grounded in multimodal trajectory prediction, using a deep neural network with LSTM-based social pooling to predict the trajectories of surrounding vehicles. These trajectories are used to compute forward-looking risk assessments along the ego vehicle's path, guiding its navigation. Our method aims to replicate human driving behaviors by learning parameters that emulate human decision-making during driving. We ensure that our model exhibits robust generalization capabilities by conducting simulations, employing real-world driving data to validate the accuracy of our approach in modeling human behavior. The results reveal that our model effectively captures human behavior, showcasing its versatility in modeling human drivers in diverse highway scenarios."
DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for Trajectory Prediction,"Rezaul Karim, Soheil Mohamad Alizadeh Shabestary, Amir Rasouli","York University,Huawei Technologies Canada",Intelligent Transportation Systems I,"Predicting temporally consistent road users' trajectories in a multi-agent setting is a challenging task due to the unknown characteristics of agents and their varying intentions. Besides using semantic map information and modeling interactions, it is important to build an effective mechanism capable of reasoning about behaviors at different levels of granularity. To this end, we propose Dynamic goal quErieS with temporal Transductive alIgNmEnt (DESTINE) method. Unlike prior approaches, our approach 1) dynamically predicts agents' goals irrespective of particular road structures, such as lanes, allowing the method to produce a more accurate estimation of destinations; 2) achieves map-compliant predictions by generating future trajectories in a coarse-to-fine fashion, where the coarser predictions at a lower frame rate serve as intermediate goals; and 3) uses an attention module designed to temporally align predicted trajectories via a masked attention operation. Using the common Argoverse benchmark dataset, we show that our method achieves state-of-the-art performance on various metrics, and further investigate the contributions of proposed modules via comprehensive ablation studies."
Parallel Optimization with Hard Safety Constraints for Cooperative Planning of Connected Autonomous Vehicles,"Zhenmin Huang, Haichao Liu, Shaojie Shen, Jun Ma","The Hong Kong University of Science and Technology,Hong Kong University of Science and Technology",Intelligent Transportation Systems I,"The development of connected autonomous vehicles (CAVs) facilitates the enhancement of traffic efficiency in complicated scenarios. Difficulties remain unsolved in developing an effective and efficient coordination strategy for CAVs. In this paper, we formulate the cooperative autonomous driving task of CAVs as an optimal control problem with safety conditions enforced as hard constraints, and propose a computationally-efficient parallel optimization framework to generate strategies for CAVs with the travel efficiency improved and the hard safety constraints satisfied. Specifically, all constraints involved are addressed appropriately with convex approximation, such that the convexity property of the reformulated optimization problem is exhibited. Then, a parallel optimization algorithm is presented to solve the reformulated optimization problem, with an embodied iterative nearest neighbor search strategy to determine the optimal passing sequence. It is noteworthy that the travel efficiency is enhanced and the computation burden is considerably alleviated with the proposed innovation development. We also examine the proposed method in CARLA simulator and perform thorough comparisons to demonstrate the effectiveness and efficiency of the proposed approach."
Editing Driver Character: Socially-Controllable Behavior Generation for Interactive Traffic Simulation,"Wei-jer Chang, Chen Tang, Chenran Li, Yeping Hu, Masayoshi Tomizuka, Wei Zhan","University of California, Berkeley,University of California Berkeley,University of California,Univeristy of California, Berkeley",Intelligent Transportation Systems I,"Traffic simulation plays a crucial role in evaluating and improving autonomous driving planning systems. After being deployed on public roads, autonomous vehicles need to interact with human road participants with different social preferences (e.g., selfish or courteous human drivers). To ensure that autonomous vehicles take safe and efficient maneuvers in different interactive traffic scenarios, we should be able to evaluate autonomous vehicles against reactive agents with different social characteristics in the simulation environment. We propose a socially-controllable behavior generation (SCBG) model for this purpose, which allows the users to specify the level of courtesy of the generated trajectory while ensuring realistic and human-like trajectory generation through learning from real-world driving data. Specifically, we define a novel and differentiable measure to quantify the level of courtesy of driving behavior, leveraging marginal and conditional behavior prediction models trained from real-world driving data. The proposed courtesy measure allows us to auto-label the courtesy levels of trajectories from real-world driving data and conveniently train an SCBG model generating trajectories based on the input courtesy values. We examine SCBG on the WOMD and show that we are able to control the SCBG model to generate realistic behaviors with desired courtesy levels. SCBG is able to identify different motion patterns of courteous behaviors according to the scenarios."
Cognitive-Digital-Twin-Based Driving Assistance,"Junyu Diao, Renzhi Tang, Yi Gu, Sen Tian, Zhihao Jiang","Shanghaitech University,ShanghaiTech University,Shanghaitech Technology,Southwestern University of Finance and Economics",Intelligent Transportation Systems I,"Advanced driver assistance systems (ADAS) have been developed to enhance driving safety by issuing timely warnings to drivers. However, current ADAS do not take into account the driver's cognitive state when delivering warnings, which can result in false alarms and impact the driver's trust in the system. To address this issue, we propose a Cognitive Digital-twin-based Assistance System (CDAS) that issues warnings tailored to the driver's perception of the driving environment and driving style. In this paper, we present a model of the driver's decision-making process that explicitly captures their perception of the driving environment, their utility evaluation of predicted future environments, and their driving style in terms of minimum acceptable risk. The cognitive digital twin of the driver is then created and updated by minimizing the discrepancy between the predicted and actual behaviors of the driver. With the cognitive digital twin, the CDAS warns the driver when there is a significant discrepancy between the predicted driving strategy based on partial observation and that based on full observation. This approach can more accurately identify risks that the driver is not aware of and provide warnings only when necessary. We conducted human and simulated experiments in a virtual driving environment, and our results demonstrate that our proposed CDAS has a similar perception of risky behaviors compared to humans. Furthermore, the digital twin learning framework can identi"
POLITE: Preferences Combined with Highlights in Reinforcement Learning,"Simon Holk, Daniel Marta, Iolanda Leite",KTH Royal Institute of Technology,Human-Robot Interaction I,"Many solutions to address the challenge of robot learning have been devised, namely through exploring novel ways for humans to communicate complex goals and tasks in reinforcement learning (RL) setups. One way that experienced recent research interest directly addresses the problem by considering human feedback as preferences between pairs of trajectories (sequences of state-action pairs). However, when simply attributing a single preference to a pair of trajectories that contain many agglomerated steps, key pieces of information are lost in the process. We amplify the initial definition of preferences to account for highlights: state-action pairs of relatively high information (high/low reward) within a preferred trajectory. To include the additional information, we design novel regularization methods within a preference learning framework. To this extent, we present our method which is able to greatly reduce the necessary amount of preferences, by permitting the highlighting of favoured trajectories, in order to reduce the entropy of the credit assignment. We show the effectiveness of our work in both simulation and a user study, which analyzes the feedback given and its implications. We also use the total collected feedback to train a robot policy for socially compliant trajectories in a simulated social navigation environment. We release code and video examples at https://sites.google.com/view/rl-polite"
CoFRIDA: Self-Supervised Fine-Tuning for Human-Robot Co-Painting,"Peter Schaldenbrand, Gaurav Parmar, Jun-yan Zhu, James Mccann, Jean Oh",Carnegie Mellon University,Human-Robot Interaction I,"Prior robot painting and drawing work, such as FRIDA, has focused on decreasing the sim-to-real gap and expanding input modalities for users, but the interaction with these systems generally exists only in the input stages. To support interactive, human-robot collaborative painting, we introduce the Collaborative FRIDA (CoFRIDA) robot painting framework, which can co-paint by modifying and engaging with content already painted by a human collaborator. To improve text-image alignment, FRIDA's major weakness, our system uses pre-trained text-to-image models; however, pre-trained models in the context of real-world co-painting do not perform well because they (1) do not understand the constraints and abilities of the robot and (2) cannot perform co-painting without making unrealistic edits to the canvas and overwriting content. We propose a self-supervised fine-tuning procedure that can tackle both issues, allowing the use of pre-trained state-of-the-art text-image alignment models with robots to enable co-painting in the physical world. Our open-source approach, CoFRIDA, creates paintings and drawings that match the input text prompt more clearly than FRIDA, both from a blank canvas and one with human created work. More generally, our fine-tuning procedure successfully encodes the robot's constraints and abilities into a pre-trained text-to-image model, showcasing promising results as an effective method for reducing sim-to-real gaps."
MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments,"Junwei Zheng, Jiaming Zhang, Kailun Yang, Kunyu Peng, Rainer Stiefelhagen","Karlsruhe Institute of Technology,Hunan University",Human-Robot Interaction I,"People with Visual Impairments (PVI) typically recognize objects through haptic perception. Knowing objects and materials before touching is desired by the target users but under-explored in the field of human-centered robotics. To fill this gap, in this work, a wearable vision-based robotic system, MateRobot, is established for PVI to recognize materials and object categories beforehand. To address the computational constraints of mobile platforms, we propose a lightweight yet accurate model MateViT to perform pixel-wise semantic segmentation, simultaneously recognizing both objects and materials. Our methods achieve respective 40.2% and 51.1% of mIoU on COCOStuff-10K and DMS datasets, surpassing the previous method with +5.7% and +7.0% gains. Moreover, on the field test with participants, our wearable system reaches a score of $28$ in the NASA-Task Load Index, indicating low cognitive demands and ease of use. Our MateRobot demonstrates the feasibility of recognizing material property through visual cues and offers a promising step towards improving the functionality of wearable robots for PVI. The source code has been made publicly available at https://junweizheng93.github.io/publications/MATERobot/MATE Robot.html."
Robot-Assisted Navigation for Visually Impaired through Adaptive Impedance and Path Planning,"Pietro Balatti, Idil Özdamar, Doganay Sirintuna, Luca Fortini, Mattia Leonori, Juan M. Gandarias, Arash Ajoudani","Istituto Italiano di Tecnologia,HRI, Lab., Istituto Italiano di Tecnologia. Dept. of Informatics,University of Malaga",Human-Robot Interaction I,"This paper presents a framework to navigate visually impaired people through unfamiliar environments by means of a mobile manipulator. The Human-Robot system consists of three key components: a mobile base, a robotic arm, and the human subject who gets guided by the robotic arm via physically coupling their hand with the cobot's end-effector. These components, receiving a goal from the user, traverse a collision-free set of waypoints in a coordinated manner, while avoiding static and dynamic obstacles through an obstacle avoidance unit and a novel human guidance planner. With this aim, we also present a legs tracking algorithm that utilizes 2D LiDAR sensors integrated into the mobile base to monitor the human pose. Additionally, we introduce an adaptive pulling planner responsible for guiding the individual back to the intended path if they veer off course. This is achieved by establishing a target arm end-effector position and dynamically adjusting the impedance parameters in real-time through a impedance tuning unit. To validate the framework we present a set of experiments both in laboratory settings with 12 healthy blindfolded subjects and a proof-of-concept demonstration in a real-world scenario."
Incremental Learning of Full-Pose Via-Point Movement Primitives on Riemannian Manifolds,"Tilman Daab, Noémie Jaquier, Christian R. G. Dreher, Andre Meixner, Franziska Krebs, Tamim Asfour",Karlsruhe Institute of Technology (KIT),Human-Robot Interaction I,"Movement primitives (MPs) are compact representations of robot skills that can be learned from demonstrations and combined into complex behaviors. However, merely equipping robots with a fixed set of innate MPs is insufficient to deploy them in dynamic and unpredictable environments. Instead, the full potential of MPs remains to be attained via adaptable, large-scale MP libraries. In this paper, we propose a set of seven fundamental operations to incrementally learn, improve, and re-organize MP libraries. To showcase their applicability, we provide explicit formulations of the spatial operations for libraries composed of Via-Point Movement Primitives (VMPs). By building on Riemannian manifold theory, our approach enables the incremental learning of all parameters of position and orientation VMPs within a library. Moreover, our approach stores a fixed number of parameters, thus complying with the essential principles of incremental learning. We evaluate our approach to incrementally learn a VMP library from sequentially-provided motion capture data."
Supernumerary Robotic Limbs to Support Post-Fall Recoveries for Astronauts,"Erik Ballesteros, Sang-Yoep Lee, Kalind Carpenter, Harry Asada","Massachusetts Institute of Technology,Seoul National University,Jet Propulsion Laboratory,MIT",Human-Robot Interaction I,"This paper proposes the utilization of Supernumerary Robotic Limbs (SuperLimbs) for augmenting astronauts during an Extra-Vehicular Activity (EVA) in a partial-gravity environment. We investigate the effectiveness of SuperLimbs in assisting astronauts to their feet following a fall. Based on preliminary observations from a pilot human study, we categorized post-fall recoveries into a sequence of statically stable poses called ``waypoints"". The paths between the waypoints can be modeled with a simplified kinetic motion applied about a specific point on the body. Following the characterization of post-fall recoveries, we designed a task-space impedance control with high damping and low stiffness, where the SuperLimbs provide an astronaut with assistance in post-fall recovery while keeping the human-in-the-loop scheme. In order to validate this control scheme, a full-scale wearable analog space suit was constructed and tested with a SuperLimbs prototype. Results from the experimentation found that without assistance, astronauts would impulsively exert themselves to perform a post-fall recovery, which resulted in high energy consumption and instabilities maintaining an upright posture, concurring with prior NASA studies. When the SuperLimbs provided assistance, the astronaut's energy consumption and deviation in their tracking as they performed a post-fall recovery was reduced considerably."
Lissajous Curve-Based Vibrational Orbit Control of a Flexible Vibrational Actuator with a Structural Anisotropy,"Yuto Miyazaki, Mitsuru Higashimori","Graduate School of Engineering, Osaka University,Osaka University",Mechanisms and Design,"This paper proposes a novel flexible vibrational actuator with a structural anisotropy and its control method to diversify the vibrational behavior. First, the analytical model of the proposed actuator, which comprises a rectangular cross-sectional flexible beam and a rotational-type motor, is introduced. Regarding the structural anisotropy, the rotational axis of the motor is nonparallel to both principal axes of bending stiffness of the beam. Then, the vibrational phenomenon of the actuator is theoretically revealed. It is shown that using the synthetic wave input constituting two sine waves based on the resonance frequencies for the principal axes of the beam, the vibrational orbit of the tip of the beam can be controlled in the same manner as the Lissajous curve. Finally, the proposed method is experimentally validated. The Lissajous curve-based vibrational orbit control is performed using a prototype actuator. Furthermore, an application to underactuated-type locomotor is demonstrated."
Design and Modeling of a Nested Bi-Cavity-Based Soft Growing Robot for Grasping in Constrained Environments,"Haochen Yong, Fukang Xu, Chenfei Li, Han Ding, Zhigang Wu",Huazhong University of Science and Technology,Mechanisms and Design,"Soft growing robots with unique navigation (tip extension by eversion) hold great promise in rescue, medical, and industrial applications. Equipping them with grasping capability would enhance their usefulness in constrained environments for various applications. However, in traditional designs, the tipâ€™s eversion naturally conflicts with grasping, and the addition of grippers at the tip would limit navigation inevitably in constrained environments. To realize grasping in such scenes without extra devices, we propose a nested bi-cavity-based growing soft robot (BIBOT). The new design consists of two coaxially nested cavities, where the inner and outer cavities extend synchronously by inversion and eversion of the film rolls. Such a bi-cavity design enables the BIBOT to navigate and grasp without relative movements between the body and environment, and avoids contact between the object and its surroundings as well. Further, a kinematics model is established and verified to precisely control its lengthening and steering by a feed mechanism. Finally, its capability in a constrained environment is demonstrated by navigating and grasping an object in a curved pipe with a variable internal diameter."
Dynamic Modeling of Wing-Assisted Inclined Running with a Morphing Multi-Modal Robot,"Eric Sihite, Alireza Ramezani, Gharib Morteza","California Institute of Technology,Northeastern University,CALTECH",Mechanisms and Design,"Robot designs can take many inspirations from nature, where there are many examples of highly resilient and fault-tolerant locomotion strategies to navigate complex terrains by using multi-functional appendages. For example, Chukar and Hoatzin birds can repurpose their wings for quadrupedal walking and wing-assisted incline running (WAIR) to climb steep surfaces. We took inspiration from nature and designed a morphing robot with multi-functional thruster-wheel appendages that allows the robot to change its mode of locomotion by transforming into a rover, quad-rotor, mobile inverted pendulum (MIP), and other modes. In this work, we derive a dynamic model and formulate a nonlinear model predictive controller to perform WAIR to showcase the unique capabilities of our robot. We implemented the model and controller in a numerical simulation and experiments to show their feasibility and the capabilities of our transforming multi-modal robot."
Optimized Design and Fabrication of Skeletal Muscle Actuators for Bio-Syncretic Robots,"Lianchao Yang, Chuang Zhang, Ruiqian Wang, Yiwei Zhang, Lianqing Liu","Shenyang Institute of Automation, Chinese Academy of Sciences.,Shenyang Institute of Automation Chinese Academy of Sciences,Shenyang Institute of automation,Chinese Academy of Sciences,Shenyang Institute of Automation, Chinese Academy of Sciences,Shenyang Institute of Automation",Mechanisms and Design,"In recent years, bio-syncretic robots actuated by living materials have received widespread attention. Among the common living materials, engineered skeletal muscle tissue (eSKT) has been the focus of researchers due to its high contraction force and good controllability. However, the current performance of eSKT is far from that of natural skeletal muscle tissue. In this paper, an optimized design method for eSKTs has been proposed. By combining simulation analysis with experiments, the eSKTs with multiple strips have been developed. The results show that under a specific volume (250 Î¼L), the optimized strip structures can enhance the stability of eSKT and facilitate the penetration of nutrients and oxygen, leading to improved fusion of myoblasts and the directional arrangement of myotubes, thus improving the performance of eSKT. The eSKT with multiple strips exhibits a significant contraction force and has been successfully utilized in a bio-syncretic robot to demonstrate its actuation capability. This work may provide insights into the development of the field of bio-syncretic robots and even tissue engineering."
Stochastic Implicit Neural Signed Distance Functions for Safe Motion Planning under Sensing Uncertainty,"Carlos Quintero-Pena, Wil Thomason, Zachary Kingston, Anastasios Kyrillidis, Lydia Kavraki",Rice University,Planning under Uncertainty II,"Motion planning under sensing uncertainty is critical for robots in unstructured environments, to guarantee safety for both the robot and any nearby humans. Most work on planning under uncertainty does not scale to high-dimensional robots such as manipulators, assumes simplified geometry of the robot or environment, or requires per-object knowledge of noise. Instead, we propose a method that directly models sensor-specific aleatoric uncertainty to find safe motions for high-dimensional systems in complex environments, without exact knowledge of environment geometry. We combine a novel implicit neural model of stochastic signed distance functions with a hierarchical optimization-based motion planner to plan low-risk motions without sacrificing path quality. Our method also explicitly bounds the risk of the path, offering trustworthiness. We empirically validate that our method produces safe motions and accurate risk bounds and is safer than baseline approaches."
Constrained Hierarchical Monte Carlo Belief-State Planning,"Arec Jamgochian, Hugo Buurmeijer, Kyle Wray, Anthony Corso, Mykel Kochenderfer","Stanford University,n/a",Planning under Uncertainty II,"Optimal plans in Constrained Partially Observable Markov Decision Processes (CPOMDPs) maximize reward objectives while satisfying hard cost constraints, generalizing safe planning under state and transition uncertainty. Unfortunately, online CPOMDP planning is extremely difficult in large or continuous problem domains. In many large robotic domains, hierarchical decomposition can simplify planning by using tools for low-level control given high-level action primitives (options). We introduce Constrained Options Belief Tree Search (COBeTS) to leverage this hierarchy and scale online search-based CPOMDP planning to large robotic problems. We show that if primitive option controllers are defined to satisfy assigned constraint budgets, then COBeTS will satisfy constraints anytime. Otherwise, COBeTS will guide the search towards a safe sequence of option primitives, and hierarchical monitoring can be used to achieve runtime safety. We demonstrate COBeTS in several safety-critical, constrained partially observable robotic domains, showing that it can plan successfully in continuous CPOMDPs while non-hierarchical baselines cannot."
Estimating 3D Uncertainty Field: Quantifying Uncertainty for Neural Radiance Fields,"Jianxiong Shen, Ruijie Ren, Adrià Ruiz, Francesc Moreno-Noguer","IRI, CSIC-UPC,IRI-CSIC,INRIA,CSIC",Planning under Uncertainty II,"Current methods based on Neural Radiance Fields (NeRF) significantly lack the capacity to quantify uncertainty in their predictions, particularly on the unseen space including the occluded and outside scene content. This limitation hinders their extensive applications in robotics, where the reliability of model predictions has to be considered for robotic exploration and planning in the unknown environments. To address this, we propose a novel approach to estimate a 3D Uncertainty Field based on the learned incomplete scene geometry, which explicitly identifies these unseen regions in the scene. By considering the accumulated transmittance along each camera ray, our Uncertainty Field infers 2D pixel-wise uncertainty, exhibiting high values for rays directly casting towards occluded or external scene content. To quantify the uncertainty on the learned surface, we model a stochastic radiance field. Our experiments demonstrate that our approach is the only one that can explicitly reason about high uncertainty both on 3D unseen regions and its involved 2D rendered pixels, compared with recent methods. Furthermore, we illustrate that our designed uncertainty field is ideally suited for real-world robotics tasks, such as next-best-view selection."
Online Adaptation of Sampling-Based Motion Planning with Inaccurate Models,"Marco Faroni, Dmitry Berenson","Politecnico di Milano,University of Michigan",Planning under Uncertainty II,"Robotic manipulation relies on analytical or learned models to simulate the system dynamics. These models are often inaccurate and based on offline information, so that the robot planner is unable to cope with mismatches between the expected and the actual behavior of the system (e.g., the presence of an unexpected obstacle). These situations require the robot to use information gathered online to correct its planning strategy and adapt to the actual system response. We propose a sampling-based motion planning approach that uses an estimate of the model error and online observations to correct the planning strategy at each new replanning. Our approach adapts the cost function and the sampling distribution of a sampling-based kinodynamic motion planner when the outcome of the executed transitions is different from the expected one (e.g., when the robot unexpectedly collides with an obstacle) so that future trajectories will avoid unreliable motions. To infer the properties of a new transition, we introduce the notion of context-awareness, i.e., we store local environment information for each executed transition and avoid new transitions with context similar to previous unreliable ones. This is helpful for leveraging online information even if the simulated transitions are far (in the state-and-action space) from the executed ones. Simulation and experimental results show that the proposed approach increases the success rate in execution and reduces the number of replannings needed to reach the goal."
Autonomous 3D Exploration in Large-Scale Environments with Dynamic Obstacles,"Emil Wiman, Ludvig Widén, Mattias Tiger, Fredrik Heintz","Linköping University,AI and Integrated Computer Systems (AIICS), Linköping University",Planning under Uncertainty II,"Exploration in dynamic and uncertain real-world environments is an open problem in robotics and it constitutes a foundational capability of autonomous systems operating in most of the real-world. While 3D exploration planning has been extensively studied, the environments are assumed static or only reactive collision avoidance is carried out. We propose a novel approach to not only avoid dynamic obstacles but also include them in the plan itself, to deliberately exploit the dynamic environment in the agent's favor. The proposed planner, Dynamic Autonomous Exploration Planner (DAEP), extends AEP to explicitly plan with respect to dynamic obstacles. Furthermore, addressing prior errors within AEP in DAEP has resulted in enhanced exploration within static environments. To thoroughly evaluate exploration planners in such settings we propose a new enhanced benchmark suite with several dynamic environments, including large-scale outdoor environments. DAEP outperforms state-of-the-art planners in dynamic and large-scale environments and is shown to be more effective at both exploration and collision avoidance."
MTG: Mapless Trajectory Generator with Traversability Coverage for Outdoor Navigation,"Jing Liang, Peng Gao, Xuesu Xiao, Adarsh Jagan Sathyamoorthy, Mohamed Elnoor, Ming C. Lin, Dinesh Manocha","University of Maryland,University of Massachussets Amherst,George Mason University,University of Maryland at College Park",Planning under Uncertainty II,"We present a novel learning-based trajectory generation algorithm for outdoor robot navigation. Our goal is to compute collision-free paths that also satisfy the environment-specific traversability constraints. Our approach is designed for global planning using limited onboard robot perception in mapless environments while ensuring comprehensive coverage of all traversable directions. Our formulation uses a Conditional Variational Autoencoder (CVAE) generative model that is enhanced with traversability constraints and an optimization formulation used for the coverage. We highlight the benefits of our approach over state-of-the-art trajectory generation approaches and demonstrate its performance in challenging and large outdoor environments, including around buildings, across intersections, along trails, and off-road terrain, using a Clearpath Husky and a Boston Dynamics Spot robot. In practice, our approach results in a 6% improvement in coverage of traversable areas and an 89% reduction in trajectory portions residing in non-traversable regions. Our video is here: https://youtu.be/3eJ2soAzXnU"
IBBT: Informed Batch Belief Trees for Motion Planning under Uncertainty,"Dongliang Zheng, Panagiotis Tsiotras",Georgia Tech,Planning under Uncertainty II,"In this work, we propose the Informed Batch Belief Trees (IBBT) algorithm for motion planning under motion and sensing uncertainties. The original stochastic motion planning problem is divided into a deterministic motion planning problem and a graph search problem. First, we solve the deterministic planning problem using Rapidly-exploring Random Graph (RRG) to construct a nominal trajectory graph. Then, an informed cost-to-go heuristic for the original problem is computed based on the nominal trajectory graph. Finally, we grow a belief tree by searching the graph using the proposed heuristic. IBBT interleaves batch state sampling, nominal trajectory graph construction, heuristic computing, and searching over the graph to find belief space motion plans. IBBT is an anytime, incremental algorithm. With an increasing number of batches of samples added to the graph, the algorithm finds improved plans. IBBT is efficient by reusing results between sequential iterations. The belief tree search is an ordered search guided by an informed heuristic. We test IBBT in different planning environments. Our numerical investigation confirms that IBBT finds non-trivial motion plans and is faster compared with previous similar methods."
Integrating Predictive Motion Uncertainties with Distributionally Robust Risk-Aware Control for Safe Robot Navigation in Crowds,"Kanghyun Ryu, Negar Mehr","University of California, Berkeley,University of California Berkeley",Planning under Uncertainty II,"Ensuring safe navigation in human-populated environments is crucial for autonomous mobile robots. Although recent advances in machine learning offer promising methods to predict human trajectories in crowded areas, it remains unclear how one can safely incorporate these learned models into a control loop due to the uncertain nature of human motion, which can make predictions of these models imprecise. In this work, we address this challenge and introduce a distributionally robust chance-constrained model predictive control (DRCC-MPC) which: (i) adopts a probability of collision as a pre-specified, interpretable risk metric, and (ii) offers robustness against discrepancies between actual human trajectories and their predictions. We consider the risk of collision in the form of a chance constraint, providing an interpretable measure of robot safety. To enable real-time evaluation of chance constraints, we consider conservative approximations of chance constraints in the form of distributionally robust Conditional Value at Risk constraints. The resulting formulation offers computational efficiency as well as robustness with respect to out-of-distribution human motion. With the parallelization of a sampling-based optimization technique, our method operates in real-time, demonstrating successful and safe navigation in a number of case studies with real-world pedestrian data."
A GP-Based Robust Motion Planning Framework for Agile Autonomous Robot Navigation and Recovery in Unknown Environments,"Nicholas Mohammad, Jacob Higgins, Nicola Bezzo",University of Virginia,Planning under Uncertainty II,"For autonomous mobile robots, uncertainties in the environment and system model can lead to failure in the motion planning pipeline, resulting in potential collisions. In order to achieve a high level of robust autonomy, these robots should be able to proactively predict and recover from such failures. To this end, we propose a Gaussian Process (GP) based model for proactively detecting the risk of future motion planning failure. When this risk exceeds a certain threshold, a recovery behavior is triggered that leverages the same GP model to find a safe state from which the robot may continue towards the goal. The proposed approach is trained in simulation only and can generalize to real world environments on different robotic platforms. Simulations and physical experiments demonstrate that our framework is capable of both predicting planner failures and recovering the robot to states where planner success is likely, all while producing agile motion."
ReC-Gripper: A Reconfigurable Combined Suction and Fingered Gripper for Various Logistics Picking and Stowing Tasks,"Seunghwan Um, Heeyeon Jeong, Chunsoo Kim, Issac Rhee, Hyouk Ryeol Choi","SungKyunKwan University,Sungkyunkwan University,SKKU",Mechanism Design II,"This article presents a gripper comprising finger and suction with reconfigurable attributes. With the reconfiguration feature, the proposed gripper has a configuration suitable for different working environments of logistic order picking. The finger part of the gripper was configured with the parallelogram remote center of motion (RCM) mechanism to implement reconfigurable features. With the RCM mechanism, the gripper implements the function of zeroed offset, which removes the gap between the finger and the suction gripper, and the function of the supporting finger. The gripper shows higher grasping stability and practicality than existing grippers in order-picking tasks. First, the design of the mechanism and model constituting the gripper is described. Afterward, a quantitative evaluation of the performance of this gripper compared to the existing ones in the bin and shelf environment is conducted. In this section, the gripper shows 32.912% improved performance in representative tasks. Finally, the practical aspects of this gripper are described through a quantitative evaluation."
Development of the Assembling System for Structure Transformable Humanoid with Attach-Lock-Detachable Magnetic Coupling,"Tasuku Makabe, Kei Okada, Masayuki Inaba",The University of Tokyo,Mechanism Design II,"We propose the method to adapt humanoids the ability to change the body structures that modular robots have by using Attach-Lock-Detachable Magnetic Couplings(ALDMag) to give the ability to detach and attach the robot body with an arm-type robot, and the system to manage the connection state of modularized body elements. Robots and we can use the ALDMag to attach and detach mechanical and electrical connections without actuators. Using xacro for writing the file of the robot model description of each module, we can construct a system that allows the robot to attach and detach modules during task operation. We demonstrated the effectiveness of the proposed method by achieving assembly experiments of a small robot with a life-size arm and experiments with environmental contacts by the small robot."
Design of a Deployable Continuum Robot Using Elastic Kirigami-Origami,"Yunong Li, Hailin Huang, Bing Li","Harbin Institute of Technology, Shenzhen,Harbin Institute of Technology (Shenzhen)",Mechanism Design II,"Inspired by Yoshimura origami, this study presents a novel deployable modular continuum robot that achieves configuration maintenance by utilizing active cables and passive elastic deformation of kirigami-origami. The synchronous motion of each module is improved by using slider-crank mechanisms. Using screw theory, the comprehensive kinematics of the proposed deployable kirigami-origami robot were analyzed, explaining how elastic restoring force is generated in each module. A physical prototype was developed, and the performance of this origami-inspired continuum robot was evaluated by comparing the motion properties of the proposed robot with the robot without elastic rings and synchronism mechanisms. Besides, position accuracy, trajectory tracking ability, stiffness, and load capacity experiments were also conducted. By integrating a pneumatic soft hand at the end of the proposed robot, an object-grasping experiment was conducted to verify the feasibility."
Design and Control of a Transformable Multi-Mode Mobile Robot,"Haoran Li, Yongzhong Bu, Yongjian Bu, Shixin Mao, Yisheng Guan, Haifei Zhu","Guangdong University of Technology,University of Science and Technology of China",Mechanism Design II,"Conventional mobile robots typically include a single locomotion mode and require additional arms to transport objects. To address the challenges of traversing in diverse environments and transporting objects, a novel transformable multi-mode Mecanum-wheeled mobile robot is proposed in this paper. Owing to its unique foreleg design, the robot could operate either in the quadrilateral four-wheel mode, collinear four-wheel mode, or upright two-wheel mode; it could even smoothly switch between any two modes by re-arranging the foreleg wheels. When standing with its forelegs raised, the robot can carry objects and transport them to a predetermined destination. The design and operational modes of the robot were explored in detail. The kinematics and control of the different operational modes were analyzed and experimentally verified. The results indicate that the developed robot can perform versatile locomotion to accomplish object transportation in diverse environments by utilizing its foreleg-wheel mechanisms. Furthermore, the robot experiences an additional angular velocity because of an asymmetric arrangement of the front and rear Mecanum wheels, which differs from conventional symmetric arrangements."
HyperLeg: Biomechanics-Inspired High-DOF Leg and Toe Mechanism for Highly Dynamic Motions,"Do-yun Kim, Seong-Ho Yun, Joongkyung Lee, Jongjun Yoon, Dongyun Nam, Chan-young Maeng, Yong-Jae Kim","KOREATECH University,Koreatech,Korea University of Technology and Education (KOREATECH),Korea University of Technology and Education",Mechanism Design II,"A human foot with high degrees of freedom (DOF) that has multi-DOF toe joints and a two-DOF ankle joint provides multiple benefits, such as increased stride length and walking speed, impact mitigation, and enhanced balancing. However, creating such mechanisms for legged robots has been challenging due to increased complexity, heavy weight, and vulnerability to impact. In this paper, a novel leg and toe mechanism inspired by human biomechanics, featuring a one-DOF knee joint, two-DOF ankle joint, and one-DOF toe joint, is developed. All actuators are located at the proximal part of the thigh frame to minimize the distal mass. High payload timing belts and unique linkage mechanisms are utilized in the transmission to achieve high backdrivability and high joint stiffness. Actuation torques are intentionally coupled inspired by human anatomy, enduring the high propulsive force to the ground for dynamic movements, such as jumping. The implemented leg and toe mechanisms weigh 8.16 kg, and the height from the ground to the hip center is 786 mm. The proposed mechanism has been proven to be effective through force test and distance jump experiments."
Design of a Towing System by Multi Autonomous Sailboats,"Cheng Liang, Bairun Lin, Huihuan Qian","Shenzhen Institute of Artificial Intelligence and Robotics for S,The Chinese University of Hong Kong, Shenzhen",Mechanism Design II,"For researchers or administrators of relevant institutions who need to collect hydrological data of a certain water area, using autonomous sailboats to tow floating detection equipment is an energy-saving and convenient scheme for deploying detectors. However, due to the limited pulling force provided by a single autonomous sailboat, this scheme is not suitable for the floating equipment with large mass. This paper proposes a new approach for multiple autonomous sailboats to tow floating objects. A system of linear arrangement and connection of two autonomous sailboats is considered as an appropriate solution for towing heavy floating objects because of its ability to provide greater pulling force. The main part of the article introduces a new design of multi sailboats towing system which can tow floating objects to sail with or against wind. Repetitive experiments have been conducted at the test site equipped with motion capture system to find the best strategy to control the sails and rudder, in order to increase towing systemâ€™s pulling force and tacking success rate. Three connection modes are proposed, compared and tested. The best one is applied to the sailboats towing system and improves its performance."
Non-Intrusive LiDAR Protection Module Emulating Bio-Inspired Wiping Motion for Outdoor Unmanned Vehicles,"Youngrae Kim, Seunghyun Lim, Hanmin Lee, Seokchan Kim, Jichul Kim, Dongwon Yun","Daegu Gyeongbuk Institute of Science and Technology (DGIST), Dae,DGIST,Korea Institute of Machinery & Materials,Korea Institute of Machinery and Materials,Daegu Gyeongbuk Institute of Science and Technology (DGIST)",Mechanism Design II,"In this paper, we have developed a protection module for Light Detection and Ranging (LiDAR) sensors used in outdoor unmanned vehicles. Bio-inspired wiping motion was figured to have more efficient and excellent wiping performance than conventional cleaning methods for LiDAR sensors. An water wiping experiment confirmed that the finger wiping motion removed 35% more water than the translational wiping motion. Also, the theoretical analysis for the existence of an optimal rotational speed at maximum wiping performance was verified to be consistent with the experiment. The LiDAR distortion experiment results demonstrated no data distortion, showing an average error of up to 0.40% for detecting obstacles even when the acrylic cover rotates. Finally, a contamination protection experiment was conducted for water, powder, soil, and mud. As a result, although there was a change in the number of pointcloud and a decrease in the intensity of the sensor data after contamination, it was validated that the number of pointclouds and average intensity of data could be restored to at least 97% and 67% after being cleaned."
Lightweight Human-Friendly Robotic Arm Based on Transparent Hydrostatic Transmissions,"Marco Bolignari, Gianluca Rizzello, Luca Zaccarian, Marco Fontana","University of Trento,Saarland University,LAAS-CNRS and University of Trento,Scuola Superiore Sant'Anna",Mechanism Design II,"We present theoretical and experimental results regarding the development and the control of a two-link robotic arm with remotized actuation via rolling diaphragm hydrostatic transmissions. We propose a dynamical model capturing the essential dynamics of the developed transmission/robot ensemble and implement a control strategy consisting of two nested loops, the inner one performing high-bandwidth joint torque regulation and the outer one producing various types of compliance responses for effective humanâ€“robot interactions. Extensive sets of experiments, testing both the low-level torque controller and the high-level compliance controller, confirm the effectiveness of the proposed hardware-software remotization architecture."
OSCaR: An Origami-Inspired Shape-Changing Robot for Ground Coverage Tasks,"Zirui Fan, Hongying Zhang",National University of Singapore,Mechanism Design II,"This paper introduces a novel origami-inspired shape-changing robot OSCaR. The objective is to enhance the adaptability of vehicles engaged in ground coverage tasks, such as floor cleaning. The robot exhibits two distinct configurations: it can fold itself for agile navigation through tight spaces, and unfold to cover larger areas efficiently. The folding pattern has a deploy-to-stow ratio of 3 in the width dimension, and a kinematic model is established to simulate the deployment process for the pattern. The hinge design employs rolling contact elements to mitigate collision among the panels, particularly in regions with multiple colinear crease lines. Furthermore, the design exhibits one degree of freedom and features pivots, making it easy to actuate with motors. The system design of the prototype is also presented, including its structure, an embedded hardware system, and an upper computer software. The results show that the robot has great adaptability in complex environments."
Robust MITL Planning under Uncertain Navigation Times,"Alexis Linard, Anna Gautier, Daniel Duberg, Jana Tumova","KTH Royal Institute of Technology,KTH - Royal Institute of Technology",Formal Methods in Robotics and Automation II,"In environments like offices, the duration of a robot's navigation between two locations may vary over time. For instance, reaching a kitchen may take more time during lunchtime since the corridors are crowded with people heading the same way. In this work, we address the problem of routing in such environments with tasks expressed in Metric Interval Temporal Logic (MITL) - a rich robot task specification language that allows us to capture explicit time requirements. Our objective is to find a strategy that maximizes the temporal robustness of the robot's MITL task. As the first step towards a solution, we define a Mixed-integer linear programming approach to solving the task planning problem over a Varying Weighted Transition System, where navigation durations are deterministic but vary depending on the time of day. Then, we apply this planner to optimize for MITL temporal robustness in Markov Decision Processes, where the navigation durations between physical locations are uncertain, but the time-dependent distribution over possible delays is known. Finally, we develop a receding horizon planner for Markov Decision Processes that preserves guarantees over MITL temporal robustness. We show the scalability of our planning algorithms in simulations of robotic tasks."
Exploiting Transformer in Sparse Reward Reinforcement Learning for Interpretable Temporal Logic Motion Planning,"Hao Zhang, Hao Wang, Zhen Kan",University of Science and Technology of China,Formal Methods in Robotics and Automation II,"Automaton based approaches have enabled robots to perform various complex tasks. However, most existing automaton based algorithms highly rely on the manually customized representation of states for the considered task, limiting its applicability in deep reinforcement learning algorithms. To address this issue, by incorporating Transformer into reinforcement learning, we develop a Double-Transformer- guided Temporal Logic framework (T2TL) that exploits the structural feature of Transformer twice, i.e., first encoding the LTL instruction via the Transformer module for efficient understanding of task instructions during the training and then encoding the context variable via the Transformer again for improved task performance. Particularly, the LTL instruction is specified by co-safe LTL. As a semantics-preserving rewriting operation, LTL progression is exploited to decompose the complex task into learnable sub-goals, which not only converts non-Markovian reward decision processes to Markovian ones, but also improves the sampling efficiency by simultaneous learning of multiple sub-tasks. An environment-agnostic LTL pre-training scheme is further incorporated to facilitate the learning of the Transformer module resulting in an improved representation of LTL. The simulation results demonstrate the effectiveness of the T2TL framework."
Stochastic Games for Interactive Manipulation Domains,"Karan Muvvala, Andrew Wells, Morteza Lahijanian, Lydia Kavraki, Vardi Moshe","University of Colorado Boulder,Rice University",Formal Methods in Robotics and Automation II,"As robots become more prevalent, the complexity of robot-robot, robot-human, and robot-environment interactions increases. In these interactions, a robot needs to consider not only the effects of its own actions, but also the effects of other agents' actions and the possible interactions between agents. Previous works have considered reactive synthesis, where the human/environment is modeled as a deterministic, adversarial agent; as well as probabilistic synthesis, where the human/environment is modeled via a Markov chain. While they provide strong theoretical frameworks, there are still many aspects of human-robot interaction that cannot be fully expressed and many assumptions that must be made in each model. In this work, we propose stochastic games as a general model for human-robot interaction, which subsumes the expressivity of all previous representations. In addition, it allows us to make fewer modeling assumptions and leads to more natural and powerful models of interaction. We introduce the semantics of this abstraction and show how existing tools can be utilized to synthesize strategies to achieve complex tasks with guarantees. Further, we discuss the current computational limitations and improve the scalability by two orders of magnitude by a new way of constructing models for PRISM-Games."
Active Inference for Reactive Temporal Logic Motion Planning,"Ziyang Chen, Zhangli Zhou, Lin Li, Zhen Kan",University of Science and Technology of China,Formal Methods in Robotics and Automation II,"Reactive planning enables the robots to deal with dynamic events in uncertain environments. However, existing methods heavily rely on the predefined hard-coded robot behaviors, e.g, a pre-coded temporal logic formula that specifies how robot should react. Little attention has been paid for autonomous generation of reactive tasks specifications during the runtime. As a first attempt towards this goal, this work develops a real-time decision-making and motion planning framework. It allows the robot to follow a global task planned offline while taking proactive decisions and generating temporal logic specifications for local reactive tasks when encountering dynamic events. Specifically, inspired by the causal knowledge graph, a proposition graph is developed, based on which the decision module encode the environment and the task as the Boolean logic and linear temporal logic (LTL), respectively. Based on the established proposition graph and perceived environment, the agent can autonomously generate an LTL formula to realize the local temporary task. A joint sampling algorithm is then developed, in which the automaton states of local and global task are jointly considered to generate a feasible planning that satisfies both global and local tasks. Experiments demonstrate the effectiveness of the proposed decision-making and motion planning."
Fast Task Allocation of Heterogeneous Robots with Temporal Logic and Inter-Task Constraints,"Lin Li, Ziyang Chen, Hao Wang, Zhen Kan",University of Science and Technology of China,Formal Methods in Robotics and Automation II,"This work develops a fast task allocation framework for heterogeneous multi-robot systems subject to both temporal logic and inter-task constraints. The considered inter-task constraints include unrelated tasks, compatible tasks, and exclusive tasks. To specify such inter-task relationships, we extend conventional atomic proposition to batch atomic propositions, which gives rise to the LTLT formula. The Task Batch Planning Decision Tree (TB-PDT) is then developed, which is a variant of conventional decision tree specialized for temporal logic and inter-task constraints. The TB-PDT is built incrementally to represent the task progress and does not require sophisticated product automaton, which significantly reduces the search space. Based on TB-PDT, the search algorithm, namely Intensive Inter- task Relationship Tree Search (IIRTS), is developed for the fast task allocation of heterogeneous multi-robot systems. It is shown that the solution time of finding a satisfactory task allocation grows almost quadratically with the number of automaton states. Extensive simulation and experiment demonstrate the validity, the effectiveness, and the transferability of IIRTS."
Skill Transfer for Temporal Task Specification,"Xinyu Liu, Ankit Jayesh Shah, Eric Rosen, Mingxi Jia, George Konidaris, Stefanie Tellex","Brown University,Massachusetts Institute of Technology,Brown",Formal Methods in Robotics and Automation II,"Deploying robots in real-world environments, such as households and manufacturing lines, requires generalization across novel task specifications without violating safety constraints. Linear temporal logic (LTL) is a widely used task specification language with a compositional grammar that naturally induces commonalities among tasks while preserving safety guarantees. However, most prior work on reinforcement learning with LTL specifications treats every new task independently, thus requiring large amounts of training data to generalize. We propose LTL-Transfer, a zero-shot transfer algorithm that composes task-agnostic skills learned during training to safely satisfy a wide variety of novel LTL task specifications. Experiments in Minecraft-inspired domains show that after training on only 50 tasks, LTL-Transfer can solve over 90% of 100 challenging unseen tasks and 100% of 300 commonly used novel tasks without violating any safety constraints. We deployed LTL-Transfer at the task-planning level of a quadruped mobile manipulator to demonstrate its zero-shot transfer ability for fetch-and-deliver and navigation tasks."
High Precision Paint Deposition Modeling Considering Variable Posture of Spray Painting Robot,"Genichiro Tanaka, Yoshinobu Takahashi, Hiroyasu Iwata","Waseda university,Waseda University",Formal Methods in Robotics and Automation II,"This study developed a high-precision paint deposition model that considers the position and direction of a spray-painting gun. Our angle-specific paint deposition model focused on the change in paint deposition due to the change in the painting angle; however, there was a problem with its versatility. We analyzed this problem, and the solution was achieved by separately modeling changes in the film thickness distribution using impact angle and spray distance, which were previously modeled together. For higher accuracy, a special function was proposed to convert the three-dimensional vector into two-dimensional coordinate values in the distribution function upper plane. To confirm the validity of our model, a painting test on an L-shaped surface was conducted, and the measured and predicted values were compared. The L-shaped surface is a typical shape in which the film thickness distribution changes with the angle; a complex path with varying distances and angles was employed. The results confirmed that the predicted values agreed well with the measured values in the L-shaped surface painting test, validating the developed model."
Verifiable Learned Behaviors Via Motion Primitive Composition: Applications to Scooping of Granular Media,"Andrew Benton, Eugen Solowjow, Prithvi Akella","Siemens,Siemens Corporation,California Institute of Technology",Formal Methods in Robotics and Automation II,"A robotic behavior model that can reliably generate behaviors from natural language inputs in real time would substantially expedite the adoption of industrial robots due to enhanced system flexibility. To facilitate these efforts, we construct a framework in which learned behaviors, created by a natural language abstractor, are verifiable by construction. Leveraging recent advancements in motion primitives and probabilistic verification, we construct a natural-language behavior abstractor that generates behaviors by synthesizing a directed graph over the provided motion primitives. If these component motion primitives are constructed according to the criteria we specify, the resulting behaviors are probabilistically verifiable. We demonstrate this verifiable behavior generation capacity in both simulation on an exploration task and on hardware with a robot scooping granular media."
"Knowledge Acquisition Plans: Generation, Combination, and Execution","Dylan Shell, Jason O'kane",Texas A&M University,Formal Methods in Robotics and Automation II,"This paper contemplates the possibility of asking robots questions and having them use their ability to go out into the environment and probe it, in combination with what they already know of the world, to provide answers. We describe a method whereby a robot system efficiently answers such questions on the basis of reasoning about observations as they are made, interrelationships between multiple pieces of evidence, and what they imply. A central idea in the approach is to maintain a separation of concerns so that managing 'what is known' is decoupled from 'how it is learned'. This idea is realized in a graph-based representation well-suited to algorithmic manipulation and composition, exposing synergies rife for optimization. We show how to use this representation to leverage both informational overlap between multiple simultaneous queries and availability of multiple robots working in concert to answer those queries. We demonstrate these ideas in a simple case study and present data illustrating how plan quality (in terms of cost to execute) can be improved through an optimization operation that is robot agnostic."
Distributed Control of a Limited Angular Field-Of-View Multi-Robot System in Communication-Denied Scenarios: A Probabilistic Approach,"Mattia Catellani, Lorenzo Sabattini",University of Modena and Reggio Emilia,Multi-Robot Systems II,"Multi-robot systems are gaining popularity over single-agent systems for their advantages. Although they have been studied in agriculture, search and rescue, surveillance, and environmental exploration, real-world implementation is limited due to agent coordination complexities caused by communication and sensor limitations. In this work, we propose a probabilistic approach to allow coordination among robots in communication-denied scenarios, where agents can only rely on visual information from a camera with a limited angular field-of-view. Our solution utilizes a particle filter to analyze uncertainty in the location of neighbors, together with Control Barrier Functions to address the exploration-exploitation dilemma that arises when robots must balance the mission goal with seeking information on undetected neighbors. This technique was tested with virtual robots required to complete a coverage mission, analyzing how the number of deployed robots affects performances and making a comparison with the ideal case of isotropic sensors and communication. Despite an increase in the amount of time required to fulfill the task, results have shown to be comparable to the ideal scenario in terms of final configuration achieved by the system."
Assessing Reputation to Improve Team Performance in Heterogeneous Multi-Robot Coverage,"Mela Coffey, Alyssa Pierson",Boston University,Multi-Robot Systems II,"When agents in a multi-robot team have limited knowledge about their relative performance, their teammates, or the environment, robots must observe individual performance variations and adapt accordingly. We propose robot reputation to assess the historical performance of agents and make future adaptations in a persistent coverage task. We consider a heterogeneous multi-robot team, where robots are equipped with different capabilities to serve discrete events in an environment. We utilize a heterogeneous coverage control approach to partition the space according to robot capabilities and the estimated probability density, such that the robot is responsible for serving the events in its assigned region. As the team serves events, we assign each robot a reputation, which is then used to adjust the size of a robot's region, thus adjusting the amount of space a robot is responsible for serving. Our simulations show that using reputation to weigh the size of the Voronoi cells outperforms the case where we neglect reputation."
A Robot Web for Distributed Many-Device Localisation,"Riku Murai, Joseph Ortiz, Sajad Saeedi, Paul H J Kelly, Andrew J Davison","Imperial College London,Meta,Toronto Metropolitan University",Multi-Robot Systems II,"We show that a distributed network of robots or other devices which make measurements of each other can collaborate to globally localise via efficient ad-hoc peer-to-peer communication. Our Robot Web solution is based on Gaussian Belief Propagation on the fundamental non-linear factor graph describing the probabilistic structure of all of the observations robots make internally or of each other, and is flexible for any type of robot, motion or sensor. We define a simple and efficient communication protocol which can be implemented by the publishing and reading of web pages or other asynchronous communication technologies. We show in simulations with up to 1000 robots interacting in arbitrary patterns that our solution convergently achieves global accuracy as accurate as a centralised non-linear factor graph solver while operating with high distributed efficiency of computation and communication. Via the use of robust factors in GBP, our method is tolerant to a high percentage of faulty sensor measurements or dropped communication packets. Furthermore, we showcase that the system operates on real robots with limited onboard computational resources."
Learning Decentralized Flocking Controllers with Spatio-Temporal Graph Neural Network,"Siji Chen, Yanshen Sun, Peihan Li, Lifeng Zhou, Chang-tien Lu","Virginia Tech,Drexel University",Multi-Robot Systems II,"Recently a line of researches has delved the use of graph neural networks (GNNs) for decentralized control in swarm robotics. However, it has been observed that relying solely on the states of immediate neighbors is insufficient to imitate a centralized control policy. To address this limitation, prior studies proposed incorporating $L$-hop delayed states into the computation. While this approach shows promise, it can lead to a lack of consensus among distant flock members and the formation of small clusters, consequently resulting in the failure of cohesive flocking behaviors. Instead, our approach leverages spatiotemporal GNN, named STGNN that encompasses both spatial and temporal expansions. The spatial expansion collects delayed states from distant neighbors, while the temporal expansion incorporates previous states from immediate neighbors. The broader and more comprehensive information gathered from both expansions results in more effective and accurate predictions. We develop an expert algorithm for controlling a swarm of robots and employ imitation learning to train our decentralized STGNN model based on the expert algorithm. We simulate the proposed STGNN approach in various settings, demonstrating its decentralized capacity to emulate the global expert algorithm. Further, we implemented our approach to achieve cohesive flocking, leader following and obstacle avoidance by a group of Crazyflie drones. The performance of STGNN underscores its potential as an effective and reliable approach for achieving cohesive flocking, leader following and obstacle avoidance tasks."
Simultaneous Time Synchronization and Mutual Localization for Multi-Robot System,"Xiangyong Wen, Yingjian Wang, Xi Zheng, Kaiwei Wang, Chao Xu, Fei Gao","Zhejiang University,The Hong Kong Polytechnic University",Multi-Robot Systems II,"Mutual localization stands as a foundational component within various domains of multi-robot systems. Nevertheless, in relative pose estimation, time synchronization is usually underappreciated and rarely addressed, although it significantly influences the accuracy of estimation. In this paper, we introduce time synchronization into mutual localization, to recover the time offset and relative poses between robots simultaneously. Under a constant velocity assumption in a short time, we fuse time offset estimation with our previous bearing-based mutual localization by a novel error representation. Based on the error model, we formulate a joint optimization problem and utilize semi-definite relaxation (SDR) to furnish a lossless relaxation. By solving the relaxed problem, time synchronization and relative pose estimation can be achieved when time drift between robots is limited. To enhance the application range of time offset estimation, we further propose an iterative method to recover the time offset from coarse to fine. Comparisons between the proposed method and the existing ones through extensive simulation tests present prominent benefits of time synchronization on mutual localization. Moreover, real-world experiments are conducted to show the practicality and robustness."
Enabling Large-Scale Heterogeneous Collaboration with Opportunistic Communications,"Fernando Cladera, Zachary Ravichandran, Ian Douglas Miller, M. Ani Hsieh, Camillo Jose Taylor, Vijay Kumar",University of Pennsylvania,Multi-Robot Systems II,"Multi-robot collaboration in large-scale environments with limited-sized teams and without external infrastructure is challenging, since the software framework required to support complex tasks must be robust to unreliable and intermittent communication links. In this work, we present MOCHA (Multi-robot Opportunistic Communication for Heterogeneous Collaboration), a framework for resilient multi-robot collaboration that enables large-scale exploration in the absence of continuous communications. MOCHA is based on a gossip communication protocol that allows robots to interact opportunistically whenever communication links are available, propagating information on a peer-to-peer basis. We demonstrate the performance of MOCHA through real-world experiments with commercial-off-the-shelf (COTS) communication hardware. We further explore the systemâ€™s scalability in simulation, evaluating the performance of our approach as the number of robots increase and communication ranges vary. Finally, we demonstrate how MOCHA can be tightly integrated with the planning stack of autonomous robots. We show a communication-aware planning algorithm for a high-altitude aerial robot executing a collaborative task while maximizing the amount of information shared with ground robots."
AG-CVG: Coverage Planning with a Mobile Recharging UGV and an Energy-Constrained UAV,"Nare Karapetyan, Ahmad Bilal Asghar, Amisha Bhaskar, Guangyao Shi, Dinesh Manocha, Pratap Tokekar","Woods Hole Oceanographic Institution,University of Maryland,University of Maryland, College Park,University of Southern California",Multi-Robot Systems II,"In this paper, we present an approach for coverage path planning for a team of an energy-constrained Unmanned Aerial Vehicle (UAV) and an Unmanned Ground Vehicle (UGV). Both the UAV and the UGV have predefined areas that they have to cover. The goal is to perform complete coverage by both robots while minimizing the coverage time. The UGV can also serve as a mobile recharging station. The UAV and UGV need to occasionally rendezvous for recharging. We propose a heuristic method to address this NP-Hard planning problem. Our approach involves initially determining coverage paths without factoring in energy constraints. Subsequently, we cluster segments of these paths and employ graph matching to assign UAV clusters to UGV clusters for efficient recharging management. We perform numerical analysis on real-world coverage applications and show that compared with a greedy approach our method reduces rendezvous overhead on average by 11.33%. We demonstrate proof-of-concept with a team of a VOXL m500 drone and a Clearpath Jackal ground vehicle, providing a complete system from the offline algorithm to the field execution."
A Non-Cubic Space-Filling Modular Robot,"Tyler Hummer, Sam Kriegman",Northwestern University,Multi-Robot Systems II,"Space-filling building blocks of diverse shape permeate nature at all levels of organization, from atoms to honeycombs, and have proven useful in artificial systems, from molecular containers to clay bricks. But, despite the wide variety of space-filling polyhedra known to mathematics, only the cube has been explored in robotics. Thus, here we roboticize a non-cubic space-filling shape: the rhombic dodecahedron. This geometry offers an appealing alternative to cubes as it greatly simplifies rotational motion of one cell about the edge of another, and increases the number of neighbors each cell can communicate with and hold on to. To better understand the challenges and opportunities of these and other space-filling machines, we manufactured 48 rhombic dodecahedral cells and used them to build various superstructures. We report locomotive ability of some of the structures we built, and discuss the dis/advantages of the different designs we tested. We also introduce a strategy for genderless passive docking of cells that generalizes to any polyhedra with radially symmetrical faces. Future work will allow the cells to freely roll/rotate about one another so that they may realize the full potential of their unique shape."
Optimal Containment Control of Multiple Quadrotors Via Reinforcement Learning,"Ming Cheng, Hao Liu, Deyuan Liu, Haibo Gu, Xiangke Wang","Beihang University,National University of Defense Technology",Multi-Robot Systems II,"This paper explores the optimal containment control problem for nonlinear and underactuated quadrotors with multiple team leaders governed by nonlinear dynamics, employing the reinforcement learning. A cascade controller is formulated, comprising a position control component to ensure containment achievement and an attitude control component to govern rotational channel. The proposed optimal control protocols derived from historical data collected from quadrotor systems without requirement for exact knowledge of vehicle dynamics. The simulation illustrates the effectiveness of the proposed controller in managing a quadrotor team with multiple leaders."
Ensemble Latent Space Roadmap for Improved Robustness in Visual Action Planning,"Martina Lippi, Michael Welle, Andrea Gasparri, Danica Kragic","University of Roma Tre,KTH Royal Institute of Technology,Università degli Studi Roma Tre,KTH",Vision Systems,"Planning in learned latent spaces helps to decrease the dimensionality of raw observations. In this work, we propose to leverage the ensemble paradigm to enhance the robustness of latent planning systems. We rely on our Latent Space Roadmap (LSR) framework, which builds a graph in a learned structured latent space to perform planning. Given multiple LSR framework instances, that differ either on their latent spaces or on the parameters for constructing the graph, we use the action information as well as the embedded nodes of the produced plans to define similarity measures. These are then utilized to select the most promising plans. We validate the performance of our Ensemble LSR (ENS-LSR) on simulated box stacking and grape harvesting tasks as well as on a real-world robotic T-shirt folding experiment."
Direct 3D Model-Based Object Tracking with Event Camera by Motion Interpolation,"Kang Yufan, Guillaume Caron, Ryoichi Ishikawa, Adrien Escande, Kevin Chappellet, Ryusuke Sagawa, Takeshi Oishi","The University of Tokyo,CNRS,INRIA,National Institute of Advanced Industrial Science andTechnology",Vision Systems,"Event cameras are recent sensors that measure intensity changes in each pixel asynchronously. It is being used due to lower latency and higher temporal resolution compared to traditional frame-based camera. We propose a method of 3D model-based object tracking directly from events captured by event camera. To enable reliable and accurate tracking of objects, we use a new event representation and predict brightness increment images with motion interpolation. Results of object tracking show the new methods significantly improves tracking duration and robustness, both for perspective and fisheye cameras. Our implementation succeeds in tracking objects when the camera speed is reaching 2 m/s."
Using Specularities to Boost Non-Rigid Structure-From-Motion,"Agniva Sengupta, Makki Karim, Adrien Bartoli","Institut Pascal,UCA",Vision Systems,"Non-rigid structure-from-motion reconstructs the time-varying 3D shape of a deforming object from 2D point correspondences in monocular images. Despite promising use-cases such as the grasping of deformable objects and visual navigation in a non-rigid environment, NRSfM has had limited applications in robotics due to a lack of sufficient accuracy. To remedy this, we propose a new method which boosts the accuracy of NRSfM using sparse surface normals. Surface normal information is available from many sources, including structured lighting, homography decomposition of infinitesimal planes and shape priors. However, these sources are not always available. We thus propose a widely available new source of surface normals: the specularities. Our first technical contribution is a method which detects specular highlights and reconstructs the surface normals from it. It assumes that the light source is approximately localised, which is widely applicable in robotics applications such as endoscopy. Our second technical contribution is an NRSfM method which exploits a sparse surface normal set. For that, we propose a novel convex formulation and a globally optimal solution method. Experiments on photo-realistic synthetic data and real household and medical data show that the proposed method outperforms existing NRSfM methods."
Tracking Snake-Like Robots in the Wild Using Only a Single Camera,"Jingpei Lu, Florian Richter, Shan Lin, Michael Yip","University of California San Diego,University of California, San Diego",Vision Systems,"Robot navigation within complex environments requires precise state estimation and localization to ensure robust and safe operations. For ambulating mobile robots like robot snakes, traditional methods for sensing require multiple embedded sensors or markers, leading to increased complexity, cost, and increased points of failure. Alternatively, deploying an external camera in the environment is very easy to do, and marker-less state estimation of the robot from this camera's images is an ideal solution: both simple and cost-effective. However, the challenge in this process is in tracking the robot under larger environments where the cameras may be moved around without extrinsic calibration, or maybe when in motion (e.g., a drone following the robot). The scenario itself presents a complex challenge: single-image reconstruction of robot poses under noisy observations. In this paper, we address the problem of tracking ambulatory mobile robots from a single camera. The method combines differentiable rendering with the Kalman filter. This synergy allows for simultaneous estimation of the robot's joint angle and pose while also providing state uncertainty which could be used later on for robust control. We demonstrate the efficacy of our approach on a snake-like robot in both stationary and non-stationary (moving) cameras, validating its performance in both structured and unstructured scenarios. The results achieved show an average error of 0.05 m in localizing the robot's base position and 6 degrees in joint state estimation. We believe this novel technique opens up possibilities for enhanced robot mobility and navigation in future exploratory and search-and-rescue missions."
Multi-Object Tracking by Hierarchical Visual Representations,"Jinkun Cao, Jiangmiao Pang, Kris Kitani","Carnegie Mellon University,Shanghai AI Laboratory",Vision Systems,We propose a new visual hierarchical representation paradigm for multi-object tracking. It is more effective to discriminate between objects by attending to objectsâ€™ compositional visual regions and contrasting with the background contextual information instead of sticking to only the semantic visual cue such as bounding boxes. This compositional-semantic-contextual hierarchy is flexible to be integrated in different appearance-based multi-object tracking methods. We also propose an attention-based visual feature module to fuse the hierarchical visual representations. The proposed method achieves state-of-the-art accuracy and time efficiency among query-based methods on multiple multi-object tracking benchmarks.
AgriSORT: A Simple Online Real-Time Tracking-By-Detection Framework for Robotics in Precision Agriculture,"Leonardo Saraceni, Ionut Marian Motoi, Daniele Nardi, Thomas Alessandro Ciarfuglia",Sapienza University of Rome,Vision Systems,"The problem of multi-object tracking (MOT) consists in detecting and tracking all the objects in a video sequence while keeping a unique identifier for each object. It is a challenging and fundamental problem for robotics. In precision agriculture the challenge of achieving a satisfactory solution is amplified by extreme camera motion, sudden illumination changes, and strong occlusions. Most modern trackers rely on the appearance of objects rather than motion for association, which can be ineffective when most targets are static objects with the same appearance, as in the agricultural case. To this end, on the trail of SORT, we propose AgriSORT, a simple, online, real-time tracking-by-detection pipeline for precision agriculture based only on motion information that allows for accurate and fast propagation of tracks between frames. The main focuses of AgriSORT are efficiency, flexibility, minimal dependencies, and ease of deployment on robotic platforms. We test the proposed pipeline on a novel MOT benchmark specifically tailored for the agricultural context, based on video sequences taken in a table grape vineyard, particularly challenging due to strong self-similarity and density of the instances. Both the code and the dataset are available for future comparisons. Both the code and the dataset are available for future comparisons at: https://github.com/Lio320/AgriSORT"
Tightly Coupled Visual-Inertial-UWB Indoor Localization System with Multiple Position-Unknown Anchors,"Chao Hu, Ping Huang, Wei Wang",Harbin Engineering University,Vision Systems,"In this letter, we perform a tightly-coupled fusion of a monocular camera, a 6-DoF IMU, and multiple position-unknown Ultra-wideband (UWB) anchors to construct an indoor localization system with both accuracy and robustness. Prior to this, there have been several works that have achieved satisfactory results by fusing UWB ranging measurements with visual-inertial system. However, these approaches still have some limitations: 1) these approaches either require the UWB anchor position to be calibrated in advance or the UWB anchor position estimation method used is not robust enough; 2) these approaches do not allow for dynamic changes to the number of UWB anchors in a tightly coupled estimator. Our approach uses visual object detection algorithm to provide UWB anchor initial position and refine it in the factor graph, using chi-square test algorithm to identify UWB ranging outliers. Based on the above two ideas, we implement a tightly coupled estimator that dynamically adjusts the number of UWB anchors, i.e. adding them to the factor graph when their ranging measurements are available and discarding them when their ranging measurements are outliers. These ideas improve the efficiency and robustness of the fusion about UWB ranging measurements with the visual-inertial system, as well as the easy setup of UWB anchors. Experimental results show that the proposed method outperforms previous methods in terms of estimating anchor position and improving localization accuracy."
Stereo-NEC: Enhancing Stereo Visual-Inertial SLAM Initialization with Normal Epipolar Constraints,"Weihan Wang, Chieh Chou, Ganesh Sevagamoorthy, Kevin Chen, Zheng Chen, Ziyue Feng, Youjie Xia, Feiyang Cai, Yi Xu, Philippos Mordohai","Stevens Institute of Technology,InnoPeak Technology,OPPO US Research Center,University of Michigan,Indiana University Bloomington,Clemson University,Stony Brook University",Vision Systems,"We propose an accurate and robust initialization approach for stereo visual-inertial SLAM systems. Unlike the current state-of-the-art method, which heavily relies on the accuracy of a pure visual SLAM system to estimate inertial variables without updating camera poses, potentially compromising accuracy and robustness, our approach offers a different solution. We realize the crucial impact of precise gyroscope bias estimation on rotation accuracy. This, in turn, affects trajectory accuracy due to the accumulation of translation errors. To address this, we first independently estimate the gyroscope bias and use it to formulate a maximum a posteriori problem for further refinement. After this refinement, we proceed to update the rotation estimation by performing IMU integration with gyroscope bias removed from gyroscope measurements. We then leverage robust and accurate rotation estimates to enhance translation estimation via 3-DoF bundle adjustment. Moreover, we introduce a novel approach for determining the success of the initialization by evaluating the residual of the normal epipolar constraint. Extensive evaluations on the EuRoC dataset illustrate that our method excels in accuracy and robustness. It outperforms ORB-SLAM3, the current leading stereo visual-inertial initialization method, in terms of absolute trajectory error and relative rotation error, while maintaining competitive computational speed. Notably, even with 5 keyframes for initialization, our method consistently surpasses the state-of-the-art approach using 10 keyframes in rotation accuracy."
Nvblox: GPU-Accelerated Incremental Signed Distance Field Mapping,"Alexander Millane, Helen Oleynikova, Emilie Wirbel, Remo Steiner, Vikram Ramasamy, David Tingdahl, Roland Siegwart","NVIDIA,ETH Zurich,Valeo,Nvidia,nvidia",Vision Systems,"Dense, volumetric maps are essential to enable robot navigation and interaction with the environment. To achieve low latency, dense maps are typically computed on-board the robot, often on computationally constrained hardware. Previous works leave a gap between CPU-based systems for robotic mapping which, due to computation constraints, limit map resolution or scale, and GPU-based reconstruction systems which omit features that are critical to robotic path planning, such as computation of the Euclidean Signed Distance Field (ESDF). We introduce a library, nvblox, that aims to fill this gap, by GPU-accelerating robotic volumetric mapping. Nvblox delivers a significant performance improvement over the state of the art, achieving up to a 177Ã— speed-up in surface reconstruction, and up to a 31Ã— improvement in distance field computation, and is available open-source."
Multi-Resolution Planar Region Extraction for Uneven Terrains,"Yinghan Sun, Linfang Zheng, Hua Chen, Wei Zhang","Southern University of Science and Technology,University of Birmingham, Southern University of Science and Tec",RGB-D Sensing and Perception I,"This paper studies the problem of extracting planar regions in uneven terrains from unordered point cloud measurements. Such a problem is critical in various robotic applications such as robotic perceptive locomotion. While ex- isting approaches have shown promising results in effectively extracting planar regions from the environment, they often suffer from issues such as low computational efficiency or loss of resolution. To address these issues, we propose a multi-resolution planar region extraction strategy in this paper that balances the accuracy in boundaries and computational efficiency. Our method begins with a pointwise classification preprocessing module, which categorizes all sampled points according to their local geometric properties to facilitate multi- resolution segmentation. Subsequently, we arrange the catego- rized points using an octree, followed by an in-depth analysis of nodes to finish multi-resolution plane segmentation. The efficiency and robustness of the proposed approach are verified via synthetic and real-world experiments, demonstrating our methodâ€™s ability to generalize effectively across various uneven terrains while maintaining real-time performance, achieving frame rates exceeding 35 FPS."
RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction,"Isaac Kasahara, Shubham Agrawal, Kazim Selim Engin, Nikhil Chavan-dafle, Shuran Song, Volkan Isler","Samsung Research America,Columbia University,University of Minnesota",RGB-D Sensing and Perception I,"General scene reconstruction refers to the task of estimating the full 3D geometry and texture of a scene containing previously unseen objects. In many practical applications such as AR/VR, autonomous navigation, and robotics, only a single view of the scene may be available, making the scene reconstruction task challenging. In this paper, we present a method for scene reconstruction by structurally breaking the problem into two steps: rendering novel views via inpainting and 2D to 3D scene lifting. Specifically, we leverage the generalization capability of large visual language models (Dalle-2) to inpaint the missing areas of scene color images rendered from different views. Next, we lift these inpainted images to 3D by predicting normals of the inpainted image and solving for the missing depth values. By predicting for normals instead of depth directly, our method allows for robustness to changes in depth distributions and scale. With rigorous quantitative evaluation, we show that our method outperforms multiple baselines while providing generalization to novel objects and scenes. Code and data is available at https://samsunglabs.github.io/RIC-project-page/."
Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction,"Yuhao Yang, Jun Wu, Yue Wang, Mr Zhang Guangjian, Rong Xiong","Chongqing University of Technology,Zhejiang University",RGB-D Sensing and Perception I,"To address the problem that traditional geometric registration based estimation methods only exploit the CAD model implicitly, which leads to their dependence on observation quality and deficiency to occlusion, this paper proposes a bidirectional correspondence prediction network with point-wise attention aware mechanism that not only requires the model points to predict the correspondence, but also explicitly models the geometric similarities between observations and the model prior. Our key insight is that the correlations between each model point and scene point provide essential information for learning point-pair matches. To further tackle the correlation noises brought by feature distribution divergence, we design a simple but effective pseudo-siamese network to improve feature homogeneity. Experimental results on the public datasets of LineMOD, YCB-Video, and Occ-LineMOD show that the proposed method achieves better performance than other state-of-the-art methods under the same evaluation criteria. Its robustness in estimating poses is greatly improved, especially in an environment with severe occlusions."
Stereo-LiDAR Depth Estimation with Deformable Propagation and Learned Disparity-Depth Conversion,"Ang Li, Anning Hu, Wei Xi, Danping Zou, Wenxian Yu","Shanghai Jiao Tong University,Midea,Shanghai Jiao Ton University",RGB-D Sensing and Perception I,"Accurate and dense depth estimation with stereo cameras and LiDAR is an important task for automatic driving and robotic perception. While sparse hints from LiDAR points have improved cost aggregation in stereo matching, their effectiveness is limited by the low density and non-uniform distribution. To address this issue, we propose a novel stereo-LiDAR depth estimation network. Our network includes a deformable propagation module for generating a semi-dense hint map and a confidence map by propagating sparse hints using a learned deformable window. These maps then guide cost aggregation in stereo matching. To reduce the triangulation error in depth recovery from disparity, especially in distant regions, we introduce a disparity-depth conversion module. Our method is both accurate and efficient. The experimental results on benchmark tests show its superior performance."
Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration,"Siddharth Tourani, Jayaram Gurram, Sarvesh Thakur, Muhammad Haris Khan, Madhava Krishna, Dinesh Reddy Narapureddy","IIIT Hyderabad,International Institute of Information Technology, Hyderabad,IIIT HYDERABAD,Mohamed Bin Zayed University of Artificial Intelligence,Amazon",RGB-D Sensing and Perception I,"With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has become available. This prompts the question of how to utilize this data for geometric reasoning of scenes. While many RGB-D registration methods rely on geometric and feature-based similarity, we take a different approach. We use cycle-consistent keypoints as salient points to enforce spatial coherence constraints during matching, improving correspondence accuracy. Additionally, we introduce a novel pose block that combines a GRU recurrent unit with transformation synchronization, blending historical and multi-view data. Our approach surpasses previous self-supervised registration methods on ScanNet and 3DMatch, even outperforming some older supervised methods. We also integrate our components into existing methods, showing their effectiveness."
MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats,"Shenghai Yuan, Yizhuo Yang, Thien Hoang Nguyen, Thien-Minh Nguyen, Jianfei Yang, Fen Liu, Jianping Li, Han Wang, Lihua Xie","NANYANG TECHNOLOGICAL UNIVERSITY,Nangyang technological Univercity,University of Sydney,Nanyang Technological University,Guangdong University of Technology,Nanyang Technological University, Singapore,NanyangTechnological University",RGB-D Sensing and Perception I,"Introducing MMAUD: a Multi-Modal Anti-UAV Dataset, developed in response to the evolving challenges posed by small unmanned aerial vehicles (UAVs). These UAVs have the potential to transport harmful payloads or independently cause damage, necessitating comprehensive exploration of countermeasures. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on aerial detection, UAV-type classification, and trajectory estimationâ€”a perspective often overlooked but laden with substantial risks. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique aerial perspective vital for addressing real-world scenarios with higher fidelity compared to datasets reliant on specific vantage points. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions. Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement UAV threat assessments. MMAUD's dataset collection process follows a methodical strategy, selecting industrial sites characterized by ambient machinery noise to mirror real-world scenarios. This approach enhances the dataset's applicability, capturing nuanced challenges faced during proximate vehicular operations. MMAUD emerges as an indispensable resource for scholarly investigation and practical research, facilitated by meticulous methodologies. It plays a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond. Explore the dataset, codes, and designs at https://github.com/ntu-aris/MMAUD."
SupeRGB-D: Zero-Shot Instance Segmentation in Cluttered Indoor Environments,"Evin PÄ±nar Örnek, Aravindhan Krishnan, Shreekant Gayaka, Cheng-hao Kuo, Arnab Sen, Nassir Navab, Federico Tombari","TU Munich,Amazon Lab,,,,Amazon,Technische Universität München",RGB-D Sensing and Perception I,"Object instance segmentation is a key challenge for indoor robots navigating cluttered environments with many small objects. Limitations in 3D sensing capabilities often make it difficult to detect every possible object. While deep learning approaches may be effective for this problem, manually annotating 3D data for supervised learning is time-consuming. In this work, we explore zero-shot instance segmentation (ZSIS) from RGB-D data to identify unseen objects in a semantic category-agnostic manner. We introduce a zero-shot split for Tabletop Objects Dataset (TOD-Z) to enable this study and present a method that uses annotated objects to learn the objectness of pixels and generalize to unseen object categories in cluttered indoor environments. Our method, SupeRGB-D, groups pixels into small patches based on geometric cues and learns to merge the patches in a deep agglomerative clustering fashion. SupeRGB-D outperforms existing baselines on unseen objects while achieving similar performance on seen objects. We further show competitive results on the real dataset OCID. With its lightweight design (0.4 MB memory requirement), our method is extremely suitable for mobile and robotic applications. Additional DINO features can increase the performance with a higher memory requirement. The dataset split and code is available under www.github.com/evinpinar/supergb-d."
Mean Shift Mask Transformer for Unseen Object Instance Segmentation,"Yangxiao Lu, Yuqiao Chen, Nicholas Ruozzi, Yu Xiang","the University of Texas at Dallas,University of Texas at Dallas,The University of Texas at Dallas",RGB-D Sensing and Perception I,"Segmenting unseen objects from images is a critical perception skill that a robot needs to acquire. In robot manipulation, it can facilitate a robot to grasp and manipulate unseen objects. Mean shift clustering is a widely used method for image segmentation tasks. However, the traditional mean shift clustering algorithm is not differentiable, making it difficult to integrate it into an end-to-end neural network training framework. In this work, we propose the Mean Shift Mask Transformer (MSMFormer), a new transformer architecture that simulates the von Mises-Fisher (vMF) mean shift clustering algorithm, allowing for the joint training and inference of both the feature extractor and the clustering. Its central component is a hypersphere attention mechanism, which updates object queries on a hypersphere. To illustrate the effectiveness of our method, we apply MSMFormer to unseen object instance segmentation. Our experiments show that MSMFormer achieves competitive performance compared to state-of-the-art methods for unseen object instance segmentation."
SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion Using a 3D Recurrent U-Net,"Helin Cao, Sven Behnke",University of Bonn,RGB-D Sensing and Perception I,"We introduce SLCF-Net, a novel approach for the Semantic Scene Completion (SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates missing geometry and semantics in a scene from sequences of RGB images and sparse LiDAR measurements. The images are semantically segmented by a pre-trained 2D U-Net and a dense depth prior is estimated from a depth-conditioned pipeline fueled by Depth Anything. To associate the 2D image features with the 3D scene volume, we introduce Gaussian-decay Depth-prior Projection (GDP). This module projects the 2D features into the 3D volume along the line of sight with a Gaussian-decay function, centered around the depth prior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden 3D U-Net state using the sensor motion and design a novel loss to ensure temporal consistency. We evaluate our approach on the SemanticKITTI dataset and compare it with leading SSC approaches. The SLCF-Net excels in all SSC metrics and shows great temporal consistency."
Overparametrization Helps Offline-To-Online Generalization of Closed-Loop Control from Pixels,"Mathias Lechner, Ramin Hasani, Alexander Amini, Tsun-Hsuan Johnson Wang, Thomas Henzinger, Daniela Rus","Massachusetts Institute of Technology,Massachusetts Institute of Technology (MIT),IST Austria,MIT",Imitation Learning,"There is an ever-growing zoo of modern neural network models that can efficiently learn end-to-end control from visual observations. These advanced deep models, ranging from convolutional to vision transformers, from small to gigantic networks, have been extensively tested on offline image classification tasks. In this paper, we study these vision models with respect to the open-loop training to closed-loop generalization abilities, i.e., deployment realizes a causal feedback loop that is not present during training. This causality gap typically emerges in robotics applications such as autonomous driving, where a network is trained to imitate the control commands of a human. In this setting, two situations arise: 1) Closed-loop testing in-distribution, where the test environment shares properties with those of offline training data. 2) Closed-loop testing under distribution shifts and out-of-distribution. Contrary to recently reported results, we show that emph{under proper training guidelines}, all vision architectures perform indistinguishably well on in-distribution deployment, resolving the causality gap. In situation 2, We observe that scale is the strongest factor in improving closed-loop generalization regardless of the choice of the model architecture. Our results predict the trend that in the future we will see larger and larger models being used in offline-training-online-deployment imitation learning tasks in robotic applications."
Hierarchical Human-To-Robot Imitation Learning for Long-Horizon Tasks Via Cross-Domain Skill Alignment,"Zhenyang Lin, Yurou Chen, Zhiyong Liu","University of Chinese Academy of Sciences,Chinese Academy of Sciences,Institute of Automation Chinese Academy of Sciences",Imitation Learning,"For a general-purpose robot, it is desirable to imitate human demonstration videos that can effectively solve long-horizon tasks and perform novel ones. Recent advances in skill-based imitation learning have shown that extracting skill embedding from raw human videos is a promising paradigm to enable robots to cope with long-horizon tasks. However, generalization to unseen tasks in a different domain with a human prompt video poses a significant challenge due to the big embodiment and environment difference. To this end, we present Hierarchical Human-to-Robot Imitation Learning (H2RIL) that learns the mapping of cross-domain sensorimotor skills and utilizes it to generalize to unseen tasks given a human video in a different environment. To allow for generalizing zero-shot across environments and embodiments, H2RIL leverages task-agnostic play data for low-level policy training and paired human-robot data for both semantic and temporal skill embedding alignment. Extensive experiments in a simulated kitchen environment demonstrate that H2RIL significantly outperforms other prior baselines and is capable of generalizing to composable new tasks and adapting to Out-of-Distribution (OOD) tasks."
Policy Optimization by Looking Ahead for Model-Based Offline RL,"Yang Liu, Marius Hofert",The University of Hong Kong,Imitation Learning,"Offline reinforcement learning (RL) aims to optimize the policy, based on pre-collected data, to maximize the cumulative rewards after performing a sequence of actions. Existing approaches learn a value function from historical data, then guide the update of policy parameters by maximizing the value function at a single time. Driven by the gap between maximizing the cumulative rewards of RL and the greedy strategy of existing methods, we propose an approach of policy optimization by looking ahead (POLA) to mitigate the gap. Concretely, we optimize the policy on both current and future states where the future states are predicted by a transition model. A trajectory contains numerous actions before the task is done. Performing the best action at each time does not mean an optimal trajectory in the end. We need to allow sub-optimal or negative actions occasionally. But existing methods focus on generating the optimal action at each time according to the maximizing Q-value principle. This motivates our looking ahead approach. Besides, hidden confounding factors may affect the decision making process. To that end, we incorporate the correlations among dimensions of the state into the policy, providing more information about the environment for the policy to make decisions. Empirical results on the Mujoco dataset show the effectiveness of the proposed approach."
DINOBot: Robot Manipulation Via Retrieval and Alignment with Vision Foundation Models,"Norman Di Palo, Edward Johns",Imperial College London,Imitation Learning,"We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its end-effector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of visual foundation models enables unprecedented learning efficiency and generalisation. Videos and code are available at https://www.robot-learning.uk/dinobot"
Rank2Reward: Learning Shaped Reward Functions from Passive Video,"Daniel Yang, Davin Tjia, Jacob Herman Berg, Dima Damen, Pulkit Agrawal, Abhishek Gupta","Massachusetts Institute of Technology,University of Washington,University of Bristol,MIT",Imitation Learning,"Teaching robots novel skills with demonstrations via human-in-the-loop data collection techniques like kinesthetic teaching or teleoperation puts a heavy burden on human supervisors. In contrast to this paradigm, it is often significantly easier to provide raw, action-free visual data of tasks being performed. Moreover, this data can even be mined from video datasets or the web. Ideally, this data can serve to guide robot learning for new tasks in novel environments, informing both ""what"" to do and ""how"" to do it. A powerful way to encode both the ""what"" and the ""how"" is to infer a well-shaped reward function for reinforcement learning. The challenge is determining how to ground visual demonstration inputs into a well-shaped and informative reward function. We propose a technique Rank2Reward for learning behaviors from videos of tasks being performed without access to any low-level states and actions. We do so by leveraging the videos to learn a reward function that measures incremental ""progress"" through a task by learning how to temporally rank the video frames in a demonstration. By inferring an appropriate ranking, the reward function is able to guide reinforcement learning by indicating when task progress is being made. This ranking function can be integrated into an adversarial imitation learning scheme resulting in an algorithm that can learn behaviors without exploiting the learned reward function. We demonstrate the effectiveness of Rank2Reward at learning behaviors from raw video on a number of tabletop manipulation tasks in both simulations and on a real-world robotic arm. We also demonstrate how Rank2Reward can be easily extended to be applicable to web-scale video datasets."
A Generalized Acquisition Function for Preference-Based Reward Learning,"Evan Ellis, Gaurav Ghosal, Stuart Jonathan Russell, Anca Dragan, Erdem Bıyık","UC Berkeley,Carnegie Mellon University,University of California, Berkeley,University of California Berkeley,University of Southern California",Imitation Learning,"Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic environment, an assistive robotics environment with domain transfer, and a natural language processing problem with real datasets demonstrate the superior performance of our querying method over the state-of-the-art information gain method."
Human-Robot Deformation Manipulation Skill Transfer: Sequential Fabric Unfolding Method for Robots,"Tianyu Fu, Yunfeng Bai, Cheng Li, Fengming Li, Chaoqun Wang, Rui Song","Shandong University,Shandong Jianzhu University,shandong university",Imitation Learning,"Deformable object manipulation has been considered a challenging task for robots for its complex dynamics and the infinite-dimensional configuration space. Fabric unfolding manipulation takes on critical significance in the textile industry and household services. Accordingly, enabling robots to possess the above-mentioned skill has been confirmed as a crucial and challenging task. In this study, a general framework is developed for transferring human skills to robots in fabric unfolding manipulation. The developed framework comprises two key components (i.e., behavior cloning to learn human unfolding policy and learning from demonstration to transfer unfolding actions). A mixture density network is introduced, with the aim of addressing the multimodality in human policy. Moreover, task parameter weighting is considered during action generalization to adapt to a wide variety of unfolding scenarios. As revealed by the experimental results of this study, the framework can successfully unfold fabrics of different colors and sizes, and its performance can be comparable to human-level operation. Furthermore, the framework also can be applied to garment unfolding, and experiments suggest that it exhibits generalization."
Model Optimization in Deep Learning Based Robot Control for Autonomous Driving,"Sergio Paniego Blanco, Nikhil Paliwal, José M. Cañas","Universidad Rey Juan Carlos,Saarland University, Germany",Imitation Learning,"Deep Learning (DL) has been successfully used in robotics for perception tasks and end-to-end robot control. In the context of autonomous driving, this work explores and compares a variety of alternatives for model optimization to solve the visual lane-follow application in urban scenarios with an imitation learning approach. The optimization techniques include quantization, pruning, fine-tuning (retraining), and clustering, covering all the options available at the most common DL frameworks. TensorRT optimization for specific cutting-edge hardware devices has been also explored. For the comparison, offline metrics such as mean squared error and inference time are used. In addition, the optimized models have been evaluated in an online fashion using the autonomous driving state-of-the-art simulator CARLA and an assessment tool called Behavior Metrics, which provides holistic quantitative fine-grain data about robot performance. Typically the performance of robot applications depends both on the quality of the control decisions and also on their frequency. The studied optimized models significantly increase inference frequency without losing decision quality. The impact of each optimization alone has also been measured. This speed-up allows us to successfully run DL robot-control applications even in limited computing hardware. All the work presented here is open-source, including models, weights, assessment tool, and dataset, for easy replication and extension."
Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy,"Chenyang Cao, Zichen Yan, Renhao Lu, Junbo Tan, Xueqian Wang","Tsinghua University,Center for Artificial Intelligence and Robotics, Graduate School",Reinforcement Learning I,"Offline goal-conditioned reinforcement learning (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based Supervised Learning (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a benchmark based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GCRL algorithms and one offline safe RL algorithm. As a result, our method outperforms the existing state-of-the-art methods to a large extent. Furthermore, we validate the practicality and effectiveness of RbSL by deploying it on a real Panda manipulator. Code is available at https://github.com/Sunlighted/RbSL.git."
Reinforcement Learning in a Safety-Embedded MDP with Trajectory Optimization,"Fan Yang, Wenxuan Zhou, Zuxin Liu, Ding Zhao, David Held","University of Michigan,Carnegie Mellon University",Reinforcement Learning I,"Safe Reinforcement Learning (RL) plays an important role in applying RL algorithms to safety-critical real-world applications, addressing the trade-off between maximizing rewards and adhering to safety constraints. This work introduces a novel approach that combines RL with trajectory optimization to manage this trade-off effectively. Our approach embeds safety constraints within the action space of a modified Markov Decision Process (MDP). The RL agent produces a sequence of actions that are transformed into safe trajectories by a trajectory optimizer, thereby effectively ensuring safety and increasing training stability. This novel approach excels in its performance on challenging Safety Gym tasks, achieving significantly higher rewards and near-zero safety violations during inference. The method's real-world applicability is demonstrated through a safe and effective deployment in a real robot task of box-pushing around obstacles. Further insights are available from the videos on our website: https://sites.google.com/view/safemdp"
Distributional Reinforcement Learning with Sample-Set Bellman Update,"Weijian Zhang, Jianshu Wang, Yu Yang","Nanjing University,National Key Laboratory for Novel Software Technology, Nanjing U",Reinforcement Learning I,"Distributional Reinforcement Learning (DRL) not only endeavors to optimize expected returns but also strives to accurately characterize the full distribution of these returns, a key aspect in enhancing risk-aware decision-making. Previous DRL implementations often inappropriately treat statistical estimations as concrete samples, which undermines the integrity of learning. While several studies have addressed this issue, they frequently give rise to new complications, including computational burdens and diminished stochastic behavior. In our work, we present a novel DRL framework that leverages the Gaussian mixture model to adeptly depict the distribution of returns. This approach ensures precise, authentic sampling critical for robust learning, while also preserving computational tractability. Through extensive evaluation of a diverse array of 59 Atari games, our method not only surpasses the efficacy of prior DRL algorithms but also presents formidable competition to contemporary top-tier RL algorithms, signifying a substantial advancement in the field."
Learning Adaptive Safety for Multi-Agent Systems,"Luigi Berducci, Shuo Yang, Rahul Mangharam, Radu Grosu","TU Wien,University of Pennsylvania",Reinforcement Learning I,"Ensuring safety in dynamic multi-agent systems is challenging due to limited information about the other agents. Control Barrier Functions (CBFs) are showing promise for safety assurance but current methods make strong assumptions about other agents and often rely on manual tuning to balance safety, feasibility, and performance. In this work, we delve into the problem of adaptive safe learning for multi-agent systems with CBF. We show how emergent behaviour can be profoundly influenced by the CBF configuration, highlighting the necessity for a responsive and dynamic approach to CBF design. We present ASRL, a novel adaptive safe RL framework, to fully automate the optimization of policy and CBF coefficients, to enhance safety and long-term performance through reinforcement learning. By directly interacting with the other agents, ASRL learns to cope with diverse agent behaviours and maintains the cost violations below a desired limit. We evaluate ASRL in a multi-robot system and competitive multi-agent racing, against learning-based and control-theoretic approaches. We empirically demonstrate the efficacy of ASRL, and assess generalization and scalability to out-of-distribution scenarios."
Contrastive Initial State Buffer for Reinforcement Learning,"Nico Messikommer, Yunlong Song, Davide Scaramuzza",University of Zurich,Reinforcement Learning I,"In Reinforcement Learning, the trade-off between exploration and exploitation poses a complex challenge for achieving efficient learning from limited samples. While recent works have been effective in leveraging past experiences for policy updates, they often overlook the potential of reusing past experiences for data collection. Independent of the underlying RL algorithm, we introduce the concept of a Contrastive Initial State Buffer, which strategically selects states from past experiences and uses them to initialize the agent in the environment in order to guide it toward more informative states. We validate our approach on two complex robotic tasks without relying on any prior information about the environment: (i) locomotion of a quadruped robot traversing challenging terrains and (ii) a quadcopter drone racing through a track. The experimental results show that our initial state buffer achieves higher task performance than the nominal baseline while also speeding up training convergence."
Safety Optimized Reinforcement Learning Via Multi-Objective Policy Optimization,"Homayoun Honari, Mehran Ghafarian Tamizi, Homayoun Najjaran",University of Victoria,Reinforcement Learning I,"Safe reinforcement learning (Safe RL) refers to a class of techniques that aim to prevent RL algorithms from violating constraints in the process of decision-making and exploration during trial and error. In this paper, a novel model-free Safe RL algorithm, formulated based on the multi-objective policy optimization framework is introduced where the policy is optimized towards optimality and safety, simultaneously. The optimality is achieved by the environment reward function that is subsequently shaped using a safety critic. The advantage of the textit{Safety Optimized RL (SORL)} algorithm compared to the traditional Safe RL algorithms is that it omits the need to constrain the policy search space. This allows SORL to find a natural tradeoff between safety and optimality without compromising the performance in terms of either safety or optimality due to strict search space constraints. Through our theoretical analysis of SORL, we propose a condition for SORL's converged policy to guarantee safety and then use it to introduce an aggressiveness parameter that allows for fine-tuning the mentioned tradeoff. The experimental results obtained in seven different robotic environments indicate a considerable reduction in the number of safety violations along with higher, or competitive, policy returns, in comparison to six different state-of-the-art Safe RL methods. The results demonstrate the significant superiority of the proposed SORL algorithm in safety-critical applications."
Differentially Encoded Observation Spaces for Perceptive Reinforcement Learning,"Lev Grossman, Brian Plancher","Berkshire Grey,Barnard College, Columbia University",Reinforcement Learning I,"Perceptive deep reinforcement learning (DRL) has lead to many recent breakthroughs for complex AI systems leveraging image-based input data. Applications of these results range from super-human level video game agents to dexterous, physically intelligent robots. However, training these perceptive DRL-enabled systems remains incredibly compute and memory intensive, often requiring huge training datasets and large experience replay buffers. This poses a challenge for the next generation of field robots that will need to be able to learn on the edge in order to adapt to their environments. In this paper, we begin to address this issue through differentially encoded observation spaces. By reinterpreting stored image-based observations as a video, we leverage lossless differential video encoding schemes to compress the replay buffer without impacting training performance. We evaluate our approach with three state-of-the-art DRL algorithms and find that differential image encoding reduces the memory footprint by as much as 14.2x and 16.7x across tasks from the Atari 2600 benchmark and the DeepMind Control Suite (DMC) respectively. These savings also enable large-scale perceptive DRL that previously required paging between flash and RAM to be run entirely in RAM, improving the latency of DMC tasks by as much as 27%."
Projected Task-Specific Layers for Multi-Task Reinforcement Learning,"Josselin Somerville Roberts, Julia Di",Stanford University,Reinforcement Learning I,"Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm."
BiÂ²Lane: Bi-Directional Temporal Refinement with Bi-Level Feature Aggregation for 3D Lane Detection,"Chengxin Li, Yihui Hu, Zewen Zheng, Xiang Gao, Yongqiang Mou, Peng Nie, Jun Li","South China Normal University,GAC R&D CENTER,Guangdong University of Technology,Guangzhou Automobile Group Co Ltd",Reinforcement Learning I,"Monocular 3D lane detection has recently received increasing research attention in autonomous driving due to its application effectiveness and simplicity. However, depending solely on the limited semantic information from a single image makes current monocular detection methods unable to deal with complex scenarios, such as occluded, blurred, and unaligned scenes. In this study, we introduce an end-to-end framework named BiÂ²Lane which models temporal dependency in a continuous sequence. It recurrently utilizes detected lanes within historical frames as prior information to achieve robust lane detection. Additionally, BiÂ²Lane employs temporal reverse refinement together with temporal forward refinement to achieve bi-directional temporal refinement (BDTR) while maintaining a robust temporal dependency. For the refined features of different frames, we design a bi-level feature aggregation module (BLFA) to fuse them in both point-level and line-level manners, enabling a comprehensive feature representation to deal with complicated road scenes. Extensive experiments conducted on the OpenLane dataset demonstrate the superiority of BiÂ²Lane, achieving a notable F1 score of 63.8% using a simple ResNet50 backbone, surpassing the performance of existing state-of-the-art methods."
Exploitation-Guided Exploration for Semantic Embodied Navigation,"Justin Wasserman, Girish Chowdhary, Abhinav Gupta, Unnat Jain","University of Illinois at Urbana-Champaign,University of Illinois at Urbana Champaign,Carnegie Mellon University,Indian Institute of Technology Kanpur",Vision-Based Navigation,"In the recent progress in embodied navigation and sim-to-robot transfer, modular policies have emerged as a de facto framework. However, there is more to compositionality beyond the decomposition of the learning load into modular components. In this work, we investigate a principled way to syntactically combine these components. Particularly, we propose Exploitation-Guided Exploration (XGX) where separate modules for exploration and exploitation come together in a novel and intuitive manner. We configure the exploitation module to take over in the deterministic final steps of navigation i.e. when the goal becomes visible. Crucially, an exploitation module teacher-forces the exploration module and continues driving an overridden policy optimization. XGX, with effective decomposition and novel guidance, improves the state-of-the-art performance on the challenging object navigation task from 70% to 73%. Along with better accuracy, through targeted analysis, we show that XGX is also more efficient at goal-conditioned exploration. Finally, we show sim-to-real transfer to robot hardware and XGX performs over two-fold better than the best baseline from simulation benchmarking."
Teach and Repeat Navigation: A Robust Control Approach,"Payam Nourizadeh, Michael Milford, Tobias Fischer","QUT Centre for Robotics,Queensland University of Technology",Vision-Based Navigation,"Robot navigation requires an autonomy pipeline that is robust to environmental changes and effective in varying conditions. Teach and Repeat (T&R) navigation has shown high performance in autonomous repeated tasks under challenging circumstances, but research within T&R has predominantly focused on motion planning as opposed to robust motion control. In this paper, we propose a novel T&R system based on a robust motion control technique for a skid-steering mobile robot using sliding-mode control that effectively handles uncertainties due to sensor noises, parametric uncertainties, and wheel-terrain interaction. We theoretically demonstrate that the proposed T&R system is globally stable and robust while considering the uncertainties of the closed-loop system. When deployed on a Clearpath Jackal robot, we show the global stability of the proposed system in both indoor and outdoor environments covering different terrains, outperforming previous state-of-the-art methods that had a higher mean average trajectory error and became unstable in these challenging environments. This paper makes an important step towards long-term autonomous T&R navigation with ensured safety guarantees."
LOC-ZSON: Language-Driven Object-Centric Zero-Shot Object Retrieval and Navigation,"Tianrui Guan, Yurou Yang, Harry Cheng, Muyuan Lin, Richard Kim, Rajasimman Madhivanan, Arnab Sen, Dinesh Manocha","University of Maryland,Amazon,Amazon.com LLC,Amazon, Lab,,,,Amazon.com",Vision-Based Navigation,"In this paper, we present LOC-ZSON, a novel Language-driven Object-Centric image representation for object navigation task within complex scenes. We propose an object-centric image representation and corresponding losses for visual-language model (VLM) fine-tuning, which can handle complex object-level queries. In addition, we design a novel LLM-based augmentation and prompt templates for stability during training and zero-shot inference. We implement our method on Astro robot and deploy it in both simulated and real-world environments for zero-shot object navigation. We show that our proposed method can achieve an improvement of 1.38-13.38% in terms of text-to-image recall on different benchmark settings for the retrieval task. For object navigation, we show the benefit of our approach in simulation and real world, showing 5% and 16.67% improvement in terms of navigation success rate, respectively."
Real-Time Localization for Closed-Loop Control of Assistive Furniture,"Lixuan Tang, Chuanfang Ning, George Adaimi, Auke Ijspeert, Alexandre Alahi, Anastasia Bolotnikova","EPFL,École Polytechnique Fédérale de Lausanne (EPFL)",Vision-Based Navigation,"For people with limited mobility, navigating in cluttered indoor environments is challenging. In this work, we propose a mobile assistive furniture suite that is designed to ease the life of people with special needs in indoor movement. To enable intelligent coordination of this system, a key component is the localization of each mobile furniture. The challenge is to assess the state of an arbitrary living environment so that the estimation can be used as a realtime feedback signal for autonomous closed-loop control of mobile furniture. We propose a perception pipeline that addresses these challenges. A machine learning model is designed and trained to jointly achieve multi-object semantic keypoint detection and classification in camera images. The synthetic data generation is employed to augment the training set and boost the model performance. A robust point cloud registration uses the detected semantic keypoints and depth information to estimate poses of the furniture. Tracking is applied to achieve smooth estimation. A high-performance accelerator that optimizes the efficiency of using heterogeneous devices is applied to achieve real-time performance. This visual perception pipeline is used in closed-loop control to steer the mobile furniture from initial to a desired location demonstrated in experiments on real hardware."
Uncertainty-Aware Hybrid Paradigm of Nonlinear MPC and Model-Based RL for Offroad Navigation: Exploration of Transformers in the Predictive Model,"Faraz Lotfi, Khalil Virji, Farnoosh Faraji, Lucas Berry, Andrew Holliday, David Paul Meger, Gregory Dudek",McGill University,Vision-Based Navigation,"In this paper, we investigate a hybrid scheme that combines nonlinear model predictive control (MPC) and model-based reinforcement learning (RL) for navigation planning of an autonomous model car across offroad, unstructured terrains without relying on predefined maps. Our innovative approach takes inspiration from BADGR, an LSTM-based network that primarily concentrates on environment modeling, but distinguishes itself by substituting LSTM modules with transformers to greatly elevate the performance our model. Addressing uncertainty within the system, we train an ensemble of predictive models and estimate the mutual information between model weights and outputs, facilitating dynamic horizon planning through the introduction of variable speeds. Further enhancing our methodology, we incorporate a nonlinear MPC controller that accounts for the intricacies of the vehicle's model and states. The model-based RL facet produces steering angles and quantifies inherent uncertainty. At the same time, the nonlinear MPC suggests optimal throttle settings, striking a balance between goal attainment speed and managing model uncertainty influenced by velocity. In the conducted studies, our approach excels over the existing baseline by consistently achieving higher metric values in predicting future events and seamlessly integrating the vehicle's kinematic model for enhanced decision-making."
Robot Navigation in Unseen Environments Using Coarse Maps,"Chengguang Xu, Christopher Amato, Lawson L.S. Wong",Northeastern University,Vision-Based Navigation,"Metric occupancy maps are widely used in autonomous robot navigation systems. However, when a robot is deployed in an unseen environment, building an accurate metric map is time-consuming. Can an autonomous robot directly navigate in previously unseen environments using coarse maps? In this work, we propose the Coarse Map Navigator (CMN), a navigation framework that can perform robot navigation in unseen environments using different coarse maps. To do so, CMN addresses two challenges: (1) novel and realistic visual observations; (2) error and misalignment on coarse maps. To tackle novel visual observations in unseen environments, CMN learns a deep perception model that maps the visual input from various pixel spaces to the local occupancy grid space. To tackle the error and misalignment on coarse maps, CMN extends the Bayesian filter and maintains a belief directly on coarse maps using the predicted local occupancy grids as observations. Using the latest belief, CMN extracts a global heuristic vector that guides the planner to find a local navigation action. Empirical results demonstrate that CMN achieves high navigation success rates in unseen environments, significantly outperforming baselines, and is robust to different coarse maps."
Bicode: A Hybrid Blinking Marker System for Event Cameras,"Takuya Kitade, Wataru Yamada, Keiichi Ochiai, Michita Imai","NTT DOCOMO, INC.,Keio University",Vision-Based Navigation,"In the field of robotics, tag systems play an important role in various applications, such as object identification and robot control in real-world environments. While typical visual markers use two-dimensional (2D) patterns and RGB cameras for recognizing object IDs and poses, achieving long-distance recognition necessitates increasing marker size and camera magnification to ensure the required resolution. Furthermore, the growing adoption of event cameras in robotics captures rapid changes in pixel brightness but faces limitations in recognizing stationary 2D markers. Although compact blinker markers using blinking light-emitting diodes (LEDs) achieve long-distance recognition, they are constrained by the number of IDs or recognition speed when used with standard RGB cameras. In addition, recognizing object pose using only a single blinking LED presents challenges. To address these challenges, we introduce â€˜Bicode,â€™ an indoor visual marker designed for event cameras. Bicode seamlessly integrates 2D and blinker markers within a single marker unit.We have developed prototypes of 2.5, 5, and 10 cm square acrylic 2D markers, each equipped with a single LED blinking at 1 kHz, enabling recognition with an event camera. Our experiments revealed the effects of marker size, LED light quantity, recognition distance, and angle, external lighting conditions, and camera or marker movement on accuracy. Notably, using the 5 cm marker, we confirmed its compatibility to recognize IDs at distances exceeding 20 m, and pose recognition at 2.5 m was confirmed."
RAPIDFlow: Recurrent Adaptable Pyramids with Iterative Decoding for Efficient Optical Flow Estimation,"Henrique Morimitsu, Zhu Xiaobin, Roberto Marcondes Cesar Junior, Xiangyang Ji, Xu-cheng Yin","University of Science and Technology Beijing,University of São Paulo USP,Tsinghua University",Vision-Based Navigation,"Extracting motion information from videos with optical flow estimation is vital in multiple practical robot applications. Current optical flow approaches show remarkable accuracy, but top-performing methods have high computational costs and are unsuitable for embedded devices. Although some previous works have focused on developing low-cost optical flow strategies, their estimation quality has a noticeable gap with more robust methods. In this paper, we develop a novel method to efficiently estimate high-quality optical flow in embedded devices. Our proposed RAPIDFlow model combines efficient NeXt1D convolution blocks with a fully recurrent structure based on feature pyramids to decrease computational costs without significantly impacting estimation accuracy. The adaptable recurrent encoder produces multi-scale features with a single shared block, which allows us to adjust the pyramid length at inference time and make it more robust to changes in input size. Also, it enables our model to offer multiple tradeoffs between accuracy and speed to suit different applications. Experiments using a Jetson Orin NX embedded system on the MPI-Sintel and KITTI public benchmarks show that RAPIDFlow outperforms previous approaches by significant margins at faster speeds."
Modeling and Design of Lattice-Reinforced Pneumatic Soft Robots,"Dong Wang, Chengru Jiang, Guo-Ying Gu","Shanghai Jiao Tong University,Shanghai Jiaotong University",Soft Robot Materials and Design II,"Lattice metamaterials exhibit diverse functions and complex spatial deformations by rational structural design. Here, lattice metamaterials are exploited to design pneumatic soft robots with programmable bending, twisting and elongation deformations. The system comprises an elastomeric tube reinforced by lattice metamaterials. We develop an analytical framework to model the twisting, bending and elongation finite deformation taking into account the geometric orthotropy and nonlinear elasticity. We experimentally validate our modeling approach and investigate the effects of geometric patterns and input loading on the soft actuatorsâ€™ deformation. Theoretical guided design of lateral-climbing soft robots and exploration soft manipulators are demonstrated. The soft actuator could exhibit a combined twisting-bending-elongation deformation by lattice superimposition. The proposed structural design method paves the way for designing soft robots with complex and dexterous deformations."
Design and Analysis of Soft Hybrid-Driven Manipulator with Variable Stiffness and Multiple Motion Patterns,"Xin Fu, Daohui Zhang, Liyan Mo, Kai Li, Xingang Zhao","Shenyang Institute of Automation Chinese Academy of Sciences,Shenyang Institute of Automation, Chinese Academy of Sciences,Chinese Academy of Sciences(CAS), University of Chinese Academy ",Soft Robot Materials and Design II,"Abstractâ€” Soft manipulators offer the advantages of safety and adaptability. However, due to insufficient stiffness and single motion mode limitations, existing soft manipulators usually exhibit low load capacity and small working space. To address this problem, we propose a novel soft hybrid-driven manipulator with continuous stiffness control capability and multiple motion patterns (bending, rotation, and elongation). Furthermore, we develop kinematic and stiffness models based on the constant curvature assumption. The soft robot consists of a soft bellow actuator and inextensible rigid skeletons, which exhibits a high extension ratio and low input pressure. With the antagonistic actuation of tendon-pulling and air-pushing, the robot can achieve independent control over stiffness and position in three-dimensional space. The performance associated with the designed soft hybrid-driven manipulator is experimentally verified. The robot can achieve an elongation of 198% and a maximum bending angle of 240Â°. The robot can also increase stiffness by increasing internal air pressure to resist deformation caused by external loads. Additionally, tracking experiments with various trajectories in 3D space verify the accuracy of the kinematic model, which indicates that the soft manipulator possesses a large workspace and stable motion capabilities."
"Directly 3D Printed, Pneumatically Actuated Multi-Material Robotic Hand","Hanna Matusik, Chao Liu, Daniela Rus","MIT,Massachusetts Institute of Technology",Soft Robot Materials and Design II,"Soft robotic manipulators with many degrees of freedom can carry out complex tasks safely around humans. However, manufacturing of soft robotic hands with several degrees of freedom requires a complex multi-step manual process, which significantly increases their cost. We present a design of a multi-material 15 DoF robotic hand with five fingers including an opposable thumb. Our design has 15 pneumatic actuators based on a series of hollow chambers that are driven by an external pressure system. The thumb utilizes rigid joints and the palm features internal rigid structure and soft skin. The design can be directly 3D printed using a multi-material additive manufacturing process without any assembly process and therefore our hand can be manufactured for less than 300 dollars. We test the hand in conjunction with a low-cost vision-based teleoperation system on different tasks."
Soft Hand Extension Glove with Thumb Abduction and Extension Assistance,"Disheng Xie, Yujie Su, Xiangqian Shi, Zheng Li, Kai Yu Tong","The Chinese University of Hong Kong,The Chinese Unverisity of Hong Kong,the chinese university of hong kong",Soft Robot Materials and Design II,"Hand extension is crucial for stroke survivors with spasticity, where their fingers become rigid and their thumb remains curled within the palm. Due to the underactuated nature of the hand, the dominance of flexor muscles over extensors, and the limited surface area available, developing an extension glove with thumb assistance poses a challenge for researchers. This paper introduces a fully wearable soft hand extension glove based on the X-pouch and strap system, addressing the above challenges. The glove enables adequate finger extension, thumb abduction, and extension for high MAS score patients. Modelling and testing revealed extension torques of up to 2.7 Nm at the MCP joint and 0.67 Nm at the PIP and DIP joints. Performance evaluation, including comparison with existing methods, demonstrated the glove's superior extension capabilities using a model hand with realistic stiffness. Furthermore, the glove's effectiveness was confirmed through testing on a stroke patient with MAS = 2, validating its on-body functionality."
Design and Characterization of a Soft Flat Tube Twisting Actuator,"Hao Liu, Changchun Wu, Senyuan Lin, Yonghua Chen",The University of Hong Kong,Soft Robot Materials and Design II,"Soft actuators have shown advantages of adaptiveness, large deformation, and safe human-robot interaction, making them suitable for various applications. Herein, a novel soft flat tube twisting actuator (SFTTA) is proposed. The SFTTA is composed of a folded flat tube sandwiched between two silicone rubber laminates. When inflated by compressed air, the folded corners of the flat tube tend to unfold, resulting in the twist of the actuator to a helical structure. The SFTTA has great scalability. It can be fabricated through simple processes with low-cost materials. For a sample SFTTA with the size of a human finger, it can twist 5400 at an air pressure of 300 kPa. In general, SFTTA based actuators can twist 9.6 degree per millimeter in length, which is significantly larger than previously reported soft twisting actuators. Additionally, the composite-like SFTTA allows mechanical property programming through the alteration of folding patterns of the flat tube and the material structure of the elastomer laminates. Finally, an extensible soft gripper based on flat tube actuators and a robotic wrist module are developed, and their rotation is realized by the proposed SFTTA actuator."
Self-Retractable Soft Growing Robots for Reliable and Fast Retraction While Preserving Their Inherent Advantages,"Nam Gyun Kim, Dongoh Seo, Shinwoo Park, Jee-Hwan Ryu","Korea Advanced Institute of Science and Technology,KAIST",Soft Robot Materials and Design II,"Soft growing robots have garnered significant research interest owing to their unique locomotion. However, real-world applications of these robots are limited by challenges in achieving reversible and repeatable operations, particularly when faced with buckling during retraction. Although a va- riety of retraction mechanisms have been developed, many necessitate the installation of extra rigid hardware at the distal part, compromising the inherent benefits of soft growing robots. Existing soft retraction mechanisms that maintain these advantages tend to be relatively slow and rely on heavy driving fluids. This study introduces a soft retraction mechanism that depends exclusively on the existing pneumatic force, eliminating the need for additional rigid hardware, power sources, or complex control procedures. This mechanism enables rapid and reliable retraction of soft growing robots without sacrificing their inherent advantages or interfering with their inner chan- nels during retraction. The proposed mechanismâ€™s straightfor- ward structure facilitates easy integration with a wide range of tip mounts, steering mechanisms, and other application- specific soft growing robots. This research offers an analysis and experimental examination of the operating principles and behaviors of the proposed mechanism. It also presents the design guidelines and fabrication details for the mechanism, as well as a demonstration of its swift and buckling-free retraction."
"High-Curvature, High-Force, Vine Robot for Inspection","Mijaíl Jaén Mendoza Flores, Nicholas Naclerio, Elliot Hawkes","University of California Santa Barbara,University of California, Santa Barbara",Soft Robot Materials and Design II,"Robot performance has advanced considerably both in and out of the factory, however in tightly constrained, unknown environments such as inside a jet engine or the human heart, current robots are less adept. In such cases where a borescope or endoscope canâ€™t reach, disassembly or surgery are costly. One promising inspection device inspired by plant growth are â€œvine robotsâ€ that can navigate cluttered environments by extending from their tip. Yet, these vine robots are currently limited in their ability to simultaneously steer into tight curvatures and apply substantial forces to the environment. Here, we propose a plant-inspired method of steering by asymmetrically lengthening one side of the vine robot to enable high curvature and large force application. Our key development is the introduction of an extremely anisotropic, composite, wrinkled film with elastic moduli 400x different in orthogonal directions. The film is used as the vine robot body, oriented such that it can stretch over 120% axially, but only 3% circumferentially. With the addition of controlled layer jamming, this film enables a steering method inspired by plants in which the circumference of the robot is inextensible, but the sides can stretch to allow turns. This steering method and body pressure do not work against each other, allowing the robot to exhibit higher forces and tighter curvatures than previous vine robot architectures. This work advances the abilities of vine robotsâ€“and robots more generallyâ€“to not only access tightly constrained environments, but perform useful work once accessed."
Robotic Modules for a Continuum Manipulator with Variable Stiffness Joints,"Linda Paterno, Canberk Sozer, Sujit Sahu, Arianna Menciassi","Scuola Superiore Sant'Anna,The University of Sheffield,INDIAN INSTITUTE OF TECHNOLOGY PATNA, BIHTA,Scuola Superiore Sant'Anna - SSSA",Soft Robot Materials and Design II,"This study introduces a novel robotic module that integrates three spring-reinforced soft actuators for positioning the module in 3D space. This is achieved by utilizing a ball joint as the rotation center and leveraging the spring elements not only as reinforcement structures but also as inductive sensors. Additionally, soft pads are strategically placed around the ball joint to adjust the module stiffness irrespective of its position. Both actuation and stiffening mechanisms are independently controlled by pressure. Design, experimental characterization, and closed-loop control of the module are reported. In addition, a multifunctional manipulator that is built by integrating three modules in a series is demonstrated. A specific architecture has been pursued to reduce the overall number of fluidic tubes required when adding a new module. It resulted in a manipulator with continuum soft actuators, but independent variable stiffness joints, which are the key feature for guaranteeing different bending angles of each segment. Results show that a single module can bend up to 30Â° omnidirectionally, its stiffness can increase up to 95% in a controllable way, and the output voltage change of the springs can be employed for position sensing. This design offers a highly compact, lightweight, and low-cost solution exploitable in a wide range of applications, from medical to rescue missions, where actions behind obstacles in highly confined areas are needed."
"A Modular, Tendon Driven Variable Stiffness Manipulator with Internal Routing for Improved Stability and Increased Payload Capacity","Kyle L. Walker, Alix Partridge, Hsing-yu Chen, Rahul Ramachandran, Adam A. Stokes, Kenjiro Tadakuma, Lucas Cruz Da Silva, Francesco Giorgio-Serchi","The National Robotarium,Univeristy of Bristol,The National Robotarium, Heriot-Watt University,University of Edinburgh,Tohoku University,SENAI CIMATEC",Soft Robot Materials and Design II,"Stability and reliable operation under a spectrum of environmental conditions is still an open challenge for soft and continuum style manipulators. The inability to carry sufficient load and effectively reject external disturbances are two drawbacks which limit the scale of continuum designs, preventing widespread adoption of this technology. To tackle these problems, this work details the design and experimental testing of a modular, tendon driven bead-style continuum manipulator with tunable stiffness. By embedding the ability to independently control the stiffness of distinct sections of the structure, the manipulator can regulate itâ€™s posture under greater loads of up to 1kg at the end-effector, with reference to the flexible state. Likewise, an internal routing scheme vastly improves the stability of the proximal segment when operating the distal segment, reducing deviations by at least 70.11%. Operation is validated when gravity is both tangential and perpendicular to the manipulator backbone, a feature uncommon in previous designs. The findings presented in this work are key to the development of larger scale continuum designs, demonstrating that flexibility and tip stability under loading can co-exist without compromise."
EgoPAT3Dv2: Predicting 3D Action Target from 2D Egocentric Vision for Human-Robot Interaction,"Irving Fang, Yuzhong Chen, Yifan Wang, Jianghan Zhang, Qiushi Zhang, Jiali Xu, Xibo He, Weibo Gao, Hao Su, Yiming Li, Chen Feng","New York University,Xi'an Jiaotong University,North Carolina State University",Deep Learning for Visual Perception II,"A robot's ability to anticipate the 3D action target location of a hand's movement from egocentric videos can greatly improve safety and efficiency in human-robot interaction (HRI). While previous research predominantly focused on semantic action classification or 2D target region prediction, we argue that predicting the action target's 3D coordinate could pave the way for more versatile downstream robotics tasks, especially given the increasing prevalence of headset devices. This study expands EgoPAT3D, the sole dataset dedicated to egocentric 3D action target prediction. We augment both its size and diversity, enhancing its potential for generalization. Moreover, we substantially enhance the baseline algorithm by introducing a large pre-trained model and human prior knowledge. Remarkably, our novel algorithm can now achieve superior prediction outcomes using solely RGB images, eliminating the previous need for 3D point clouds and IMU input. Furthermore, we deploy our enhanced baseline algorithm on a real-world robotic platform to illustrate its practical utility in straightforward HRI tasks. The demonstrations showcase the real-world applicability of our advancements and may inspire more HRI use cases involving egocentric vision. All code and data are open-sourced and can be found on the project website."
Distribution-Aware Continual Test-Time Adaptation for Semantic Segmentation,"Jiayi Ni, Senqiao Yang, Ran Xu, Jiaming Liu, Xiaoqi Li, Wenyu Jiao, Zehui Chen, Yi Liu, Shanghang Zhang","Peking University,Harbin Institute of Technology, Shenzhen,Beijing University of Posts and Telecommunications,University of Washington,University of Science and Technology of China,Baidu Inc.",Deep Learning for Visual Perception II,"Since autonomous driving systems usually face dynamic and ever-changing environments, continual test-time adaptation (CTTA) has been proposed as a strategy for transferring deployed models to continually changing target domains. However, the pursuit of long-term adaptation often introduces catastrophic forgetting and error accumulation problems, which impede the practical implementation of CTTA in the real world. Recently, existing CTTA methods mainly focus on utilizing a majority of parameters to fit target domain knowledge through self-training. Unfortunately, these approaches often amplify the challenge of error accumulation due to noisy pseudo-labels, and pose practical limitations stemming from the heavy computational costs associated with entire model updates. In this paper, we propose a distribution-aware tuning (DAT) method to make the semantic segmentation CTTA efficient and practical in real-world applications. DAT adaptively selects and updates two small groups of trainable parameters based on data distribution during the continual adaptation process, including domain-specific parameters (DSP) and task-relevant parameters (TRP). Specifically, DSP exhibits sensitivity to outputs with substantial distribution shifts, effectively mitigating the problem of error accumulation. In contrast, TRP are allocated to positions that are responsive to outputs with minor distribution shifts, which are fine-tuned to avoid the catastrophic forgetting problem. In addition, since CTTA is a temporal task, we introduce the Parameter Accumulation Update (PAU) strategy to collect the updated DSP and TRP in target domain sequences. We conducted extensive experiments on two widely-used semantic segmentation CTTA benchmarks, achieving competitive performance and efficiency compared to previous state-of-the-art methods."
STNet: Spatio-Temporal Fusion-Based Self-Attention for Slip Detection in Visuo-Tactile Sensors,"Jin Lu, Bangyan Niu, Huan Ma, Zhu Jiafeng, Jingjing Ji",Huazhong University of Science and Technology,Deep Learning for Visual Perception II,"Slip detection plays a pivotal role in the dexterity of robotics, improving the reliability and precision of manipulations but also contributing to safety, efficiency, and adaptability. Deep learning-based slip detection algorithms commonly difficult to concentrate on key features when faced with dense 3D shape data obtained by visuo-tactile sensors. Data from noncontact locations can interfere with slip judgements and the ignorance of interframe linkage can also lead to slip detection failure. In this paper, a new spatio-temporal sequences fusion-based self-attention, STNet, is proposed to perform slip detection by allocating more attention to the object-sensor contact area when processing complex 3D shape data. A binocular visuo-tactile system (BVTS) is designed and fabricated for dataset construction. The entire 3D shape dataset containing 4 motion patterns, including stationary, pressing, rolling and slipping. Self-attention architecture with and without spatio-temporal sequences fusion mechanism (denoted as STNet and TemNet, respectively) are trained based on the same dataset. The experiments show the validity of STNet, which can reach 98.91% slip detection accuracy. Meanwhile, the ablation studies confirm the effectiveness of the spatio-temporal sequences fusion mechanism."
Commonsense Spatial Knowledge-Aware 3-D Human Motion and Object Interaction Prediction,Sang Uk Lee,Motional,Deep Learning for Visual Perception II,"We propose a novel 3-D human motion and object interaction prediction model that is aware of commonsense knowledge about human--object interaction. We jointly predict human joint motion and human--object interactions. The two prediction results are combined to enforce commonsense knowledge, such as ``if the human right hand is predicted to be in contact with an object after 1 second, the distance between the right hand and an object should also be predicted to be small,'' explicit to the model. Our model uses the raw point cloud representation of the surrounding objects in the environment as input. Using raw point cloud representation allows us to model commonsense knowledge easily and improve accuracy. In particular, it does not require a separate perception system (e.g., object classification, object pose estimation, and so on), as in previous studies, and thus is robust to perception errors. Our model applies a cross-attention mechanism to fuse the environmental point cloud and past human joint poses. The surrounding environment context and past human joint poses are two heterogeneous inputs and cross-attention can be a powerful approach to fuse them. Our model is validated on the KIT Whole-Body Human Motion (WBHM) dataset."
High-Degrees-Of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning,"Lennart Schulze, Hod Lipson","Columbia University,Columbia university",Deep Learning for Visual Perception II,"A robot self-model is a task-agnostic representation of the robot's physical morphology that can be used for motion planning tasks in the absence of a classical geometric kinematic model. In particular, when the latter is hard to engineer or the robot's kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of the robot's workspace dimension. We demonstrate the capabilities of this model on motion planning tasks as an exemplary downstream application."
Language-Conditioned Affordance-Pose Detection in 3D Point Clouds,"Toan Nguyen, Minh Nhat Vu, Baoru Huang, Tuan Van Vo, Thuy Tuong Vy Truong, Ngan Le, Thieu Vo, Hoai Bac Le, Anh Nguyen","FPT Software,TU Wien, Austria,Imperial College London,University of Arkansas,Ton Duc Thang University,VNUHCM-University of Science,University of Liverpool",Deep Learning for Visual Perception II,"Affordance detection and pose estimation are of great importance in many robotic applications. Their combination helps the robot gain an enhanced manipulation capability, in which the generated pose can facilitate the corresponding affordance task. Previous methods for affodance-pose joint learning are limited to a predefined set of affordances, thus limiting the adaptability of robots in real-world environments. In this paper, we propose a new method for language-conditioned affordance-pose joint learning in 3D point clouds. Given a 3D point cloud object, our method detects the affordance region and generates appropriate 6-DoF poses for any unconstrained affordance label. Our method consists of an open-vocabulary affordance detection branch and a language-guided diffusion model that generates 6-DoF poses based on the affordance text. We also introduce a new high-quality dataset for the task of language-driven affordance-pose joint learning. Intensive experimental results demonstrate that our proposed method works effectively on a wide range of open-vocabulary affordances and outperforms other baselines by a large margin. In addition, we illustrate the usefulness of our method in real-world robotic applications. Our code and dataset are publicly available at https://3dapnet.github.io."
Multi-Object RANSAC: Efficient Plane Clustering Method in a Clutter,"Seunghyeon Lim, Youngjae Yoo, Jun Ki Lee, Byoung-Tak Zhang",Seoul National University,Deep Learning for Visual Perception II,"In this paper, we propose a novel method for plane clustering specialized in cluttered scenes using an RGB-D camera and validate its effectiveness through robot grasping experiments. Unlike existing methods, which focus on large-scale indoor structures, our approach---Multi-Object RANSAC emphasizes cluttered environments that contain a wide range of objects with different scales. It enhances plane segmentation by generating subplanes in Deep Plane Clustering (DPC) module, which are then merged with the final planes by post-processing. DPC rearranges the point cloud by voting layers to make subplane clusters, trained in a self-supervised manner using pseudo-labels generated from RANSAC. Multi-Object RANSAC demonstrates superior plane instance segmentation performances over other recent RANSAC applications. We conducted an experiment on robot suction-based grasping, comparing our method with vision-based grasping network and RANSAC applications. The results from this real-world scenario showed its remarkable performance surpassing the baseline methods, highlighting its potential for advanced scene understanding and manipulation."
Utilizing Inpainting for Training Keypoint Detection Algorithms towards Markerless Visual Servoing,"Sreejani Chatterjee, Duc [email protected], Berk Calli",Worcester Polytechnic Institute,Deep Learning for Visual Perception II,"This paper presents a novel strategy to train keypoint detection models for robotics applications. Our goal is to develop methods that can robustly detect and track natural features on robotic manipulators. Such features can be used for vision-based control and pose estimation purposes, when placing artificial markers (e.g. ArUco) on the robotâ€™s body is not possible or practical in runtime. Prior methods require accurate camera calibration and robot kinematic models in order to label training images for the keypoint locations. In this paper, we remove these dependencies by utilizing inpainting methods: In the training phase, we attach ArUco markers along the robotâ€™s body and then label the keypoint locations as the center of those markers. We, then, use an inpainting method to reconstruct the parts of the robot occluded by the ArUco markers. As such, the markers are artificially removed from the training images, and labeled data is obtained to train markerless keypoint detection algorithms without the need for camera calibration or robot models. Using this approach, we trained a model for realtime keypoint detection and used the inferred keypoints as control features for an adaptive visual servoing scheme. We obtained successful control results with this fully model-free control strategy, utilizing natural robot features in the runtime and not requiring camera calibration or robot models in any stage of this process."
Visual-Policy Learning through Multi-Camera View to Single-Camera View Knowledge Distillation for Robot Manipulation Tasks,"Cihan Acar, Kuluhan Binici, Alp TekÄ±rdag, Yan Wu","Institute for Infocomm Research (I,R), A*STAR,National University of Singapore,Nanyang Technological University,A*STAR Institute for Infocomm Research",Deep Learning in Grasping and Manipulation II,"The use of multi-camera views simultaneously has been shown to improve the generalization capabilities and performance of visual policies. However, using multiple cameras in real-world scenarios can be challenging. In this study, we present a novel approach to enhance the generalization performance of vision-based Reinforcement Learning (RL) algorithms for robotic manipulation tasks. Our proposed method involves utilizing a technique known as knowledge distillation, in which a ``teacher'' policy pre-trained with multiple camera viewpoints guides a ``student'' policy in learning from a single camera viewpoint. To enhance the student policy's robustness against camera location perturbations, it is trained using data augmentation and extreme viewpoint changes. As a result, the student policy learns robust visual features that allow it to locate the object of interest accurately and consistently, regardless of the camera viewpoint. The efficacy and efficiency of the proposed method were evaluated both in simulation and real-world environments. The results demonstrate that the single-view visual student policy can successfully learn to grasp and lift a challenging object, which was not possible with a single-view policy alone. Furthermore, the student policy demonstrates zero-shot transfer capability, where it can successfully grasp and lift objects in real-world scenarios for unseen visual configurations."
Symmetric Models for Visual Force Policy Learning,"Colin Kohler, Anuj Shrivatsav Srikanth, Eshan Arora, Robert Platt",Northeastern University,Deep Learning in Grasping and Manipulation II,"While it is generally acknowledged that force feedback is beneficial to robotic control, applications of policy learning to robotic manipulation typically only leverage visual feedback. Recently, symmetric neural models have been used to significantly improve the sample efficiency and performance of policy learning across a variety of robotic manipulation domains. This paper explores an application of symmetric policy learning to visual-force problems. We present Symmetric Visual Force Learning (SVFL), a novel method for robotic control which leverages visual and force feedback. We demonstrate that SVFL can significantly outperform state of the art baselines for visual force learning and report several interesting empirical findings related to the utility of learning force feedback control policies in both general manipulation tasks and scenarios with low visual acuity."
"Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models","Yixuan Huang, Jialin Yuan, Chanho Kim, Pupul Pradhan, Bryan Chen, Fuxin Li, Tucker Hermans","University of Utah,Oregon State University",Deep Learning in Grasping and Manipulation II,"Robots need to have a memory of previously observed, but currently occluded objects to work reliably in realistic environments. We investigate the problem of encoding object-oriented memory into a multi-object manipulation reasoning and planning framework. We propose DOOM and LOOM, which leverage transformer relational dynamics to encode the history of trajectories given partial-view point clouds and an object discovery and tracking engine. Our approaches can perform multiple challenging tasks including reasoning with occluded objects, novel objects appearance, and object reappearance. Throughout our extensive simulation and real-world experiments, we find that our approaches perform well in terms of different numbers of objects and different numbers of distractor actions. Furthermore, we show our approaches outperform an implicit memory baseline."
Learning to Dexterously Pick or Separate Tangled-Prone Objects for Industrial Bin Picking,"Xinyi Zhang, Yukiyasu Domae, Weiwei Wan, Kensuke Harada","Osaka University,The National Institute of Advanced Industrial Science and Techno",Deep Learning in Grasping and Manipulation II,"Industrial bin picking for tangled-prone objects requires the robot to either pick up untangled objects or perform separation manipulation when the bin contains no isolated objects. The robot must be able to flexibly perform appropriate actions based on the current observation. It is challenging due to high occlusion in the clutter, elusive entanglement phenomena, and the need for skilled manipulation planning. In this paper, we propose an autonomous, effective and general approach for picking up tangled-prone objects for industrial bin picking. First, we learn PickNet - a network that maps the visual observation to pixel-wise possibilities of picking isolated objects or separating tangled objects and infers the corresponding grasp. Then, we propose two effective separation strategies: Dropping the entangled objects into a buffer bin to reduce the degree of entanglement; Pulling to separate the entangled objects in the buffer bin planned by PullNet - a network that predicts position and direction for pulling from visual input. To efficiently collect data for training PickNet and PullNet, we embrace the self-supervised learning paradigm using an algorithmic supervisor in a physics simulator. Real-world experiments show that our policy can dexterously pick up tangled-prone objects with success rates of 90%. We further demonstrate the generalization of our policy by picking a set of unseen objects. Supplementary material, code, and videos can be found at https://xinyiz093"
Learning Fabric Manipulation in the Real World with Human Videos,"Robert Lee, Jad Chakra, Fangyi Zhang, Peter Corke","Australian Centre for Robotic Vision,Queensland University of Technology",Deep Learning in Grasping and Manipulation II,"Fabric manipulation is a long-standing challenge in robotics due to the enormous state space and complex dynamics. Learning approaches stand out as promising for this domain as they allow us to learn behaviours directly from data. Most prior methods however rely heavily on simulation, which is still limited by the large sim-to-real gap of deformable objects or rely on large datasets. A promising alternative is to learn fabric manipulation directly from watching humans perform the task. In this work, we explore how demonstrations for fabric manipulation tasks can be collected directly by humans, providing an extremely natural and fast data collection pipeline. Then, using only a handful of such demonstrations, we show how a pick-and-place policy can be learned and deployed on a real robot, without any robot data collection at all. We demonstrate our approach on a fabric smoothing and folding task, showing that our policy can reliably reach folded states from crumpled initial configurations. Videos, code, dataset and trained models are available on the project website: https://sites.google.com/view/foldingbyhand"
HAGrasp: Hybrid Action Grasp Control in Cluttered Scenes Using Deep Reinforcement Learning,"Kai-tai Song, Hsiang-Hsi Chen",National Yang Ming Chiao Tung University,Deep Learning in Grasping and Manipulation II,"Robotic autonomous grasp requires the system to perform multiple functions such as gripper and robot control, making it a task with hybrid output nature. Existing methods based on closed-loop deep reinforcement learning rely on external models for termination evaluation. To achieve more effective grasp for novel objects, we propose a new autonomous grasp control scheme termed HAGrasp that considers the complete point cloud of the workspace. It integrates grasp pose estimation, end-effector pose evaluation, and motion planning of the robotic arm into a single model, enhancing the success rate while reducing computational load. We present a closed-loop grasp control system based on deep reinforcement learning. This control system can perform grasp tasks while dynamically adjusting to avoid end-effector collisions. The design of hybrid-action reinforcement learning module is trained with unified latent action space and further improve generalization, achieving real-time autonomous grasp control. Real robot experiments show that our method has 74.2% success rate for grasping 7 unseen objects. Comparative experiments show that the proposed HAGrasp outperforms open-loop baseline Contact-Graspnet in both success rate and inference time. It is demonstrated that with integrated multi-view input and sim-to-real training design, our method improves real-world applications of autonomous grasp."
Dual-Critic Deep Reinforcement Learning for Push-Grasping Synergy in Cluttered Environment,"Jiakang Zhong, Yew Wee Wong, Jiong Jin, Yong Song, Xianfeng Yuan, Xiaoqi Chen","Swinburne University of Technology,Shandong University,South China University of Technology",Deep Learning in Grasping and Manipulation II,"Robotic push-grasping in densely cluttered environments presents significant challenges due to unbalanced synergy and redundancy between both actions, leading to decreased grasp efficiency. In this paper, a novel double-critic deep reinforcement learning framework is introduced to optimize the push-grasping synergy for robotic manipulation in such environments, aiming to significantly reduce pre-grasping redundancy. This framework incorporates two distinct Deep Q-learning critics: Critic I selects the best course of actions based on the current state derived from visual interpretation, whereas Critic II evaluates the success rate of the current state-action pairing. To further refine the push-grasping synergy, an active double-step learning mechanism is introduced to optimize the training reward function for the pushing action, thereby enhancing its effectiveness through increased intentionality. Simulations show that the proposed framework outperforms contemporary counterparts, notably in grasping success rate and action efficiency. Finally, the framework's generalization and adaptability are demonstrated by conducting real-world experiments using novel objects without the need of retraining."
DefGoalNet: Contextual Goal Learning from Demonstrations for Deformable Object Manipulation,"Bao Thach, Tanner Watts, Shing-hei Ho, Tucker Hermans, Alan Kuntz",University of Utah,Deep Learning in Grasping and Manipulation II,"Shape servoing, a robotic task dedicated to controlling objects to desired goal shapes, is a promising approach to deformable object manipulation. An issue arises, however, with the reliance on the specification of a goal shape. This goal has been obtained either by a laborious domain knowledge engineering process or by manually manipulating the object into the desired shape and capturing the goal shape at that specific moment, both of which are impractical in various robotic applications. In this paper, we solve this problem by developing a novel neural network DefGoalNet, which learns deformable object goal shapes directly from a small number of human demonstrations. We demonstrate our methodâ€™s effectiveness on various robotic tasks, both in simulation and on a physical robot. Notably, in the surgical retraction task, even when trained with as few as 10 demonstrations, our method achieves a median success percentage of nearly 90%. These results mark a substantial advancement in enabling shape servoing methods to bring deformable object manipulation closer to practical, real-world applications."
Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation,"Annie Xie, Lisa Lee, Ted Xiao, Chelsea Finn","Stanford University,Google",Deep Learning in Grasping and Manipulation II,"What makes generalization hard for imitation learning in visual robotic manipulation? This question is difficult to approach at face value, but the environment from the perspective of a robot can often be decomposed into enumerable factors of variation, such as the lighting conditions or the placement of the camera. Empirically, generalization to some of these factors have presented a greater obstacle than others, but existing work sheds little light on precisely how much each factor contributes to the generalization gap. Towards an answer to this question, we study imitation learning policies in simulation and on a real robot language-conditioned manipulation task to quantify the difficulty of generalization to different (sets of) factors. We design a simulated benchmark of 19 tasks with 11 factors of variation to facilitate more controlled evaluations of generalization. From our study, we determine an ordering of factors based on generalization difficulty, that is consistent across simulation and our real robot setup. Videos and code are available at: https://sites.google.com/stanford.edu/gengap-icra"
Transformer-Based Prediction of Human Motions and Contact Forces for Physical Human-Robot Interaction,"Alessia Fusco, Valerio Modugno, Dimitrios Kanoulas, Alessandro Rizzo, Marco Cognetti","Politecnico di Torino,University College London,LAAS-CNRS and Université Toulouse III - Paul Sabatier",Physical Human-Robot Interaction II,"In this paper, we propose a transformer-based architecture for predicting contact forces during a physical human-robot interaction. Our Neural Network is composed of two main parts: a Multi-Layer Perceptron called Transducer and a Transformer. The former estimates, based on the kinematic data from a motion capture suit, the current contact forces. The latter predicts -- taking as input the same kinematic data and the output of the Transducer -- the human motions and the contact forces over a time window in the future. We validated our approach by testing the network on directions of motions that were not provided in the training set. We also compared our approach to a purely Transformer-based network, showing a better prediction accuracy of the contact forces."
SynH2R: Synthesizing Hand-Object Motions for Learning Human-To-Robot Handovers,"Sammy Christen, Lan Feng, Wei Yang, Yu-Wei Chao, Otmar Hilliges, Jie Song","ETH Zurich,ETH ZURICH,NVIDIA,ETHZ",Physical Human-Robot Interaction II,"Vision-based human-to-robot handover is an important and challenging task in human-robot interaction. Recent work has attempted to train robot policies by interacting with dynamic virtual humans in simulated environments, where the policies can later be transferred to the real world. However, a major bottleneck is the reliance on human motion capture data, which is expensive to acquire and difficult to scale to arbitrary objects and human grasping motions. In this paper, we introduce a framework that can generate plausible human grasping motions suitable for training the robot. To achieve this, we propose a hand-object synthesis method that is designed to generate handover-friendly motions similar to humans. This allows us to generate synthetic training and testing data with 100x more objects than previous work. In our experiments, we show that our method trained purely with synthetic data is competitive with state-of-the-art methods that rely on real human motion data both in simulation and on a real system. In addition, we can perform evaluations on a larger scale compared to prior work. With our newly introduced test set, we show that our model can better scale to a large variety of unseen objects and human motions compared to the baselines."
Proactive Robot Control for Collaborative Manipulation Using Human Intent,"Zhanibek Rysbek, Siyu Li, Afagh Mehri Shervedani, Milos Zefran","University of Illinois at Chicago,University of Illinois Chicago",Physical Human-Robot Interaction II,"Collaborative manipulation task often requires negotiation using explicit or implicit communication. An important example is determining where to move when the goal destination is not uniquely specified, and who should lead the motion. This work is motivated by the ability of humans to communicate the desired destination of motion through back-and-forth force exchanges. Inherent to these exchanges is also the ability to dynamically assign a role to each participant, either taking the initiative or deferring to the partner's lead. In this paper, we propose a hierarchical robot control framework that emulates human behavior in communicating a motion destination to a human collaborator and in responding to their actions. At the top level, the controller consists of a set of finite-state machines corresponding to different levels of commitment of the robot to its desired goal configuration. The control architecture is loosely based on the human strategy observed in the human-human experiments, and the key component is a real-time intent recognizer that helps the robot respond to human actions. We describe the details of the control framework, feature engineering and training process of the intent recognition. The proposed controller was implemented on a UR10e robot (Universal Robots) and evaluated through human studies. The experiments show that the robot correctly recognizes and responds to human input, communicates its intent clearly, and resolves conflict. We report success rates and draw comparisons with human-human experiments to demonstrate the effectiveness of the approach."
Human Modeling in Physical Human-Robot Interaction: A Brief Survey,"Cheng Fang, Luka Peternel, Ajay Seth, Massimo Sartori, Katja Mombaur, Eiichi Yoshida","University of Southern Denmark,Delft University of Technology,University of Twente,Karlsruhe Institute of Technology,Tokyo University of Science",Physical Human-Robot Interaction II,"The advancement and development of human modeling have greatly benefited from principles used in robotics, for instance, multibody dynamics laid the foundations for physics engines of human movement simulation, and the robotics and control theory were used to contextualize human sensorimotor control. There are many common interests and interconnections between the fields of human modeling and robotics. In recent years, as robots have become safer and smarter, they actively participate in our lives and help us in various scenarios. Roboticists need tools and data from human modeling to build next-generation robots that better assist humans. In this survey, we focus on the connections between physical human-robot interaction and human modeling. On one hand, human neuromusculoskeletal and sensorimotor control models provide novel insights into the human response that robots can utilize to improve human performance. On the other hand, robots are becoming instrumental in quantifying the performance of the (neuro)musculoskeletal system. Thus, the combined use of human modeling and robotic methods in physical human-robot interaction can lead to both improved human understanding and functional assistance."
Exploring Transformers and Visual Transformers for Force Prediction in Human-Robot Collaborative Transportation Tasks,"Jose Enrique Dominguez-Vidal, Alberto Sanfeliu","Institut de Robòtica i Informàtica Industrial, CSIC-UPC,Universitat Politècnica de Cataluyna",Physical Human-Robot Interaction II,"In this paper, we analyze the possibilities offered by Deep Learning State-of-the-Art architectures such as Transformers and Visual Transformers in generating a prediction of the humanâ€™s force in a Human-Robot collaborative object transportation task at a middle distance. We outperform our previous predictor by achieving a success rate of 93.8% in testset and 90.9% in real experiments with 21 volunteers predicting in both cases the force that the human will exert during the next 1 s. A modification in the architecture allows us to obtain a second output from the model with a velocity prediction, which allows us to improve the capabilities of our predictor if it is used to estimate the trajectory that the human-robot pair will follow. An ablation test is also performed to verify the relative contribution to performance of each input."
Exploring the Effect of Base Compliance on Physical Human-Robot Collaboration,"Ziqi Wang, Marc Garry Carmichael","University of Technology Sydney,Centre for Autonomous Systems",Physical Human-Robot Interaction II,"Mobile physical human-robot collaboration (pHRC) using collaborative robots (cobots) and mobile robots has attracted much research attention. Many researchers have focused on improving the control performance to comply with human intentions. However, a problem that generally exists with mobile pHRC but often gets neglected is the impact of non-rigid components e.g. deformable tyres, suspension systems and uneven terrain on human interaction experience and task performance. To fullfil this current research gap, we carried out an investigation on the above-mentioned problem by altering a cobotâ€™s base rigidity level (also referred to as base compliance level or BCL) during pHRC experiments. We explored how the task performance is affected by base compliance as well as human operatorâ€™s experience and cobot control parameters. Measurements include the human operatorâ€™s physical effort, task velocity, and task error. From the experimental results, it is discovered that base compliance has a significant impact on task accuracy as it can easily excite the system if an inadequate control strategy is deployed. Furthermore, through ANOVA, it is discovered that the influence of base compliance can be minimized and system excitation can be avoided by sufficient human operator training and the appropriate selection of cobotâ€™s control parameters."
Experimental and Simulation-Based Estimation of Interface Power During Physical Human-Robot Interaction in Hand Exoskeletons,"Saad Yousaf, Gaurav Mukherjee, Raymond King, Ashish Deshpande","The University of Texas at Austin,University of Washington,Oculus VR,The University of Texas",Physical Human-Robot Interaction II,"Even the best wearable robots face challenges with power losses in the system, especially at the physical attachment interface. While some sources for power loss are inherent to the system, such as human soft tissue or musculoskeletal joint damping, other sources such as soft padding materials and bias strap forces can be modulated to optimize interface power transmission. Few methods currently exist for estimating power loss at physical human-robot interfaces, especially for upper-body exoskeletons. This letter presents a novel method to estimate interface power from experimental data in a wearable hand device, along with a simulation model for predicting interaction behavior by incorporating viscoelastic properties at the attachment interface. The experimental method is implemented with the Maestro hand exoskeleton, and repeatability of the interface power estimation is confirmed with pilot human testing. Simulation results are compared with experimental estimation of interface power, showing agreement of trends and validating the use of a simulation model to predict physical human-robot interaction behavior. These findings highlight the advantages of multi-body simulations as a tool to perform modular, inexpensive, and predictive investigations in physical human-robot interaction, without affecting the real-world mechatronic system or hindering the subjectâ€™s safety. The proposed tools can optimize the design of wearable robots for seamless integration with the human body."
A Personalizable Controller for the Walking Assistive omNi-Directional Exo-Robot (WANDER),"Andrea Fortuna, Marta Lorenzini, Mattia Leonori, Juan M. Gandarias, Pietro Balatti, Younggeol Cho, Elena De Momi, Arash Ajoudani","Politecnico di Milano,Istituto Italiano di Tecnologia,University of Malaga,Istituto Italiano di Tecnologia (IIT)",Physical Human-Robot Interaction II,"Preserving and encouraging mobility in the elderly and adults with chronic conditions is of paramount importance. However, existing walking aids are either inadequate to provide sufficient support to users' stability or too bulky and poorly maneuverable to be used outside hospital environments. In addition, they all lack adaptability to individual requirements. To address these challenges, this paper introduces WANDER, a novel Walking Assistive omNi-Directional Exo-Robot. It consists of an omnidirectional platform and a robust aluminum structure mounted on top of it, which provides partial body weight support. A comfortable and minimally restrictive coupling interface embedded with a force/torque sensor allows to detect users' intentions, which are translated into command velocities by means of a variable admittance controller. An optimization technique based on users' preferences, i.e., Preference-Based Optimization (PBO) guides the choice of the admittance parameters (i.e., virtual mass and damping) to better fit subject-specific needs and characteristics. Experiments with twelve healthy subjects exhibited a significant decrease in energy consumption and jerk when using WANDER with PBO parameters as well as improved user performance and comfort. The great interpersonal variability in the optimized parameters highlights the importance of personalized control settings when walking with an assistive device, aiming to enhance users' comfort and mobility while ensuring reliable physical support."
Lightweight and Flexible Prosthetic Wrist with Shape Memory Alloy (SMA)-Based Artificial Muscle and Elliptic Rolling Joint,"Kyujin Hyeon, Chongyoung Chung, Jihyeong Ma, Ki-Uk Kyung","KAIST,Korea Advanced Institute of Science and Technology (KAIST),Korea Advanced Institute of Science and Technology,Korea Advanced Institute of Science & Technology (KAIST)",Prosthetics and Exoskeletons II,"This paper proposes a novel prosthetic wrist that emulates the anatomical structure of the human wrist, specifically the wrist bones and muscles responsible for wrist movements. To achieve a range of motion (ROM) and load-bearing capacity comparable to the human wrist joint, we designed an elliptic rolling joint as an artificial wrist joint, mimicking the two-row structures of carpal bones. The joint offers two degrees of freedom (DOFs) and can support high loads while also providing adequate ROM. In addition, we designed the artificial muscles using the properties of human muscles, such as moment arm and displacement, and implemented them as shape memory alloy (SMA) spring-based actuators. The resulting prosthetic wrist, incorporating the artificial joint and artificial muscles, is lightweight at only 50g and can perform functional ranges of motion, including 53Â° for flexion, 50Â° for extension, 40Â° for radial deviation, and 42Â° for ulnar deviation. The use of SMA spring actuators confers restoring force and flexibility to the prosthetic wrist, allowing it to withstand external disturbances. Furthermore, the proposed wrist can be utilized as a robotic wrist, affording two additional DOFs, the ability to lift loads more than 20 times its weight, and variable joint stiffness."
Ankle Exoskeleton with a Symmetric 3 DoF Structure for Plantarflexion Assistance,"Miha Dezman, Charlotte Dorothea Marquardt, Tamim Asfour","Karlsruhe Institute of Technology,Karlsruhe Institute of Technology (KIT)",Prosthetics and Exoskeletons II,"Ankle exoskeletons can assist the ankle joint and reduce the metabolic cost of walking. However, many existing ankle exoskeletons constrain the natural 3 degrees of freedom (DoF) of the ankle to limit the exoskeleton's weight and mechanical complexity, thereby compromising comfort and kinematic compatibility with the user. This paper presents a novel ankle exoskeleton frame design that allows for 3 DoF ankle motion using a symmetric parallel frame design principle resulting in a strong frame while weighing 1.8 kg. Furthermore, a cable routing method is proposed to actuate the plantarflexion of the ankle. The kinematic compatibility of the proposed exoskeleton frame is evaluated in straight- and curve-walking scenarios with four users. The study demonstrates that the exoskeleton frame adapts to the natural 3 DoF ankle motion and the range of motion (RoM) during walking. The actuation in plantarflexion is evaluated in a stationary torque experiment demonstrating the ability of the frame to transfer large torque loads of up to 57.4 Nm. This work contributes to the design and development of more flexible and adaptable ankle exoskeletons for walking assistance."
Design of a Front-Enveloping Powered Exoskeleton Considering Optimal Distribution of Actuating Torques and Center of Mass,"Jeongsu Park, Kyeongsu Shi, Hyojun An, Gunhee Lee, Seunghwan Kim, Chanyoung Ko, Taeyeon Kim, Hyeongjun Kim, Kyoungchul Kong","KAIST,Korea Advanced Institute of Science and Technology,Korea Advanced Institute of Science and Technology (KAIST),Korea Institute of Science and Technology",Prosthetics and Exoskeletons II,"Traditionally, powered exoskeletons have predominantly featured a back-enveloping design due to its simplicity in both implementation and user donning. However, this design results in a backward shift of the center of mass (CoM) in the sagittal plane. This paper identifies the limitations of existing design approaches and determines the optimal anterior-posterior (A/P) CoM position considering factors like actuating power, balance in the neutral posture, and user's hand workspace. Our optimization analysis recommends placing the CoM in front of the user. We address historical constraints on front-enveloping designs and propose solutions. Furthermore, we validate the usability of our designed exoskeleton through testing with a complete paraplegic user."
Real-Time Locomotion Transitions Detection: Maximizing Performances with Minimal Resources,"Zeynep Özge Orhan, Andrea Dal Prete, Anastasia Bolotnikova, Marta Gandolla, Auke Ijspeert, Mohamed Bouri","EPFL,Politecnico di Milano",Prosthetics and Exoskeletons II,"Assistive devices, such as exoskeletons and prostheses, have revolutionized the field of rehabilitation and mobility assistance. Efficiently detecting transitions between different activities, such as walking, stair ascending and descending, and sitting, is crucial for ensuring adaptive control and enhancing user experience. We present an approach for real-time transition detection, aimed at optimizing the processing-time performance. By establishing activity-specific threshold values through trained machine learning models, we effectively distinguish motion patterns and we identify transition moments between locomotion modes. This threshold-based method improves real-time embedded processing time performance by up to 11 times compared to machine learning approaches. The efficacy of the developed finite-state machine is validated using data collected from three different measurement systems. Moreover, experiments with healthy participants were conducted on an active pelvis orthosis to validate the robustness and reliability of our approach. The proposed algorithm achieved high accuracy in detecting transitions between activities. These promising results show the robustness and reliability of the method, reinforcing its potential for integration into practical applications."
ExoRecovery: Push Recovery with a Lower-Limb Exoskeleton Based on Stepping Strategy,"Zeynep Özge Orhan, Milad Shafiee, Vincent Juillard, Joel Coelho Oliveira, Auke Ijspeert, Mohamed Bouri",EPFL,Prosthetics and Exoskeletons II,"Balance loss is a significant challenge in lower-limb exoskeleton applications, as it can lead to potential falls, thereby impacting user safety and confidence. We introduce a control framework for omnidirectional recovery step planning by online optimization of step duration and position in response to external forces. We map the step duration and position to a human-like foot trajectory, which is then translated into joint trajectories using inverse kinematics. These trajectories are executed via an impedance controller, promoting cooperation between the exoskeleton and the user. Moreover, our framework is based on the concept of the divergent component of motion, also known as the Extrapolated Center of Mass, which has been established as a consistent dynamic for describing human movement. This real-time online optimization framework enhances the adaptability of exoskeleton users under unforeseen forces thereby improving the overall user stability and safety. To validate the effectiveness of our approach, simulations, and experiments were conducted. Our push recovery experiments employing the exoskeleton in zero-torque mode (without assistance) exhibit an alignment with the exoskeleton's recovery assistance mode, that shows the consistency of the control framework with human intention. To the best of our knowledge, this is the first cooperative push recovery framework for the lower-limb human exoskeleton that relies on the simultaneous adaptation of intra-stride parameters in both frontal and sagittal directions. The proposed control scheme has been validated with human subject experiments."
Pilot Comparison of Customized and Generalized Hip-Knee-Ankle Exoskeleton Torque Profiles,"Gwendolyn Bryan, Patrick W. Franks, Seungmoon Song, Steven H. Collins","IHMC,Skip,Northeastern,Stanford University",Prosthetics and Exoskeletons II,"Optimized assistance patterns have produced the greatest exoskeleton benefits to energy expenditure of any strategy to date. This strategy may be effective due to the customization of the applied torque profiles to the user as well as the locomotion condition; however, it is currently unclear how sensitive participants are to their unique torque profile. To investigate, we applied previously optimized hip-knee-ankle torque profiles to expert users (N=3; 1.25 m/s; 0 deg incline). The participants walked with the profile optimized to them, the two profiles optimized to the other two participants, and the average of the three torque profiles while we measured their energy expenditure. Relative to walking with the device turned off, on average, participants experienced a 47.5% (range 12%) metabolic reduction when walking with the torque profile optimized to them and a 46% (range 15%) reduction when walking with the other profiles. Interestingly, within-subject performance was more consistent than across subjects (P1: 52% range 5%, P2: 49% range 6%, P3: 39% range 3%) suggesting that, for expert users of some devices, there may be a range of nearly equally effective torque profiles to reduce the metabolic cost of walking. The torque timing was remarkably similar across the four torque profiles while the torque magnitude varied; participants may be much more sensitive to torque timing than torque magnitude, and there may be a set of torque timing parameters that are generally effective."
Task-Space Control of a Powered Ankle Prosthesis,"David Kelly, Ryan Posh, Patrick Wensing",University of Notre Dame,Prosthetics and Exoskeletons II,"Powered lower-limb prostheses have shown promise in helping individuals with amputation regain functionality that passive prostheses cannot provide. However, the best method for controlling these devices in coordination with their users is still an open research topic. While powered devices can replicate normative joint kinematics and kinetics, active control also holds the potential to shape system-level characteristics such as the center of mass (CoM) that play an important role in balance. Controlling the prosthesis based on these system-level, or task-space, variables would further represent a new way of coordinating the user and their device. This paper explores the initial implementation of task-space control for a powered ankle prosthesis, characterizing the emergent outcomes of this new coordination strategy. One able-bodied subject walked using a bypass adapter while prosthesis torques were commanded based on reference ground reaction force (GRF) and CoM trajectories. The subject could walk comfortably and continuously at their preferred walking speed, achieving normative ankle torques and joint trajectories despite not tracking explicit joint-level references in stance."
Integrating Computer Vision in Exosuits for Adaptive Support and Reduced Muscle Strain in Industrial Environments,"Francesco Missiroli, Pietro Mazzoni, Nicola Lotti, Enrica Tricomi, Francesco Braghin, Loris Roveda, Lorenzo Masia","Heidelberg University,POLITECNICO DI MILANO,Politecnico di Milano,SUPSI-IDSIA",Prosthetics and Exoskeletons II,"Exosuits are wearable technologies that improve physical capabilities and mobility providing support during various activities. Although primarily intended for medical rehabilitation, there is growing interest in utilizing exosuits in industrial environments to prevent work-related musculoskeletal disorders (WMSDs) by ensuring continuous joints support. However, achieving synchronization between the exosuit and human motion, as well as effectively controlling interactions with the surroundings, presents ongoing challenges. The integration of computer vision techniques, particularly object recognition algorithms, can greatly assist exosuits in understanding the user's environment and adapting their behaviour accordingly. To address this issue, we have developed a control strategy for a soft exosuit that employs computer vision to collaboratively offer tailored assistance to the elbow, alleviating joint stress during interactions with objects of various natures and weights. We conducted a study to assess the effectiveness of the integrated system, which merges object recognition and gravity compensation within a built-in structure of the robotic exosuit. The findings confirmed that the suggested solution notably minimized muscle strain during dynamic activities, exhibiting a consistent correlation with the mass of the object being lifted, namely reducing by 45% and 54% respectively the Biceps activity while lifting the MW and HW compared to the 32% of the ""Dynamic Arm"". The int"
Vision and Tactile-Based Continuous Multimodal Intention and Attention Recognition for Safer Physical Human-Robot Interaction,"Christopher Yee Wong, Lucas Vergez, Wael Suleiman","McGill University,Arts et Métiers Institute of Technology,University of Sherbrooke",Multi-Modal Perception for HRI II,"Employing skin-like sensors on robots enhances both the safety and usability of collaborative robots by adding the capability to detect human contact. Unfortunately, simple binary tactile sensors alone cannot determine the context of the human contact---whether it is a deliberate interaction or an unintended collision that requires safety manoeuvres. Many published methods classify discrete interactions using more advanced tactile sensors or by analysing joint torques. Instead, we propose to augment the intention recognition capabilities of simple binary tactile sensors by adding a robot-mounted camera for human analysis. Different interaction characteristics, including touch location, human pose, and gaze direction, are used to train a supervised machine learning algorithm to classify whether a touch is intentional or not with an F1-score of 86%. We demonstrate that multimodal intention recognition is significantly more accurate than monomodal analyses. Furthermore, our method continuously monitors interactions that fluidly change between intentional or unintentional. If deemed unintentional, the proposed intention and attention recognition algorithm can activate safety features to prevent unsafe interactions. We also employ a feature reduction technique that reduces the number of inputs to five to achieve a more generalized low-dimensional classifier. This simplification both reduces the amount of training data required and improves real-world classification accuracy."
Towards Unified Interactive Visual Grounding in the Wild,"Jie Xu, Hanbo Zhang, Qingyi Si, Yifeng Li, Xuguang Lan, Tao Kong","Xi'an Jiaotong University,Bytedance Research,Chinese Academy of Sciences,ByteDance",Multi-Modal Perception for HRI II,"Interactive visual grounding in Human-Robot Interaction (HRI) is challenging yet practical due to the inevitable ambiguity in natural languages. It requires robots to disambiguate the userâ€™s input by active information gathering. Previous approaches often rely on predefined templates to ask disambiguation questions, resulting in performance reduction in realistic interactive scenarios. In this paper, we propose TiO, an end-to-end system for interactive visual grounding in human-robot interaction. Benefiting from a unified formulation of visual dialog and grounding, our method can be trained on a joint of extensive public data, and show superior generality to diversified and challenging open-world scenarios. In the experiments, we validate TiO on GuessWhat?! and InViG benchmarks, setting new state-of-the-art performance by a clear margin. Moreover, we conduct HRI experiments on the carefully selected 150 challenging scenes as well as real-robot platforms. Results show that our method demonstrates superior generality to diversified visual and language inputs with a high success rate. Codes and demos are available on https://jxu124.github.io/TiO/."
"Think, Act and Ask: Open-World Interactive Personalized Robot Navigation","Yinpei Dai, Run Peng, Sikai Li, Joyce Chai","University of Michigan,University of Michigan, Ann Arbor",Multi-Modal Perception for HRI II,"Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION), which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion and the efficiency of navigation and interaction remains challenging for all methods. We further provide more findings on the impact of diverse user feedback forms on the agentsâ€™ performance."
PROGrasp: Pragmatic Human-Robot Communication for Object Grasping,"Gi-cheon Kang, Junghyun Kim, Jaein Kim, Byoung-Tak Zhang",Seoul National University,Multi-Modal Perception for HRI II,"Interactive Object Grasping (IOG) is the task of identifying and grasping the desired object via human-robot natural language interaction. Current IOG systems assume that a human user initially specifies the target object's category (e.g., bottle). Inspired by pragmatics, where humans often convey their intentions by relying on context to achieve goals, we introduce a new IOG task, Pragmatic-IOG, and the corresponding dataset, Intention-oriented Multi-modal Dialogue (IM-Dial). In our proposed task scenario, an intention-oriented utterance (e.g., ""I am thirsty"") is initially given to the robot. The robot should then identify the target object by interacting with a human user. Based on the task setup, we propose a new robotic system that can interpret the user's intention and pick up the target object, Pragmatic Object Grasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules for visual grounding, question asking, object grasping, and most importantly, answer interpretation for pragmatic inference. Experimental results show that PROGrasp is effective in offline (i.e., target object discovery) and online (i.e., IOG with a physical robot arm) settings. Code and data are available at https://github.com/gicheonkang/prograsp."
Enhancing Tactile Sensing in Robotics: Dual-Modal Force and Shape Perception with EIT-Based Sensors and MM-CNN,"Haofeng Chen, Xuanxuan Yang, Gang Ma, Yucheng Wang, Xiaojie Wang","University of Science and Technology of China,Chinese Academy of Sciences,university of science and technology of china,Hefei Institutes of Physical Science, Chinese Academy of Science,Chinese academy of sciences",Multi-Modal Perception for HRI II,"Electrical Impedance Tomography (EIT)-based tactile sensors offer durability, scalability, and cost-effective manufacturing. However, simultaneously reconstructing force and shape from boundary measurements remains challenging due to EITâ€™s inherent location dependencies and image artifacts. This study presents a model-driven multimodal convolutional neural network (MM-CNN) for joint EIT-based force and shape sensing. The hybrid approach combines physics-inspired voltage preprocessing with an attention-based network to overcome EITâ€™s limitations. The preprocessing network applies a linearized one-step inverse solution with Tikhonov regularization to convert raw boundary voltage into a noise-reduced 2D image. The image reconstruction network uses an attention mechanism to focus on salient features, addressing location dependency issues. Quantitative metrics show that MM-CNN outperforms traditional EIT algorithms like NOSER and TV, reducing location dependency and improving shape discrimination. MMCNN enables unified force and shape modalities, validated through real-contact experiments, enhancing EIT tactile systems for human-robot interaction by incorporating physical knowledge with deep learning."
CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents,"Jeongeun Park, Seungwon Lim, Joonhyung Lee, Sangbeom Park, Minsuk Chang, Youngjae Yu, Sungjoon Choi","Korea University,Yonsei University,Google",Multi-Modal Perception for HRI II,"In this paper, we focus on inferring whether the given user command is clear, ambiguous, or infeasible in the context of interactive robotic agents utilizing large language models (LLMs). To tackle this problem, we first present an uncertainty estimation method for LLMs to classify whether the command is certain (i.e., clear) or not (i.e., ambiguous or infeasible). Once the command is classified as uncertain, we further distinguish it between ambiguous or infeasible commands leveraging LLMs with situational aware context in a zero-shot manner. For ambiguous commands, we disambiguate the command by interacting with users via question generation with LLMs. We believe that proper recognition of the given commands could lead to a decrease in malfunction and undesired actions of the robot, enhancing the reliability of interactive robot agents. We present a dataset for robotic situational awareness, consisting of pairs of high-level commands, scene descriptions, and labels of command type (i.e., clear, ambiguous, or infeasible). We validate the proposed method on the collected dataset and pick-and-place tabletop simulation environment. Finally, we demonstrate the proposed approach in real-world human-robot interaction experiments."
Assisting Group Discussions Using Desktop Robot Haru,"Fei Tang, Chuanxiong Zheng, Hongqi Yu, Lei Zhang, Eric Nichols, Randy Gomez, Guangliang Li","Ocean University of China,ocean university of china,Honda Research Institute Japan,Honda Research Institute Japan Co., Ltd.",Multi-Modal Perception for HRI II,"Socially assistive robots are potentially to be integrated with human daily lives in the near future, and expected to be able to improve group dynamics when interacting with groups of people in social settings. In this paper, we developed a system with desktop robot Haru to assist group discussions. The system consists of three modules: a dialogue assistance module, which facilitates Haru to speak to users and answer questions in a free way; a dialogue balance module to encourage participation of users in the discussion with verbal behaviors; an autonomous gazing behavior module trained via deep reinforcement learning in simulation and deployed on physical Haru in reality, which can show politeness during group discussion, e.g., gazing to the speaking member, looking to the middle when both members are talking or silent, looking at the least spoken person when encouraging her. Results of user study with 40 subjects show the significant effectiveness of our system in assisting group discussion."
Assessment and Benchmarking of XoNLI: A Natural Language Processing Interface for Industrial Exoskeletons,"Olmo Alonso Moreno Franco, Raajshekhar Parameswari Neelakandan, Christian Di Natali, Darwin G. Caldwell, Jesus Ortiz","Istituto Italiano di Tecnologia,Istituto Italiano di Technologia,Istituto Italiano di Tecnologia (IIT)",Multi-Modal Perception for HRI II,"Industrial exoskeletons are a potential solution for reducing work-related musculoskeletal disorders during carrying or lifting tasks. Having sensors, electrical/pneumatic actuators, and control systems, active exoskeletons present a more versatile control system because it is possible to select different assistive strategies based on the performed task. From this perspective, human-machine interaction is required to safely open basic exoskeleton domains to the user and provide an adaptable setup system. This article presents the assessment and benchmarking of the novel XoLab Natural Language Interface, a voice user interface for interaction and configuration of industrial active exoskeletons. The evaluation of the novel interface was performed by 17 participants who completed the setup and operational activities while wearing the XoTrunk exoskeleton. The benchmark consisted of a comparison of the presented device with previous adaptable interfaces for the exoskeleton: the user command interface and the monitor system interface. The results showed that although the novel interface presented a considerable lag in the time response, it was more attractive than the standard one. However, the user command interface obtained favourable results over the standard interface in terms of perspicuity and efficiency."
Advancing Virtual Reality Interaction: A Ring-Shaped Controller and Pose Tracking,"Zhuqing Zhang, Dongxuan Li, Jiayao Ma, Yijia He, Pan Ji, Rong Xiong, Hongdong Li, Yue Wang","Zhejiang University,Peking University,Institute of Automation, Chinese Academy of Sciences,Tencent,Australian National university and NICTA",Multi-Modal Perception for HRI II,"Ensuring robust tracking of controllers' movement is critical for human-robot interaction in virtual reality (VR) scenarios. This paper proposes a robust tracking algorithm based on a novel wearable ring-shaped controller equipped with an inertial measurement unit (IMU) and a light-emitting diode (LED). This novel controller design allows users to free up their hands for more immersive experiences. To track the controller's motion accurately and robustly, we resort to various forms of visual measurements, including 6 DoF and 5 DoF pose measurements from hand gesture detection, as well as 3 DoF position measurement and 2 DoF image measurement derived from the LED. We theoretically analyze the performances of these observation models and propose an optimal observation model combination scheme. Moreover, the necessity and rationale of online estimating system gravity are illustrated. The effectiveness of our tracking method is validated through extensive experiments."
Tactile Embeddings for Multi-Task Learning,"Yiyue Luo, Murphy Wonsick, Jessica Hodgins, Brian Okorn","Massachusetts Institute of Technology,Boston Dynamics AI Institute,Carnegie Mellon University",Force and Tactile Sensing II,"Tactile sensing plays a pivotal role in human perception and manipulation tasks, allowing us to intuitively understand task dynamics and adapt our actions in real time. Transferring such tactile intelligence to robotic systems would help intelligent agents understand task constraints and accurately interpret the dynamics of both the objects they are interacting with and their own operations. While significant progress has been made in imbuing robots with this tactile intelligence, challenges persist in effectively utilizing tactile information due to the diversity of tactile sensor form factors, manipulation tasks, and learning objectives involved. To address this challenge, we present a unified tactile embedding space capable of predicting a variety of task-centric qualities over multiple manipulation tasks. We collect tactile data from human demonstrations across various tasks and leverage this data to construct a shared latent space for task stage classification, object dynamics estimation, and tactile dynamics prediction. Through experiments and ablation studies, we demonstrate the effectiveness of our shared tactile latent space for more accurate and adaptable tactile networks, showing an improvement of up to 84% over the single-task training."
AllSight: A Low-Cost and High-Resolution round Tactile Sensor with Zero-Shot Learning Capability,"Osher Azulay, Nimrod Curtis, Rotem Sokolovsky, Guy Levistky, Daniel Slomovik, Guy Lilling, Avishai Sintov","Tel Aviv University,Tel-Aviv University",Force and Tactile Sensing II,"Tactile sensing is a necessary capability for a robotic hand to perform fine manipulations and interact with the environment. Optical sensors are a promising solution for high-resolution contact estimation. Nevertheless, they are usually not easy to fabricate and require individual calibration in order to acquire sufficient accuracy. In this letter, we propose AllSight, an optical tactile sensor with a round 3D structure designed for robotic in-hand manipulation tasks. AllSight is mostly 3D printed including a novel and simplified fabrication processes. This makes it low-cost, modular, durable and in the size of a human thumb while with a large contact surface. We show the ability of AllSight to learn and estimate a full contact state, i.e., contact position, forces and torsion. With that, an experimental benchmark between various configurations of illumination and contact elastomers are provided. Furthermore, the robust design of AllSight provides it with a unique zero-shot capability such that a practitioner can fabricate the open-source design and have a ready-to-use state estimation model. A set of experiments demonstrates the accurate state estimation performance of AllSight."
9DTact: A Compact Vision-Based Tactile Sensor for Accurate 3D Shape Reconstruction and Generalizable 6D Force Estimation,"Changyi Lin, Han Zhang, Jikai Xu, Lei Wu, Huazhe Xu","Carnegie Mellon University,Tsinghua University, Shanghai Qi Zhi Institute,Huazhong University of Science and Technology,Shanghai Qi Zhi In,Huazhong University of Science and Technology,Tsinghua University",Force and Tactile Sensing II,"The advancements in vision-based tactile sensors have boosted the aptitude of robots to perform contact-rich manipulation, particularly when precise positioning and contact state of the manipulated objects are crucial for successful execution. In this work, we present 9DTact, a straightforward yet versatile tactile sensor that offers 3D shape reconstruction and 6D force estimation capabilities. Conceptually, 9DTact is designed to be highly compact, robust, and adaptable to various robotic platforms. Moreover, it is low-cost and easy-to-fabricate, requiring minimal assembly skills. Functionally, 9DTact builds upon the optical principles of DTact and is optimized to achieve 3D shape reconstruction with enhanced accuracy and efficiency. Remarkably, we leverage the optical and deformable properties of the translucent gel so that 9DTact can perform 6D force estimation without the participation of auxiliary markers or patterns on the gel surface. More specifically, we collect a dataset consisting of approximately 100,000 image-force pairs from 175 complex objects and train a neural network to regress the 6D force, which can generalize to unseen objects. To promote the development and applications of vision-based tactile sensors, we open-source both the hardware and software of 9DTact, along with a comprehensive video tutorial, all of which are available at https://linchangyi1.github.io/9DTact."
GelFinger: A Novel Visual-Tactile Sensor with Multi-Angle Tactile Image Stitching,"Zhonglin Lin, Jiaquan Zhuang, Yufeng Li, Xianyu Wu, Shan Luo, Daniel Fernandes Gomes, Feng Huang, Zheng Yang","Fuzhou University,FUZHOU UNIVERSITY,King's College London,Kings College London",Force and Tactile Sensing II,"Visual-tactile sensors that use a camera to capture the deformation of a soft gel layer have become popular in recent years. However, these sensors have a limited receptive field, which can hinder their ability to perceive tactile information effectively. In this paper, we propose a novel visual-tactile sensor named GelFinger that closely resembles the human finger and is well-suited for detecting various complex surfaces. The GelFinger sensor is equipped with an embedded miniature motor that allows for the adaptation of the camera pose and the scanning of a large contact area. During the detection process, the camera rotates to multiple angles to capture the tactile image of the contact area. To stitch together the tactile images obtained at different camera poses, we use an As-Projective-As-Possible image stitching algorithm to form a global view of the contact. We demonstrate the effectiveness of the GelFinger sensor in assessing large surfaces by using it to reconstruct curved crack outlines. Comparative experimental results show that the proposed sensor can effectively detect cracks and has the potential to assist humans in detecting defects on curved surfaces of infrastructure such as pipelines."
StereoTac: A Novel Visuotactile Sensor That Combines Tactile Sensing with 3D Vision,"Etienne Roberge, Guillaume Fornes, Jean-Philippe Roberge","École de technologie supérieure,ENSEIRB-MATMECA, Bordeaux INP",Force and Tactile Sensing II,"Combining 3D vision with tactile sensing could unlock a greater level of dexterity for robots and improve several manipulation tasks. However, obtaining a close-up 3D view of the location where manipulation contacts occur can be challenging, particularly in confined spaces, cluttered environments, or without installing more sensors on the end effector. In this context, this paper presents StereoTac, a novel vision-based sensor that combines tactile sensing with 3D vision. The proposed sensor relies on stereoscopic vision to capture a 3D representation of the environment before contact and uses photometric stereo to reconstruct the tactile imprint generated by an object during contact. To this end, two cameras were integrated in a single sensor, whose interface is made of a transparent elastomer coated with a thin layer of paint with a level of transparency that can be adjusted by varying the sensorâ€™s internal lighting conditions. We describe the sensorâ€™s fabrication and evaluate its performance for both tactile perception and 3D vision. Our results show that the proposed sensor can reconstruct a 3D view of a scene just before grasping and perceive the tactile imprint after grasping, allowing for monitoring of the contact during manipulation."
An Investigation of Multi-Feature Extraction and Super-Resolution with Fast Microphone Arrays,"Eric T. Chang, Runsheng Wang, Peter Ballentine, Jingxi Xu, Trey Smith, Brian Coltin, Ioannis Kymissis, Matei Ciocarlie","Columbia University,NASA Ames Research Center,Carnegie Mellon University",Force and Tactile Sensing II,"In this work, we use MEMS microphones as vibration sensors to simultaneously classify texture and estimate contact position and velocity. Vibration sensors are an important facet of both human and robotic tactile sensing, providing fast detection of contact and onset of slip. Microphones are an attractive option for implementing vibration sensing as they offer a fast response and can be sampled quickly, are affordable, and occupy a very small footprint. Our prototype sensor uses only a sparse array (8-9 mm spacing) of distributed MEMS microphones ("
Model-Based Compliance Discrimination Via Soft Tactile Optical Sensing and Optical Flow Computation: A Biomimetic Approach,"Giulia Pagnanelli, Simone Ciotti, Nathan Lepora, Antonio Bicchi, Matteo Bianchi","University of Pisa,University of Bristol,Fondazione Istituto Italiano di Tecnologia",Force and Tactile Sensing II,"Soft tactile optical sensors have opened up new possibilities for endowing artificial robotic hands with advanced touch-related properties; however, their use for compliance discrimination has been poorly investigated and mainly relies on data-driven methods. Discrimination of object compliance is crucial for enabling accurate and purposeful object manipulation. Humans retrieve this information primarily using the contact area spread rate (CASR) over their fingertips. CASR can be defined as the integral of tactile flow, which describes the movement of iso-strain surfaces within the fingerpad. This work presents the first attempt to discriminate compliance through soft optical tactile sensing based on a computational model of human tactile perception that relies on CASR and tactile flow concepts. To this aim, we used a soft optical biomimetic sensor that transduces surface deformation via movements of marked pins, similar to the function of intermediate ridges in the human fingertip. We acquired images of markers' movements during the interaction with silicone specimens with different compliance at different indenting forces. Then, we computed the optical flow as a tactile flow approximation and its divergence to estimate the CASR. Our model-based approach can accurately discriminate the compliance levels of the specimens, both when the sensor probed the surface perpendicularly and with different inclinations. Finally, we used the relation between specimen compliance and the"
Bi-Touch: Bimanual Tactile Manipulation with Sim-To-Real Deep Reinforcement Learning,"Yijiong Lin, Alex Church, Max Yang, Haoran Li, John Lloyd, Dandan Zhang, Nathan Lepora","University of Bristol,Cambrian,Imperial College London",Force and Tactile Sensing II,"Bimanual manipulation with tactile feedback will be key to human-level robot dexterity. However, this topic is less explored than single-arm settings, partly due to the availability of suitable hardware along with the complexity of designing effective controllers for tasks with relatively large state-action spaces. Here we introduce a dual-arm tactile robotic system (Bi-Touch) based on the Tactile Gym 2.0 setup that integrates two affordable industrial-level robot arms with low-cost high-resolution tactile sensors (TacTips). We present a suite of bimanual manipulation tasks tailored towards tactile feedback: bi-pushing, bi-reorienting and bi-gathering. To learn effective policies for challenging tasks in simulation, we contribute several efforts, such as introducing appropriate reward functions and proposing a novel goal-update mechanism with deep reinforcement learning. We also apply these policies to real-world settings with a zero-shot sim-to-real approach. Our analysis highlights and addresses some challenges met during the sim-to-real application, e.g. the learned policy tended to squeeze an object in the bi-reorienting task due to the sim-to-real gap. Finally, we demonstrate the generalizability and robustness of this system by experimenting with different unseen objects with applied perturbations in the real world. These tasks and our system can also serve as a benchmark for bimanual tactile manipulation. Code will be openly released at https://github.com/ac-93/tact"
AcTExplore: Active Tactile Exploration on Unknown Objects,"Amir Hossein Shahid Zadeh, Seong Jong Yoo, Pavan Mantripragada, Chahat Deep Singh, Cornelia Fermuller, Yiannis Aloimonos","University of Maryland,University of Maryland, College Park",Force and Tactile Sensing II,"Tactile exploration plays a crucial role in under- standing object structures for fundamental robotics tasks such as grasping and manipulation. However, efficiently exploring such objects using tactile sensors is challenging, primarily due to the large-scale unknown environments and limited sensing coverage of these sensors. To this end, we present AcTExplore, an active tactile exploration method driven by reinforcement learning for object reconstruction at scales that automatically explores the object surfaces in a limited number of steps. Through sufficient exploration, our algorithm incrementally collects tactile data and reconstructs 3D shapes of the objects as well, which can serve as a representation for higher-level downstream tasks. Our method achieves an average of 95.97% IoU coverage on unseen YCB objects while just being trained on primitive shapes."
Terrestrial Locomotion of PogoX: From Hardware Design to Energy Shaping and Step-To-Step Dynamics Based Control,"Yi Wang, Jiarong Kang, Zhiheng Chen, Xiaobin Xiong","Columbia University,University of Wisconsin Madison,University of Wisconsin-Madison",Legged Robots II,"We present a novel controller design on a robotic locomotor that combines an aerial vehicle with a spring-loaded leg. The main motivation is to enable the terrestrial locomotion capability on aerial vehicles so that they can carry heavy loads: heavy enough that flying is no longer possible, e.g., when the thrust-to-weight ratio (TWR) is small. The robot is designed with a pogo-stick leg and a quadrotor, and thus it is named as PogoX. We show that with a simple and lightweight spring-loaded leg, the robot is capable of hopping with TWR"
"Learning Emergent Gaits with Decentralized Phase Oscillators: On the Role of Observations, Rewards, and Feedback","Jenny Zhang, Steve Heim, Se Hwan Jeon, Sangbae Kim",Massachusetts Institute of Technology,Legged Robots II,"We present a minimal phase oscillator model for learning quadrupedal locomotion. Each of the four oscillators is coupled only to itself and its corresponding leg through local feedback of the ground reaction force, which can be interpreted as an observer feedback gain. We interpret the oscillator itself as a latent contact state-estimator. Through a systematic ablation study, we show that the combination of phase observations, simple phase-based rewards, and the local feedback dynamics induces policies that exhibit emergent gait preferences, while using a reduced set of simple rewards, and without prescribing a specific gait. The code is open-source, and a video synopsis available at https://youtu.be/1NKQ0rSV3jU."
Bio-Inspired Gait Transitions for Quadruped Locomotion,"Joseph Elliot Humphreys, Jun Li, Yuhui Wan, Haibo Gao, Chengxu Zhou","University of Leeds,Harbin Institute of Technology,University of leeds,University College London",Legged Robots II,
Optimizing Dynamic Balance in a Rat Robot Via the Lateral Flexion of a Soft Actuated Spine,"Yuhong Huang, Zhenshan Bing, Zitao Zhang, Genghang Zhuang, Kai Huang, Zhenshan Bing","Technische Universität München,Technical University of Munich,Sun Yat-Sen University,Sun Yat-sen University,Tech. Univ. Muenchen TUM",Legged Robots II,"Balancing oneself using the spine is a physiological alignment of the body posture in the most efficient manner by the muscular forces for mammals. For this reason, we can see many disabled quadruped animals can still stand or walk even with three limbs. This paper investigates the optimization of dynamic balance during trot gait based on the spatial relationship between the center of mass (CoM) and support area influenced by spinal flexion. During trotting, the robot balance is significantly influenced by the distance of the CoM to the support area formed by diagonal footholds. In this context, lateral spinal flexion, which is able to modify the position of footholds, holds promise for optimizing balance during trotting. This paper explores this phenomenon using a rat robot equipped with a soft actuated spine. Based on the lateral flexion of the spine, we establish a kinematic model to quantify the impact of spinal flexion on robot balance during trot gait. Subsequently, we develop an optimized controller for spinal flexion, designed to enhance balance without altering the leg locomotion. The effectiveness of our proposed controller is evaluated through extensive simulations and physical experiments conducted on a rat robot.Compared to both a non-spine based trot gait controller and a trot gait controller with lateral spinal flexion, our proposed optimized controller effectively improves the dynamic balance of the robot and retains the desired locomotion during trotting."
SLoMo: A General System for Legged Robot Motion Imitation from Casual Videos,"John Zhang, Shuo Yang, Gengshan Yang, Arun Bishop, Swaminathan Gurumurthy, Deva Ramanan, Zachary Manchester","Carnegie Mellon University,Meta",Legged Robots II,"We present SLoMo: a first-of-its-kind framework for transferring skilled motions from casually captured â€œin- the-wildâ€ video footage of humans and animals to legged robots. SLoMo works in three stages: 1) synthesize a physically plausible reconstructed key-point trajectory from monocular videos; 2) optimize a dynamically feasible reference trajectory for the robot offline that includes body and foot motion, as well as a contact sequence that closely tracks the key points; 3) track the reference trajectory online using a general-purpose model-predictive controller on robot hardware. Traditional motion imitation for legged motor skills often requires expert anima- tors, collaborative demonstrations, and/or expensive motion- capture equipment, all of which limits scalability. Instead, SLoMo only relies on easy-to-obtain videos, readily available in online repositories such as YouTube. It converts videos into motion primitives that can be executed reliably by real- world robots. We demonstrate our approach by transferring the motions of cats, dogs, and humans to example robots including a quadruped (on hardware) and a humanoid (in simulation)."
Introducing the Carpal-Claw: A Mechanism to Enhance High-Obstacle Negotiation for Quadruped Robots,"Victor Barasuol, Sinan Emre, Vivian Suzano Medeiros, Angelo Bratta, Claudio Semini","Istituto Italiano di Tecnologia,University of São Paulo",Legged Robots II,"The capability of a quadruped robot to negotiate obstacles is tightly connected to its leg workspace and joint torque limits. When facing terrain where the height of obstacles is close to the leg length, the locomotion robustness and safety are reduced since more dynamic motions are required to traverse it. In this paper, we introduce a new mechanism called the Carpal-Claw, which enables quadruped robots to negotiate higher obstacles and adds safety to the locomotion by allowing the robot to negotiate obstacles under static and quasi-static locomotion and regular joint torque demands. The design of the mechanism is detailed, as well as the methodology to exploit the mechanism in the locomotion control framework. The Carpal-Claw functionality is validated through various experiments on a very high obstacle and stairs-like terrains using an Aliengo robot. We demonstrate how Aliengo can safely descend a step height of 40cm, which is 80% of its leg length. To the best knowledge of the authors, this is the first time a mechanism like the C-Claw is proposed for improving quadruped robot locomotion over high obstacles."
SpaceHopper: A Small-Scale Legged Robot for Exploring Low-Gravity Celestial Bodies,"Alexander Spiridonov, Fabio Buehler, Moriz Berclaz, Valerio Antonio Schelbert, Jorit Geurts, Elena Krasnova, Emma Steinke, Jonas Toma, Joschua Wüthrich, Recep Polat, Wim Zimmermann, Philip Arm, Nikita Rudin, Hendrik Kolvenbach, Marco Hutter","ETH Zurich,ETH Zürich,ZHAW School of Engineering,ZHAW,ETH Zurich, NVIDIA",Legged Robots II,"We present SpaceHopper, a three-legged, small-scale robot designed for future mobile exploration of asteroids and moons. The robot weighs 5.2kg and has a body size of 245mm while using space-qualifiable components. Furthermore, SpaceHopper's design and controls make it well-adapted for investigating dynamic locomotion modes with extended flight-phases. Instead of gyroscopes or fly-wheels, the system uses its three legs to reorient the body during flight in preparation for landing. We control the leg motion for reorientation using Deep Reinforcement Learning policies. In a simulation of Ceres' gravity (0.029g), the robot can reliably jump to commanded positions up to 6m away. Our real-world experiments show that SpaceHopper can successfully reorient to a safe landing orientation within 9.7 degree inside a rotational gimbal and jump in a counterweight setup in Earth's gravity. Overall, we consider SpaceHopper an important step towards controlled jumping locomotion in low-gravity environments."
ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse Quadruped Robots,"Milad Shafiee, Guillaume Bellegarda, Auke Ijspeert",EPFL,Legged Robots II,"Learning a locomotion policy for quadruped robots has traditionally been constrained to a specific robot morphology, mass, and size. The learning process must usually be repeated for every new robot, where hyperparameters and reward function weights must be re-tuned to maximize performance for each new system. Alternatively, attempting to train a single policy to accommodate different robot sizes, while maintaining the same degrees of freedom (DoF) and morphology, requires either complex learning frameworks, or mass, inertia, and dimension randomization, which leads to prolonged training periods. In our study, we show that drawing inspiration from animal motor control allows us to effectively train a single locomotion policy capable of controlling a diverse range of quadruped robots. The robot differences encompass: a variable number of DoFs, (i.e. 12 or 16 joints), three distinct morphologies, a broad mass range spanning from 2 kg to 200 kg, and nominal standing heights ranging from 18 cm to 100 cm. Our policy modulates a representation of the Central Pattern Generator (CPG) in the spinal cord, effectively coordinating both frequencies and amplitudes of the CPG to produce rhythmic output (Rhythm Generation), which is then mapped to a Pattern Formation (PF) layer. Across different robots, the only varying component is the PF layer, which adjusts the scaling parameters for the stride height and length. Subsequently, we evaluate the sim-to-real transfer by testing the single policy on both the Unitree Go1 and A1 robots. Remarkably, we observe robust performance, even when adding a 15 kg load, equivalent to 125% of the A1 robotâ€™s nominal mass."
Safety-Critical Coordination of Legged Robots Via Layered Controllers and Forward Reachable Set Based Control Barrier Functions,"Jeeseop Kim, Jaemin Lee, Aaron Ames","Caltech,California Institute of Technology",Motion Control II,"This paper presents a safety-critical approach to the coordination of robots in dynamic environments. To this end, we leverage control barrier functions (CBFs) with the forward reachable set to guarantee the safe coordination of the robots while preserving a desired trajectory via a layered controller. The top-level planner generates a safety-ensured trajectory for each agent, accounting for the dynamic constraints in the environment. This planner leverages high-order CBFs based on the forward reachable set to ensure safety-critical coordination control, i.e., guarantee the safe coordination of the robots during locomotion. The middle-level trajectory planner employs single rigid body (SRB) dynamics to generate optimal ground reaction forces (GRFs) to track the safety-ensured trajectories from the top-level planner. The whole-body motions to adhere to the optimal GRFs while ensuring the friction cone condition at the end of each stance leg are generated from the low-level controller. The effectiveness of the approach is demonstrated through simulation and hardware experiments."
Safety-Critical Control of Quadrupedal Robots with Rolling Arms for Autonomous Inspection of Complex Environments,"Jaemin Lee, Jeeseop Kim, Wyatt Ubellacker, Tamas G. Molnar, Aaron Ames","California Institute of Technology,Caltech,Wichita State University",Motion Control II,"This paper presents a safety-critical control framework tailored for quadruped robots equipped with a roller arm, particularly when performing locomotive tasks such as autonomous robotic inspection in complex, multi-tiered environments. In this study, we consider the problem of operating a quadrupedal robot in distillation columns, locomoting on column trays and transitioning between these trays with a roller arm. To address this problem, our framework encompasses the following key elements: 1) Trajectory generation for seamless transitions between columns, 2) Foothold re-planning in regions deemed unsafe, 3) Safety-critical control incorporating control barrier functions, 4) Gait transitions based on safety levels, and 5) A low-level controller. Our comprehensive framework, comprising these components, enables autonomous and safe locomotion across multiple layers. We incorporate reduced-order and full-body models to ensure safety, integrating safety-critical control and footstep re-planning approaches. We validate the effectiveness of our proposed framework through practical experiments involving a quadruped robot equipped with a roller arm, successfully navigating and transitioning between different levels within the column tray structure."
Robust and Remote Center of Cyclic Motion Control for Redundant Robots with Partially Unknown Structure,"Long Jin, Kun Liu, Mei Liu",Lanzhou University,Motion Control II,"Remote center of motion (RCM) describes a robot with a rod-like end-effector operating through a hole in the interface separating the internal space from the external space. Considering that the control of RCM may be influenced by perturbations (noises) and that the end-effector is frequently replaced to complete different tasks, the structural information related to the robot manipulator and its rod-like end-effector may contain errors. This paper proposes an acceleration-level remote center of cyclic motion (ARC$^{2}$M) control scheme, which takes into account the cyclic motion index and the physical limitations of robot manipulators to achieve repetitive motion planning and RCM control at the acceleration level. Additionally, a parameter calculation method is proposed to compute unknown parameters of the end-effector under the influence of noise. Kalman filter and a neural dynamics-based method are employed to address noises effects, and related theoretical analyses are given. To validate the proposed ARC$^2$M scheme, simulations and physical experiments are carried out. The source code is available at https://github.com/LongJin-lab/ARCM."
Safe Risk-Averse Bayesian Optimization for Controller Tuning,"Christopher König, Miks Ozols, Anastasiia Makarova, Efe Balta, Andreas Krause, Alisa Rupenyan","Inspire AG,ETH Zurich,Zurich University of Applied Sciences",Motion Control II,"Controller tuning and parameter optimization are crucial in system design to improve both the controller and underlying system performance. Bayesian optimization has been established as an efficient model-free method for controller tuning and adaptation. Standard methods, however, are not enough for high-precision systems to be robust with respect to unknown input-dependent noise and stable under safety constraints. In this work, we present a novel data-driven approach, RAGoOSe, for safe controller tuning in the presence of heteroscedastic noise, combining safe learning with risk-averse Bayesian optimization. We demonstrate the method for synthetic benchmark and compare its performance to established BO-based tuning methods. We further evaluate RaGoose performance on a real precision-motion system utilized in semiconductor industry applications and compare it to the built-in auto-tuning routine."
Phase Synthesis for Spatial Locomotion Control of Retractable Worm Robots,"Zhongcheng Wang, Shiwei Yuan, Manfen Dou, Jianhua Yang, Bin Liang","Northwestern Polytechnical University,Tsinghua University",Motion Control II,"Retractable worm robots possess hyper-flexibility, allowing them to work in confined spaces that are difficult for humans. However, the spatial locomotion control of these robots remains challenging due to the robots' large degrees of freedom. To address this challenge, we propose a phase synthesis (PS) scheme for retractable worm robots. The scheme combines an undulating gait inspired by caterpillars with three-dimensional movement commands. We first introduce the kinematics model and real-world prototype of our retractable worm robot, called RW-Robot, and then we introduce footstep phases to express the timing of segments' spatial movement. According to the length of movement periods, we classify the movement into short-term movements and long-term movements and compress their patterns in the frequency domain. Our PS scheme aligns the patterns according to the footstep phases to generate new gaits of spatial locomotion. We evaluate the scheme in real-world experiments, including steering and climbing a slope. The experimental results indicate that our scheme allows the RW-Robot to perform flexible spatial locomotion from simple user input."
Enhanced Robust Motion Control Based on Unknown System Dynamics Estimator for Robot Manipulators,"Xinyu Jia, Jun Yang, Lu Kaixin, Yongping Pan, Haoyong Yu","National University of Singapore,Faculty of Engineering, National University of Singapore,Sun Yat-sen University",Motion Control II,"To achieve high-accuracy manipulation in the presence of unknown disturbances, we propose two novel efficient and robust motion control schemes for high-dimensional robot manipulators. Both controllers incorporate an unknown system dynamics estimator (USDE) to estimate disturbances without requiring acceleration signals and the inverse of inertia matrix. Then, based on the USDE framework, an adaptive-gain controller and a super-twisting sliding mode controller are designed to speed up the convergence of tracking errors and strengthen anti-perturbation ability. The former aims to enhance feedback portions through error-driven control gains, while the latter exploits finite-time convergence of discontinuous switching terms. We analyze the boundedness of control signals and the stability of the closed-loop system in theory, and conduct real hardware experiments on a robot manipulator with seven degrees of freedom (DoF). Experimental results verify the effectiveness and improved performance of the proposed controllers, and also show the feasibility of implementation on high-dimensional robots."
Model-Free Control of a Class of High-Precision Scanning Motion Systems with Piezoceramic Actuators,"Yazan Al-rawashdeh, Mohammad Al Saaideh, Marcel Heertjes, Mohammad Al Janaideh","Memorial University of Newfoundland,Eindhoven University of Technology,University of Guelph",Motion Control II,"To enhance the precision of coarse long-stroke motion axes, complementary short-stroke fine positioning stages are usually introduced. Being mechanically attached, the motion of the combined positioning stages needs to be controlled and synchronized. Therefore, typically suitable model-based controllers of fine stages are designed according to the so- phisticated models and identification techniques used. Due to their appealing features, Piezocermamic-based fine positioning stages were successfully utilized in many applications, which recently sparked their use in high-acceleration motion found in wafer scanners, for example, where high-precision motion is required despite the resulting high inertial forces involved. Unfortunately, hard nonlinear behavior is associated with piezo- electric actuators, which adds to the complexity of modeling, control, and synchronization processes. To overcome such a burden, in this study, the design procedure of a model-free control and synchronization technique of piezocermamic-based fine positioning stages is introduced and verified experimentally using a representative precision motion system comprising a planner stage and a uni-axial fine stage under step-and-scan trajectories commonly used in wafer scanners. Despite its sim- plicity, the herein proposed design procedure can be seamlessly extended to other robotics and automation applications"
Constrained Nonlinear Disturbance Observer for Robotic Systems,"Jiwan Han, Daehyung Park, Min Jun Kim","Korea Advanced Institute of Science and Technology,Korea Advanced Institute of Science and Technology, KAIST,KAIST",Motion Control II,"Disturbance observer (DOB) is a well-known two-loop control structure that imparts robustness to a controller with a simple implementation. As a nonlinear DOB for the robotic systems, we proposed so-called nonlinear robust internal-loop compensator (NRIC) framework in our previous work. In this paper, we further extend the NRIC in such a way that an optimization scheme can be embedded in the control structure. The proposed method is called constrained NRIC (C-NRIC), because the optimization allows us to impose constraints, by which a controller acquires additional properties. As a particular use case of the C-NRIC framework, we design contact-responsive motion controllers that enables a robot to react to unknown interactions while accurately tracking the desired trajectory in free motion. The effectiveness of such designs is validated through the real-world experiments."
An Integrated Position-Velocity-Force Method for Safety-Enhanced Shared Control in Robot-Assisted Surgical Cutting,"Xilin Xiao, Xiaojian Li, Shi Yudong, Jin Fang, Ling Li, Pengfei He, Hangjie Mo","Hefei University of Technology,HeFei University of Technology,City University of HongKong",Motion Control II,"Numerous studies have emphasized the application of autonomous intelligence in human-robot shared control to enhance surgical convenience and efficiency. However, the neglect of human dominance may reduce surgical safety. This paper developed a safety-enhanced human-robot shared control method by intelligently allocating control authority, with the surgeon remaining the leader during the surgical procedure. Three controllers are designed initially, including a master hand position (MP) controller and a master hand velocity (MV) controller related to the surgeon's manipulation, and a planned trajectory tracking (PT) controller related to the robot. In precision surgical manipulation scenarios, precise tracking of the human's operation is achieved by combining MP and MV controllers, while a combination of MV and PT controllers is developed in high-efficiency surgical scenarios, which relaxes the requirement for precise tracking of hand position and enables precise robot assistance guided by the velocity of human hand. The autonomous scenarios and controllers switching are accomplished through a motion fusion mechanism, which is achieved via optimizing evaluation functions that are reliant on future states. Furthermore, a force feedback mechanism is proposed to help human understand the intent of autonomous control to improve safety. The feasibility and effectiveness of this method have been validated through simulations and experiments."
DopUS-Net: Quality-Aware Robotic Ultrasound Imaging Based on Doppler Signal,"Zhongliang Jiang, Felix Duelmer, Nassir Navab","Technical University of Munich,TU Munich",Medical Robots II,"Medical ultrasound (US) is widely used to evaluate and stage vascular diseases, in particular for the preliminary screening program, due to the advantage of being radiation-free. However, automatic segmentation of small tubular structures (e.g., the ulnar artery) from cross-sectional US images is still challenging. To address this challenge, this paper proposes the DopUS-Net and a vessel re-identification module that leverage the Doppler effect to enhance the final segmentation result. Firstly, the DopUS-Net combines the Doppler images with B-mode images to increase the segmentation accuracy and robustness of small blood vessels. It incorporates two encoders to exploit the maximum potential of the Doppler signal and recurrent neural network modules to preserve sequential information. Input to the first encoder is a two-channel duplex image representing the combination of the grey-scale Doppler and B-mode images to ensure anatomical spatial correctness. The second encoder operates on the pure Doppler images to provide a region proposal. Secondly, benefiting from the Doppler signal, this work first introduces an online artery re-identification module to qualitatively evaluate the real-time segmentation results and automatically optimize the probe pose for enhanced Doppler images. This quality-aware module enables the closed-loop control of robotic screening to further improve the confidence and robustness of image segmentation. The experimental results demonstrate that the prop"
Robotic Craniomaxillofacial Osteotomy System Using Acoustic 3D Registration,"Jiayu Zhu, Runzhe Han, Mengning Yuan, Bimeng Jie, Shanshan Du, Yang He, Runshi Zhang, Junchen Wang","Beihang University,Peking University School and Hospital of Stomatology,Peking University",Medical Robots II,"Osteotomy holds a pivotal position among the fundamental procedures in craniomaxillofacial (CMF) surgery. However, there are inherent challenges and risks associated with ensuring the recuperation of occlusion, safeguarding the facial nerves and blood vessels, as well as preserving facial aesthetics. In this study, a non-invasive image-to-patient registration method for navigation/robotic CMF surgery based on intraoperative freehand ultrasound (US) 3D reconstruction is proposed. Building upon this, a CMF osteotomy robotic system with compliant human-robot interaction and osteotomy trajectory planning was devised. In the freehand US 3D reconstruction and registration experiments, the registration errors for human volunteers and phantoms were consistently less than 1 mm. In robot osteotomy experiments based on the resulting registration, the average osteotomy error was below 1.5 mm. The proposed US 3D reconstruction-based registration method is non-invasive and radiation-free, and shows the promising accuracy which is suitable for CMF robotic or navigation systems."
Elliptical Torus-Based Six-Axis FBG Force Sensor with In-Situ Calibration for Condition Monitoring of Orthopedic Surgical Robot,"Tianliang Li, Chen Zhao, Yuhang Wen, Fayin Chen, Yuegang Tan, Zude Zhou",Wuhan University of Technology,Medical Robots II,"Six-axis force/moment (6-A F/M) sensors make surgical robots effectively sense intraoperative force feedback and drilling status information, reducing the operating challenges and psychological burden of doctors, which also improves the quality and safety of surgery. However, it is difficult for current commercial electrical 6-A F/M sensors to adapt the electromagnetic environment in the operating room, and status changes after installation can also reduce accuracy. At the same time, there is a strong vibration coupling of low-frequency force information, leading to low identification accuracy and slow response speed in the drilling and milling status. Aiming at these problems, an elliptical torus-based 6-A fiber optic F/M sensor and its in-situ calibration method for orthopedic surgical robot force sensing are proposed. Furthermore, combined with the multichannel one-dimensional convolutional gated recurrent unit (M1-DCGRU), a fast and accurate identification of seven drilling stages was realized. The final force sensing error is less than 7.1%, and the drilling state identification accuracy is at least 93.9%. The designed sensor has higher accuracy, is compatible with magnetic resonance imaging (MRI), and accurately identifies finer drilling stages without relying on other sensors."
Vision-And-Force-Based Compliance Control for a Posterior Segment Ophthalmic Surgical Robot,"Ning Wang, Xiaodong Zhang, Danail Stoyanov, Hongbing Zhang, Agostino Stilli","Xi'an Jiaotong University,Xi’an Jiaotong University,University College London,The First Affiliated Hospital of Northwestern University",Medical Robots II,"In ophthalmic surgery, particularly in procedures involving the posterior segment, clinicians face significant challenges in maintaining precise control of hand-held instruments without damaging the fundus tissue. Typical targets of this type of surgery are the internal limiting membrane (ILM) and the epiretinal membrane (ERM) which have an average thickness of only 60 Î¼m and 2 Î¼m, respectively, making it challenging, even for experienced clinicians utilising dedicated ophthalmic surgical robots, to peel these delicate membranes successfully without damaging the healthy tissue. Minimal intra-operative motion errorswhen driving both hand-held and robotic-assisted surgical tools may result in significant stress on the delicate tissue of the fundus, potentially causing irreversible damage to the eye. To address these issues, this work proposes an intra-operative vision-and-force-based compliance control method for a posterior segment ophthalmic surgical robot. This method aims to achieve compliance control of the surgical instrument in contact with the tissue to minimise the risk of tissue damage. In this work we demonstrate that we can achieve a maximum motion error for the end effector (EE) of our ophthalmic robot of just 8 Î¼m, resulting in a 64% increase in motion accuracy compared to our previous work where the system was firstly introduced. The results of the proposed compliance control demonstrate consistent performance in the force range of 40 mN during mem"
A Hybrid Admittance Control Algorithm for Automatic Robotic Cranium-Milling,"Chen Qian, Zhen Li, Qiang Ye, Pei Cong Ge, Jizong Zhao, Gui-Bin Bian","Institute of Automation, Chinese Academy of Sciences,Beijing Tiantan Hospital, Capital Medical University",Medical Robots II,"Prior robot-assisted cranium-milling studies only considered controlling the force in the skull's vertical direction and neglected the milling cutter's feed force. Additionally, achieving stable force control in multiple directions is challenging for robots due to the uneven skull surface. Here a hybrid admittance control algorithm incorporating a model-free adaptive nonlinear force control and fuzzy control algorithms is proposed to accomplish effective automatic cranial-milling tasks. First, a pure data-driven model-free adaptive control method based on partial form dynamic linearization is used to control the vertical force. Second, fuzzy control minimizes the total error of both the vertical and feed force by adaptively adjusting the milling cutter's velocity and position. 42 ex vivo animal skull-milling experiments conducted by the automatic robotic cranium-milling system indicate that when using the proposed control algorithm, the force error percentage can be maintained below 5.0% within 3 s and the maximal root mean square error percentages for vertical and feed force are 1.85% and 1.94%, respectively. Moreover, no instances of dura mater damage are observed and the robotic system exhibits a high level of autonomy as it performs the skull milling task with minimal human involvement throughout the entire experiment. The results suggest the potential for advancing the intelligence level of neurosurgery in the future."
Preliminary Study of Fingertip and Wrist Motion Based Haptic Controller for Robotically Assisted Micro and Supermicrosurgy,"Muneaki Miyasaka, Pepijn Van Esch, Atsushi Morikawa, Kotaro Tadano","Riverfield Inc.,Riverfield inc.,Tokyo Institute of Technology",Medical Robots II,"One issue of robotic microsurgery is that compared to manual surgery, the operation time tends to be longer due to high motion scaling. To address this issue, we developed a new controller that can provide the accuracy required for microsurgery without a high scaling factor by utilizing fingertip and wrist motions. Also, for the better outcome of surgery, the proposed controller has a force feedback function which is not available for the existing controllers for microsurgical robots. A challenge of designing such a controller is associated with the size requirement. For conventional microsurgery, surgeons perform surgical procedures while looking at the eyepieces of a surgical microscope and the same applies for robotic microsurgery. The only space available to manipulate controllers is a narrow space between the patient/surgical bed and the surgeon. To satisfy this constraint, the proposed controller is integrated with a handrest and the controller's DOFs are strategically allocated. In this work, as the first step of addressing the issue of prolonged operational time, we built a prototype controller and evaluated the accuracy and task space with simulations. The results indicated that by using the fingertip and wrist motions with a scaling factor of 3x, 0.5 mm diameter circles could be traced with a mean bidirectional precision of 0.0485 mm. Also, 10.0 mm diameter circles were traceable with the same scaling factor."
Haptic-Assisted Collaborative Robot Framework for Improved Situational Awareness in Skull Base Surgery,"Hisashi Ishida, Manish Sahu, Adnan Munawar, Nimesh Nagururu, Deepa Galaiya, Peter Kazanzides, Francis Creighton, Russell H. Taylor","Johns Hopkins University,Johns Hopkins,Johns Hopkins University School of Medicine,Johns Hopkins School of Medicine,The Johns Hopkins University",Medical Robots II,"Skull base surgery is a demanding field in which surgeons operate in and around the skull while avoiding critical anatomical structures including nerves and vasculature. While image-guided surgical navigation is the prevailing standard, limitation still exists, requiring personalized planning and recognizing the irreplaceable role of a skilled surgeon. This paper presents a collaboratively controlled robotic system tailored for assisted drilling in skull base surgery. Our central hypothesis posits that this collaborative system, enriched with haptic assistive modes to enforce virtual fixtures, holds the potential to significantly enhance surgical safety, streamline efficiency, and alleviate the physical demands on the surgeon. The paper describes the intricate system development work required to enable these virtual fixtures through haptic assistive modes. To validate our system's performance and effectiveness, we conducted initial feasibility experiments involving a medical student and two experienced surgeons. The experiment focused on drilling around critical structures following cortical mastoidectomy, utilizing dental stone phantom and cadaveric models. Our experimental results demonstrate that our proposed haptic feedback mechanism enhances the safety of drilling around critical structures compared to systems lacking haptic assistance. With the aid of our system, surgeons were able to safely skeletonize the critical structures without breaching any critical structure even under obstructed view of the surgical site."
Intelligent Disinfection Robot with High-Touch Surface Detection and Dynamic Pedestrian Avoidance,"Yunfei Luan, Muhang He, Yudong Tian, Chengjie Lin, Yunhan Fang, Zihao Zhao, Jianxin Yang, Yao Guo","Shanghai Jiao Tong University,Shanghai Jiaotong University",Medical Robots II,"The increasing awareness of public health issues has highlighted the need for effective disinfection of crowded indoor public areas, leading to the development of automated disinfection robots. However, most of the existing robots spray disinfectant in all areas, and they are still immature to navigate in densely populated environments. Hence, in this paper, we design a new disinfection robotic system consisting of a mobile platform, an RGB-D camera, and a robotic arm with a spray disinfection device. To address the above challenges, we propose a vision-based method for accurately detecting high-touch areas in the surroundings, enabling the disinfection robot to achieve superior disinfection efficiency. In addition, we propose a dynamic pedestrian avoidance method, namely Socially Aware APF (SA-APF), which can predict the movement trend of pedestrians and plan the path in real-time. Both simulated and real-world experiments are conducted to demonstrate the effectiveness of our disinfection robot system, especially highlighting the ability to detect high-touch areas and navigate in the environment while avoiding dynamic pedestrians."
Toward a Framework Integrating Augmented Reality and Virtual Fixtures for Safer Robot-Assisted Lymphadenectomy,"Ziyang Chen, Ke Fan, Laura Cruciani, Matteo Fontana, Lorenzo Muraglia, Francesco Ceci, Laura Travaini, Giancarlo Ferrigno, Elena De Momi","Politecnico di Milano,European Institute of Oncology",Medical Robots II,"Lymphadenectomy generally accompanies various oncology surgeries to remove infected cancer cells. However, there are two limitations in robot-assisted lymphadenectomy: 1) lymph nodes are not visible during operation since they are hidden by the superficial fat layer; 2) intra-operative bleeding may occur during lymph node removal caused by collisions between surgical instruments and delicate blood vessels (arteries or veins) near the lymph nodes. Therefore, we propose a framework integrating augmented reality and virtual fixtures to address these limitations. Augmented reality intra-operatively visualizes the hidden lymph nodes by projecting the corresponding 3D pre-operative model, and virtual fixtures are used to provide force feedback to surgeons to avoid possible collisions when they operate the surgical instruments to resect the lymph nodes surrounding the blood vessel. Ten human subjects were invited to perform an emulated lymphadenectomy based on the da Vinci robot in a dry lab. Experimental results demonstrated that the proposed framework can keep localizing the hidden lymph nodes, and reduce the number of collisions (21% and 48% reduction rates using two different force models compared to the standard setup, respectively) between the instruments and the delicate blood vessel during lymph node resection. It shows the potential to enhance the safety of robot-assisted lymphadenectomy."
Hyblock: Hardware Realization and Control of Modular Hydraulic Robots with Dowel Connectors,"Sang-Ho Hyon, Ryo Ando, Eiji Sono, Shunichi Sugimoto, Yasushi Saito","Ritsumeikan University,KYB-YS Co. Ltd",Robotics and Automation in Construction,"This paper presents the hardware design and development of Hyblock, a modular hydraulic robot for heavy-duty application such as construction. The robot is equipped with a simple docking mechanism called a C-type expansion dowel and a novel hydraulic circuit MHSB that matches the modular structure. In this paper, we first report on the design of the robot hardware including the dowel and hydraulic circuit, then present preliminary experiments on pressure-based torque control and docking control using proximal magnetic sensors. Next, we propose a framework for dynamic reconfiguration and task-space motion control built on the concept of dowel connectors. Simulation results demonstrate that a collective modular robot achieves desired motion tasks while keeping all normal contact forces of the connectors being lower-bound. The results are also explained in the supplementary video."
PLASTR: Planning for Autonomous Sampling-Based Trowelling,"Mads Alber Kuhlmann-jørgensen, Johannes Pankert, Lukasz Leszek Pietrasik, Marco Hutter","ETH Zurich,ETH Zuerich",Robotics and Automation in Construction,"Plaster is commonly used in the construction industry to finish walls and ceilings, but the application is labor-intensive and physically strenuous, which motivates the need for automation. We present PLASTR, a receding horizon optimization-based planning algorithm for robotic plaster trowelling. It samples trowelling sequence rollouts from a new plaster simulator and weights them according to the flatness of the finished wall. The proposed simulator approximates the real-world plaster-trowel interaction adequately while allowing execution orders of magnitude faster than real-time. We evaluate PLASTR in simulation and on a real-world test setup and compare it to two handcrafted heuristic baseline algorithms. PLASTR performs equal to or better than the best heuristic in terms of material coverage for both simulated and real-world experiments while being 50% more efficient in terms of trowelled distance."
Self-Reconfigurable Robots for Collaborative Discrete Lattice Assembly,"Miana Smith, Amira Abdel-rahman, Neil Gershenfeld","MIT,Massachusetts Institute of Technology",Robotics and Automation in Construction,"We present a robotic system for the assembly of 3D discrete lattice structures in which the robots are able to self-reproduce, such that the assembly system may scale its own parallelization. Robots and structures are made from a set of compatible building blocks, or voxels, which can be assembled and reassembled into more complex structures. Robotic modules are made by combining actuators with a functional voxel, which routes electrical power and signals. Robotic modules then assemble into reconfigurable robots via a reversible solder joint. The robot assembles higher performance structures using a set of construction voxels, which do not contain electrical features. This paper describes the design, development, and evaluation of this assembly system, including the robotic hardware, lattice material, and planning and controls methods. We demonstrate the system through a set of fundamental assembly tasks: the robot assembling another robot, and the two robots collaborating to assemble a small structure."
LiSTA: Geometric Object-Based Change Detection in Cluttered Environments,"Joseph Rowell, Lintong Zhang, Maurice Fallon","University of Oxford, Oxford Robotics Institute,University of Oxford",Robotics and Automation in Construction,"We present LiSTA (LiDAR Spatio-Temporal Analysis), a system to detect probabilistic object-level change over time using multi-mission SLAM. Many applications require such a system, including construction, robotic navigation, long-term autonomy, and environmental monitoring. We focus on the semi-static scenario where objects are added, subtracted, or changed in position over weeks or months. Our system combines multi-mission LiDAR SLAM, volumetric differencing, object instance description, and correspondence grouping using learned descriptors to keep track of an open set of objects. Object correspondences between missions are determined by clustering the object's learned descriptors. We demonstrate our approach using datasets collected in a simulated environment and a real-world dataset captured using a LiDAR system mounted on a quadruped robot monitoring an industrial facility containing static, semi-static, and dynamic objects. Our method demonstrates superior performance in detecting changes in semi-static environments compared to existing methods."
Scalable Underwater Assembly with Reconfigurable Visual Fiducials,"Samuel Lensgraf, Ankita Sarkar, Adithya Pediredla, Devin Balkcom, Alberto Quattrini Li",Dartmouth College,Robotics and Automation in Construction,"We present a scalable combined localization infrastructure deployment and task planning algorithm for underwater assembly. Infrastructure is autonomously modified to suit the needs of manipulation tasks based on an uncertainty model on the infrastructure's positional accuracy. Our uncertainty model can be combined with the noise characteristics from multiple devices. For the task planning problem, we propose a layer-based clustering approach that completes the manipulation tasks one cluster at a time. We employ movable visual fiducial markers as infrastructure and an autonomous underwater vehicle (AUV) for manipulation tasks. The proposed task planning algorithm is computationally simple, and we implement it on AUV without any offline computation requirements. Combined hardware experiments and simulations over large datasets show that the proposed technique is scalable to large areas."
Automatic Loading of Unknown Material with a Wheel Loader Using Reinforcement Learning,"Daniel Eriksson, Reza Ghabcheloo, Geimer Marcus","Tampere University,Karlsruhe Institute of Technology",Robotics and Automation in Construction,"Loading multiple different materials with wheel loaders is a challenging task because various materials require different loading techniques. It's, therefore, difficult to find a single controller capable of handling them all. One solution is to use a base controller and fine-tune it for different materials. Reinforcement Learning (RL) automates this process without the need for collecting additional human-annotated data. We investigated the feasibility of this approach using a full-size 24-tonnes wheel loader in the real world and demonstrated that it's possible to fine-tune a neural network controller that was originally trained with imitation learning on blasted rock for use with an unknown gravel material, requiring 20 bucket fillings. Additionally, we showcased the adaptability of a controller pre-trained on woodchips for an unknown gravel material, requiring 40 bucket fillings. We also proposed a novel reward function for the material loading task. Finally, we examined how the sampling time of the reinforcement learning algorithm affects convergence speed and adaptability. Our results demonstrate that it's optimal to match the sampling time of the RL algorithm to the delays of the wheel loader's hydraulic actuators."
Learning Adaptive Policies for Autonomous Excavation under Various Soil Conditions by Adversarial Domain Sampling,"Takayuki Osa, Naoto Osajima, Masanori Aizawa, Tatsuya Harada","University of Tokyo,Kyushu Institute of Technology,Komatsu Ltd.,The University of Tokyo",Robotics and Automation in Construction,"Excavation is a frequent task in construction. In this context, automation is expected to reduce hazard risks and labor-intensive work. To this end, recent studies have investigated using reinforcement learning (RL) to automate construction machines. One of the challenges in applying RL to excavation tasks concerns obtaining skills adaptable to various conditions. When the conditions of soils differ, the optimal plans for efficiently excavating the target area will significantly differ. In existing meta-learning methods, the domain parameters are often uniformly sampled; this implicitly assumes that the difficulty of the task does not change significantly for different domain parameters. In this study, we empirically show that uniformly sampling the domain parameters is insufficient when the task difficulty varies according to the task parameters. Correspondingly, we develop a framework for learning a policy that can be generalized to various domain parameters in excavation tasks. We propose two techniques for improving the performance of an RL method in our problem setting: adversarial domain sampling and domain parameter estimation with a sensitivity-aware importance weight. In the proposed adversarial domain sampling technique, the domain parameters leading to low expected Q-values are actively sampled during the training phase. We empirically show that our approach outperforms existing meta-learning and domain adaptation methods for excavation tasks."
Robotic Inspection and Subsurface Defect Mapping Using Impact-Echo and Ground Penetrating Radar,"Ejup Hoxha, Jinglun Feng, Diar Sanakov, Jizhong Xiao","The City College of New York,New York University",Robotics and Automation in Construction,"Concrete infrastructure often develops a variety of internal flaws that cannot be detected through visual inspection alone, and must be regularly inspected with other methods to maintain structural integrity. It has been demonstrated through previous studies that relying solely on a single non-destructive evaluation (NDE) method can be insufficient in providing a comprehensive evaluation of the structure's condition. In addition, manual NDE data collection can be labor-intensive for on-site engineers. This paper presents a robotic inspection system that uses vision-based positioning and tags NDE measurement with pose information to reveal and map subsurface defects. The system consists of three modules: 1) an Omni-directional robotic data collection platform equipped with a Realsense D435i camera for localization, an impact-echo (IE) sensor, and a ground penetrating radar (GPR), to perform automatic NDE data collection; 2) an IE data processing module that utilizes both learning-based and classical methods to interpret the IE data and reveal subsurface objects; 3) a GPR data processing module to reconstruct underground targets and create a 3D map for better visualization. Field testing demonstrates that the robotic system significantly increases the data collection speed, and the correlation of findings from both IE and GPR sensors give a comprehensive evaluation of concrete structures that will benefit the inspection and maintenance industry of civil infrastructure."
Ospreys-Inspired Self-Takeoff Strategy of an Eagle-Scale Flapping-Wing Robot: System Design and Flight Experiments,"Haoyu Wang, Wenfu Xu, Linpo Hou, Erzhen Pan","Harbin Institute of Technology,Shenzhen,Harbin Institute of Technology, Shenzhen",Bioinspired Flight and Swimming,"In this work, we achieved self-takeoff of an eagle-scale flapping-wing robot for the first time. Inspired by the takeoff process of ospreys, we propose a bio-inspired takeoff strategy, then discuss the dynamic model and the requirements for self-takeoff. Based on the requirements of flight strategy, we designed a system with two parts, including a flapping-wing aircraft with a wingspan of 1.8m and a take-off weight of 870g, and an auxiliary platform with initial pitch angle adjustment function. In order to explore the differences in the take-off process under different conditions, we conduct the flight experiments under different time-averaged thrust-to-weight ratios (0.745-0.876) and launch angles (45Â°-90Â°). The results of flight experiments confirmed the theoretical analysis that the flapping-wing robot can achieve self-takeoff with no potential energy cost and maintain high maneuverability (The video shows a rapid climb immediately after takeoff) even when the time-averaged thrust-to-weight ratio is smaller than 1. This is significantly different from conventional rotary-wing and vertical take-off and landing (VTOL) UAVs. This work solves the challenge of self-takeoff for large-scale flapping-wing robots using a designable method, and demonstrates the superior performance potential of flapping-wing robots compared to conventional UAVs."
Design and Analysis of Adaptive Flipper with Origami Structure for Frog-Inspired Swimming Robot,"Shuqi Wang, Jizhuang Fan, Yitao Pan, Gangfeng Liu, Yubin Liu","Harbin Institute of Technology,Robot Research Institute, Harbin Institute of Technology,Harbin Institude of Technology",Bioinspired Flight and Swimming,"Flippers are important components for improving the locomotion efficiency and stability of bionic underwater robots. A novel origami-based adaptive flipper is presented to address a lack of environmental adaptability and low performance efficiency due to the structural design or inherent characteristics of its main constituent materials. The design decision process and locomotion principle of the flipper are introduced in detail. It can exhibit better adaptive deformation under the action of hydrodynamics without affecting the propulsion efficiency. Kinematics and simulation analysis are performed to characterize the influence of structural parameters on the motion performance. Experimental swimming results show that compared with ordinary flippers, the locomotion efficiency is greatly improved with the help of origami flippers. The origami flipper also shows good adaptability when in contact with the external environment and overcomes the inability of open-close flippers to cross a 90Â° corner, which shows the rationality of the structural design and the feasibility of its application in underwater robots."
Model-Based Approach for Lateral Maneuvers of Bird-Size Ornithopter,"Ernesto Sanchez-laulhe, Álvaro César Satué Crespo, Saeed Rafee Nekoo, Aníbal Ollero","University of Malaga,GRVC Robotics Lab., Universidad de Sevilla,GRVC Robotics Lab, Universidad de Sevilla,AICIA. G,,,,,,,,",Bioinspired Flight and Swimming,"A model-based approach for lateral maneuvering of flapping wing UAVs in closed spaces is presented. Bird-size ornithopters do not have asymmetric variables in the wing due to mechanical complexity, so they rely upon the tail for lateral maneuvering. The prototype E-Flap can deflect the vertical tail to make maneuvers out of the longitudinal plane. This work defines simplified equations for the steady turning maneuver based on the body roll angle. The relation between the velocity of the prototype and the turning radius is also stated. Then, an approach to the attitude is proposed, defining the relation between the deflection of the vertical tail and the roll angle. We prove that, even though this deflection causes a yaw moment, the coupling between yaw and roll dynamics generates also a roll rate. To validate this simplified model, a simple control is presented for continuous circular trajectory tracking inside an indoor flight zone. The objective is to track circular trajectories of a radius 2 times greater than the wingspan at a constant height. Results show a very good agreement between the theoretical and experimental turning radius. In addition, the direct relation between the vertical tail deflection and the roll rate of the ornithopter is identified. Even though the desired radius is not reached, the FWUAV is capable of maintaining a closed turning maneuver for several laps. Therefore, the insight provided by the model proves to be an appropriate approach for aggressive lateral maneuvers of bird-size ornithopters."
A General Kinematic Model of Fish Locomotion Enables Robot Fish to Master Multiple Swimming Motions,"Yong Zhong, Zicun Hong, Yuhan Li, Junzhi Yu","South China University of Technology,Chinese Academy of Sciences",Bioinspired Flight and Swimming,"Fish locomotion which adopts body and/or caudal fin swimming mode consists of different motions, such as Cruising-straight, Cruising-turn, and various fast turns, etc. Currently, there is no single mathematical model that could illustrate all these motions. Thus, for scientists and engineers, it is quite cumbersome and complicated to model and control different motions with multiple principles. In this paper, we proposed a general kinematic model to illustrate the kinematics of all aforementioned swimming motions. The model is synthesized by a nonlinear oscillator and a traveling wave equation. By changing four parameters extracted from the model, the kinematic model can demonstrate all the aforementioned swimming motions with different amplitudes and frequencies. To verify the model, we built a multi-joint robotic fish, and developed its dynamic model and control method to perform all the maneuvers under the guidance of the general kinematic model. Through this systematic methodology, one can easily study the principles of different swimming motions and design the multi-motions controller for a robotic fish through only one governing kinematic model."
Adaptation of Flipper-Mud Interactions Enables Effective Terrestrial Locomotion on Muddy Substrates,"Shipeng Liu, Boyuan Huang, Feifei Qian",University of Southern California,Bioinspired Flight and Swimming,"Moving on natural muddy terrains, where soil composition and water content vary significantly, is complex and challenging. To understand how mud properties and robot-mud interaction strategies affect locomotion performance on mud, we study the terrestrial locomotion of a mudskipper-inspired robot on synthetic mud with precisely-controlled ratios of sand, clay, and water. We observed a non-monotonic dependence of the robot speed on mud water content. Robot speed was the largest on mud with intermediate levels of water content (25%-26%), but decreased significantly on higher or lower water content. Measurements of mud reaction force revealed two distinct failure mechanisms. At high water content, the reduced mud shear strength led to a large slippage of robot appendages and a significantly reduced step length. At low water content, the increased mud suction force caused appendage entrapment, resulting in a large negative displacement in the robot body during the swing phase. A simple model successfully captured the observed robot performance, and informed adaptation strategies that increased robot speed by more than 200%. Our study is a beginning step to extend robot mobility beyond simple substrates towards a wider range of complex, heterogeneous terrains."
RoboTwin: A Platform to Study Hydrodynamic Interactions in Schooling Fish,"Liang Li, Li-ming Chao, Siyuan Wang, Oliver Deussen, Iain D. Couzin","Max-Planck Institute of Animal Behavior,Max Planck Institute of Animal Behavior,University of Konstanz",Bioinspired Flight and Swimming,"By living and moving in groups, ï¬sh can gain many beneï¬ts, such as heightened predator detection, greater hunting efï¬ciency, more accurate environmental sensing, and energetic saving. Although the beneï¬ts of hydrodynamic interactions in schooling ï¬sh have drawn growing interest in ï¬elds such as biology, physics, and engineering, and multiple hypotheses for how such beneï¬ts may arise have been proposed, it is still largely unknown which mechanisms ï¬sh employ to obtain hydrodynamic beneï¬ts, such as in increased thrust, or improved movement efï¬ciency. One main bottleneck has been the difï¬culty in collecting detailed sensory information, corresponding locomotory responses and hydrodynamic information from real schooling fish. In this paper, we present the RoboTwin platform designed to aid such data collection: it allows us to replay the dynamic movements and body posture kinematics of real fish in fish-like robots, allowing us to measure the power cost, thrust, and detailed flow fields, all of which is extremely challenging for real animals. We mutually verified our platform with our previously-proposed mechanism of energy saving ('vortex phase matching') by flow visualization through PIV (particle image velocimetry). Our results demonstrate the effectiveness of our design and highlight the potential of RoboTwin for future applications in exploring further hydrodynamic interactions among schooling fish."
Real-Time Estimation for the Swimming Direction of Robotic Fish Based on IMU Sensors,"Shikun Li, Yufan Zhai, Chen Wang, Guangming Xie",Peking University,Bioinspired Flight and Swimming,"An increasing number of underwater robots inspired by Carangidae are developed, which is characterized by high efficiency and flexibility. However, estimating the swimming direction of these robotic fish is challenging due to the constant swinging of the head during movement, which complicates precise control. In this study, we installed two low-cost inertial measurement unit (IMU) sensors separately on the head and tail parts of a double-joint robotic fish and presented a method for accurately and timely estimating the swimming direction. Firstly, we effectively compensated for the yaw angle drift of the IMU sensors through a fused Kalman Filter. Furthermore, we propose the Anti-Shake Estimation (ASE) algorithm to calculate the real-time swimming direction using filtered yaw angles at a high updating rate of 100Hz. Finally, we applied the method to swimming direction feedback control for evaluation and comparison. The results show that our ASE method performs better than other existing methods in straight-line swimming experiments. The experiment of S-curve swimming also demonstrates the effectiveness of our method in complex missions."
Tunable Stiffness Caudal Peduncle Leads to Higher Swimming Speed without Extra Energy,"Sijia Liu, Chunbao Liu, Yunhong Liang, Luquan Ren, Lei Ren","Jilin University,University of Manchester",Bioinspired Flight and Swimming,"Tuning body stiffness like fish to improve swimming efficiency and speed has been adopted by many fish-inspired robotics. However, it is unknown whether the energy saved from improved efficiency can compensate for the energy consumption brought by tuning stiffness itself. To explore this issue, we develop a robotic fish with a tunable stiffness caudal peduncle (TSCP), simultaneously allowing for untethered swimming and online tunable stiffness, and conduct a series of tests. We first apply interchangeable caudal peduncles to our robot to explore the effect of a tunable stiffness mechanism and determine the optimal tunable stiffness interval. The results show that tunable stiffness can significantly improve the response of robot at different frequencies. Then we develop the TSCP by embedding shape memory alloy wire into a silicone matrix. TSCP can adjust the stiffness in real time through current and increase the initial stiffness by up to 57.4%. More importantly, we incorporate the cost of tunable stiffness into the total cost of transport compared to previous robots for the first time. The cost of maintaining medium and maximum stiffness accounts for 8.72% and 17.87% of the total cost of transport, respectively. As a result, TSCP increases the swimming speed by up to 35.5% and reduces the Strouhal number by up to 21.9% at high frequencies without extra power."
A Novel Fish-Inspired Self-Adaptive Approach to Collective Escape of Swarm Robots Based on Neurodynamic Models,"Junfei Li, Simon X. Yang",University of Guelph,Bioinspired Flight and Swimming,"Fish schools present high-efficiency group behaviors to collective migration and dynamic escape from the predator through simple individual interactions. The purpose of this research is to infuse swarm robots with â€œfish-likeâ€ intelligence that will enable safe navigation and efficient cooperation, and successful completion of escape tasks in changing environments. In this paper, a novel fish-inspired self-adaptive approach is proposed for the collective escape of swarm robots. A bio-inspired neural network (BINN) is introduced to generate collision-free escape trajectories through the dynamics of neural activity and the combination of attractive and repulsive forces. In addition, a neurodynamics-based self-adaptive mechanism is proposed to improve the self-adaptive performance of the swarm robots in dynamic environments. Similar to fish escape maneuvers, simulations and real-robot experiments show that the swarm robots can collectively leave away from the threat and respond to sudden environmental changes. Several comparison studies demonstrated that the proposed approach can significantly improve the effectiveness, efficiency, and flexibility of swarm robots in complex environments."
RUMP: Robust Underwater Motion Planning in Dynamic Environments of Fast Moving Obstacles,"Herman Biørn Amundsen, Torben Falleth Olsen, Marios Xanthidis, Martin Føre, Eleni Kelasidi","NTNU,SINTEF Ocean AS,SINTEF Ocean",Marine Robotics II,"Robust underwater motion planning of autonomous underwater vehicles (AUVs) in dynamic cluttered environments is a problem that has yet to be addressed in depth. Due to advances in technology and computational capacity, AUVs are expected to operate safely and autonomously in increasingly challenging environments, necessitating methods that are able to safely navigate robots in real-time. Though, most solutions remain overly cautious and conservative. This paper proposes RUMP, a novel locally-optimal motion planning framework for robust real-time autonomous underwater navigation in 3D cluttered environments consisting of observed static and dynamic obstacles. The problem is modeled using path-optimization and can be solved in real-time with a common non-linear solver. The constructed objective function allows deciding the local goal during optimization to both maximize safety within a planning horizon and minimize the expected distance to the target position. Furthermore, path safety is considered for the entire transition between consecutive states, utilizing a novel approach for continuous spatiotemporal collision checks. The proposed formulation provides safe performance even in environments with obstacles that may move orders of magnitude faster than the AUV itself. Simulation experiments, in different challenging scenarios, showcase robustness and efficient real-time performance of more than 16 Hz."
Metrically Scaled Monocular Depth Estimation through Sparse Priors for Underwater Robots,"Luca Ebner, Gideon Billings, Stefan Bernard Williams","ETH Zurich,University of Sydney, Australian Center for Field Robotics,University of Sydney",Marine Robotics II,"In this work, we address the problem of real-time dense depth estimation from monocular images for mobile underwater vehicles. We formulate a deep learning model that fuses sparse depth measurements from triangulated features to improve the depth predictions and solve the problem of scale ambiguity. To allow prior inputs of arbitrary sparsity, we apply a dense parameterization method. Our model extends recent state-of-the-art approaches to monocular image based depth estimation, using an efficient encoder-decoder backbone and modern lightweight transformer optimization stage to encode global context. The network is trained in a supervised fashion on the forward-looking underwater dataset, FLSea. Evaluation results on this dataset demonstrate significant improvement in depth prediction accuracy by the fusion of the sparse feature priors. In addition, without any retraining, our method achieves similar depth prediction accuracy on a downward looking dataset we collected with a diver operated camera rig, conducting a survey of a coral reef. The method achieves real-time performance, running at 24 FPS on a NVIDIA Jetson Xavier NX, 160 FPS on a NVIDIA RTX 2080 GPU and 7 FPS on a single Intel i9-9900K CPU core, making it suitable for direct deployment on embedded GPU systems. The implementation of this work is made publicly available at https://github.com/ebnerluca/uw_depth."
Model-Based Underwater 6D Pose Estimation from RGB,"Davide Sapienza, Elena Govi, Sara Aldhaheri, Bertgona Marko, Eloy Roura, Èric Pairet Artau, Micaela Verucchi, Paola Ardón","Unimore,TII,Technology Innovation Institute,University of Modena and Reggio Emilia",Marine Robotics II,"Object pose estimation underwater allows an autonomous system to perform tracking and intervention tasks. Nonetheless, underwater target pose estimation is remarkably challenging due to, among many factors, limited visibility, light scattering, cluttered environments, and constantly varying water conditions. An approach is to employ sonar or laser sensing to acquire 3D data, however, the data is not clear and the sensors expensive. For this reason, the community has focused on extracting pose estimates from RGB input. In this work, we propose an approach that leverages 2D object detection to reliably compute 6D pose estimates in different underwater scenarios. We test our proposal with 4 objects with symmetrical shapes and poor texture spanning across 33, 920 synthetic and 10 real scenes. All objects and scenes are made available in an open-source dataset that includes annotations for object detection and pose estimation. When benchmarking against similar end-to-end methodologies for 6D object pose estimation, our pipeline provides estimates that are 8% more accurate. We also demonstrate the real-world usability of our pose estimation pipeline on an underwater robotic manipulator in a reaching task."
SONIC: Sonar Image Correspondence Using Pose Supervised Learning for Imaging Sonars,"Samiran Gode, Akshay Hinduja, Michael Kaess",Carnegie Mellon University,Marine Robotics II,"In this paper, we address the challenging problem of data association for underwater SLAM through a novel method for sonar image correspondence using learned features. We introduce SONIC (SONar Image Correspondence), a pose-supervised network designed to yield robust feature correspondence capable of withstanding viewpoint variations. The inherent complexity of the underwater environment stems from the dynamic and frequently limited visibility conditions,restricting vision to a few meters of often featureless expanses.This makes camera-based systems suboptimal in most open water application scenarios. Consequently, multibeam imaging sonars emerge as the preferred choice for perception sensors. However, they too are not without their limitations. While imaging sonars offer superior long-range visibility compared to cameras, their measurements can appear different from varying viewpoints. This inherent variability presents formidable challenges in data association, particularly for feature-based methods. Our method demonstrates significantly better performance in generating correspondences for sonar images which will pave the way for more accurate loop closure constraints and sonar-based place recognition. Code as well as simulated and real-world datasets are made public on https://github.com/rpl-cmu/sonic to facilitate further development in the field."
CVAE-SM: A Conditional Variational Autoencoder with Style Modulation for Efficient Uncertainty Quantification,"Amin Ullah, Taiqing Yan, Fuxin Li",Oregon State University,Marine Robotics II,"Deep learning has brought transformative advancements to object segmentation, especially in marine robotics contexts such as waste management and subaquatic infrastructure oversight. However, a central challenge persists: calibrating the prediction confidence of the model to ensure robust and reliable outcomes, especially within the demanding underwater environment. Existing solutions for estimating uncertainty are often computationally intensive and have largely centered around Bayesian neural networks or ensemble methods. In this paper, we present a Conditional Variational Autoencoder-based framework (CVAE-SM), which is capable of generating diverse latent codes for improved uncertainty quantification in image segmentation. Our method, enhanced by a style modulator, merges content features, and latent codes more effectively, leading to refined prediction of uncertainty levels. We further introduce a dataset of perturbed underwater images to benchmark uncertainty quantification in this domain. The proposed model not only surpasses peers in segmentation metrics but also matches ensemble models in uncertainty predictions, all while being 2.5 times faster."
Beyond NeRF Underwater: Learning Neural Reflectance Fields for True Color Correction of Marine Imagery,"Tianyi Zhang, Matthew Johnson-Roberson",Carnegie Mellon University,Marine Robotics II,"Underwater imagery often exhibits distorted coloration as a result of light-water interactions, which complicates the study of benthic environments in marine biology and geography. In this research, we propose an algorithm to restore the true color (albedo) in underwater imagery by jointly learning the effects of the medium and neural scene representations. Our approach models water effects as a combination of light attenuation with distance and backscattered light. The proposed neural scene representation is based on a neural reflectance field model, which learns albedos, normals, and volume densities of the underwater environment. We introduce a logistic regression model to separate water from the scene and apply distinct light physics during training. Our method avoids the need to estimate complex backscatter effects in water by employing several approximations, enhancing sampling efficiency and numerical stability during training. The proposed technique integrates underwater light effects into a volume rendering framework with end-to-end differentiability. Experimental results on both synthetic and real-world data demonstrate that our method effectively restores true color from underwater imagery, outperforming existing approaches in terms of color consistency."
CaveSeg: Deep Semantic Segmentation and Scene Parsing for Autonomous Underwater Cave Exploration,"Adnan Abdullah, Titon Barua, Reagan Tibbetts, Zijie Chen, Md Jahidul Islam, Ioannis Rekleitis","University of Florida,University of South Carolina,Mississippi State University",Marine Robotics II,"In this paper, we present CaveSeg - the first visual learning pipeline for semantic segmentation and scene parsing for AUV navigation inside underwater caves. We address the problem of scarce annotated training data by preparing a comprehensive dataset for semantic segmentation of underwater cave scenes. It contains pixel annotations for important navigation markers (e.g., caveline, arrows), obstacles (e.g., ground plain and overhead layers), scuba divers, and open areas for servoing. Through comprehensive benchmark analyses on cave systems in USA, Mexico, and Spain locations, we demonstrate that robust deep visual models can be developed based on CaveSeg for fast semantic scene parsing of underwater cave environments. In particular, we formulate a novel transformer-based model that is computationally light and offers near real-time execution in addition to achieving state-of-the-art performance. Finally, we explore the design choices and implications of semantic segmentation for visual servoing by AUVs inside underwater caves. The proposed model and benchmark dataset open up promising opportunities for future research in autonomous underwater cave exploration and mapping."
Discovering Biological Hotspots with a Passively Listening AUV,"Seth McCammon, Stewart Jamieson, T. Aran Mooney, Yogesh Girdhar","Woods Hole Oceanographic Institution,Massachusetts Institute of Technology,Woods Hole Oceanographic Instituttion",Marine Robotics II,"We present a novel system which blends multiple distinct sensing modalities in audio-visual surveys to assist marine biologists in collecting datasets for understanding the ecological relationship of fish and other organisms with their habitats on and around coral reefs. Our system, designed for the CUREE AUV, uses four hydrophones to determine the bearing to biological sound sources through beamforming. These observations are merged in a Bayesian Occupancy Grid to produce a 2D map of the acoustic activity of a coral reef. Simultaneously, the AUV uses unsupervised topic modeling to identify different benthic habitats. Combining these maps allows us to determine the level of acoustic activity within each habitat. We demonstrated the system in field trials on reefs in the U.S. Virgin Islands, where it was able to autonomously discover the favored habitats of snapping shrimp (genus Alpheus)."
A Du-Octree Based Cross-Attention Model for LiDAR Geometry Compression,"Mingyue Cui, Mingjian Feng, Junhua Long, Daosong Hu, Shuai Zhao, Kai Huang","Sun Yat-sen University,SUN YAT-SEN UNIVERSITY,Sun Yat-Sen University",Marine Robotics II,"Point cloud compression is an essential technology for the efficient storage and transmission of 3D data. Previous methods usually use hierarchical tree data structures for encoding the spatial sparseness of point clouds. However, the node context within the tree is not fully discovered since the feature space among nodes varies significantly. To address this problem, we innovatively represent the LiDAR points in a two-octree structure instead of using traditional single-octree coding, and then design the cross-attention model to capture the hierarchical features between different octrees, of which each octree incorporates a transformer-based deep entropy and an arithmetic encoder. Besides, we introduce the untied cross-aware position encoding with principal component analysis and different projection matrices, which enhances the correlations over two octrees's attention feature embeddings. Experimental results show that our method outperforms the previous state-of-the-art works, achieving up to 8.2% Bpp savings on point cloud benchmark datasets with different lasers."
Energy Consumption Modelling of Coaxial-Rotor in Vortex Ring State for Controllable High-Speed Descending,"Jiawei Sun, Xiang Zhou, Taoze Ban, Jiannan Zhao, Feng Shuang","Guangxi University,Co., Ltd. Mystical Bow Technology",Mechanics and Control II,"The ability to fast climb and descend is crucial for Unmanned Aerial Vehicle (UAV) applications in the mountains. The slower descent speed will affect the UAV's working efficiency in reaching the rescue area. However, during the fast descent of the rotorcraft, a chaotic flow field rampages as the rotorcraft falls into its wake flow. This is known as the vortex ring. Therefore, the safe descent velocity of consumer UAVs is usually limited to approximately 3m/s. This limitation reduces the potential of UAVs to execute tasks in mountainous and plateau regions. To broaden the task capability constrained by the maximum descending speed, it is necessary to jointly analyze the flow field and the energy consumption during descending. Existing research mainly focused on how to avoid entering the vortex ring instead of offering sufficient power to fly with it. In this paper, in order to achieve an efficient rotorcraft for rescuing in mountainous and plateaus, we break through the maximum-descending-speed of a coaxial rotors UAV. Hence, a power consumption managing pipeline is proposed to extend the power tolerance of the UAV. Specifically, a theoretic model for the coaxial rotors is proposed to analyze the induced velocity and energy consumption during vertical descending. Then, the theoretic model is verified to be consistent with the Computational Fluid Dynamics (CFD) and wind tunnel experiment results. Finally, we optimized the tolerance of the power and dynamic system according to the theoretic model. With this pipeline, our real-time flight achieved 8m/s controlled vertical-descent-speed (CVDS), which is a leading result in both quadrotors and coaxial UAVs."
"Equilibria, Stability, and Sensitivity for the Aerial Suspended Beam Robotic System Subject to Parameter Uncertainty","Chiara Gabellieri, Marco Tognon, Dario Sanalitro, Antonio Franchi","University of Twente,Inria Rennes,University of Catania,University of Twente / Sapienza University of Rome",Mechanics and Control II,"This work studies how parametric uncertainties affect the cooperative manipulation of a cable-suspended beam-shaped load by means of two aerial robots not explicitly communicating with each other. In particular, the work sheds light on the impact of the uncertain knowledge of the model parameters available to an established communication-less force-based controller. First, we find the closed-loop equilibrium configurations in the presence of the aforementioned uncertainties, and then we study their stability. Hence, we show the fundamental role played in the robustness of the load attitude control by the internal force induced in the manipulated object by non-vertical cables. Furthermore, we formally study the sensitivity of the attitude error to such parametric variations, and we provide a method to act on the load position error in the presence of the uncertainties. Eventually, we validate the results through an extensive set of numerical tests in a realistic simulation environment including underactuated aerial vehicles and sagging-prone cables, and through hardware experiments."
Aerial Tensile Perching and Disentangling Mechanism for Long-Term Environmental Monitoring,"Tian Lan, Luca Romanello, Mirko Kovac, Sophie Franziska Armanini, Başaran Bahadır Koçer","Technical University of Munich,TUM,Imperial College London",Mechanics and Control II,"Aerial robots show significant potential for forest canopy research and environmental monitoring by providing data collection capabilities at high spatial and temporal resolutions. However, limited flight endurance hinders their application. Inspired by natural perching behaviors, we propose a multi-modal aerial robot system that integrates tensile perching for energy conservation and a suspended actuated pod for data collection. The system consists of a quadrotor drone, a slewing ring mechanism allowing 360Â° tether rotation, and a streamlined pod with two ducted propellers connected via a tether. Winding and unwinding the tether allows the pod to move within the canopy, and activating the propellers allows the tether to be wrapped around branches for perching or disentangling. We experimentally determined the minimum counterweights required for stable perching under various conditions. Building on this, we devised and evaluated multiple perching and disentangling strategies. Comparisons of perching and disentangling maneuvers demonstrate energy savings that could be further maximized with the use of the pod or of tether winding. These approaches can reduce energy consumption to only 22% and 1.5%, respectively, compared to a drone disentangling maneuver. We also calculated the minimum idle time required by the proposed system after the system perching and motor shut down to save energy on a mission, which is 48.9% of the operating time. Overall, the integrated system expands the operational capabilities and enhances the energy efficiency of aerial robots for long-term monitoring tasks."
Millimeter-Level Pick and Peg-In-Hole Task Achieved by Aerial Manipulator,"Meng Wang, Zeshuai Chen, Kexin Guo, Xiang Yu, Youmin Zhang, Lei Guo, Wei Wang","Beihang University,Concordia University,China Aerospace Science and Technology Corporation, Beijing Inst",Mechanics and Control II,"Achieving accurate control performance of the end-effector is critical for practical applications of aerial manipulator. However, due to the presence of floating-base disturbance from the UAV platform and the kinematic error amplification effect from multi-link structure of the manipulator, it is extremely challenging to ensure the high-precision performance of aerial manipulator. Building upon the philosophy of disturbance rejection, we propose a predictive optimization scheme that allows aerial manipulator to successfully execute millimeter-level flying pick and peg-in-hole task. Firstly, the error amplification effect of the floating base is quantitatively analyzed by virtue of the aerial manipulator kinematics. Intuitively, it is found that if the further motion of the UAV platform is well predicted, the manipulator can directly counteract the floating disturbance by following a modified reference trajectory. Hence, a learning-based prediction approach is leveraged to rapidly forecast the UAV platform motion online. Subsequently, an optimization controller is formulated to follow the reference trajectory by incorporating multiple practical constraints of aerial manipulator."
Lumped Drag Model Identification and Real-Time External Force Detection for Rotary-Wing Micro Aerial Vehicles,"Lucas Waelti, Alcherio Martinoli",EPFL,Mechanics and Control II,"This work focuses on understanding and identifying the drag forces applied to a rotary-wing Micro Aerial Vehicle (MAV). We propose a lumped drag model that concisely describes the aerodynamical forces the MAV is subject to, with a minimal set of parameters. We only rely on commonly available sensor information onboard a MAV, such as accelerometer data, pose estimate, and throttle commands, which makes our method generally applicable. The identification uses an offline gradient-based method on flight data collected over specially designed trajectories. The identified model allows us to predict the aerodynamical forces experienced by the aircraft due to its own motion in real-time and, therefore, will be useful to distinguish them from external perturbations, such as wind or physical contact with the environment. The results show that we are able to identify the drag coefficients of a rotary-wing MAV through onboard flight data and observe the close correlation between the motion of the MAV, the measured external forces, and the predicted drag forces."
Flight Validation of a Global Singularity-Free Aerodynamic Model for Flight Control of Tail Sitters,"Krishna Murali, Elena Ponce Moreno, Leandro Lustosa","ISAE-SUPAERO,ISAE-Supaero",Mechanics and Control II,"This work validates through flight tests a previously developed wide-envelope singularity-free aerodynamic framework, called phi-theory, for modeling dual-engine tail-sitting flying-wing vehicles for optimization-based control. The phi-theory methodology imposes a specific geometry on aerodynamic coefficients that leads to polynomial differential equations of motion amenable to semidefinite programming optimization. Through phi-theory, we illustrate a typical predicted longitudinal and lateral flight envelope of a tail-sitting vehicle, which, while commonplace for fixed-wing aircraft in performance textbooks, is a novel figure that generalizes fixed-wing doghouse plots to tail-sitting vehicles. This flight envelope figure suggests a novel, natural and intuitive remote piloting interface that we validate in flight tests. Furthermore, we further validate phi-theory through the computation of flight features in simulation and their subsequent observation in flight tests."
Empirical Study of Ground Proximity Effects for Small-Scale Electroaerodynamic Thrusters,"Grant Nations, Charles Luke Nelson, Daniel S. Drew",University of Utah,Mechanics and Control II,"Electroaerodynamic (EAD) propulsion, where thrust is produced by collisions between electrostatically-accelerated ions and neutral air, is a potentially transformative method for indoor flight owing to its silent and solid-state nature. Like rotors, EAD thrusters exhibit changes in performance based on proximity to surfaces. Unlike rotors, they have no fragile and quickly spinning parts that have to avoid those surfaces; taking advantage of the efficiency benefits from proximity effects may be a route towards longer-duration indoor operation of ion-propelled fliers. This work presents the first empirical study of ground proximity effects for EAD propulsors, both individually and as quad-thruster arrays. It focuses on multi-stage ducted centimeter-scale actuators suitable for use on small robots envisioned for deployment in human-proximal and indoor environments. Three specific effects (ground, suckdown, and fountain lift), each occurring with a different magnitude at a different spacing from the ground plane, are investigated and shown to have strong dependencies on geometric parameters including thruster-to-thruster spacing, thruster protrusion from the fuselage, and inclusion of flanges or strakes. Peak thrust enhancement ranging from 300 to 600% is found for certain configurations operated in close proximity (0.2 mm) to the ground plane and as much as a 20% increase is measured even when operated centimeters away."
The Weighted Markov-Dubins Problem,"Deepak Prakash Kumar, Swaroop Darbha, Satyanarayana Gupta Manyam, David Casbeer","Texas A&M University,TAMU,Infoscitex corp.,AFRL",Mechanics and Control II,"In this letter, a variation of the classical Markov- Dubins problem is considered, which deals with curvature constrained least-cost paths in a plane with prescribed initial and final configurations, different bounds for the sinistral and dextral curvatures, and penalties Î¼L and Î¼R for the sinistral and dextral turns, respectively. The addressed problem generalizes the classical Markov-Dubins problem and the asymmetric sinistral/dextral Markov-Dubins problem. The proposed formulation can be used to model an Unmanned Aerial Vehicle (UAV) with a penalty associated with a turn due to the required additional thrust to maintain altitude and airspeed while turning, or a UAV with different curvature bounds and costs for the sinistral and dextral turns due to hardware failures. Using optimal control theory, the main result of this letter shows that the optimal path belongs to a set of at most 21 candidate paths, each comprising of at most five segments. Unlike in the classical Markov-Dubins problem, the CCC path, which is a candidate path for the classical Markov-Dubins problem, is not optimal for the weighted Markov-Dubins problem. Moreover, the obtained list of candidate paths for the weighted Markov-Dubins problem reduces to the standard CSC and CCC paths and the corresponding degenerate paths when Î¼L and Î¼R approach zero."
The Price of a Safe Flight: Risk Cost Based Path Planning,"Aliaksei Pilko, Andy Oakey, Mario Ferraro, James Scanlan",University of Southampton,Mechanics and Control II,A risk aware UAS path planning methodology is proposed using monetary value as the sole cost metric. A third party ground risk model is used to generate a non-uniform costmap for a modified A* heuristic search. The Value of a Prevented Fatality provides a basis to convert fatality risk to monetary value terms as a Human Value at Risk (HVaR) measure. Additional operating and UAS Capital Value at Risk (CVaR) costs are modelled to provide a holistic monetary cost model for path cost minimisation. A number of future cost variants are investigated based upon prior work for a realistic urban-rural mix logistics case study in Southern England. Results show increasingly risk averse paths with decreasing future UAS operating costs.
Tight Fusion of Odometry and Kinematic Constraints for Multiple Aerial Vehicles in Physical Interconnection,"Yingjun Fan, Chuanbeibei Shi, Ganghua Lai, Ruiheng Zhang, Yushu Yu, Fuchun Sun, Yiqun Dong","Beijing Institute of Technology,Univeristy of Bristol,Tsinghua University,Nanyang Technological University",Multi-Robot SLAM,"Integrated aerial Platforms (IAPs), comprising multiple aircrafts, are typically fully actuated and hold significant potential for aerial manipulation tasks. Differing from a multiple aerial swarm, the aircrafts within the IAP are interconnected, presenting promising opportunities for enhancing localization. Incorporating the physical constraints of these multiple aircrafts to improve the accuracy and reliability of integrated aircraft positioning and navigation systems is a challenging yet highly significant problem. In this paper, we introduce a distributed multi-aircraft visual-inertial-range odometry system that analyzes the position, velocity, and attitude constraints within the IAP. Leveraging constraint relationships in the IAP, we propose corresponding methods that tightly fuse visual-inertial-range odometry and kinematic constraints to optimize odometry accuracy. Our system's performance is validated using a collected dataset, resulting in a notable 28.7$%$ reduction in drift compared to the baseline."
Robust Multi-Robot Global Localization with Unknown Initial Pose Based on Neighbor Constraints,"Yaojie Zhang, Haowen Luo, Weijun Wang, Wei Feng","Shenzhen Institute of Advanced Technologyï¼ŒChinese Academy,Guangzhou Institute of Advanced Technology,Chinese Academy of Sc,Shenzhen Institutes of Advanced Technology, Chinese Academy of S",Multi-Robot SLAM,"Multi-robot global localization (MR-GL) with unknown initial positions in a large scale environment is a challenging task. The key point is the data association between different robots' viewpoints. It also makes traditional Appearance-based localization methods unusable. Recently, researchers have utilized the object's semantic invariance to generate a semantic graph to address this issue. However, previous works lack robustness and are sensitive to the overlap rate of maps, resulting in unpredictable performance in real-world environments. In this paper, we propose a data association algorithm based on neighbor constraints to improve the robustness of the system. We demonstrate the effectiveness of our method on three different datasets, indicating a significant improvement in robustness compared to previous works."
Swarm-SLAM: Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems,"Pierre-yves Lajoie, Giovanni Beltrame","École Polytechnique de Montréal,Ecole Polytechnique de Montreal",Multi-Robot SLAM,"Collaborative Simultaneous Localization And Mapping (C-SLAM) is a vital component for successful multi-robot operations in environments without an external positioning system, such as indoors, underground or underwater. In this paper, we introduce Swarm-SLAM, an open-source C-SLAM system that is designed to be scalable, flexible, decentralized, and sparse, which are all key properties in swarm robotics. Our system supports lidar, stereo, and RGB-D sensing, and it includes a novel inter-robot loop closure prioritization technique that reduces communication and accelerates convergence. We evaluated our ROS 2 implementation on five different datasets, and in a real-world experiment with three robots communicating through an ad-hoc network. Our code is publicly available: https://github.com/MISTLab/Swarm-SLAM"
AutoFusion: Autonomous Visual Geolocation and Online Dense Reconstruction for UAV Cluster,"Yizhu Zhang, Shuhui Bu, Yifei Dong, Zhang Yu, Kun Li, Lin Chen","Northwestern Polytechnical University,NorthWestern Polytechnical University",Multi-Robot SLAM,"Real-time dense reconstruction using Unmanned Aerial Vehicle (UAV) is becoming increasingly popular in large-scale rescue and environmental monitoring tasks. However, due to the energy constraints of a single UAV, the efficiency can be greatly improved through the collaboration of multi-UAVs. Nevertheless, when faced with unknown environments or the loss of Global Navigation Satellite System (GNSS) signal, most multi-UAV SLAM systems can't work, making it hard to construct a global consistent map. In this paper, we propose a real-time dense reconstruction system called AutoFusion for multiple UAVs, which robustly supports scenarios with lost global positioning and weak co-visibility. A method for Visual Geolocation and Matching Network (VGMN) is suggested by constructing a graph convolutional neural network as a feature extractor. It can acquire geographical location information solely through images. We also present a real-time dense reconstruction framework for multi-UAV with autonomous visual geolocation. UAV agents send images and relative positions to the ground server, which processes the data using VGMN for multi-agent geolocation optimization, including initialization, pose graph optimization, and map fusion. Extensive experiments demonstrate that our system can efficiently and stably construct large-scale dense maps in real-time with high accuracy and robustness."
CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic Swarms,"Shipeng Zhong, Hongbo Chen, Yuhua Qi, Dapeng Feng, Zhiqiang` Chen, Wu Jin, Weisong Wen, Ming Liu","SUN YAT-SEN UNIVERSITY,Sun Yat-Sen University,Sun Yat-sen University,UESTC,Hong Kong Polytechnic University,Hong Kong University of Science and Technology (Guangzhou)",Multi-Robot SLAM,"Collaborative state estimation using heterogeneous multi-sensors is a fundamental prerequisite for robotic swarms operating in GPS-denied environments, presenting a formidable research challenge. In this work, we propose a centralized system designed to facilitate collaborative LiDAR-ranging-inertial state estimation in expansive environments, enabling robotic swarms to operate without the need for anchor deployment. The system optimally distributes computationally intensive tasks to a potent central server, thereby alleviating the computational burden on individual robots for local odometry calculations. The server back-end establishes a global reference by harnessing the shared data, reï¬ning the joint pose graph optimization through place recognition, global optimization, and the removal of redundant data to ensure an precise and robust collaborative state estimation. Extensive evaluations of our system using both public and our custom datasets showcase notable improvements in the accuracy of collaborative SLAM estimates. Furthermore, our system demonstrates its competence in large-scale missions, where ten robots collaborate seamlessly in performing SLAM tasks. To benefit the community, we will open-source our code at https://github.com/PengYu-team/Co-LRIO."
Relative Localization Estimation for Multiple Robots Via the Rotating Ultra-Wideband Tag,"Jinxin Liu, Guoqiang Hu","Nanyang Technological University,Nanyang Technological University,",Multi-Robot SLAM,"Most distributed algorithms for robot coordination require relative location information, but how to obtain relative locations in a distributed manner is still a primary problem to address in multi-robot applications. In order to obtain the relative locations between robots, no matter whether they are in relative motion or stationary situation, we design a rotating ultra-wideband tag to provide the persistency of excitation condition and two estimation algorithms to estimate the relative locations in a distributed manner. Moreover, our approach relies only on on-board sensors and requires only one ultra-Wideband tag per robot, eliminating the need for any ground anchors, thus allowing deployment in GNSS-denied environments without range restrictions. The proposed approach in this letter is also tested in simulations and experiments to verify the theoretical findings and effectiveness in practice."
Asynchronous Multiple LiDAR-Inertial Odometry Using Point-Wise Inter-LiDAR Uncertainty Propagation,"Minwoo Jung, Sangwoo Jung, Ayoung Kim",Seoul National University,Multi-Robot SLAM,"In recent years, multiple Light Detection and Ranging (LiDAR) systems have grown in popularity due to their enhanced accuracy and stability from the increased field of view (FOV). However, integrating multiple LiDARs can be challenging, attributable to temporal and spatial discrepancies. Common practice is to transform points among sensors while requiring strict time synchronization or approximating transformation among sensor frames. Unlike existing methods, we elaborate the inter-sensor transformation using continuous time (CT) inertial measurement unit (IMU) modeling and derive associated ambiguity as a point-wise uncertainty. This uncertainty, modeled by combining the state covariance with the acquisition time and point range, allows us to alleviate the strict time synchronization and to overcome FOV difference. The proposed method has been validated on both public and our datasets and is compatible with various LiDAR manufacturers and scanning patterns. We open-source the code for public access at https://github.com/minwoo0611/MA-LIO."
AutoMerge: A Framework for Map Assembling and Smoothing in City-Scale Environments,"Peng Yin, Shiqi Zhao, Haowen Lai, Ruohai Ge, Ji Zhang, Howie Choset, Sebastian Scherer","Carnegie Mellon University,University of California San Diego,University of Pennsylvania,Carnegie Mellon Univeristy",Multi-Robot SLAM,"In the era of advancing autonomous driving and increasing reliance on geospatial information, high-precision mapping not only demands accuracy but also flexible construction. Current approaches mainly rely on expensive mapping devices, which are time-consuming for city-scale map construction and vulnerable to erroneous data associations without accurate GPS assistance. We present AutoMerge, a novel framework for merging large-scale maps that surpasses these limitations, which (i) provides robust place recognition performance despite differences in both translation and viewpoint, (ii) is capable of identifying and discarding incorrect loop closures caused by perceptual aliasing, and (iii) effectively associates and optimizes large-scale and numerous map segments in the real-world scenario. AutoMerge utilizes multi-perspective fusion and adaptive loop closure detection for accurate data associations, and it uses incremental merging to assemble large maps from individual trajectory segments given in random order and with no initial estimations. Furthermore, AutoMerge performs posegraph optimization after assembling the segments to smooth the merged map globally. We demonstrate AutoMe"
TP3M: Transformer-Based Pseudo 3D Image Matching with Reference Image,"Liming Han, Zhaoxiang Liu, Shiguo Lian",China Unicom,Localization II,"Image matching is still challenging in such scenes with large viewpoints or illumination changes or with low textures. In this paper, we propose a Transformer-based pseudo 3D image matching method. It upgrades the 2D features extracted from the source image to 3D features with the help of a reference image and matches to the 2D features extracted from the destination image by the coarse-to-fine 3D matching. Our key discovery is that by introducing the reference image, the source image's fine points are screened and furtherly their feature descriptors are enriched from 2D to 3D, which improves the match performance with the destination image. Experimental results on multiple datasets show that the proposed method achieves the state-of-the-art on the tasks of homography estimation, pose estimation and visual localization especially in challenging scenes."
Adaptive Outlier Thresholding for Bundle Adjustment in Visual SLAM,"Alejandro Fontán Villacampa, Javier Civera, Michael Milford","Queensland University of Technology,Universidad de Zaragoza",Localization II,"State-of-the-art V-SLAM pipelines utilize robust cost functions and outlier rejection techniques to remove incorrect correspondences. However, these methods are typically fine-tuned to overfit certain benchmarks and struggle to adapt effectively to changes in the application domain or environmental conditions. This renders them impractical for many robotic applications in which robustness in a wide variety of conditions is essential. In this paper we introduce a novel distribution-based approach for online outlier rejection that reduces the necessity for scene-specific fine-tuning while simultaneously improving the overall SLAM performance. Through experiments across 3 different public datasets, we show that our approach consistently outperforms state-of-the-art methods in various real-world settings. Our code is available at https://github.com/alejandrofontan/ORB_SLAM2_Distribution"
From Satellite to Ground: Satellite Assisted Visual Localization with Cross-View Semantic Matching,"Xiyue Guo, Haocheng Peng, Junjie Hu, Hujun Bao, Guofeng Zhang","Zhejiang University,The Chinese University of Hong Kong, Shenzhen",Localization II,"One of the key challenges of visual Simultaneous Localization and Mapping (SLAM) in large-scale environments is how to effectively use global localization to correct the cumulative errors from long-term tracking. This challenge presents itself in two main aspects: first, the difficulty for robots in revisiting previous locations to perform loop closure, and second, the considerable memory resources required to maintain point-cloud-based global maps. Recent solutions have resorted into neural networks, using satellite images as the references for ground-level localization. However, most of these methods merely provide cross-view patch-matching results, which leads to unfeasible in integration with the SLAM system. To address these issues, we present a semantic-based cross-view localization method. This approach combines semantic information with a reward and penalty mechanism, enabling us to obtain a global probability map and achieve precise 3-degree-of-freedom (3-DoF) localization. Based on that, we develop a SLAM system that capitalizes on satellite imagery for global localization. This strategy effectively bridges the gap between SLAM and real-world coordinates while also substantially reducing accumulated errors. Our experimental results demonstrate that our global localization method significantly outperforms existing satellite-based systems. Moreover, in scenarios where the robot struggles to find loop closures, employing our localization method improves the SLAM accuracy."
Self-Supervised Learning of Monocular Visual Odometry and Depth with Uncertainty-Aware Scale Consistency,"Changhao Wang, Guanwen Zhang, Wei Zhou","Northwestern Polytechnical University,Northwestern Polytechnical University,",Localization II,"The inherent scale ambiguity issue greatly limits the performance of monocular visual odometry. In recent years, a variety of methods have been proposed for self-supervised learning of ego-motion and depth estimation, incorporating specifically designed scale-consistency constraints that utilize estimated depth as a reference. However, these existing methods neglect the influence of the depth uncertainty introduced by the dominant photometric loss, which leads to unreliable depth estimation in difficult regions and detrimentally affects scale alignment. To solve these problems, we introduces a feature-based visual odometry learning system with an effective scale recovery strategy in this paper. Additionally, we propose a learning method to estimate the photometric-sensitive depth uncertainty for guiding the scale recovery. The proposed method is evaluated on KITTI odometry, and the experimental results demonstrate that our system can predict scale-consistent trajectories from monocular videos and achieves state-of-the-art performance. Moreover, the proposed method achieves competitive performance on KITTI depth estimation."
Unifying Local and Global Multimodal Features for Place Recognition in Aliased and Low-Texture Environments,"Alberto García-hernández, Riccardo Giubilato, Klaus H. Strobl, Javier Civera, Rudolph Triebel","Universidad de Zaragoza,German Aerospace Center (DLR)",Localization II,"Perceptual aliasing and weak textures pose significant challenges to the task of place recognition, hindering the performance of Simultaneous Localization and Mapping (SLAM) systems. This paper presents a novel model, called UMF (standing for Unifying Local and Global Multimodal Features) that 1) leverages multi-modality by cross-attention blocks between vision and LiDAR features, and 2) includes a re-ranking stage that re-orders based on local feature matching the top-k candidates retrieved using a global representation. Our experiments, particularly on sequences captured on a planetary-analogous environment, show that UMF outperforms significantly previous baselines in those challenging aliased environments. Since our work aims to enhance the reliability of SLAM in all situations, we also explore its performance on the widely used RobotCar dataset, for broader applicability. Code and models are available at https://github.com/DLR-RM/UMF."
RELEAD: Resilient Localization with Enhanced LiDAR Odometry in Adverse Environments,"Zhiqiang` Chen, Hongbo Chen, Yuhua Qi, Shipeng Zhong, Dapeng Feng, Wu Jin, Weisong Wen, Ming Liu","SUN YAT-SEN UNIVERSITY,Sun Yat-Sen University,Sun Yat-sen University,UESTC,Hong Kong Polytechnic University,Hong Kong University of Science and Technology (Guangzhou)",Localization II,"LiDAR-based localization is valuable for applications like mining surveys and underground facility maintenance. However, existing methods can struggle when dealing with uninformative geometric structures in challenging scenarios. This paper presents RELEAD, a LiDAR-centric solution designed to address scan-matching degradation. Our method enables degeneracy-free point cloud registration by solving constrained ESIKF updates in the front end and incorporates multisensor constraints, even when dealing with outlier measurements, through graph optimization based on Graduated Non-Convexity (GNC). Additionally, we propose a robust Incremental Fixed Lag Smoother (rIFL) for efficient GNC-based optimization. RELEAD has undergone extensive evaluation in degenerate scenarios and has outperformed existing state-of-the-art LiDAR-Inertial odometry and LiDAR-Visual-Inertial odometry methods."
Semantic-Focused Patch Tokenizer with Multi-Branch Mixer for Visual Place Recognition,"Zhenyu Xu, Ren Ziliang, Qieshi Zhang, Lou Jie, Dacheng Tao, Jun Cheng","CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems,Dongguan University of Technology,Shenzhen Institutes of Advanced Technology, Chinese Academy of S,China Nuclear Power Operations Co., Ltd.,The University of Sydney,Shenzhen Institutes of Advanced Technology",Localization II,"Visual Place Recognition (VPR) is critical for navigation and loop closure in autonomous driving tasks, mitigating the impact of shift errors caused by dynamic changes in the environment. Due to the limited ability of backbone networks and extreme environmental changes, current methods fail to capture foundational semantic details that include the distinctive attributes for unique place identification. To address this problem, we propose a new visual token-guided VPR framework that contains a semantic-focused patch tokenizer and a multi-branch Mixer. To mitigate the inference from place-unrelated objects, the semantic-focused patch tokenizer exploits attention-based channel selection and spatial partition, which efficiently captures important semantic information within the channels and preserve spatial relationships among the backbone features. To extract abstract features with spatial structure information, the multi-branch Mixer utilizes a multi-branch structure to aggregate local and global position information, improving the robustness of global representations to environmental changes. Experimental results demonstrate that our method outperforms state-of-the-art methods, achieving 85.3% Recall@1 on the MSLS_val dataset and 59.1% Recall@1 on the Nordland dataset when using ResNet18 as the backbone."
FF-LINS: A Consistent Frame-To-Frame Solid-State-LiDAR-Inertial State Estimator,"Hailiang Tang, Tisheng Zhang, Xiaoji Niu, Liqiang Wang, Linfu Wei, Liu Jingnan",Wuhan University,Localization II,"Most of the existing LiDAR-inertial navigation systems are based on frame-to-map registrations, leading to inconsistency in state estimation. The newest solid-state LiDAR with a non-repetitive scanning pattern makes it possible to achieve a consistent LiDAR-inertial estimator by employing a frame-to-frame data association. In this letter, we propose a robust and consistent frame-to-frame LiDAR-inertial navigation system (FF-LINS) for solid-state LiDARs. With the INS-centric LiDAR frame processing, the keyframe point-cloud map is built using the accumulated point clouds to construct the frame-to-frame data association. The LiDAR frame-to-frame and the inertial measurement unit (IMU) preintegration measurements are tightly integrated using the factor graph optimization, with online calibration of the LiDAR-IMU extrinsic and time-delay parameters. The experiments on the public and private datasets demonstrate that the proposed FF-LINS achieves superior accuracy and robustness than the state-of-the-art systems. Besides, the LiDAR-IMU extrinsic and time-delay parameters are estimated effectively, and the online calibration notably improves the pose accuracy."
VioLA: Aligning Videos to 2D LiDAR Scans,"Jun-Jee Chao, Kazim Selim Engin, Nikhil Chavan-dafle, Bhoram Lee, Volkan Isler","University of Minnesota,Samsung Research America,SRI International",Localization II,"We study the problem of aligning a video that captures a local portion of an environment to the 2D LiDAR scan of the entire environment. We introduce a method (VioLA) that starts with building a semantic map of the local scene from the image sequence, then extracts points at a fixed height for registering to the LiDAR map. Due to reconstruction errors or partial coverage of the camera scan, the reconstructed semantic map may not contain sufficient information for registration. To address this problem, VioLA makes use of a pre-trained text-to-image inpainting model paired with a depth completion model for filling in the missing scene content in a geometrically consistent fashion to support pose registration. We evaluate VioLA on two real-world RGB-D benchmarks, as well as a self-captured dataset of a large office scene. Notably, our proposed point completion module improves the pose registration performance by up to 20%."
Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps,"Katie Luo, Xinshuo Weng, Yan Wang, Shuang Wu, Jie Li, Kilian Weinberger, Yue Wang, Marco Pavone","Cornell University,NVIDIA Corporation,NVIDIA,Nvidia,Toyota Research Institute,USC,Stanford University",Mapping I,"Autonomous driving has traditionally relied heavily on costly and labor-intensive High Definition (HD) maps, hindering scalability. In contrast, Standard Definition (SD) maps are more affordable and have worldwide coverage, offering a scalable alternative. In this work, we systematically explore the effect of SD maps for real-time lane-topology understanding. We propose a novel framework to integrate SD maps into online map prediction and propose a Transformer-based encoder, SMERF, to leverage priors in SD maps for the lane-topology prediction task. This enhancement consistently and significantly boosts (by up to 60%) lane detection and topology prediction on current state-of-the-art online map prediction methods without bells and whistles and can be immediately incorporated into any Transformer-based lane-topology method. Code is available at https://github.com/NVlabs/SMERF."
3QFP: Efficient Neural Implicit Surface Reconstruction Using Tri-Quadtrees and Fourier Feature Positional Encoding,"Shuo Sun, Malcolm Mielle, Achim J. Lilienthal, Martin Magnusson","Orebro University,Schindler,Örebro University",Mapping I,"Neural implicit surface representations are currently receiving a lot of interest as a means to achieve high-fidelity surface reconstruction at a low memory cost, compared to traditional explicit representations. However, state-of-the-art methods still struggle with excessive memory usage and non-smooth surfaces. This is particularly problematic in large-scale applications with sparse inputs, as is common in robotics use cases. To address these issues, we first introduce a sparse structure, tri-quadtrees, which represents the environment using learnable features stored in three planar quadtree projections. Secondly, we concatenate the learnable features with a Fourier feature positional encoding. The combined features are then decoded into signed distance values through a small multi-layer perceptron. We demonstrate that this approach facilitates smoother reconstruction with a higher completion ratio with fewer holes. Compared to two recent baselines, one implicit and one explicit, our approach requires only 10%â€“50% as much memory, while achieving competitive quality"
Towards Large-Scale Incremental Dense Mapping Using Robot-Centric Implicit Neural Representation,"Jianheng Liu, Haoyao Chen","Harbin Institute of Technology Shenzhen, P.R. China,Harbin Institute of Technology, Shenzhen",Mapping I,"Large-scale dense mapping is vital in robotics, digital twins, and virtual reality. Recently, implicit neural mapping has shown remarkable reconstruction quality. However, incremental large-scale mapping with implicit neural representations remains problematic due to low efficiency, limited video memory, and the catastrophic forgetting phenomenon. To counter these challenges, we introduce the Robot-centric Implicit Mapping (RIM) technique for large-scale incremental dense mapping. This method employs a hybrid representation, encoding shapes with implicit features via a multi-resolution voxel map and decoding signed distance fields through a shallow MLP. We advocate for a robot-centric local map to boost model training efficiency and curb the catastrophic forgetting issue. A decoupled scalable global map is further developed to archive learned features for reuse and maintain constant video memory consumption. Validation experiments demonstrate our method's exceptional quality, efficiency, and adaptability across diverse scales and scenes over advanced dense mapping methods using range sensors. Our system's code will be accessible at https://github.com/HITSZ-NRSL/RIM.git."
Camera Relocalization in Shadow-Free Neural Radiance Fields,"Shiyao Xu, Caiyun Liu, Yuantao Chen, Zhenxin Zhu, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou","Institute for AI Industry Research, Tsinghua University,Xi'an University of Architecture and Technology,Beihang University,Tsinghua University, Peking University,Tsinghua University",Mapping I,"Camera relocalization is a crucial problem in computer vision and robotics. Recent advancements in neural radiance fields (NeRFs) have shown promise in synthesizing photo-realistic images. Several works have utilized NeRFs for refining camera poses, but they do not account for lighting changes that can affect scene appearance and shadow regions, causing a degraded pose optimization process. In this paper, we propose a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization. We implement our scene representation upon a hash-encoded NeRF which significantly boosts up the pose optimization process. To account for the noisy image gradient computing problem in grid-based NeRFs, we further propose a re-devised truncated dynamic low-pass filter (TDLF) and a numerical gradient averaging technique to smoothen the process. Experimental results on several datasets with varying lighting conditions demonstrate that our method achieves state-of-the-art results in camera relocalization under varying lighting conditions. Code and data will be made publicly available."
QuadricsNet: Learning Concise Representation for Geometric Primitives in Point Clouds,"Ji Wu, Huai Yu, Wen Yang, Gui-Song Xia",Wuhan University,Mapping I,"This paper presents a novel framework to learn a concise geometric primitive representation for 3D point clouds. Different from representing each type of primitive individually, we focus on the challenging problem of how to achieve a concise and uniform representation robustly. We employ quadrics to represent diverse primitives with only 10 parameters and propose the first end-to-end learning-based framework, namely QuadricsNet, to parse quadrics in point clouds. The relationships between quadrics mathematical formulation and geometric attributes, including the type, scale and pose, are insightfully integrated for effective supervision of QuaidricsNet. Besides, a novel pattern-comprehensive dataset with quadrics segments and objects is collected for training and evaluation. Experiments demonstrate the effectiveness of our concise representation and the robustness of QuadricsNet. Our code is available at url{https://github.com/MichaelWu99-lab/QuadricsNet}."
ERASOR++: Height Coding Plus Egocentric Ratio Based Dynamic Object Removal for Static Point Cloud Mapping,"Jiabao Zhang, Yu Zhang",Zhejiang University,Mapping I,"Mapping plays a crucial role in location and navigation within automatic systems. However, the presence of dynamic objects in 3D point cloud maps generated from scan sensors can introduce map distortion and long traces, thereby posing challenges for accurate mapping and navigation. To address this issue, we propose ERASOR++, an enhanced approach based on the Egocentric Ratio of Pseudo Occupancy for effective dynamic object removal. To begin, we introduce the Height Coding Descriptor, which combines height difference and height layer information to encode point cloud. Subsequently, we propose the Height Stack Test, Ground Layer Test, and Surrounding Point Test methods to precisely and efficiently identify the dynamic bins within point cloud bins, thus overcoming the limitations of prior approaches. Through extensive evaluation on open-source datasets, our approach demonstrates superior performance in terms of precision and efficiency compared to existing methods. Furthermore, the techniques described in our work hold promise for addressing various challenging tasks or aspects through subsequent migration."
H2-Mapping: Real-Time Dense Mapping Using Hierarchical Hybrid Representation,"Chenxing Jiang, Hanwen Zhang, Peize Liu, Zehuan Yu, Hui Cheng, Boyu Zhou, Shaojie Shen","The Hong Kong University of Science and Technology,Sun Yat-Sen University,The Hong Kong University of Science and Technology, Robotic Inst,Hong Kong University of Science and Technology,Sun Yat-sen University",Mapping I,"Constructing a high-quality dense map in real-time is essential for robotics, AR/VR, and digital twins applications. As Neural Radiance Field (NeRF) greatly improves the mapping performance, in this paper, we propose a NeRF-based mapping method that enables higher-quality reconstruction and real-time capability even on edge computers. Specifically, we propose a novel hierarchical hybrid representation that leverages implicit multiresolution hash encoding aided by explicit octree SDF priors, describing the scene at different levels of detail. This representation allows for fast scene geometry initialization and makes scene geometry easier to learn. Besides, we present a coverage-maximizing keyframe selection strategy to address the forgetting issue and enhance mapping quality, particularly in marginal areas. To the best of our knowledge, our method is the first to achieve high-quality NeRF-based mapping on edge computers of handheld devices and quadrotors in real-time. Experiments demonstrate that our method outperforms existing NeRF-based mapping methods in geometry accuracy, texture realism, and time consumption."
Uncertainty-Aware 3D Object-Level Mapping with Deep Shape Priors,"Ziwei Liao, Jun Yang, Jingxing Qian, Angela P. Schoellig, Steven Lake Waslander","University of Toronto,TU Munich",Mapping I,"3D object-level mapping is a fundamental problem in robotics, which is especially challenging when object CAD models are unavailable during inference. We propose a framework that can reconstruct high-quality object-level maps for unknown objects. Our approach takes multiple RGB-D images as input and outputs dense 3D shapes and 9-DoF poses (including 3 scale parameters) for detected objects. The core idea is to leverage a learnt generative model for a category of object shapes as priors and to formulate a probabilistic, uncertainty-aware optimization framework for 3D reconstruction. We derive a probabilistic formulation that propagates shape and pose uncertainty through two novel loss functions. Unlike current state-of-the-art approaches, we explicitly model the uncertainty of the object shapes and poses during our optimization, resulting in a high-quality object-level mapping system. Moreover, the estimated shape and pose uncertainties, which we demonstrate can accurately reflect the true errors of our object maps, can be useful for downstream robotics tasks such as active vision. We perform extensive evaluations on indoor and outdoor real-world datasets, achieving substantial improvements over state-of-the-art methods. Our code is available at https://github.com/TRAILab/UncertainShapePose."
RoboHop: Segment-Based Topological Map Representation for Open-World Visual Navigation,"Sourav Garg, Krishan Rana, Mehdi Hosseinzadeh, Lachlan Mares, Niko Suenderhauf, Feras Dayoub, Ian Reid","University of Adelaide,Queensland University of Technology,The Australian Institute for Machine Learning (AIML) -- The Univ,The University of Adelaide",Mapping I,"Mapping is crucial for spatial reasoning, planning and robot navigation. Existing approaches range from metric, which require precise geometry-based optimization, to purely topological, where image-as-node based graphs lack explicit object-level reasoning and interconnectivity. In this paper, we propose a novel topological representation of an environment based on `image segments', which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features. Unlike 3D scene graphs, we create a purely topological graph with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids. This unveils a `continuous sense of a place', defined by inter-image persistence of segments along with their intra-image neighbours. It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval. Using real-world data, we show how our proposed map representation can be used to i) generate navigation plans in the form of `hops over segments' and ii) search for target objects using natural language queries describing spatial relations of objects. Furthermore, we quantitatively analyze data association at the segment level, which underpins inter-image connectivity during mapping and segment-level localization when revisiting the same place. Finally, we show preliminary trials on segment-level `hopping' based zero-shot real-world navigation. Project page with supplementary details: oravus.github.io/RoboHop/."
Grasp Manipulation Relationship Detection Based on Graph Sample and Aggregation,"Jiayuan Luo, Yaxin Liu, Han Wang, Mengyuan Ding, Xuguang Lan",Xi'an Jiaotong University,Grasping II,"In multi-object stacking scenarios, exploring the relationships among objects and determining the correct sequence of operations are crucial for robotic manipulation. However, previous algorithms inefficiently combine global and local information, often focusing solely on the local features of objects or the interactions of object features at a global level. This approach leads to imbalanced distribution of features and the generation of redundant or missing relationships in complex scenes, such as multi-object stacking and partial occlusion. To address this issue, we have developed a grasp manipulation relationship detection algorithm called Graph Sampling Aggregation Network for Visual Manipulation Relationship Detection (GSAGED). This algorithm assists robots in detecting targets in complex scenes and determining the appropriate grasping order. Firstly, the Positional Encoding Module in GSAGED enhances object feature information by considering global contexts. Secondly, the Graph Sampling Aggregation method effectively integrates global and local information, relieving imbalanced distribution of features. Finally, we applied the developed algorithm to a physical robot for grasping. Experimental results on the Visual Manipulation Relationship Dataset (VMRD) and the large-scale relational grasp dataset named REGRAD demonstrate that our method significantly improves the accuracy of relationship detection in complex scenes and exhibits robust generalization capabilities in real-world applications."
Acoustic Soft Tactile Skin (AST Skin),"S. Vishnu Rajendran, Willow Mandil, Kiyanoush Nazari, Simon Parsons, Amir Masoud Ghalamzan Esfahani","University of Lincoln,University of Surrey",Grasping II,"This paper presents a novel acoustic soft tactile (AST) skin technology operating with sound waves. In this innovative approach, the sound waves generated by a speaker travel in channels embedded in a soft membrane and get modulated due to a deformation of the channel when pressed by an external force and received by a microphone at the end of the channel. The sensor leverages regression and classification methods for estimating the normal force and its contact location. Our sensor can be affixed to any robot part, e.g., end effectors or arm. We tested several regression and classifier methods to learn the relation between sound wave modulation, the applied force, and its location, respectively and picked the best-performing models for force and location predictions. The best skin configurations yield more than 93% of the force estimation within Â±1.5 N tolerances for a range of 0-30+1 N and contact locations with over 96% accuracy. We also demonstrated the performance of AST Skin technology for a real-time gripping force control application."
Domain Randomization for Sim2real Transfer of Automatically Generated Grasping Datasets,"Johann Huber, François Hélénon, Hippolyte Christian Sébastien Watrelot, Faiz Ben Amar, Stephane Doncieux","ISIR, Sorbonne Université,Sorbonne Université,Sorbonne Université ISIR,Université Pierre et Marie Curie, Paris ,,Sorbonne University",Grasping II,"Robotic grasping refers to making a robotic system pick an object by applying forces and torques on its surface. Many recent studies use data-driven approaches to address grasping, but the sparse reward nature of this task made the learning process challenging to bootstrap. To avoid constraining the operational space, an increasing number of works propose grasping datasets to learn from. But most of them are limited to simulations. The present paper investigates how automatically generated grasps can be exploited in the real world. More than 7000 reach-and-grasp trajectories have been generated with Quality-Diversity (QD) methods on 3 different arms and grippers, including parallel fingers and a dexterous hand, and tested in the real world. Conducted analysis on the collected measure shows correlations between several Domain Randomization-based quality criteria and sim-to-real transferability. Key challenges regarding the reality gap for grasping have been identified, stressing matters on which researchers on grasping should focus in the future. A QD approach has finally been proposed for making grasps more robust to domain randomization, resulting in a transfer ratio of 84% on the Franka Research 3 arm."
Kinematic Synergy Primitives for Human-Like Grasp Motion Generation,"Julia Starke, Tamim Asfour","Karlsruhe Institute of Technology,Karlsruhe Institute of Technology (KIT)",Grasping II,"Grasping with five-fingered humanoid hands is a complex control problem. Throughout the entire grasping motion, all finger joints need to be coordinated to achieve a stable grasp. Grasp synergies provide a simplified, low-dimensional representation of grasp postures and motions, that can be used for the description of human grasps as well as the generation of novel, human-like grasps. However, the abstract synergy representation complicates the association of relevant high-level grasp parameters, as for example the grasp type and final posture or the grasp speed. Therefore, it is difficult to control these grasp characteristics in the synergy space. This paper presents an adaptable representation for kinematic grasping motions in synergy space, that allows the generation of novel, human-like grasps under direct control of high-level grasp parameters. It is based on via-point movement primitives trained on synergy trajectories of human grasping motions. The representation using synergy primitives allows for a straightforward adaptation of grasp characteristics while preserving the essential grasping motion learned from human demonstration. The kinematic synergy primitives have a low reproduction error of 3.9% of the maximum finger joint angle and are able to generate successful grasps on a simulated human hand and a real prosthetic hand."
VFAS-Grasp: Closed Loop Grasping with Visual Feedback and Adaptive Sampling,"Pedro Piacenza, Jiacheng Yuan, Jinwook Huh, Volkan Isler","Samsung Research America,University of Minnesota,Samsung",Grasping II,"We consider the problem of closed-loop robotic grasping and present a novel planner which uses Visual Feedback and an uncertainty-aware Adaptive Sampling strategy (VFAS) to close the loop. At each iteration, our method VFAS-Grasp builds a set of candidate grasps by generating random perturbations of a seed grasp. The candidates are then scored using a novel metric which combines a learned grasp-quality estimator, the uncertainty in the estimate and the distance from the seed proposal to promote temporal consistency. Additionally, we present two mechanisms to improve the efficiency of our sampling strategy: We dynamically scale the sampling region size and number of samples in it based on past grasp scores. We also leverage a motion vector field estimator to shift the center of our sampling region. We demonstrate that our algorithm can run in real time (20 Hz) and is capable of improving grasp performance for static scenes by refining the initial grasp proposal. We also show that it can enable grasping of slow moving objects, such as those encountered during human to robot handover."
The Fractal Hand-II: Reviving a Classic Mechanism for Contemporary Grasping Challenges,"Malcolm Tisdale, Joel Burdick","The California Institute of Technology,California Institute of Technology",Grasping II,"This paper and its companion propose a new fractal robotic gripper, drawing inspiration from the century-old Fractal Vise. The unusual synergistic properties allow it to passively conform to diverse objects using only one actuator. Designed to be easily integrated with prevailing parallel jaw grippers, it alleviates the complexities tied to perception and grasp planning, especially when dealing with unpredictable object poses and geometries. We build on the foundational principles of the Fractal Vise to a broader class of gripping mechanisms and address the limitations that had led to its obscurity. Two Fractal Fingers, coupled with a closing actuator, can form an adaptive and synergistic Fractal Hand. We articulate a design methodology for low-cost, easy-to-fabricate, large workspace, and compliant Fractal Fingers. The companion paper delves into the kinematics and grasping properties of a specific class of Fractal Fingers and Hands."
ICGNet: A Unified Approach for Instance-Centric Grasping,"René Zurbrügg, Yifan Liu, Francis Engelmann, Suryansh Kumar, Marco Hutter, Vaishakh Patil, Fisher Yu","ETH Zürich,ETH Zurich,RSL ETH Zurich",Grasping II,"Accurate grasping is the key to several robotic tasks including assembly and household robotics. Executing a successful grasp in a cluttered environment requires multiple levels of scene understanding: First, the robot needs to analyze the geometric properties of individual objects to find feasible grasps. These grasps need to be compliant with the local object geometry. Second, for each proposed grasp, the robot needs to reason about the interactions with other objects in the scene. Finally, the robot must compute a collision-free grasp trajectory while taking into account the geometry of the target object. Most grasp detection algorithms directly predict grasp poses in a monolithic fashion, which does not capture the composability of the environment. In this paper, we introduce an end-to-end architecture for object-centric grasping. The method uses pointcloud data from a single arbitrary viewing direction as an input and generates an instance-centric representation for each partially observed object in the scene. This representation is further used for object reconstruction and grasp detection in cluttered table-top scenes. We show the effectiveness of the proposed method by extensively evaluating it against state-of-the-art methods on synthetic datasets, indicating superior performance for grasping and reconstruction. Additionally, we demonstrate real-world applicability by decluttering scenes with varying numbers of objects. Videos and Code at icgraspnet.github.io"
The Grasp Reset Mechanism: An Automated Apparatus for Conducting Grasping Trials,"Kyle Dufrene, Keegan Nave, Josh Campbell, Ravi Balasubramanian, Cindy Grimm","Oregon State University,Southwest Research Institute",Grasping II,"Advancing robotic grasping and manipulation requires the ability to test algorithms and/or train learning models on large numbers of grasps. Towards the goal of more advanced grasping, we present the Grasp Reset Mechanism (GRM), a fully automated apparatus for conducting large-scale grasping trials. The GRM automates the process of resetting a grasping environment, repeatably placing an object in a fixed location and controllable 1-D orientation. It also collects data and swaps between multiple objects enabling robust dataset collection with no human intervention. We also present a standardized state machine interface for control, which allows for integration of most manipulators with minimal effort. In addition to the physical design and corresponding software, we include a dataset of 1,020 grasps. The grasps were created with a Kinova Gen3 robot arm and Robotiq 2F-85 Adaptive Gripper to enable training of learning models and to demonstrate the capabilities of the GRM. The dataset includes ranges of grasps conducted across four objects and a variety of orientations. Manipulator states, object pose, video, and grasp success data are provided for every trial."
Model-Based Runtime Monitoring with Interactive Imitation Learning,"Huihan Liu, Shivin Dass, Roberto Martín-martín, Yuke Zhu","University of Texas, Austin,UT Austin,University of Texas at Austin,The University of Texas at Austin",Grasping II,"Robot learning methods have recently made great strides but generalization and robustness challenges still hinder their widespread deployment. Failing to detect potential failures and learn to solve them renders state-of-the-art learning systems not combat-ready for high-stakes tasks. Recent advancements in interactive imitation learning have proposed a promising framework for human-robot teaming, enabling the robots to operate safely and to continually improve their performances through deployment data. Nonetheless, existing methods typically require constant human supervision and preemptive feedback, limiting their usability in realistic domains. In this work, we aim to endow a robot with the ability to monitor and detect errors during runtime task execution. We introduce MoMo, a model-based runtime monitoring algorithm that learns from deployment data to detect system anomalies and anticipate failures. Unlike prior work that cannot foresee future failures or requires failure experiences for training, MoMo learns a latent-space dynamics model and a failure classifier that combined enable MoMo to simulate future action outcomes and detect out-of-distribution states and high-risk situations preemptively. We train MoMo within an interactive imitation learning framework, where it continually updates the model from the experiences of the human-robot team collected from trustworthy deployments. Consequently, our method reduces the human workload needed over time while ensuring reliable task execution. We demonstrate that MoMo outperforms the baselines across system-level and unit-test metrics, with on average 23% and 40% higher success rates in simulation and on physical hardware, respectively. More information at https://ut-austin-rpl.github.io/sirius-runtime-monitor/"
"The Fractal Hand--I: A Non-Anthropomorphic, but Synergistic, Adaptable Gripper","Joel Burdick, Malcolm Tisdale","California Institute of Technology,The California Institute of Technology",Grippers and Other End-EffectorsII,"This paper introduces a novel Fractal Hand robotic gripper. The hand has only 1 actuator, but (2^{n+1}-1) joints, where n is a design parameter that defines the depth of the fingers' tree structures. The hand is synergistic in its operation (because its joint movements are highly coupled through the hand's interaction with the grasped object), but it is not anthropomorphic.The basic finger and hand geometry, governing kinematics, and quasi-statics mechanics of rigid and compliant versions of the hand are developed. These analyses remarkably show that under very mild constraints on the hand design, the hand is compliantly stable at every equilibrium condition.Therfore, the Fractal Hand adapts to a very wide range of planar objects with a single design. A companion paper introduces a design methodology for this new class of robot hands, and multiple prototypes."
The Double-Scoop Gripper: A Tendon-Driven Soft-Rigid End-Effector for Food Handling Exploiting Constraints in Narrow Spaces,"Leonardo Franco, Enrico Turco, Valerio Bo, Maria Pozzi, Monica Malvezzi, Domenico Prattichizzo, Gionata Salvietti","University of Siena,Istituto Italiano di Tecnologia",Grippers and Other End-EffectorsII,"Food handling is a challenging task for robotic grippers, which are required to manipulate highly deformable and fragile items, that can be easily damaged. Moreover, ingredients for the preparation of the different dishes are usually stored in small containers that are often not easily accessible. This paper introduces an innovative soft-rigid, tendon-driven gripper: the Double-Scoop Gripper (DSG). Its two-fingered design exploits a specialized structure to cope with constrained spaces (e.g., containers in narrow shelves). The DSG can delicately grasp objects of various shapes by employing two scoop-shaped fingertips that can form a single plate when fingers are flexed. Data obtained from an on-board camera are used to detect the food item features and plan the grasping strategy that better exploits the possible environmental constraints regulating the opening of the two fingers and the approaching direction of the gripper. DSG capabilities are verified with experiments conducted using real food ingredients within a pick-and-place setup to evaluate both the grasping and the releasing capability of the gripper. Obtained results are promising and suggest that this approach could be particularly advantageous in the context of automated food serving"
Co-Designing Manipulation Systems Using Task-Relevant Constraints,"Apoorv Vaish, Oliver Brock","TU Berlin,Technische Universität Berlin",Grippers and Other End-EffectorsII,"A robotic system's hardware and control policy must be co-optimized to ensure they complement each other to interact robustly with the environment. However, this combined search is extremely high-dimensional and intractable without a suitable underlying representation. This paper uses environmental constraints to structure the co-design space for manipulation. We show that task-relevant constraints encode regions of the search space containing reasonable co-design solutions. Furthermore, this underlying representation renders a co-design space amenable to gradient-based optimization. For efficient search, we present the co-design Jacobian that describes how the robot's motion varies with control as well as hardware design changes. This Jacobian exploits the structure induced by environmental constraints for iterative design updates in the co-design space. Using these two conceptual tools, we co-design manipulators, grippers, and multi-fingered hands, showing that environmental constraints are an effective representation for co-designing diverse manipulation systems. Our methodology scales well with increased co-design parameters, rendering the co-design of complex, high-dimensional manipulation systems feasible."
Squirrel-Inspired Tendon-Driven Passive Gripper for Agile Landing,"Stanley Wang, Duyi Kuang, Sebastian Lee, Robert Full, Hannah Stuart","University of California, Berkeley,University of California at Berkeley,UC Berkeley",Grippers and Other End-EffectorsII,"Squirrels exhibit agile leaping between tree branches, often using non-prehensile gripping with compliant and passively adaptive fingers. We aim to test the utility of such gripping in agile robotic maneuvering. In the present study, we first examine the parametric design of a squirrel-inspired underactuated gripper for passive landing on impact. We fix the geometry of the gripper and vary the joint stiffness and contact conditions. We find that stiffer fingers with soft foam pads enlarge the landing sufficiency region. Specifically, friction appears to enlarge horizontal error tolerance, while joint stiffness and pad damping allow for higher impact speeds. Thus, these features should be considered in the design of future agile robot hands and feet that include high impact landings on rods with pose inaccuracy."
HASHI: Highly Adaptable Seafood Handling Instrument for Manipulation in Industrial Settings,"Austin Allison, Nathaniel Hanson, Sebastian Wicke, Taskin Padir","Northeastern University,Massachusetts Institute of Technology",Grippers and Other End-EffectorsII,"The seafood processing industry provides fertile ground for robotics to impact the future-of-work from multiple perspectives including productivity, worker safety, and quality of work life. The robotics research challenge is the realization of flexible and reliable manipulation of soft, deformable, slippery, spiky and scaly objects. In this paper, we propose a novel robot end effector, called HASHI, that employs chopstick-like appendages for precise and dexterous manipulation. This gripper is capable of in-hand manipulation by rotating its two constituent sticks relative to each other and offers control of objects in all three axes of rotation by imitating human use of chopsticks. HASHI delicately positions and orients food through embedded 6-axis force-torque sensors. We derive and validate the kinematic model for HASHI, as well as demonstrate grip force and torque readings from the sensorization of each chopstick. We also evaluate the versatility of HASHI through grasping trials of a variety of real and simulated food items with varying geometry, weight, and firmness."
All the Feels: A Dexterous Hand with Large-Area Tactile Sensing,"Raunaq Mahesh Bhirangi, Abigail Defranco, Jacob Adkins, Carmel Majidi, Abhinav Gupta, Tess Hellebrekers, Vikash Kumar","Carnegie Mellon University,University of Alberta,Meta AI Research,Meta AI",Grippers and Other End-EffectorsII,"High cost and lack of reliability has precluded the widespread adoption of dexterous hands in robotics. Furthermore, the lack of a viable tactile sensor capable of sensing over the entire area of the hand impedes the rich, low-level feedback that would improve learning of dexterous manipulation skills. This paper introduces an inexpensive, modular, and robust platform - the D'Manus - aimed at resolving these challenges while satisfying the large-scale data collection demands of deep robot learning paradigms. Studies on human manipulation point to the criticality of low-level tactile feedback in performing everyday dexterous tasks. The D'Manus comes with ReSkin sensing on the entire surface of the palm as well as the fingertips. We also demonstrate the generalizability of tactile models trained with the fully integrated system in a tactile-aware task - bin-picking and sorting. Code, documentation, design files, detailed assembly instructions, trained models, task videos, and all supplementary materials required to recreate the setup can be found on https://sites.google.com/view/dmanus."
Soft and Rigid Object Grasping with Cross-Structure Hand Using Bilateral Control-Based Imitation Learning,"Koki Yamane, Yuki Saigusa, Sho Sakaino, Toshiaki Tsuji","University of Tsukuba,Saitama University",Grippers and Other End-EffectorsII,"Object grasping is an important ability required for various robot tasks. In particular, tasks that require precise force adjustments during operation, such as grasping an unknown object or using a grasped tool, are difficult for humans to program in advance. Recently, AI-based algorithms that can imitate human force skills have been actively explored as a solution. In particular, bilateral control-based imitation learning achieves human-level motion speeds with environmental adaptability, only requiring human demonstration and without programming. However, owing to hardware limitations, its grasping performance remains limited, and tasks that involves grasping various objects are yet to be achieved. Here, we developed a cross-structure hand to grasp various objects. We experimentally demonstrated that the integration of bilateral control-based imitation learning and the cross-structure hand is effective for grasping various objects and harnessing tools."
GRASP: Grocery Robotâ€™s Adhesion and Suction Picker,"Amar Hajj-ahmad, Lukas Kaul, Carolyn Chen, Mark Cutkosky","Stanford University,Toyota Research Institute",Grippers and Other End-EffectorsII,"We present a solution to the separate challenges faced by suction cups and gecko adhesives for one-sided grasping of heavy, irregular items. The gripping technology combines suction with adhesion for grasping and placing a wide range of objects in packed spaces. Applications include shopping and restocking in retail and warehouse settings where products vary in size and weight and are packed tightly, which limits access. A single suction cup is compact enough to reach and grasp the smallest items (down to 5 cm in size) but cannot provide the shear force needed for handling bulky items. Gecko-inspired adhesion provides extra lifting capability for objects up to 2.3 kg, using a 7.6 x 12.7 cm adhesive swatch â€“ 2.5x heavier than with suction alone. The adhesive is fabricated on a flexible nylon fabric. A small fan blows gently to help the fabric conform to irregular surfaces prior to lifting."
Improved Generalization of Probabilistic Movement Primitives for Manipulation Trajectories,"Xueyang Yao, Yinghan Chen, Bryan Patrick Tripp",University of Waterloo,Grippers and Other End-EffectorsII,"Imitation learning methods have proven effective in learning robotic tasks by leveraging multiple human-controlled demonstrations. However, existing approaches often struggle to generalize across a wide range of tasks, such as extrapolating to unseen object locations, incorporating via-point modulation, accurately modeling orientation, handling trajectories with multiple options, and capturing aiming actions. In this study, we propose a novel framework that combines ideas from task-parameterized Gaussian mixture models and probabilistic movement primitives to address these limitations and satisfy all the aforementioned properties within a single framework. We conduct comprehensive evaluations of our approach on four real-life tasks: pick-and-place, water pouring, shooting a hockey puck into a net, and sweeping."
Road Obstacle Detection Based on Unknown Objectness Scores,"Chihiro Noguchi, Toshiaki Ohgushi, Masao Yamanaka","Toyota Motor Corporation,TOYOTA MOTOR CORPORATION,TOYOTA MOTOR CORPORATION.",Object Detection I,"The detection of unknown traffic obstacles is vital to ensure safe autonomous driving. The standard object-detection methods cannot identify unknown objects that are not included under predefined categories. This is because object-detection methods are trained to assign a background label to pixels corresponding to the presence of unknown objects. To address this problem, the pixel-wise anomaly-detection approach has attracted increased research attention. Anomaly-detection techniques, such as uncertainty estimation and perceptual difference from reconstructed images, make it possible to identify pixels of unknown objects as out-of-distribution (OoD) samples. However, when applied to images with many unknowns and complex components, such as driving scenes, these methods often exhibit unstable performance. The purpose of this study is to achieve stable performance for detecting unknown objects by incorporating the object-detection fashions into the pixel-wise anomaly detection methods. To achieve this goal, we adopt a semantic-segmentation network with a sigmoid head that simultaneously provides pixel-wise anomaly scores and objectness scores. Our experimental results show that the objectness scores play an important role in improving the detection performance. Based on these results, we propose a novel anomaly score by integrating these two scores, which we term as unknown objectness score. Quantitative evaluations show that the proposed method outperforms state-of-the-art methods when applied to the publicly available datasets."
PVTransformer: Point-To-Voxel Transformer for Scalable 3D ObjectDetection,"Zhaoqi Leng, Pei Sun, Tong He, Dragomir Anguelov, Mingxing Tan","Waymo LLC,Waymo,Waymo Research",Object Detection I,"3D object detectors for point clouds often rely on a pooling-based PointNet to encode sparse points into grid-like voxels or pillars. In this paper, we identify that the common PointNet design introduces an information bottleneck that limits 3D object detection accuracy and scalability. To address this limitation, we propose PVTransformer: a transformer-based point-to-voxel architecture for 3D detection. Our key idea is to replace the PointNet pooling operation with an attention module, leading to a better point-to-voxel aggregation function. Our design respects the permutation invariance of sparse 3D points while being more expressive than the pooling-based PointNet. Experimental results show our PVTransformer achieves much better performance compared to the latest 3D object detectors. On the widely used Waymo Open Dataset, our PVTransformer achieves state-of-the-art 76.5 mAPH L2, outperforming the prior art of SWFormer by +1.7 mAPH L2."
Object-Centric Cross-Modal Feature Distillation for Event-Based Object Detection,"Lei Li, Alexander Liniger, Mario Millhaeusler, Vagia Tsiminaki, Yuanyou Li, Dengxin Dai","ETH Zurich,Huawei Zurich,Huawei",Object Detection I,"Event cameras are gaining popularity due to their unique properties, such as their low latency and high dynamic range. One task where these benefits can be crucial is real-time object detection. However, RGB detectors still outperform event-based detectors due to the sparsity of the event data and missing visual details. In this paper, we propose a cross-modality feature distillation method that can focus on regions where the knowledge distillation works best to shrink the detection performance gap between these two modalities. We achieve this by using an object-centric slot attention mechanism that can iteratively decouple feature maps into object-centric features and corresponding pixel-features used for distillation. We evaluate our novel distillation approach on a synthetic and a real event dataset with aligned grayscale images as a teacher modality. We show that object-centric distillation allows to significantly improve the performance of the event-based student object detector, nearly halving the performance gap with respect to the teacher."
Hierarchical Point Attention for Indoor 3D Object Detection,"Manli Shu, Le Xue, Ning Yu, Roberto Martín-martín, Caiming Xiong, Tom Goldstein, Juan Carlos Niebles, Ran Xu","University of Maryland, College Park,Salesforce Research,Netflix,University of Texas at Austin,Salesforce Inc.,University of Maryland,Stanford University,Salesforce",Object Detection I,"3D object detection is an essential vision technique for various robotic systems, such as augmented reality and domestic robots. Transformers as versatile network architectures have recently seen great success in 3D point cloud object detection. However, the lack of hierarchy in a plain transformer restrains its ability to learn features at different scales. Such limitation makes transformer detectors perform worse on smaller objects and affects their reliability in indoor environments where small objects are the majority. This work proposes two novel attention operations as generic hierarchical designs for point-based transformer detectors. First, we propose Aggregated Multi-Scale Attention (MS-A) that builds multi-scale tokens from a single-scale input feature to enable more fine-grained feature learning. Second, we propose Size-Adaptive Local Attention (Local-A) with adaptive attention regions for localized feature aggregation within bounding box proposals. Both attention operations are model-agnostic network modules that can be plugged into existing point cloud transformers for end-to-end training. We evaluate our method on two widely used indoor detection benchmarks. By plugging our proposed modules into the state-of-the-art transformer-based 3D detectors, we improve the previous best results on both benchmarks, with more significant improvements on smaller objects."
Frame Fusion with Vehicle Motion Prediction for 3D Object Detection,"Xirui Li, Feng Wang, Naiyan Wang, Chao Ma","Shanghai Jiao Tong University,TuSimple",Object Detection I,"In LiDAR-based 3D detection, history point clouds contain rich temporal information helpful for future prediction. In the same way, history detections should contribute to future detections. In this paper, we propose a detection enhancement method, namely FrameFusion, which improves 3D object detection results by fusing history detection frames. In FrameFusion, we ``forward'' history frames to the current frame and apply weighted Non-Maximum-Suppression on dense bounding boxes to obtain a fused frame with merged boxes. To ``forward'' frames, we use vehicle motion models to estimate the future pose of the bounding boxes. Our method is flexible in motion model selection. We explore three motion models in our work and show how the unicycle model and the bicycle model improve turning cases. On Waymo Open Dataset, our FrameFusion method consistently improves the performance of various 3D detectors by about 2.0 vehicle level 2 APH with negligible latency and slightly enhances the performance of the temporal fusion method MPPNet. We also conduct extensive experiments on motion model selection."
FG-PFE: Fine-Grained Pillar Feature Encoding Via Spatio-Temporal Virtual Grid for 3D Object Detection,"Konyul Park, Yecheol Kim, Junho Koh, Byungwoo Park, Jun Won Choi","Hanyang University,Seoul National University",Object Detection I,"Autonomous vehicles require real-time, high- performance 3D object detectors to guarantee system robust- ness and safety. Recent point cloud-based 3D object detectors are mainly categorized into three types based on input representation: point-based, voxel-based, and pillar-based. Among these, pillar-based models are most suitable for onboard deployment due to their light architecture. Despite their advantages, pillar-based methods often underperform compared to voxel-based and point-based, largely due to their coarse representation and simplistic architecture design. While most recent research has aimed to improve the backbone network to address this performance gap, we argue that there exist a room for improvement for the Pillar Feature Encoding (PFE) stage. We demonstrate that with sufficient representational power, pillar-based methods can achieve performance comparable to other representations. To achieve this, we introduce fine-grained pillar feature encoding (FG-PFE), which utilizes spatio-temporal virtual (STV) grids for fine-grained representation. We also present the attentive pillar aggregation module designed to selectively aggregate essential pillar features. Extensive experiments conducted on the nuScenes dataset show that our FG-PFE not only requires less computational power but also achieves significant performance gains compared to the baseline."
Efficient Semantic Segmentation for Compressed Video,"Jiaxin Cai, Qi Li, Yulin Shen, Jia Pan, Wenxi Liu","Fuzhou University,University of Hong Kong",Object Detection I,"Robots, constrained by limited onboard computing resources, often encounter situations wherein high-resolution and high-bit-rate videos captured by their cameras necessitate compression before further analysis. In this paper, we propose a novel video semantic segmentation paradigm for compressed video. Specifically, our framework draws the inspiration from the principle of Wavelet Transform, and thus we design the network structure, WTDecomNet, approximating the decomposition of high-resolution image into its low-resolution counterpart and axial details. The aim is to well preserve the image content through decomposition and maintain model efficiency by obtaining semantics from low-resolution image. To facilitate this purpose, we propose an efficient axial subband approximation module for extracting axial details and a lightweight temporal alignment module for associating keyframes and non-keyframes of compressed video. Through comprehensive experiments, we show that our model can achieve the state-of-the-art performance on public benchmarks. Especially on CamVid, comparing to baseline, our proposed model reduces the computational overhead by 70% while improving mIoU by 4%."
Cross-Cluster Shifting for Efficient and Effective 3D Object Detection in Autonomous Driving,"Chen Zhili, Trung Kien Pham, Maosheng Ye, Zhiqiang Shen, Qifeng Chen","Hong Kong University of Science and Technology,The Hong Kong University of Science and Technology,HKUST,MBZUAI",Object Detection I,"We present a new 3D point-based detector model, named Shift-SSD, for precise 3D object detection in autonomous driving. Traditional point-based 3D object detectors often employ architectures that rely on a progressive downsampling of points. While this method effectively reduces computational demands and increases receptive fields, it will compromise the preservation of crucial non-local information for accurate 3D object detection, especially in the complex driving scenarios. To address this, we introduce an intriguing Cross-Cluster Shifting operation to unleash the representation capacity of the point-based detector by efficiently modeling longer-range inter-dependency while including only a negligible overhead. Concretely, the Cross-Cluster Shifting operation enhances the conventional design by shifting partial channels from neighboring clusters, which enables richer interaction with non-local regions and thus enlarges the receptive field of clusters. We conduct extensive experiments on the KITTI, Waymo, and nuScenes datasets, and the results demonstrate the state-of-the-art performance of Shift-SSD in both detection accuracy and runtime efficiency."
BEE-Net: Bridging Semantic and Instance with Gated Encoding and Edge Constraint for Efficient Panoptic Segmentation,"Xinyang Huang, Guanghui Zhang, Dongchen Zhu, Yunpeng Sun, Wenjun Shi, Gang Ye, Yang Xiao, Lei Wang, Xiaolin Zhang, Bo Li, Jiamao Li","Shanghai Institute of Microsystem and Information Technology, Ch,Shanghai Institute of Microsystem and Information Technology,Chi,Lotus Robotics,Shanghai Institute of Microsystem and Information Technology,Lotus Technology Ltd.,Shanghai Institute of Microsystem And Information Technology,Chi",Object Detection I,"Panoptic segmentation is a challenging perception task, which can help robots to comprehensively perceive the surrounding environment. In the task, we notice that semantic, instance, and panoptic have rich relations, however, which are rarely explored. In this work, we propose a novel panoptic, instance, and semantic bridged network to delve into the reciprocal relation. To make semantic and instance benefit from each other, we design a novel Gated Encoding (GE) module, incorporating complementary cues between semantic and instance heads through the gated mechanism. In addition, a novel edge-aware consistency constraint among edges of each task is presented, which exhaustedly exploit geometric constraints, to boost the segmentation quality of challenging edges. Experimental results on the Cityscapes and MS-COCO datasets demonstrate that our approach achieves state-of-the-art performance in an efficient CNN-based paradigm, attaining a balance between accuracy and efficiency."
Toward AI-Enabled Commercial Telepresence Robots to Combine Home Care Needs and Affordability,"Gloria Beraldo, Riccardo De Benedictis, Amedeo Cesta, Francesca Fracasso, Gabriella Cortellessa","National Research Council of Italy,CNR-ISTC,CNR -- National Research Council of Italy, ISTC",AI-Enabled Robotics I,"As life expectancy increases, social and health assistance requires sustainable and affordable solutions possibly usable from oneâ€™s own domestic environment. In this article, we propose a transformer-based approach combined with a task-planning system and enhanced with several AI sub-modules to run on low-cost telepresence robots in order to support more advanced and autonomous assistance services. The proposed system allows to dynamically generate and adapt in autonomy heterogeneous robotic actions according to the information emerged during the interaction. The AI-enhanced telepresence robot was assessed in an unstructured domestic environment by 10 users. The results show an accuracy of more than 95% in the expected robotâ€™s functioning. The participants judged the system efficient, useful, and intuitive, and showed a positive inclination to re-use the robot in the future. Such outcomes derive both from a proper coordination among the heterogeneous AI sub-modules in the system, and from the fast capability to frequently co-adapt the interaction."
SliceIt! - a Dual Simulator Framework for Learning Robot Food Slicing,"Cristian Beltran-Hernandez, Nicolas Erbetti, Masashi Hamaya","OMRON SINIC X,OMRON SINIC X Corporation",AI-Enabled Robotics I,"Cooking robots can enhance the home experience by reducing the burden of daily chores. However, these robots must perform their tasks dexterously and safely in shared human environments, especially when handling dangerous tools such as kitchen knives. This study focuses on enabling a robot to autonomously and safely learn food-cutting tasks. More specifically, our goal is to enable a collaborative robot or industrial robot arm to perform food-slicing tasks by adapting to varying material properties using compliance control. Our approach involves using Reinforcement Learning (RL) to train a robot to compliantly manipulate a knife, by reducing the contact forces exerted by the food items and by the cutting board. However, training the robot in the real world can be inefficient, and dangerous, and result in a lot of food waste. Therefore, we proposed SliceIt!, a framework for safely and efficiently learning robot food-slicing tasks in simulation. Following a real2sim2real approach, our framework consists of collecting a few real food slicing data, calibrating our dual simulation environment (a high-fidelity cutting simulator and a robotic simulator), learning compliant control policies on the calibrated simulation environment, and finally, deploying the policies on the real robot."
SG-Bot: Object Rearrangement Via Coarse-To-Fine Robotic Imagination on Scene Graphs,"Guangyao Zhai, Xiaoni Cai, Dianye Huang, Yan Di, Fabian Manhardt, Federico Tombari, Nassir Navab, Benjamin Busam","Technical University of Munich,Google,Technische Universität München,TU Munich",AI-Enabled Robotics I,"Object rearrangement is pivotal in robotic-environment interactions, representing a significant capability in embodied AI. In this paper, we present SG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme with a scene graph as the scene representation. Unlike previous methods that rely on either known goal priors or zero-shot large models, SG-Bot exemplifies lightweight, real-time, and user-controllable characteristics, seamlessly blending the consideration of commonsense knowledge with automatic generation capabilities. SG-Bot employs a three-fold procedureâ€”observation, imagination, and executionâ€”to adeptly address the task. Initially, objects are discerned and extracted from a cluttered scene during the observation. These objects are first coarsely organized and depicted within a scene graph, guided by either commonsense or user-defined criteria. Then, this scene graph subsequently informs a generative model, which forms a fine-grained goal scene considering the shape information from the initial scene and object semantics. Finally, for execution, the initial and envisioned goal scenes are matched to formulate robotic action policies. Experimental results demonstrate that SG-Bot outperforms competitors by a large margin."
Scalable Multi-Robot Collaboration with Large Language Models: Centralized or Decentralized Systems?,"Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, Chuchu Fan","Harvard University,Massachusetts Institute of Technology,IBM",AI-Enabled Robotics I,"A flurry of recent work has demonstrated that pre-trained large language models (LLMs) can be effective task planners for a variety of single-robot tasks. The planning performance of LLMs is significantly improved via prompting techniques, such as in-context learning or re-prompting with state feedback, placing new importance on the token budget for the context window. An under-explored but natural next direction is to investigate LLMs as multi-robot task planners. However, long-horizon, heterogeneous multi-robot planning introduces new challenges of coordination while also pushing up against the limits of context window length. It is therefore critical to find token-efficient LLM planning frameworks that are also able to reason about the complexities of multi-robot coordination. In this work, we compare the task success rate and token efficiency of four multi-agent communication frameworks (centralized, decentralized, and two hybrid) as applied to four coordination-dependent multi-agent 2D task scenarios for increasing numbers of agents. We find that a hybrid framework achieves better task success rates across all four tasks and scales better to more agents. We further demonstrate the hybrid frameworks in 3D simulations where the vision-to-text problem and dynamical errors are considered. Please see project website for prompts, videos, and codes."
Object-Centric Instruction Augmentation for Robotic Manipulation,"w lesjie, Yichen Zhu, Minjie Zhu, Jinming Li, Zhiyuan Xu, Zhengping Che, Chaomin Shen, Yaxin Peng, Dong Liu, Feifei Feng, Jian Tang","East China Normal University,Midea Group,shanghai University,Shanghai University,Midea Group (Shanghai) Co.,Ltd.,Midea Group (Shanghai) Co., Ltd.",AI-Enabled Robotics I,"Humans interpret scenes by recognizing both the identities and positions of objects in their observations. For a robot to perform tasks such as ""pick and place"", understanding both what the objects are and where they are located is crucial. While the former has been extensively discussed in the literature that uses the large language model to enrich the text descriptions, the latter remains underexplored. In this work, we introduce the Object-Centric Instruction Augmentation (OCI) framework to augment highly semantic and information-dense language instruction with position cues. We utilize a Multi-modal Large Language Model (MLLM) to weave knowledge of object locations into natural language instruction, thus aiding the policy network in mastering actions for versatile manipulation. Additionally, we present a feature reuse mechanism to integrate the vision-language features from off-the-shelf pre-trained MLLM into policy networks. Through a series of simulated and real-world robotic tasks, we demonstrate that robotic manipulator imitation policies trained with our enhanced instructions outperform those relying solely on traditional language instructions."
Learning to Play Foosball: System and Baselines,"Janosch Moos, Cedric Derstroff, Niklas Schröder, Debora Clever","TU Darmstadt, Institute for Mechatronic Systems,Technische Universität Darmstadt,TU Darmstadt, Institute of Mechatronic Systems",AI-Enabled Robotics I,"This work stages Foosball as a versatile platform for advancing scientific research, particularly in the realm of robot learning. We present an automated Foosball table along with its corresponding simulated counterpart, showcasing a diverse range of challenges through example tasks within the Foosball environment. Initial findings are shared using a simple baseline approach. Foosball constitutes a versatile learning environment with the potential to yield cutting-edge research in various fields of artificial intelligence and machine learning, notably robust learning, while also extending its applicability to industrial robotics and automation setups. To transform our physical Foosball table into a research-friendly system, we augmented it with a 2 degrees of freedom kinematic chain to control the goalkeeper rod as an initial setup with the intention to be extended to the full game as soon as possible. Our experiments reveal that a realistic simulation is essential for mastering complex robotic tasks, yet translating these accomplishments to the real system remains challenging, often accompanied by a performance decline. This emphasizes the critical importance of research in this direction. In this concern, we spotlight the automated Foosball table as an invaluable tool, possessing numerous desirable attributes, to serve as a demanding learning environment for advancing robotics and automation research."
Language-Conditioned Robotic Manipulation with Fast and Slow Thinking,"Minjie Zhu, Yichen Zhu, Jinming Li, w lesjie, Zhiyuan Xu, Zhengping Che, Chaomin Shen, Yaxin Peng, Dong Liu, Feifei Feng, Jian Tang","East China Normal University,Midea Group,shanghai University,Shanghai University,Midea Group (Shanghai) Co.,Ltd.,Midea Group (Shanghai) Co., Ltd.",AI-Enabled Robotics I,"The language-conditioned robotic manipulation aims to transfer natural language instructions into executable actions, from simple enquote{pick-and-place} to tasks requiring intent recognition and visual reasoning. Inspired by the dual-process theory in cognitive scienceâ€”which suggests two parallel systems of fast and slow thinking in human decision-makingâ€”we introduce textit{Robotics with Fast and Slow Thinking (RFST)}, a framework that mimics human cognitive architecture to classify tasks and makes decisions on two systems based on instruction types. Our RFST consists of two key components: 1) an instruction discriminator to determine which system should be activated based on the current user's instruction, and 2) a slow-thinking system that is comprised of a fine-tuned vision-language model aligned with the policy networks, which allow the robot to recognize user's intention or perform reasoning tasks. To assess our methodology, we built a dataset featuring real-world trajectories, capturing actions ranging from spontaneous impulses to tasks requiring deliberate contemplation. Our results, both in simulation and real-world scenarios, confirm that our approach adeptly manages intricate tasks that demand intent recognition and reasoning."
How to Prompt Your Robot: A Prompt Book for Manipulation Skills with Code As Policies,"Montse Gonzalez Arenas, Ted Xiao, Sumeet Singh, Vidhi Jain, Allen Z. Ren, Quan Vuong, Jacob Varley, Alexander Herzog, Isabel Leal, Sean Kirmani, Mario Prats, Dorsa Sadigh, Vikas Sindhwani, Kanishka Rao, Jacky Liang, Andy Zeng","Google Inc,Google,Carnegie Mellon University,Princeton University,UC San Diego,X, Inc. (Google),Google Deepmind,Google DeepMind,Stanford University,,Google Brain, NYC",AI-Enabled Robotics I,"Large Language Models (LLMs) have demonstrated the ability to perform semantic reasoning, planning and write code for robotics tasks. However, most methods rely on pre-existing primitives, which heavily limits their scalability to new scenarios. Additionally, they use examples prompting style where the LLM is provided few-shot examples of robot code. This presents a challenge for LLMs to implicitly infer task information, constraints, and API usage from examples alone. Meanwhile, research outside robotics has successfully studied instruction-based prompting, where providing LLMs with API documentation and detailed descriptions can improve code synthesis capabilities. However, it is not clear how to document robotics tasks and naively providing full robot APIs presents a challenge to context-length limits in LLMs. However, it is not clear how to document robotics tasks and providing full robot APIs presents a challenge to context-length limits in LLMs. In this work, we discuss how to combine different LLM prompting styles to write code for new manipulation skills. Firstly, we evaluate different prompting styles across 3 robots in a high-level sorting task, and present a collection of empirical observations: (i)including both instructions and examples improves performance, (ii)interleaving state predictions in the examples helps reasoning,(iii)instruction-based prompting benefits from human feedback. Our observations lead to a prompt recipe we refer to as PromptBook that combines: example-based, instruction-based and chain-of-thought prompting to write robot code; as well as a method to build the prompt leveraging LLMs and human feedback. Secondly, we show PromptBook can write code for new low-level manipulation skills on the fly zero-shot. The prompt extracts motion trajectories from LLMs that the robot can execute directly with an IK controller. Finally, we evaluate the new skills on a mobile manipulator with 83% success rate at picking, 50-71% at opening drawers."
A Multifidelity Sim-To-Real Pipeline for Verifiable and Compositional Reinforcement Learning,"Cyrus Neary, Christian Ellis, Aryaman Singh Samyal, Craig Lennon, Ufuk Topcu","The University of Texas at Austin,University of Massachusetts,United States Army Research Laboratory",AI-Enabled Robotics I,"We propose and demonstrate a compositional framework for training and verifying reinforcement learning (RL) systems within a multifidelity sim-to-real pipeline, in order to deploy reliable and adaptable RL policies on physical hardware. By decomposing complex robotic tasks into component subtasks and defining mathematical interfaces between them, the framework allows for the independent training and testing of the corresponding subtask policies, while simultaneously providing guarantees on the overall behavior that results from their composition. By verifying the performance of these subtask policies using a multifidelity simulation pipeline, the framework not only allows for efficient RL training, but also for a refinement of the subtasks and their interfaces in response to challenges arising from discrepancies between simulation and reality. In an experimental case study we apply the framework to train and deploy a compositional RL system that successfully pilots a Warthog unmanned ground robot."
Bridging the Sim-To-Real Gap with Dynamic Compliance Tuning for Industrial Insertion,"Xiang Zhang, Masayoshi Tomizuka, Hui Li","University of California, Berkeley,University of California,Autodesk Research",Factory/Assembly Automation,"Contact-rich manipulation tasks often exhibit a large sim-to-real gap. For instance, industrial assembly tasks frequently involve tight insertions where the clearance is less than 0.1 mm and can even be negative when dealing with a deformable receptacle. This narrow clearance leads to complex contact dynamics that are difficult to model accurately in simulation, making it challenging to transfer simulation-learned policies to real-world robots. In this paper, we propose a novel framework for robustly learning manipulation skills for real-world tasks using only the simulated data. Our framework consists of two main components: the ``Force Planner'' and the ``Gain Tuner''. The Force Planner is responsible for planning both the robot motion and desired contact forces, while the Gain Tuner dynamically adjusts the compliance control gains to accurately track the desired contact forces during task execution. The key insight of this work is that by adaptively adjusting the robot's compliance control gains during task execution, we can modulate contact forces in the new environment, thereby generating trajectories similar to those trained in simulation and narrows the sim-to-real gap. Experimental results show that our method, trained in simulation on a generic square peg-and-hole task, can generalize to a variety of real-world insertion tasks involving narrow or even negative clearances, all without requiring any fine-tuning."
Compliant Peg-In-Hole Assembly Using a Very Soft Wrist,"Qi Zhang, Zhengtao Hu, Weiwei Wan, Kensuke Harada",Osaka University,Factory/Assembly Automation,"This paper proposes to use a high-compliance soft wrist to improve the performance of robotic peg-in-hole in uncertain environments. In contrast to past research in this field, in which force control with relatively low compliance has been used, we propose a method that searching and aligning motions can be easily realized by taking advantage of high compliance wrist under the gravity. Our proposed PiH strategy is completely passive: After the peg is trapped in the hole during the hole-searching process using the spherical helix trajectory, it is guaranteed that the peg can automatically be inserted into the hole due to the effect of gravity and wrist compliance if the configuration of the peg is included in the no-escapable area. The no-escapable area can be obtained based on the potential analysis considering the contact state combined with the wrist compliance space. The effectiveness of the proposed method is experimentally verified by using the peg with various shapes and sizes."
6D Pose Estimation Based on 3D Edge Binocular Reprojection Optimization for Robotic Assembly,"Dong Li, Quan Mu, Yilin Yuan, Shiwei Wu, Hualin Hong, Ye Tian, Qian Jiang, Fei Liu","Chongqing University,Foreign Environmental Cooperation Center, Ministry of Ecology an",Factory/Assembly Automation,"Accurate 6D pose estimation of object is important for robot assembly. This letter presents a novel method for achieving high precision 6D pose estimation by exploiting the reprojection of 3D edges onto binocular RGB image pairs. Our proposed method encompasses three phases: detection, pose initialization, and pose refinement. In the detection phase, an existing detector is employed to identify the objects within the image pairs. Subsequently, the object image patch of interest is extracted and fed into an encoder-decoder network that leverages edge maps and RGB images for the purpose of initial pose estimation. To refine the initial pose and achieve precise 6D pose estimation, we introduce a novel binocular edge-map-based nonlinear optimization technique. Our primary contributions entail an improved initial pose estimation network and a novel pose optimization technique. The improved network is dedicated to enhancing the accuracy of initial pose estimation, while the optimization technique focuses on refining the precision of the estimations. Experimental results demonstrate the effectiveness of our method, yielding an average translation precision of 0.48 mm and rotation precision of 0.45 degrees. Consequently, our proposed method can be seamlessly integrated into robotic manipulation platforms to successfully execute diverse assembly tasks."
ASAP: Automated Sequence Planning for Complex Robotic Assembly with Physical Feasibility,"Yunsheng Tian, Karl Willis, Bassel Al Omari, Jieliang Luo, Pingchuan Ma, Yichen Li, Farhad Javid, Edward Gu, Joshua Jacob, Shinjiro Sueda, Hui Li, Sachin Chitta, Wojciech Matusik","MIT,Autodesk,University of Waterloo,Autodesk Research,MIT CSAIL,Texas A&M University,Autodesk Inc.",Factory/Assembly Automation,"The automated assembly of complex products requires a system that can automatically plan a physically feasible sequence of actions for assembling many parts together. In this paper, we present ASAP, a physics-based planning approach for automatically generating such a sequence for general-shaped assemblies. ASAP accounts for gravity to design a sequence where each sub-assembly is physically stable with a limited number of parts being held and a support surface. We apply efficient tree search algorithms to reduce the combinatorial complexity of determining such an assembly sequence. The search can be guided by either geometric heuristics or graph neural networks trained on data with simulation labels. Finally, we show the superior performance of ASAP at generating physically realistic assembly sequence plans on a large dataset of hundreds of complex product assemblies. We further demonstrate the applicability of ASAP on both simulation and real-world robotic setups. Project website: asap.csail.mit.edu"
Simulation-Based Approach for Automatic Roadmap Design in Multi-AGV Systems,"Tena Žužek, Rok Vrabič, Andrej Zdesar, Gašper Škulj, Igor Banfi, Matevž Bošnak, Viktor Zaletelj, Gregor Klancar","University of Ljubljana,Faculty of Mechanical Engineering, University of Ljubljana,Epilog d.o.o.,Faculty of Electrical Engineering, University of Ljubljana",Factory/Assembly Automation,"This paper addresses the problem of establishing efficient intralogistic systems, focusing on the generation of roadmaps on a given layout and the coordination of multiple Automated Guided Vehicles (AGVs). A simulation-based approach for automatic roadmap design is proposed. An event-based simulator is developed that uses ant-colony inspired optimization to generate roadmaps tailored to the specific characteristics of a given intralogistic problem, i.e., the plant layout, fleet size, statistical description of tasks, dispatching algorithm, etc. The generated solutions are evaluated with a Multi-Agent Path Finding (MAPF) simulator that uses a Safe Interval Path Planning (SIPP) algorithm. By analysing the system throughput, the optimal fleet size for the system is proposed. The approach is validated through various examples and benchmarked against existing methods in the literature."
MM4MM: Map Matching Framework for Multi-Session Mapping in Ambiguous and Perceptually-Degraded Environments,"Zhenyu Wu, Wei Wang, Chunyang Zhao, Yufeng Yue, Jun Zhang, Hongming Shen, Danwei Wang","Nanyang Technological University,Beijing Institute of Technology",Factory/Assembly Automation,"Multi-session mapping serves as the pre-requisite for autonomous robots to fulfill various long-term tasks (e.g., map updating, navigation, collaboration). However, it is challenging to implement multi-session mapping in enclosed or partially enclosed ambiguous environments (e.g., long corridors, industrial warehouses). Existing solutions either depend heavily on the matching of elementary geometric features (e.g., points, lines, and planes), which tends to fail in environments with ambiguous geometric features; or depend on the given guess of the initial transformation matrix of multiple single-session maps, which is not always obtainable and accurate enough. The ambient magnetic field has exhibited ubiquity and high distinctiveness at different location, which makes it suitable for estimating the initial transformation matrix. Thus, this paper proposes a novel probabilistic magnetic-aware Map Matching framework for Multi-session Mapping, namely MM4MM, to estimate the relative transformation of multiple single-session maps and to build the globally consistent maps in ambiguous and perceptually-degraded environments. The key novelties of this work are the designing of the hierarchical probabilistic map matching framework and the Particle Swarm Optimization strategy to associate the magnetic data of multiple sessions. Evaluations on both simulated and real world experiments demonstrate the greatly improved utility, accuracy, and robustness of multi-session mapping over the comparative methods."
Learning Generalizable Patrolling Strategies through Domain Randomization of Attacker Behaviors,"Carlos Diaz Alvarenga, Nicola Basilico, Stefano Carpin","University of California at Merced,University of Milan,University of California, Merced",Factory/Assembly Automation,"Graph-patrolling problems in the adversarial domain typically embed models and assumptions about how hostile events, from which an environment must be protected, are generated at a specific time and location. Relying upon such attacker models prevents algorithms from synthesizing strategies that can generalize in different settings, providing good performance under different and uncertain scenarios. In this paper, we propose a first method to deal with adversarial patrolling using a data driven approach. We cast the problem in an RL setting where the reward function is based on the ability to neutralize attacks that can follow an unknown strategy and that, hence, can be viewed as a black box component. We apply a policy gradient framework for optimizing action probabilities under such a reward model showing how effective patrolling strategies can be obtained from repeated attack-defense interactions between a patrolling agent and an attacker. Our results show that the data driven patroller can effectively provide protection against multiple, diverse attacker behaviors."
Combining Coordination and Independent Coverage in Multirobot Graph Patrolling,"Carlos Diaz Alvarenga, Nicola Basilico, Stefano Carpin","University of California at Merced,University of Milan,University of California, Merced",Factory/Assembly Automation,"Graph patrolling algorithms provide effective strategies to coordinate mobile robots in the context of autonomous surveillance of valuable assets. Optimizing patrolling strategies often aims at bounding the time between subsequent visits to a vertex -- a measure known in literature as idleness. In thedomain of multi-robot patrolling, two approaches have received the most attention thus far. The first involves coordinating all robots to follow a shared patrolling strategy covering the entire graph, while the second approach partitions the environment into disjoint areas that are then assigned to individual robots. Starting from these existing solutions, in this paper we introduce a new method bridging these two complementary approaches. Our technique splits the vertices of the graph into a partition that includes a shared portion of the environment patrolled collectively by all robots, along with disjoint areas allocated exclusively to individual robots. This problem in formulated in terms of minimizing the maximum weighted idleness of the graph and is shown to be NP-hard. Then, we describe an exact solution for the problem and also propose various heuristics to efficiently compute solutions for large problem instances. We evaluate and compare the proposed techniques in simulation and show that our methods in most cases produce better patrolling strategies when compared to classic solutions. Moreover, for small problem instances where the exact solution can be found, we demonstrate that our proposed heuristic has a competitive performance ratio."
Longitudinal Control Volumes: A Novel Centralized Estimation and Control Framework for Distributed Multi-Agent Sorting Systems,"James Maier, Prasanna Sriganesh, Matthew Travers",Carnegie Mellon University,Factory/Assembly Automation,"Centralized control of a multi-agent system improves upon distributed control especially when multiple agents share a common task e.g., sorting different materials in a recycling facility. Traditionally, each agent in a sorting facility is tuned individually which leads to suboptimal performance if one agent is less efficient than the others. Centralized control overcomes this bottleneck by leveraging global system state information, but it can be computationally expensive. In this work, we propose a novel framework called Longitudinal Control Volumes (LCV) to model the flow of material in a recycling facility. We then employ a Kalman Filter that incorporates local measurements of materials into a global estimation of the material flow in the system. We utilize a model predictive control algorithm that optimizes the rate of material flow using the global state estimate in real-time. We show that our proposed framework outperforms distributed control methods by 40-100 percent in simulation and physical experiments."
A Safety-Adapted Loss for Pedestrian Detection in Autonomous Driving,"Maria Lyssenko, Piyush Pimplikar, Maarten Bieshaar, Farzad Nozarian, Rudolph Triebel","Robert Bosch GmbH, University of Munich,Robert Bosch GmbH, Corporate Research, Germany,Robert Bosch GmbH,DFKI,German Aerospace Center (DLR)",Intelligent Transportation Systems II,"In safety-critical domains like autonomous driving (AD), errors by the object detector may endanger pedestrians and other vulnerable road users (VRU). As raw evaluation metrics are not an adequate safety indicator, recent works leverage domain knowledge to identify safety-relevant VRU and back-annotate the criticality of the interaction to the object detector. However, those approaches do not consider the safety factor in the deep neural network (DNN) training process. Thus, state-of-the-art DNN penalize all misdetections equally irrespective of their importance for the safe driving task. Hence, to mitigate the occurrence of safety-critical failure cases like false negatives, a safety-aware training strategy is needed to enhance the detection performance for critical pedestrians. In this paper, we propose a novel, safety-adapted loss variation that leverages the estimated per-pedestrian criticality during training. Therefore, we exploit the reachable set-based time-to-collision (TTC-RSB) metric from the motion domain along with distance information to account for the worst-case threat. Our evaluation results using RetinaNet and FCOS on the nuScenes dataset demonstrate that training the models with our safety-adapted loss function mitigates the misdetection of safety-critical pedestrians with robust performance for the general case, i.e., safety-irrelevant pedestrians."
PCB-RandNet: Rethinking Random Sampling for LiDAR Semantic Segmentation in Autonomous Driving Scene,"Xian-feng Han, Huixian Cheng, Hang Jiang, Dehong He, Guo-qiang Xiao",Southwest University,Intelligent Transportation Systems II,"Fast and efficient semantic segmentation of large-scale LiDAR point clouds is a fundamental problem in autonomous driving. To achieve this goal, the existing point-based methods mainly choose to adopt Random Sampling strategy to process large-scale point clouds. However, our quantative and qualitative studies have found that Random Sampling may be less suitable for the autonomous driving scenario, since the LiDAR points follow an uneven or even long-tailed distribution across the space, which prevents the model from capturing sufficient information from points in different distance ranges and reduces the model's learning capability. To alleviate this problem, we propose a new Polar Cylinder Balanced Random Sampling method that enables the downsampled point clouds to maintain a more balanced distribution and improve the segmentation performance under different spatial distributions. In addition, a sampling consistency loss is introduced to further improve the segmentation performance and reduce the model's variance under different sampling methods. Extensive experiments confirm that our approach produces excellent performance on both SemanticKITTI and SemanticPOSS benchmarks, achieving a 2.8% and 4.0% improvement, respectively."
STT: Stateful Tracking with Transformers for Autonomous Driving,"Longlong Jing, Ruichi Yu, Xu Chen, Zhengli Zhao, Shiwei Sheng, Colin Graber, Qi Chen, Qinru Li, Shangxuan Wu, Han Deng, Sangjin Lee, Chris Sweeney, Qiurui He, Wei-chih Hung, Tong He, Xingyi Zhou, Farshid Moussavi, Zijian Guo, Yin Zhou, Mingxing Tan, Weilong Yang, Congcong Li","Waymo,UCI,Johns Hopkins University,University of California San Diego,Waymo LLC,Google Research,Waymo Research,Waymo Inc.",Intelligent Transportation Systems II,"Tracking objects in three-dimensional space is critical for autonomous driving. To ensure safety while driving, the tracker must be able to reliably track objects across frames and accurately estimate their states such as velocity and acceleration in the present. Existing works frequently focus on the association task while either neglecting the modelâ€™s performance on state estimation or deploying complex heuristics to predict the states. In this paper, we propose STT, a Stateful Tracking model built with Transformers, that can consistently track objects in the scenes while also predicting their states accurately. STT consumes rich appearance, geometry, and motion signals through long term history of detections and is jointly optimized for both data association and state estimation tasks. Since the standard tracking metrics like MOTA and MOTP do not capture the combined performance of the two tasks in the wider spectrum of object states, we extend them with new metrics called S-MOTA and MOTPS that address this limitation. STT achieves competitive real-time performance on the Waymo Open Dataset."
SmartCooper: Vehicle Collaborative Perception under Adaptive Fusion and Judger Mechanism,"Yuang Zhang, Haonan An, Zhengru Fang, Guowen Xu, Yuan Zhou, Xianhao Chen, Yuguang Fang","Tsinghua University,Nanyang Technological University,City University of Hong Kong,The University of Hong Kong,City Universty of Hong Kong",Intelligent Transportation Systems II,"In recent years, autonomous driving has garnered significant attention due to its potential for improving road safety through collaborative perception among connected and autonomous vehicles (CAVs). However, time-varying channel variations in vehicular transmission environments demand dynamic allocation of communication resources. Moreover, in the context of collaborative perception, it is important to recognize that not all CAVs contribute valuable data, and some CAV data even have detrimental effects on collaborative perception. In this paper, we introduce SmartCooper, an adaptive collaborative perception framework that incorporates communication optimization and a judger mechanism to facilitate CAV data fusion. Our approach begins with optimizing the connectivity of vehicles while considering communication constraints. We then train a learnable encoder to dynamically adjust the compression ratio based on the channel state information (CSI). Subsequently, we devise a judger mechanism to filter the detrimental image data reconstructed by adaptive decoders. We evaluate the effectiveness of our proposed algorithm on the OpenCOOD platform. Our results demonstrate a substantial reduction in communication costs by 23.10% compared to the non-judger scheme. Additionally, we achieve a significant improvement on the average precision of Intersection over Union (AP@IoU) by 7.15% compared with state-of-the-art schemes."
A Neural-Evolutionary Algorithm for Autonomous Transit Network Design,"Andrew Holliday, Gregory Dudek",McGill University,Intelligent Transportation Systems II,"Planning a public transit network is a challenging optimization problem, but essential in order to realize the benefits of autonomous buses. We propose a novel algorithm for planning networks of routes for autonomous buses. We first train a graph neural net model as a policy for constructing route networks, and then use the policy as one of several mutation operators in a evolutionary algorithm. We evaluate this algorithm on a standard set of benchmarks for transit network design, and find that it outperforms the learned policy alone by up to 20% and a plain evolutionary algorithm approach by up to 53% on realistic benchmark instances."
UDE-Based Robust Control of a Quadrotor-Slung-Load System,"Yanhu Wang, Gan Yu, Wei Xie, Weidong Zhang, Carlos Silvestre","Shanghai Jiao Tong University,Shanghai JiaoTong University,University of Macau",Intelligent Transportation Systems II,"This article addresses the robust trajectory tracking problem for a Quadrotor-Slung-Load System (QSLS), which consists of a point-mass load and a rigid-body quadrotor connected by an inelastic cable. To construct the controller, we employ the backstepping technique and propose an Uncertainty and Disturbance Estimator (UDE) to compensate for uncertainties arising from imprecise model parameters and exogenous time-varying disturbances affecting both quadrotor and load. The main feature of the UDE is its ability to convert the robust control problem into a low-pass filter design in the frequency domain, which generates an estimate of lumped uncertainties. To streamline the design process, we utilize a coordinate transformation strategy that converts the QSLS into a configuration that resembles the dynamics of a typical quadrotor system. The proposed controller ensures uniformly ultimate boundedness of closed-loop errors in the presence of time-varying exogenous disturbances, while guaranteeing asymptotic stability when disturbances are zero. Finally, we present comprehensive simulation and experimental results to validate the effectiveness and robustness of the proposed solution."
Are You a Robot? Detecting Autonomous Vehicles from Behavior Analysis,"Fabio Maresca, Filippo Grazioli, Antonio Albanese, Vincenzo Sciancalepore, Gianpiero Negri, Xavier Costa","NEC Laboratories Europe GmbH,Flyhound Co.,Amazon Global Robotics - EU Innovation Lab,NEC Laboratories Europe",Intelligent Transportation Systems II,"The tremendous hype around autonomous driving is eagerly calling for emerging and novel technologies to support advanced mobility use cases. As car manufactures keep developing SAE level 3+ systems to improve the safety and comfort of passengers, traffic authorities need to establish new procedures to manage the transition from human-driven to fully- autonomous vehicles while providing a feedback-loop mechanism to fine-tune envisioned autonomous systems. Thus, a way to automatically profile autonomous vehicles and differentiate those from human-driven ones is a must. In this paper, we present a fully-fledged framework that monitors active vehicles using camera images and state information in order to determine whether vehicles are autonomous, without requiring any active notification from the vehicles themselves. Essentially, it builds on the cooperation among vehicles, which share their data acquired on the road feeding a machine learning model to identify autonomous cars. We extensively tested our solution and created the NexusStreet dataset, by means of the CARLA simulator, employing an autonomous driving control agent and a steering wheel maneuvered by licensed drivers. Experiments show it is possible to discriminate the two behaviors by analyzing video clips with an accuracy of âˆ¼ 80%, which improves up to âˆ¼93% when the targetâ€™s state information is available. Lastly, we deliberately degraded the state to observe how the framework performs under non-ideal data collection conditions."
RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud,"Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu","Royal College of Art,University of Edinburgh,University of Cambridge",Intelligent Transportation Systems II,"Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art. We release our code and model at https://github.com/LJacksonPan/RaTrack."
Mixed Traffic Control and Coordination from Pixels,"Michael Villarreal, Bibek Poudel, Jia Pan, Weizi Li","University of Tennessee, Knoxville,University of Tennessee Knoxville,University of Hong Kong",Intelligent Transportation Systems II,"Traffic congestion is a persistent problem in our society. Existing methods for traffic control have proven futile in alleviating current congestion levels leading researchers to explore ideas with robot vehicles given the increased emergence of vehicles with different levels of autonomy on our roads. This gives rise to mixed traffic control, where robot vehicles regulate human-driven vehicles through reinforcement learning (RL). However, most existing studies use precise observations that require domain expertise and hand engineering for each road networkâ€™s observation space. Additionally, precise observations use global information, such as environment outflow, and local information, i.e., vehicle positions and velocities. Obtaining this information requires updating existing road infrastructure with vast sensor environments and communication to potentially unwilling human drivers. We consider image observations, a modality that has not been extensively explored for mixed traffic control via RL, as the alternative: 1) images do not require a complete re-imagination of the observation space from environment to environment; 2) images are ubiquitous through satellite imagery, in-car camera systems, and traffic monitoring systems; and 3) images only require communication to equipment. In this work, we show robot vehicles using image observations can achieve competitive performance to using precise information on environments, including ring, figure eight, intersection, merge, and bottleneck. In certain scenarios, our approach even outperforms using precision observations, e.g., up to 8% increase in average vehicle velocity in the merge environment, despite only using local traffic information as opposed to global traffic information."
Exoskeleton-Mediated Physical Human-Human Interaction for a Sit-To-Stand Rehabilitation Task,"Lorenzo Vianello, Emek Baris Kucuktabak, Matthew Short, Clément Lhoste, Lorenzo Amato, Kevin Lynch, Jose Luis Pons","Shirley Ryan Ability Lab,Northwestern University, Shirley Ryan Ability Lab,Northwestern University, Shirley Ryan AbilityLab,Northwestern University,Scuola Superiore Sant'Anna,Shirley Ryan AbilityLab",Medical Robotics,"Sit-to-Stand (StS) is a fundamental daily activity that can be challenging for stroke survivors due to strength, motor control, and proprioception deficits in their lower limbs. Existing therapies involve repetitive StS exercises, but these can be physically demanding for therapists while assistive devices may limit patient participation and hinder motor learning. To address these challenges, this work proposes the use of two lower-limb exoskeletons to mediate physical interaction between therapists and patients during a StS rehabilitative task. This approach offers several advantages, including improved therapist-patient interaction, safety enforcement, and performance quantification. The whole body control of the two exoskeletons transmits online feedback between the two users, but at the same time assists in movement and ensures balance, and thus helping subjects with greater difficulty. In this study we present the architecture of the framework, presenting and discussing some technical choices made in the design."
Intraoperatively Iterative Hough Transform Based In-Plane Hybrid Control of Arterial Robotic Ultrasound for Magnetic Catheterization,"Zhengyang Li, Magejiang Yeerbulati, Qingsong Xu",University of Macau,Medical Robotics,"This paper presents an intraoperatively iterative Hough transform (IHT) based in-plane hybrid control of extracorporeal ultrasound (US) guided magnetic catheterization for arterial intervention. One uniqueness lies in that both control and tracking of the arterial robotic ultrasound end-effector have been implemented to improve performance. Firstly, the magnetic catheter model and hybrid visual/force servoing control scheme of the extracorporeal ultrasound-integrated tracking arm (EUTA) are derived based on the interaction Jacobian matrix and impedance modeling. Meanwhile, we implement a tracking method of in-plane ultrasound catheter's tip and detection of vascular boundaries utilizing intensity-level iterative Hough-transform with Iterative End-Ponit Fitting (IEPF). The effectiveness of the proposed control and tracking method has been verified by conducting in vitro experimental studies for catheter steering of a soft tissue-imitating phantom. Results show that an average steering error of 0.56 mm and signal-to-noise-ratio (SNR) of 12.2 are obtained for the ultrasound imaging at high synchronization along with a low target lost rate (15.8%) and constant-force tracking (2.50$pm$1.02 N)."
Efficient Model Learning and Adaptive Tracking Control of Magnetic Micro-Robots for Non-Contact Manipulation,"Yongyi Jia, Shu Miao, Junjian Zhou, Niandong Jiao, Lianqing Liu, Xiang Li","Tsinghua University,Shenyang Institute of Automation, Chinese Academy of Sciences,Shenyang Institute of Automation",Medical Robotics,"Magnetic microrobots can be navigated by an external magnetic field to autonomously move within living organisms with complex and unstructured environments. Potential applications include drug delivery, diagnostics, and therapeutic interventions. Existing techniques commonly impart magnetic properties to the target object, or drive the robot to contact and then manipulate the object, both probably inducing physical damage. This paper considers a non-contact formulation, where the robot spins to generate a repulsive field to push the object without physical contact. Under such a formulation, the main challenge is that the motion model between the input of the magnetic field and the output velocity of the target object is commonly unknown and difficult to analyze. To deal with it, this paper proposes a data-driven-based solution. A neural network is constructed to efficiently estimate the motion model. Then, an approximate model-based optimal control scheme is developed to push the object to track a time-varying trajectory, maintaining the non-contact with distance constraints. Furthermore, a straightforward planner is introduced to assess the adaptability of non-contact manipulation in a cluttered unstructured environment. Experimental results are presented to show the tracking and navigation performance of the proposed scheme."
Design and Implementation of a Robotized Hand-Held Dissector for Endoscopic Pulmonary Endarterectomy,"Runfeng Zhu, Xilong Hou, Wei Huang, Lei Du, Zhong Wu, Hongbin Liu, Henry Chu, Qing Xiang Zhao","The Hong Kong Polytechnic University,Hong Kong Institute of Science and Innovation Chinese Academy of,CAIR,Sichuan University,West China Hospital, Sichuan University,Hong Kong Institute of Science & Innovation, Chinese Academy of ,Hong Kong Institute of Science & Innovation, Centre for Artifici",Medical Robotics,"Severe chronic pulmonary endarterectomy needs a dissector to delicately remove proliferative intima located in the depth of the pulmonary artery. This work proposed a novel endoscopic robotized steerable dissector for this surgery, enabling easier access to curved deep artery branches. The handheld surgical dissector also provides suction and visualization for surgeons to enhance effectiveness. The steerable section is a cable-driven hinged structure, and through an antagonistic mechanism regulating the cable tension, the overall stiffness is adjusted to adapt to various surroundings. The mapping between actuation space and shape configuration and tip force estimation model are respectively established for further closed-loop control scheme, achieving adaptive positioning and safe surgery. Experiments first demonstrate the feasibility of the proposed models and ex vivo trials validated the usage and effectiveness of the robotized dissector."
Colibri5: Real-Time Monocular 5-DoF Trocar Pose Tracking for Robot-Assisted Vitreoretinal Surgery,"Shervin Dehghani, Michael Sommersperger, Mahdi Saleh, Alireza Alikhani, Benjamin Busam, Peter Gehlbach, Iulian Iordachita, Nassir Navab, M. Ali Nasseri","TUM,Technical University of Munich,Technical University Munich,Augen- klinik und Poliklinik, Klinikum rechts der Isar der Techn,Johns Hopkins Medical Institute,Johns Hopkins University,TU Munich,Technische Universitaet Muenchen",Medical Robotics,"Retinal surgery is a complex medical procedure that requires high precision dexterity to perform delicate instrument maneuvers with sub-millimeter accuracy. Minimizing the manual tremor and achieving precise and repeatable execution of surgical tasks has motivated the development of robotic platforms to overcome the limitations of manual surgery. However, specific tasks, such as instrument insertion through the trocar, are more challenging in robotic surgery than in conventional manual procedures since the robot control is often optimized for navigation inside the eye. This challenges the integration of robotic systems, creating a high cognitive load on the operator and prolonging the surgery time. Moreover, misalignment of the robot's remote center of motion (RCM) and trocar position during the procedure can lead to excessive forces between the instrument and the trocar, potentially causing patient trauma. Precise and rapid localization of the trocars enables the automation of the insertion procedure and dynamic compensation of eye motion. In this work, we present a real-time marker-less method for 3D pose tracking of trocar, achieved with only a single monocular camera. Our experiments show promising results towards real-time trocar pose estimation and tracking, achieving an average error of 3 degrees in trocar orientation estimation, with an average processing time of 15 fps. This could serve as a foundation to improve robotic systems' automation, integration, and efficiency of robotic systems for retinal surgery. The dataset created for this work is made publicly available."
Hybrid Volitional Control of a Robotic Transtibial Prosthesis Using a Phase Variable Impedance Controller,"Ryan Posh, Jonathan Allen Tittle, David Kelly, Jim Schmiedeler, Patrick Wensing",University of Notre Dame,Medical Robotics,"For robotic transtibial prosthesis control, the global tibia kinematics can be used to monitor gait cycle progression and command smooth and continuous actuation. In this work, these global tibia kinematics define a phase variable impedance controller (PVIC), which is implemented as the non-volitional base controller within a hybrid volitional control framework (PVI-HVC). The gait progression estimation and biomechanic performance of one able-bodied individual walking on a robotic ankle prosthesis via a bypass adapter are compared for three control schemes: benchmark passive controller, PVIC, and PVI-HVC. The different actuation of each had a direct effect on the global tibia kinematics, but the average deviation between the estimated and ground truth gait percentages were 1.6%, 1.8%, and 2.1%, respectively, for each controller. Both PVIC and PVI-HVC produced good agreement with able-bodied kinematic and kinetic references. As designed, PVI-HVC results were similar to those of PVIC when the user used low volitional intent, but yielded higher peak plantarflexion, peak torque, and peak power when the user commanded high volitional input in late stance. This additional torque and power also allowed the user to volitionally and continuously achieve activities beyond level walking, such as ascending ramps, avoiding obstacles, standing on tip-toes, and tapping the foot. In this way, PVI-HVC offers the kinetic and kinematic performance of the PVIC during level ground walking, along with the freedom to volitionally pursue alternative activities."
Observer-Based Distributed MPC for Collaborative Quadrotor-Quadruped Manipulation of a Cable-Towed Load,"Shaohang Xu, Yi'an Wang, Wentao Zhang, Chin Pang Ho, Lijun Zhu","Huazhong University of Science and Technology,City University of Hong Kong",Multi-Robot Systems,"This paper presents a collaborative quadrotor-quadruped robot system for the manipulation of a cable-towed payload. In particular, we aim to solve the challenge from the unknown dynamics of the cable-towed payload. To this end, we first propose novel dynamic models for both the quadrotor and the quadruped robot, taking into account the nonlinear robot dynamics and the uncertainties associated with the cable-towed load. Moreover, we design observers for the hybrid interaction between the robots and the payload. Theoretically, the convergence of these observers is analyzed using Lyapunov functions under mild technical assumptions. Finally, we seamlessly integrate the dynamics models and the observers into a distributed Model Predictive Control (MPC) framework with kinematics limitations and collision avoidance constraints. The proposed system is validated through challenging field experiments in indoor and outdoor environments, involving push disturbances, varying and unknown payloads, uneven terrains, etc."
Learning for Dynamic Subteaming and Voluntary Waiting in Heterogeneous Multi-Robot Collaborative Scheduling,"Williard Joshua Jose, Hao Zhang",University of Massachusetts Amherst,Multi-Robot Systems,"Coordinating heterogeneous robots is essential for autonomous multi-robot teaming. To execute a set of dependent tasks as quickly as possible, and to complete tasks that cannot be addressed by individual robots, it is necessary to form subteams that can collaboratively finish the tasks. It is also advantageous for robots to wait for teammates and tasks to become available in order to form better subteams or reduce the overall completion time. To enable both abilities, we introduce a new graph learning approach that formulates heterogeneous collaborative scheduling as a bipartite matching problem that maximizes a reward matrix learned via imitation learning. We design a novel graph attention transformer network (GATN) that represents the problem of collaborative scheduling as a bipartite graph, and integrates both local and global graph information to estimate the reward matrix using graph attention networks and transformers. By relaxing the constraint of one-to-one correspondence in bipartite matching, our approach allows multiple robots to address the same task as a subteam. Our approach also enables voluntary waiting by introducing an idle task that the robots can select to wait. Experimental results have shown that our approach well addresses heterogeneous collaborative scheduling with dynamic subteam formation and voluntary waiting, and outperforms the previous and baseline methods."
Uncertainty-Bounded Active Monitoring of Unknown Dynamic Targets in Road-Networks with Minimum Fleet,"Shuaikang Wang, Yiannis Kantaros, Meng Guo","Peking University,Washington University in St. Louis",Multi-Robot Systems,"Fleets of unmanned robots can be beneficial for the long-term monitoring of large areas, e.g., to monitor wild flocks, detect intruders, search and rescue. Monitoring numerous dynamic targets in a collaborative and efficient way is a challenging problem that requires online coordination and information fusion. The majority of existing works either assume a passive all-to-all observation model to minimize the summed uncertainties over all targets by all robots, or optimize over the jointed discrete actions while neglecting the dynamic constraints of the robots and unknown behaviors of the targets. This work proposes an online task and motion coordination algorithm that ensures an explicitly-bounded estimation uncertainty for the target states, while minimizing the average number of active robots. The robots have a limited-range perception to actively track a limited number of targets simultaneously, of which their future control decisions are all unknown. It includes: (i) the assignment of monitoring tasks, modeled as a flexible size multiple vehicle routing problem with time windows (m-MVRPTW), given the predicted target trajectories with uncertainty measure in the road-networks; (ii) the nonlinear model predictive control (NMPC) for optimizing the robot trajectories under uncertainty and safety constraints. It is shown that the robots can switch between active and inactive roles dynamically online as required by the unknown monitoring task. The proposed methods are validated via large-scale simulations of up to $100$ robots and targets."
Do We Run Large-Scale Multi-Robot Systems on the Edge? More Evidence for Two-Phase Performance in System Size Scaling,"Jonas Kuckling, Robin Luckey, Viktor Avrutin, Andrew Vardy, Andreagiovanni Reina, Heiko Hamann","University of Konstanz,Institute of Computer Engineering, University of Lübeck,Institute for Systems Theory and Automatic Control, University o,Memorial University of Newfoundland,Université Libre de Bruxelles",Multi-Robot Systems,"With increasing numbers of mobile robots arriving in real-world applications, more robots coexist in the same space, interact, and possibly collaborate. Methods to provide such systems with system size scalability are known, for example, from swarm robotics. Example strategies are self-organizing behavior, a strict decentralized approach, and limiting the robot-robot communication. Despite applying such strategies, any multi-robot system breaks above a certain critical system size (i.e., number of robots) as too many robots share a resource (e.g., space, communication channel). We provide additional evidence based on simulations, that at these critical system sizes, the system performance separates into two phases: nearly optimal and minimal performance. We speculate that in real-world applications that are configured for optimal system size, the supposedly high-performing system may actually live on borrowed time as it is on a transient to breakdown. We provide two modeling options (based on queueing theory and a population model) that may help to support this reasoning."
Asynchronous Distributed Smoothing and Mapping Via On-Manifold Consensus ADMM,"Daniel Mcgann, Kyle Lassak, Michael Kaess","Carnegie Mellon University,Astrobotic Technology, Inc.",Multi-Robot Systems,"In this paper we present a fully distributed, asynchronous, and general purpose optimization algorithm for Consensus Simultaneous Localization and Mapping (CSLAM). Multi-robot teams require that agents have timely and accurate solutions to their state as well as the states of the other robots in the team. To optimize this solution we develop a CSLAM back-end based on Consensus ADMM called MESA (Manifold, Edge-based, Separable ADMM). MESA is fully distributed to tolerate failures of individual robots, asynchronous to tolerate communication delays and outages, and general purpose to handle any CSLAM problem formulation. We demonstrate that MESA exhibits superior convergence rates and accuracy compare to existing state-of-the art CSLAM back-end optimizers."
Multi-Sample Long Range Path Planning under Sensing Uncertainty for Off-Road Autonomous Driving,"Matt Schmittle, Rohan Baijal, Brian Hou, Siddhartha Srinivasa, Byron Boots",University of Washington,Planning under Uncertainty III,"We focus on the problem of long-range dynamic replanning for off-road autonomous vehicles, where a robot plans paths through a previously unobserved environment while continuously receiving noisy local observations. An effective approach for planning under sensing uncertainty is deter- minization, where one converts a stochastic world into a de- terministic one and plans under this simplification. This makes the planning problem tractable, but the cost of following the planned path in the real world may be different than in the determinized world. This causes collisions if the determinized world optimistically ignores obstacles, or causes unnecessarily long routes if the determinized world pessimistically imagines more obstacles. We aim to be robust to uncertainty over potential worlds while still achieving the efficiency benefits of determinization. We evaluate algorithms for dynamic replanning on a large real-world dataset of challenging long-range planning problems from the DARPA RACER program. Our method, Dynamic Replanning via Evaluating and Aggregating Multiple Samples (DREAMS), outperforms other determinization-based approaches in terms of combined traversal time and collision cost. https://sites.google.com/cs.washington.edu/dreams/"
Perceptual Factors for Environmental Modeling in Robotic Active Perception,"David Morilla Cabello, Jonas Westheider, Marija Popovic, Eduardo Montijano","Universidad de Zaragoza,University Bonn,University of Bonn",Planning under Uncertainty III,"Accurately assessing the potential value of new sensor observations is a critical aspect of planning for active perception. This task is particularly challenging when reasoning about high-level scene understanding using measurements from vision-based neural networks. Due to appearance-based reasoning, the measurements are susceptible to several environmental effects such as the presence of occluders, variations in lighting conditions, and redundancy of information due to similarity in appearance between nearby viewpoints. To address this, we propose a new active perception framework incorporating an arbitrary number of perceptual effects in planning and fusion. Our method models the correlation with the environment by a set of general functions termed perceptual factors to construct a perceptual map, which quantifies the aggregated influence of the environment on candidate viewpoints. This information is seamlessly incorporated into the planning and fusion processes by adjusting the uncertainty associated with measurements to weigh their contributions. We evaluate our perceptual maps in a simulated environment that reproduces environmental conditions common in robotics applications. Our results show that, by accounting for environmental effects within our perceptual maps, we improve the state estimation by correctly selecting the viewpoints and considering the measurement noise correctly when affected by environmental factors. We furthermore deploy our approach on a ground robot to showcase its applicability for real-world active perception missions."
Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment,"Gokul Puthumanaillam, Xiangyu Liu, Negar Mehr, Melkior Ornik","University of Illinois Urbana-Champaign,University of Cyprus,University of California Berkeley",Planning under Uncertainty III,"Optimal decision-making presents a significant challenge for autonomous systems operating in uncertain, stochastic and time-varying environments. Environmental variability over time can significantly impact the system's optimal decision making strategy for mission completion. To model such environments, our work combines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a two-pronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environments. Our results demonstrate superior performance over standard methods, highlighting the framework's effectiveness in stochastic, uncertain, time-varying domains."
Choosing the Right Tool for the Job: Online Decision Making Over SLAM Algorithms,"Samer Nashed, Rod Grupen, Shlomo Zilberstein","University of Massachusetts Amherst,University of Massachusetts",Planning under Uncertainty III,"Nearly all state-of-the-art SLAM algorithms are designed to exploit patterns in data from specific sensing modalities, such as time-of-flight and structured light depth sensors, or RGB cameras. This specialization increases localization accuracy in domains where the given modality detects many high-quality features, but comes at the cost of decreasing performance in other, less favorable environments. For robotic systems that may experience a wide variety of sensing conditions, this difficulty in generalization presents a significant challenge. In this paper, we propose running several computationally cheap SLAM front ends in parallel and choosing the most promising feature set online. This problem is similar to the Algorithm Selection Problem (ASP), but has several complicating factors that preclude application of existing methods. We first provide an extension of the ASP formalism that captures the unique challenges in the SLAM setting, and then, based on this formalism, we propose modeling the SLAM ASP as a partially observable Markov decision process (POMDP). Our experiments show that dynamically selecting SLAM front ends, even myopically, improves localization robustness compared to selecting a static front end, and that using a POMDP policy provides even greater improvement."
ASPIRe: An Informative Trajectory Planner with Mutual Information Approximation for Target Search and Tracking,"Kangjie Zhou, Pengying Wu, Yao Su, Han Gao, Ji Ma, Hangxin Liu, Chang Liu","Peking University,Beijing Institute for General Artificial Intelligence （BIGAI）,Beijing Institute for General Artificial Intelligence (BIGAI)",Planning under Uncertainty III,"This paper proposes an informative trajectory planning approach, namely, adaptive particle filter tree with sigma point-based mutual information reward approximation (ASPIRe), for mobile target search and tracking (SAT) in cluttered environments with limited sensing field of view. We develop a novel sigma point-based approximation to accurately estimate mutual information (MI) for general, non-Gaussian distributions utilizing particle representation of the belief state, while simultaneously maintaining high computational efficiency. Building upon the MI approximation, we develop the Adaptive Particle Filter Tree (APFT) approach with MI as the reward, which features belief state tree nodes for informative trajectory planning in continuous state and measurement spaces. An adaptive criterion is proposed in APFT to adjust the planning horizon based on the expected information gain. Simulations and physical experiments demonstrate that ASPIRe achieves real-time computation and outperforms benchmark methods in terms of both search efficiency and estimation accuracy."
Preprocessing-Based Planning for Utilizing Contacts in Semi-Structured High-Precision Insertion Tasks,"Muhammad Suhail Saleem, Rishi Veerapaneni, Maxim Likhachev",Carnegie Mellon University,Planning under Uncertainty III,"In manipulation tasks like plug insertion or assembly that have low tolerance to errors in pose estimation (errors of the order of 2mm cause task failure), the utilization of touch/contact modality can aid in accurately localizing the object of interest. Motivated by this, in this work we model high-precision insertion tasks as planning problems under pose uncertainty, where we effectively utilize the occurrence of contacts (or the lack thereof) as observations to reduce uncertainty and reliably complete the task. We present a preprocessing-based planning framework for high-precision insertion in repetitive and time-critical settings, where the set of initial pose distributions (identified by a perception system) is finite. The finite set allows us to enumerate the possible planning problems that can be encountered online and preprocess a database of policies. Due to the computational complexity of constructing this database, we propose a general experience-based POMDP solver, E-RTDP-Bel, that uses the solutions of similar planning problems as experience to speed up planning queries and use it to efficiently construct the database. We show that the developed algorithm speeds up database creation by over a factor of 100, making the process computationally tractable. We demonstrate the effectiveness of the proposed framework in a real-world plug insertion task in the presence of port position uncertainty and an assembly task in simulation in the presence of pose uncertainty."
Vision-Based Uncertainty-Aware Motion Planning Based on Probabilistic Semantic Segmentation,"Ralf Römer, Armin Lederer, Samuel Tesfazgi, Sandra Hirche","Technical University of Munich,Technische Universität München",Planning under Uncertainty III,"For safe operation, a robot must be able to avoid collisions in uncertain environments. Existing approaches for motion planning under uncertainties often assume parametric obstacle representations and Gaussian uncertainty, which can be inaccurate or result in trajectories with excessive cost. While visual perception can deliver a more accurate representation of the environment, its use for safe motion planning is limited by the inherent miscalibration of neural networks and the challenge of obtaining adequate datasets. To address these limitations, we propose to employ ensembles of deep semantic segmentation networks trained with massively augmented datasets to ensure reliable probabilistic occupancy information. For avoiding conservatism during motion planning, we directly employ the probabilistic perception in a scenario-based path planning approach. A velocity scheduling scheme is applied to the path to ensure a safe motion despite tracking inaccuracies. We demonstrate the effectiveness of the massive data augmentation in combination with deep ensembles and the proposed scenario-based planning approach in comparisons to state-of-the-art methods and validate our framework in an experiment with a human hand as obstacle."
Chance-Constrained Multi-Robot Motion Planning under Gaussian Uncertainties,"Anne Theurkauf, Justin Kottinger, Nisar Ahmed, Morteza Lahijanian",University of Colorado Boulder,Planning under Uncertainty III,"We consider a chance-constrained multi-robot motion planning problem in the presence of Gaussian motion and sensor noise. Our proposed algorithm, CC-K-CBS, leverages the scalability of kinodynamic conflict-based search (K-CBS) in conjunction with the efficiency of Gaussian belief trees as used in the Belief-A framework, and inherits the completeness guarantees of Belief-A's low-level sampling-based planner. We also develop three different methods for robot-robot probabilistic collision checking, which trade off computation with accuracy. Our algorithm generates motion plans driving each robot from its initial to goal state while accounting for uncertainty evolution with chance-constrained safety guarantees. Benchmarks compare computation time to conservatism of the collision checkers, in addition to characterizing the performance of the planner as a whole. Results show that CC-K-CBS scales up to 30 robots."
Uncertainty-Aware Trajectory Planning: Using Uncertainty Quantification and Propagation in Traversability Prediction of Planetary Rovers,"Reiya Takemura, Genya Ishigami",Keio University,Planning under Uncertainty III,"Motion planning for a planetary rover involves robotic traversability such that the rover can safely travel without mobility hazards. While conventional planners have primarily assessed rover traversability index with predetermined threshold values, the traversability index cannot be precisely predicted because of the measurement uncertainty of onboard mapping and motion uncertainty. This study presents an uncertainty-aware trajectory planning algorithm for the rover on rough and loose terrains. The planning algorithm involves new metrics that quantify heteroscedastic uncertainties in the rover traversability prediction model, which are dependent on terrain characteristics and robotic state/control. Further, uncertainty propagation extends the uncertainty metrics to explicitly consider the growth of uncertainty over time steps. The uncertainty metrics are used to assess tree extensions of the sampling-based search algorithm, enabling the trajectory planner to avoid the unexpected risk of vehicle rollover and extremely high slip. Simulation study confirms that the proposed algorithm achieves up to 20 % reduction of the probability of mobility hazards in real challenging terrains."
Perching and Grasping Using a Passive Dynamic Bioinspired Gripper,"Amir Firouzeh, Jongeun Lee, Hyunsoo Yang, Dongjun Lee, Kyu-Jin Cho","EPFL,Seoul National University,Seoul National University, Biorobotics Laboratory",Joint Mechanism,"The ability to grasp objects broadens the application range of unmanned aerial vehicles (UAVs) by allowing interactions with the environment. The difficulty in performing a mid-air grasp is the high probability of impact between the UAVâ€™s foot and the target. For a successful grasp, the foot must smoothly absorb the energy of impact and simultaneously engage with the target in a short period of time. We present a bioinspired passive dynamic foot in which the claws are actuated solely by the impact energy. Our gripper simultaneously resolves the issue of smooth absorption of the impact energy and fast closure of the claws by linking the motion of an ankle linkage and the claws through soft tendons. We study the dynamics of impact and use the stiffness of the tendon as our design/control parameter to adjust the mechanics of the gripper for smooth recycling of the impact energy. Our gripper closes within 45 milliseconds after initial contact with the impacting object without requiring any controller or actuation energy. An electro-adhesive locking mechanism attached to the tendon locks the claws within 20 milliseconds after reaching closed configuration. We demonstrated the effectiven"
Self-Sensing Feedback Control of an Electrohydraulic Robotic Shoulder,"Clemens Claudio Christoph, Amirhossein Kazemipour, Michel Ryan Vogt, Yu Zhang, Robert Kevin Katzschmann","ETH Zürich,ETH Zurich",Joint Mechanism,"The human shoulder, with its glenohumeral joint, tendons, ligaments, and muscles, allows for the execution of complex tasks with precision and efficiency. However, current robotic shoulder designs lack the compliance and compactness inherent in their biological counterparts. A major limitation of these designs is their reliance on external sensors like rotary encoders, which restrict mechanical joint design and introduce bulk to the system. To address this constraint, we present a bio-inspired antagonistic robotic shoulder with two degrees of freedom powered by self-sensing hydraulically amplified self-healing electrostatic actuators. Our artificial muscle design decouples the high-voltage electrostatic actuation from the pair of low-voltage self-sensing electrodes. This approach allows for proprioceptive feedback control of trajectories in the task space while eliminating the necessity for any additional sensors. We assess the platform's efficacy by comparing it to a feedback control based on position data provided by a motion capture system. The study demonstrates closed-loop controllable robotic manipulators based on an inherent self-sensing capability of electrohydraulic actuators. The proposed architecture can serve as a basis for complex musculoskeletal joint arrangements."
Design and Validation of a Variable Stiffness Spiral Cam Actuator,"Matthew Auer, Suhrud Parag Joglekar, Hyunglae Lee",Arizona State University,Joint Mechanism,"This study presents the design and validation of a variable stiffness actuator incorporating multiple cam mechanisms. The actuator is intended for use in walking assistance, focusing on assisting individuals with diminished ankle function. This study highlights the advantages of variable stiffness actuators over traditional and other modern actuators in mobility assistance. The working principles of the proposed Variable Stiffness Spiral Cam Actuator (VS-SCA) are described, focusing on the cantilever beams with adjustable supports, main cam mechanism, and symmetric support positioning architecture utilizing an Archimedean spiral cam. The design and fabrication process are discussed, considering system design considerations, cantilever beam design, cam design, and spiral cam design. The analytical methodology used for validation is also presented, which connects the subsystems of the actuator and allows for the determination of effective torsional stiffness. The experimental validation showed that the VS-SCA provides a range of stiffness from 20 to 75 Nm/rad for dorsiflexion, necessary for providing ankle assistance during the push-off phase of walking, while maintaining low stiffness (4 - 12 Nm/rad) for plantarflexion not to hinder natural ankle motion in the swing phase."
Hybrid Force-Position Control of an Elastic Tendon-Driven Scrubbing Robot (TEDSR),"Noah Harmatz, Alina Zahra, Amir Abdelmalak, Shivam Purohit, Trevor Shin, Aaron Mazzeo","Rutgers University,Rutgers,Rutgers university",Joint Mechanism,"There is a lack of cleaning robots dedicated to the scrubbing of contaminated surfaces. Contaminated surfaces in domestic and industrial settings typically require manual scrubbing which can be costly or hazardous. To address the opportunity to automate the scrubbing of surfaces, this work focuses on the use of series elastic actuators which can apply consistent trajectories of scrubbing force. Consistent force during scrubbing increases the rate of removal for a contaminant. An elastic robot which has rigid links and low-stiffness joints can perform friction-based cleaning of surfaces with complex geometries while maintaining consistent scrubbing force. This study uses a hybrid force-position control scheme and a low-cost elastic robot to perform scrubbing. This study observes the relationship between joint stiffness in the robot and the disturbance rejection for force-based control during scrubbing. There is growing demand for automated sanitization systems in hospitals, food-processing plants, and other settings where cleanliness of surfaces is important."
Investigation on the Multi-Solution Problem of the Kinetostatics of Cable-Driven Continuum Manipulators,"Yicheng Dai, Zuan Li, Xin Wang, Han Yuan","Harbin Institute of Technology (Shenzhen),Harbin Institute of Technology, Shenzhen,Harbin Institute of Technology",Joint Mechanism,"Cable-driven continuum manipulators have gained considerable attention due to their high dexterity and inherent structural compliance, making them a popular research topic. However, previous studies have overlooked the kinetostatics of these manipulators, which can result in multi-solution problem. This issue is critical as having multiple equilibrium states can lead to erroneous estimations of the manipulator's profile. To address this issue, the kinetostatic model is presented and simulations based on both the interval analysis method and the commonly used floating-point optimization algorithm are conducted under same actuating forces and external loads. Results show that there are multiple solutions to the kinetostatics of cable-driven continuum manipulators with constant cross section or variable cross section. This paper fills a gap in the current literature and offers valuable insights for researchers in the field of cable-driven continuum manipulators."
Optimization Design Method of Tendon-Sheath Transmission Path under Curvature Constraint,"Yanan Li, Weining Lu, Yu Liu, Deshan Meng, Xueqian Wang, Bin Liang","Tsinghua University,Department of Automation, Tsinghua University,Harbin Institute of Technology,Sun Yat-Sen University,Center for Artificial Intelligence and Robotics, Graduate School",Joint Mechanism,"The application requirements of the tendon-sheath mechanism in the field of precision machinery are becoming increasingly extensive. However, the contact friction between the tendon and sheath seriously affects the transmission accuracy. In the case of unavoidable friction, optimizing the tendon transmission path to reduce tension loss and elastic deformation has become an important research direction. In this paper, the influence law of the tendon transmission path on the tension and displacement transmission is obtained using the two parameters related to the curvature of the transmission path: total bending angle and equivalent tendon length. Then, based on the optimal control theory and minimum principle, the different transmission path solutions of the minimum tension loss, the minimum tendon deformation, and the coupling of tension and displacement are obtained; the numerical optimization method verifies the correctness of the proposed theory. Finally, an optimal design of a tendon-constrained synchronous rotation mechanism for the manipulator is carried out, and the linkage performance is greatly improved by optimizing the transmission path."
Stability Analysis of Tendon Driven Continuum Robots and Application to Active Softening,"Quentin Peyron, Jessica Burgner-kahrs","Inria and CRIStAL UMR CNRS ,,,,, University of Lille,University of Toronto",Joint Mechanism,"Tendon driven continuum robots are often considered to navigate through- and operate in cluttered environments. While their compliance allows them to conform safely to obstacles, it leads them also to buckle under tendon actuation. In this work, we perform for the first time an extensive elastic stability analysis of these robots for arbitrary planar designs. The buckling phenomena are investigated and analyzed using bifurcation diagrams, complementing the current state of the art and adding new knowledge about robots composed of $n$ spacer disks. We show the existence of multiple robot configurations with different shapes, achievable with the same actuation inputs. A global stability criterion is also established which links the critical tendon force, until which the robot is stable, to the design parameters. Finally, the buckling phenomena are used to actively soften the robot for a better compromise between compliance and payload. An open loop control strategy is proposed, which can theoretically decrease the stiffness to zero while maintaining the same robot shape. Experimentally, the robot is made 4 times more compliant than it is nominally using tendon actuation only."
Elasto-Static Modelling and Identification of a Deployable Cable-Driven Parallel Robot with Compliant Masts,"Zane Zake, Stephane Caro","IRT Jules Verne,CNRS/LS,N",Joint Mechanism,"Some cable-driven parallel robots (CDPRs) can be rapidly deployed on-site. To achieve such deployability, the fixed frame is usually substituted by four masts. However, not having any rigid fixture between the masts reduces the overall stiffness of the CDPR. This paper introduces a CDPR called Rocaspect, that has four compliant masts. The robot behavior and accuracy is evaluated experimentally and three different mast models are proposed."
Torque Transmission in Double-Tendon Sheath Driven Actuators for Application in Exoskeletons,"Daniel Pérez-suay, Yu Li, Hamid Sadeghian, Abdeldjallil Naceri, Sami Haddadin",Technical University of Munich,Joint Mechanism,"Bowden cables serve as essential components in various mechanical systems, facilitating power transmission from remote actuators to specific destinations. The pretension of Bowden cables profoundly influences system performance, notably in terms of friction. This study investigates the effects of cable pretension and shape on friction and torque efficiency. A custom self-designed testbed, comprising integrated actuator units, pulleys, and a novel pretension mechanism connected by Bowden cables, is utilized to conduct experimental tests under varying parameters. This work adopts an integrated approach of experimentation, modeling, and validation, offering preliminary insights into the torque transmission characteristics of tendon driven actuator systems. Additionally, the precise model exhibits excellent conformity across a broad range of shapes and provides initial insights into hysteresis modeling attributable to cable material properties."
OpenBot-Fleet: A System for Collective Learning with Real Robots,"Matthias Mueller, Samarth Brahmbhatt, Ankur Deka, Quentin Leboutet, David Hafner, Vladlen Koltun","Intel Labs,Intel Corporation",Big Data in Robotics and Automation,"We introduce OpenBot-Fleet, a comprehensive open-source cloud robotics system for navigation. OpenBot-Fleet uses smartphones for sensing, local compute and communication, Google Firebase for secure cloud storage and off-board compute, and a robust yet low-cost wheeled robot to act in real-world environments. The robots collect task data and upload it to the cloud where navigation policies can be learned either offline or online and can then be sent back to the robot fleet. In our experiments we distribute 72 robots to a crowd of workers who operate them in homes, and show that OpenBot-Fleet can learn robust navigation policies that generalize to unseen homes with >80% success rate. OpenBot-Fleet represents a significant step forward in cloud robotics, making it possible to deploy large continually learning robot fleets in a cost-effective and scalable manner. All materials can be found at https://www.openbot.org/"
WOMD-LiDAR: Raw Sensor Dataset Benchmark for Motion Forecasting,"Kan Chen, Runzhou Ge, Hang Qiu, Rami Al-rfou, Charles Ruizhongtai Qi, Xuanyu Zhou, Zoey Zeyu Yang, Scott Ettinger, Pei Sun, Zhaoqi Leng, Mustafa Baniodeh, Ivan Bogun, Weiyue Wang, Mingxing Tan, Dragomir Anguelov","Waymo LLC,University of California, Riverside,Waymo,Cruise LLC,university of southern california,Waymo Research",Big Data in Robotics and Automation,"Widely adopted motion forecasting datasets substitute the observed sensory inputs with higher-level abstractions such as 3D boxes and polylines. These sparse shapes are inferred through annotating the original scenes with perception systems' predictions. Such intermediate representations tie the quality of the motion forecasting models to the performance of computer vision models. Moreover, the human-designed explicit interfaces between perception and motion forecasting typically pass only a subset of the semantic information present in the original sensory input. To study the effect of these modular approaches, design new paradigms that mitigate these limitations, and accelerate the development of end-to-end motion forecasting models, we augment the Waymo Open Motion Dataset (WOMD) with large-scale, high-quality, diverse LiDAR data for the motion forecasting task. The new augmented dataset WOMD-LiDAR consists of over 100,000 scenes that each spans 20 seconds, consisting of well-synchronized and calibrated high quality LiDAR point clouds captured across a range of urban and suburban geographies (https://waymo.com/open/data/motion/). Compared to Waymo Open Dataset (WOD), WOMD-LiDAR dataset contains 100x more scenes. Furthermore, we integrate the LiDAR data into the motion forecasting model training and provide a strong baseline. Experiments show that the LiDAR data brings improvement in the motion forecasting task. We hope that WOMD-LiDAR will provide new opportunities for boosting end-to-end motion forecasting models."
Increasing the Absolute Position Accuracy of Industrial Robots by Means of a Deep Continual Evidential Regression Model,"Eckart Uhlmann, Mitchel Polte, Julian Bllumberg, Sheng Yin, Gang Wang","TU Berlin, Institute for Machine Tools and Factory Management,Chongqing University",Big Data in Robotics and Automation,"The use of industrial robots represents a key technology for increasing productivity and efficiency in manufacturing. However, their low absolute position accuracy still denies the broad substitution of machine tools by industrial robots. In this paper, a data-driven method for accuracy enhancement of industrial robots under consideration of kinematic, elastic, and thermal effects is presented. A continual learning algorithm is proposed, which allows to train the model in a process-parallel manner without suffering from catastrophic forgetting. Furthermore, the model is able to determine confidence intervals of the prediction values and thus supports further processing in safety-relevant applications. The effectiveness of the model can be demonstrated using a large data stream with about 3,000 real data points. As a result, it can be shown that the absolute position accuracy of the industrial robot can be improved by 96 % with the proposed method."
SpawnNet: Learning Generalizable Visuomotor Skills from Pre-Trained Network,"Xingyu Lin, John Ian So, Sashwat Mahalingam, Fangchen Liu, Pieter Abbeel","UC Berkeley,Stanford University,University of California, Berkeley",Big Data in Robotics and Automation,"The existing internet-scale image and video datasets cover a wide range of everyday objects and tasks, bringing the potential of learning policies that generalize in diverse scenarios. Prior works have explored visual pre-training with different self-supervised objectives. Still, the generalization capabilities of the learned policies and the advantages over well-tuned baselines remain unclear from prior studies. In this work, we present a focused study of the generalization capabilities of the pre-trained visual representations at the categorical level. We identify the key bottleneck in using a frozen pre-trained visual backbone for policy learning and then propose SpawnNet, a novel two-stream architecture that learns to fuse pre-trained multi-layer representations into a separate network to learn a robust policy. Through extensive simulated and real experiments, we show significantly better categorical generalization compared to prior approaches in imitation learning settings. Open-sourced code and videos can be found on our website: https://xingyu-lin.github.io/spawnnet/."
RoboAgent: Generalization and Efficiency in Robot Manipulation Via Semantic Augmentations and Action Chunking,"Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, Vikash Kumar","Carnegie Mellon University,Meta,Meta AI",Big Data in Robotics and Automation,"The grand aim of having a single robot that can manipulate arbitrary objects in diverse settings is at odds with the paucity of robotics datasets. Acquiring and growing such datasets is strenuous due to manual efforts, operational costs, and safety challenges. A path toward such a universal agent requires an efficient framework capable of generalization but within a reasonable data budget. In this paper, we develop an efficient framework (MT-ACT) for training universal agents capable of multi-task manipulation skills using (a) semantic augmentations that can rapidly multiply existing datasets and (b) action representations that can extract performant policies with small yet diverse multi-modal datasets without overfitting. In addition, reliable task conditioning and an expressive policy architecture enables our agent to exhibit a diverse repertoire of skills in novel situations specified using task commands. Using merely 7500 demonstrations, we are able to train a single policy, RoboAgent capable of 12 unique skills, and demonstrate its generalization over 38 tasks spread across common daily activities in diverse kitchen scenes. On average, MT-ACT outperforms prior methods by over 40% in unseen situations while being more sample efficient. See https://robopen.github.io/ for video results and appendix."
Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language Models,"Ivan Kapelyukh, Yifei Ren, Ignacio Alzugaray, Edward Johns",Imperial College London,Big Data in Robotics and Automation,"We introduce Dream2Real, a robotics framework which integrates vision-language models (VLMs) trained on 2D data into a 3D object rearrangement pipeline. This is achieved by the robot autonomously constructing a 3D representation of the scene, where objects can be rearranged virtually and an image of the resulting arrangement rendered. These renders are evaluated by a VLM, so that the arrangement which best satisfies the user instruction is selected and recreated in the real world with pick-and-place. This enables language-conditioned rearrangement to be performed zero-shot, without needing to collect a training dataset of example arrangements. Results on a series of real-world tasks show that this framework is robust to distractors, controllable by language, capable of understanding complex multi-object relations, and readily applicable to both tabletop and 6-DoF rearrangement tasks."
Robot Fine-Tuning Made Easy: Pre-Training Rewards and Policies for Autonomous Real-World Reinforcement Learning,"Jingyun Yang, Max Sobol Mark, Brandon Vu, Archit Sharma, Jeannette Bohg, Chelsea Finn",Stanford University,Big Data in Robotics and Automation,"The pre-train and fine-tune paradigm in machine learning has had dramatic success in a wide range of domains because the use of existing data or pre-trained models on the internet enables quick and easy learning of new tasks. We aim to enable this paradigm in robotic reinforcement learning, allowing a robot to learn a new task with little human effort by leveraging data and models from the Internet. However, reinforcement learning often requires significant human effort in the form of manual reward specification or environment resets, even if the policy is pre-trained. We introduce RoboFuME, a reset-free fine-tuning system that pre-trains a multi-task manipulation policy from diverse datasets of prior experiences and self-improves online to learn a target task with minimal human intervention. Our insights are to utilize calibrated offline reinforcement learning techniques to ensure efficient online fine-tuning of a pre-trained policy in the presence of distribution shifts and leverage pre-trained vision language models (VLMs) to build a robust reward classifier for autonomously providing reward signals during the online fine-tuning process. In a diverse set of five real robot manipulation tasks, we show that our method can incorporate data from an existing robot dataset collected at a different institution and improve on a target task within as little as 3 hours of autonomous real-world experience. We also demonstrate in simulation experiments that our method outperforms prior works that use different RL algorithms or different approaches for predicting rewards. Project website: https://robofume.github.io"
Scaling Motion Forecasting Models with Ensemble Distillation,"Scott Ettinger, Kratarth Goel, Avikalp Srivastava, Rami Al-rfou",Waymo,Big Data in Robotics and Automation,"Motion forecasting has become an increasingly critical component of autonomous robotic systems. Onboard compute budgets typically limit the accuracy of real-time systems. In this work we propose methods of improving motion forecasting systems subject to limited compute budgets by combining model ensemble and distillation techniques. The use of ensembles of deep neural networks has been shown to improve generalization accuracy in many application domains. We first demonstrate significant performance gains by creating a large ensemble of optimized single models. We then develop a generalized framework to distill motion forecasting model ensembles into small student models which retain high performance with a fraction of the computing cost. For this study we focus on the task of motion forecasting using real world data from autonomous driving systems. We develop ensemble models that are very competitive on the Waymo Open Motion Dataset (WOMD) and Argoverse leaderboards. From these ensembles, we train distilled student models which have high performance at a fraction of the compute costs. These experiments demonstrate distillation from ensembles as an effective method for improving accuracy of predictive models for robotic systems with limited compute budgets."
Is It a Bug? Understanding Physical Unit Mismatches in Robot Software,"Paulo Canelas, Trenton Tabor, John-Paul Ore, Alcides Fonseca, Claire Le Goues, Christopher Steven Timperley","Carnegie Mellon University,North Carolina State University,LASIGE, Faculdade de Ciências da Universidade de Lisboa",Big Data in Robotics and Automation,"Robot software is abundant with variables that represent real-world physical units (e.g., meters, seconds). Operations over different units (e.g., adding meters and seconds) may be incorrect and can lead to dangerous system misbehaviors; manually detecting such mistakes is challenging. Current software analysis techniques identify such mismatches using dimensional analysis rules and ROS-specific assumptions to analyze the source code. However, these are ignorant of the fact that physical unit mismatches in robotics code are often intentional (e.g., when operating a differential drive robot), resulting in false positive bug reports that can impede robotics developer trust and productivity. In this work, we study how developers introduce physical unit mismatches by manually inspecting 180 errors detected by the software analysis technique, Phys. We identify three types of physical unit mismatches and present a taxonomy of eight high-level categories of how these errors manifest. We find that developers often make unforced and paradigmatic physical unit mismatches through differential drives, small angle approximations, and controls. We draw insights on current development to inform future research to better detect, categorize, and address meaningful physical unit mismatches."
Behavior Tree Capabilities for Dynamic Multi-Robot Task Allocation with Heterogeneous Robot Teams,"Georg Heppner, David Oberacker, Arne Roennau, Rüdiger Dillmann","FZI Forschungszentrum Informatik,FZI Forschungszentrum Informatik, Karlsruhe,FZI - Forschungszentrum Informatik - Karlsruhe",Multi-Robot Systems III,"While individual robots are becoming increasingly capable, the complexity of expected missions increases exponentially in comparison. To cope with this complexity, heterogeneous teams of robots have become a significant research interest in recent years. Making effective use of the robots and their unique skills in a team is challenging. Dynamic runtime conditions often make static task allocations infeasible, requiring a dynamic, capability-aware allocation of tasks to team members. To this end, we propose and implement a system that allows a user to specify missions using Behavior (BTs), which can then, at runtime, be dynamically allocated to the current robot team. The system allows to statically model an individual robot's capabilities within our ros_bt_py BT framework. It offers a runtime auction system to dynamically allocate tasks to the most capable robot in the current team. The system leverages utility values and pre-conditions to ensure that the allocation improves the overall mission execution quality while preventing faulty assignments. To evaluate the system, we simulated a find-and-decontaminate mission with a team of three heterogeneous robots and analyzed the utilization and overall mission times as metrics. Our results show that our system can improve the overall effectiveness of a team while allowing for intuitive mission specification and flexibility in the team composition."
Multi-Robot Cooperative Navigation in Crowds: A Game-Theoretic Learning-Based Model Predictive Control Approach,"Viet-anh Le, Vaishnav Tadiparthi, Behdad Chalaki, Hossein Nourkhiz Mahjoub, Jovin D'sa, Ehsan Moradi-Pari, Andreas Malikopoulos","University of Delaware,Honda Research Institute,Honda Research Institute USA, Inc.,Honda Research Institute US,Honda Research Institute, USA,Cornell University",Multi-Robot Systems III,"In this paper, we develop a control framework for the coordination of multiple robots as they navigate through crowded environments. Our framework comprises of a local model predictive control (MPC) for each robot and a social long short-term memory model that forecasts pedestrians' trajectories. We formulate the local MPC formulation for each individual robot that includes both individual and shared objectives, in which the latter encourages the emergence of coordination among robots. Next, we consider the multi-robot navigation and human-robot interaction, respectively, as a potential game and a two-player game, then employ an iterative best response approach to solve the resulting optimization problems in a centralized and distributed fashion. Finally, we demonstrate the effectiveness of coordination among robots in simulated crowd navigation."
Multi-Robot Human-In-The-Loop Control under Spatiotemporal Specifications,"Yixiao Zhang, Victor Nan Fernandez-Ayala, Dimos V. Dimarogonas",KTH Royal Institute of Technology,Multi-Robot Systems III,"In this work, we present a coordination strategy tailored for scenarios involving multiple agents and tasks. We devise a range of tasks using signal temporal logic (STL), each earmarked for specific agents. These tasks are then imposed through control barrier function (CBF) constraints to ensure completion. To extend existing methodologies, our framework adeptly manages interactions among multiple agents. This extension is facilitated by leveraging nonlinear model predictive control (NMPC) to compute trajectories that avoid collisions. An integral aspect of our approach is the integration of a human-in-the-loop (HIL) model. This model enables real-time integration of human directives into the coordination process. A novel task allocation protocol is embedded within the framework to guide this process. We substantiate our methodology through a series of experiments, which corroborate the viability and relevance of our algorithms."
Hypergraph-Based Multi-Robot Task and Motion Planning,"James Motes, Tan Chen, Timothy Bretl, Marco Morales, Nancy Amato","University of Illinois Urbana-Champaign,Michigan Technological University,University of Illinois at Urbana-Champaign,University of Illinois at Urbana-Champaign & Instituto Tecnológ,University of Illinois",Multi-Robot Systems III,"We present a multi-robot task and motion planning method that, when applied to the rearrangement of objects by manipulators, results in solution times up to three orders of magnitude faster than existing methods and successfully plans for problems with up to twenty objects, more than three times as many objects as comparable methods. We achieve this improvement by decomposing the planning space to consider manipulators alone, objects, and manipulators holding objects. We represent this decomposition with a hypergraph where vertices are decomposed elements of the planning spaces and hyperarcs are transitions between elements. Existing methods use based representations where vertices are full composite spaces and edges are transitions between these. Using the hypergraph reduces the representation size of the planning space-for multi-manipulator object rearrangement, the number of hypergraph vertices scales linearly with the number of either robots or objects, while the number of hyperarcs scales quadratically with the number of robots and linearly with the number of objects. In contrast, the number of vertices and edges in graph representations scales exponentially with either."
"Measurement-Limited Multi-Agent, Relative Pose Estimation for On-Orbit Inspection","Mark Mercier, David Curtis, Clark Taylor",Air Force Institute of Technology,Multi-Robot Systems III,"Relative navigation methods are a critical enabling technology for the next generation of autonomous spacecraft conducting close proximity operations. This is especially true for multi-agent inspection operations in which safety including intra-agent or agent-target collisions are a serious concern. Additionally, in an on-orbit servicing operation various failure modes of the target may result in unreliable a-priori knowledge or cooperation from the target. The main contribution of this work is the demonstration of a method for multi-agent, relative pose estimation that is robust to A) sensor blinding and B) dynamic uncertainty. This objective is accomplished leveraging GTSAM, an existing toolbox for the formulation of factor graphs, along with an algorithm for the efficient, real-time solution of such factor graphs, iSAM2. This estimation method is demonstrated in an example scenario with uncertain dynamics and sensor blinding due to sun position. Results revealed that the iSAM2-based method is capable of handling sensor blinding through leveraging an inter-agent range measurement, despite a dynamically uncertain environment."
Dynamic Targeting of Satellite Observations Incorporating Slewing Costs and Complex Observation Utility,"Akseli Kangaslahti, Alberto Candela, Jason Swope, Qing Yue, Steve Chien","University of Michigan,NASA Jet Propulsion Laboratory, Caltech,Jet Propulsion Laboratory, California Institute of Technology,Jet Propulsion Laboratory",Multi-Robot Systems III,"Maximizing the utility of limited Earth observing satellite resources is a difficult ongoing problem. Dynamic Targeting is an approach to this challenge that intelligently plans and executes primary sensor observations based on information from a lookahead sensor. However, current implementations have failed to account for realistic satellite operational constraints and have used static utility for repeat observations of the same target. To address these limitations, we implement a more general Dynamic Targeting framework that comprises a physics-based slew model, a dynamic model of observation utility, and an algorithm for gathering high-utility observations. To demonstrate this framework, we also supply complex dynamic utility models that are applicable to many missions and new algorithms for intelligently scheduling observations with slewing restrictions and changing utility, including a greedy algorithm and a depth-first search algorithm. To evaluate these algorithms, we test their performance across simulated runs through two datasets and compare to the performance of an algorithm representative of most scheduling algorithms aboard Earth science missions today as well as an intractable upper bound. We show that our algorithms have great potential to improve science return from Earth science missions."
RecNet: An Invertible Point Cloud Encoding through Range Image Embeddings for Multi-Robot Map Sharing and Reconstruction,"Nikolaos Stathoulopoulos, Mario Alberto Valdes Saucedo, Anton Koval, George Nikolakopoulos","Luleå University of Technology,Lulea University of Technology",Multi-Robot Systems III,"In the field of resource-constrained robots and the need for effective place recognition in multi-robotic systems, this article introduces RecNet, a novel approach that concurrently addresses both challenges. The core of RecNet's methodology involves a transformative process: it projects 3D point clouds into range images, compresses them using an encoder-decoder framework, and subsequently reconstructs the range image, restoring the original point cloud. Additionally, RecNet utilizes the latent vector extracted from this process for efficient place recognition tasks. This approach not only achieves comparable place recognition results but also maintains a compact representation, suitable for sharing among robots to reconstruct their collective maps. The evaluation of RecNet encompasses an array of metrics, including place recognition performance, the structural similarity of the reconstructed point clouds, and the bandwidth transmission advantages, derived from sharing only the latent vectors. Our proposed approach is assessed using both a publicly available dataset and field experiments1, confirming its efficacy and potential for real-world applications."
Object Permanence Filter for Robust Tracking with Interactive Robots,"Shaoting Peng, Margaret Wang, Julie A. Shah, Nadia Figueroa","University of Pennsylvania,Massachusetts Institute of Technology,MIT",Visual Tracking,"Object permanence, which refers to the concept that objects continue to exist even when they are no longer perceivable through the senses, is a crucial aspect of human cognitive development. In this work, we seek to incorporate this understanding into interactive robots by proposing a set of assumptions and rules to represent object permanence in multi-object, multi-agent interactive scenarios. We integrate these rules into the particle filter, resulting in the Object Permanence Filter (OPF). For multi-object scenarios, we propose an ensemble of K interconnected OPFs, where each filter predicts plausible object tracks that are resilient to missing, noisy, and kinematically or dynamically infeasible measurements. Through several interactive scenarios, we demonstrate that the proposed OPF approach provides robust tracking in human-robot interactive tasks agnostic to measurement type, even in the presence of prolonged and complete occlusion. Project webpage: https://opfilter.github.io/"
Zero-Shot Open-Vocabulary Tracking with Large-Scale Pre-Trained Models,"Wen-hsuan Chu, Adam Harley, Pavel Tokmakov, Achal Dave, Leonidas Guibas, Aikaterini Fragkiadaki","Carnegie Mellon University,Stanford University,CMU,Toyota Research Institute",Visual Tracking,"Object tracking is central to robot perception and scene understanding, allowing robots to parse a video stream in terms of moving objects with names. Tracking-by-detection has long been a dominant paradigm for object tracking of specific object categories. Recently, large-scale pre-trained models have shown promising advances in detecting and segmenting objects and parts in 2D static images in the wild. This raises the question: can we re-purpose these large-scale pre-trained static image models for open-vocabulary video tracking? In this paper, we combine an open-vocabulary detector, segmenter, and dense optical flow estimator, into a model that tracks and segments any object in 2D videos. Given a monocular video input, our method predicts object and part mask tracks with associated language descriptions, rebuilding the pipeline of Tractor with modern large pre-trained models for static image detection and segmentation: we detect open-vocabulary object instances and propagate their boxes from frame to frame using a flow-based motion model, refine the propagated boxes with the box regression module of the visual detector, and prompt an open-world segmenter with the refined box to segment the objects. We decide the termination of an object track based on the objectness score of the propagated boxes as well as forward-backward optical flow consistency. We re-identify objects across occlusions using deep feature matching. We show that our model achieves strong performance on multiple established benchmarks, and can produce reasonable tracks in manipulation data. In particular, our model outperforms previous state-of-the-art in UVO and BURST, benchmarks for open-world object tracking and segmentation, despite never being explicitly trained for tracking. We hope that our approach can serve as a simple and extensible framework for future research and enable imitation learning from videos with unconventional objects."
Multi-Correlation Siamese Transformer Network with Dense Connection for 3D Single Object Tracking,"Shihao Feng, Pengpeng Liang, Jin Gao, Erkang Cheng","Zhengzhou University,Institute of Automation Chinese Academy of Sciences,Nullmax Inc",Visual Tracking,"Point cloud-based 3D object tracking is an important task in autonomous driving. Though great advances regarding Siamese-based 3D tracking have been made recently, it remains challenging to learn the correlation between the template and search branches effectively with the sparse LIDAR point cloud data. Instead of performing correlation of the two branches at just one point in the network, in this paper, we present a multi-correlation Siamese Transformer network that has multiple stages and carries out feature correlation at the end of each stage based on sparse pillars. More specifically, in each stage, self-attention is first applied to each branch separately to capture the non-local context information. Then, cross-attention is used to inject the template information into the search area. This strategy allows the feature learning of the search area to be aware of the template while keeping the individual characteristics of the template intact. To enable the network to easily preserve the information learned at different stages and ease the optimization, for the search area, we densely connect the initial input sparse pillars and the output of each stage to all subsequent stages and the target localization network, which converts pillars to birdâ€™s eye view (BEV) feature maps and predicts the state of the target with a small densely connected convolution network. Deep supervision is added to each stage to further boost the performance as well."
Refining Pre-Trained Motion Models,"Xinglong Sun, Adam Harley, Leonidas Guibas","Stanford & UIUC,Stanford University",Visual Tracking,"Given the difficulty of manually annotating motion in video, the current best motion estimation methods are trained with synthetic data, and therefore struggle somewhat due to a train/test gap. Self-supervised methods hold the promise of training directly on real video, but typically perform worse. These include methods trained with warp error (i.e., color constancy) combined with smoothness terms, and methods that encourage cycle-consistency in the estimates (i.e., tracking backwards should yield the opposite trajectory as tracking forwards). In this work, we take on the challenge of improving state-of-the-art supervised models with self-supervised training. We find that when the initialization is supervised weights, most existing self-supervision techniques actually make performance worse instead of better, which suggests that the benefit of seeing the new data is overshadowed by the noise in the training signal. Focusing on obtaining a ""clean"" training signal from real-world unlabelled video, we propose to separate label-making and training into two distinct stages. In the first stage, we use the pre-trained model to estimate motion in a video, and then select the subset of motion estimates which we can verify with cycle-consistency. This produces a sparse but accurate pseudo-labelling of the video. In the second stage, we fine-tune the model to reproduce these outputs, while also applying augmentations on the input. We complement this boot-strapping method with simple techniques that densify and re-balance the pseudo-labels, ensuring that we do not merely train on ""easy"" tracks. We show that our method yields reliable gains over fully-supervised methods in real videos, for both short-term (flow-based) and long-range (multi-frame) pixel tracking. Our code can be found here: https://github.com/AlexSunNik/refining-motion-code."
SWTrack: Multiple Hypothesis Sliding Window 3D Multi-Object Tracking,"Sandro Papais, Robert Ren, Steven Lake Waslander",University of Toronto,Visual Tracking,"Modern robotic systems are required to operate in dense dynamic environments, requiring highly accurate real-time track identification and estimation. For 3D multi-object tracking, recent approaches process a single measurement frame recursively with greedy association and are prone to errors in ambiguous association decisions. Our method, Sliding Window Tracker (SWTrack), yields more accurate association and state estimation by batch processing many frames of sensor data while being capable of running online in real-time. The most probable track associations are identified by evaluating all possible track hypotheses across the temporal sliding window. A novel graph optimization approach is formulated to solve the multidimensional assignment problem with lifted graph edges introduced to account for missed detections and graph sparsity enforced to retain real-time efficiency. We evaluate our SWTrack implementation on the NuScenes autonomous driving dataset to demonstrate improved tracking performance."
UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking,"Chang Won Lee, Steven Lake Waslander",University of Toronto,Visual Tracking,"Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving object detection methods. The majority of tracking methods, which follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty. This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT. While there are existing works in probabilistic object detection that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in object tracking. We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic object detectors. Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19% and improves mMOTA by 2-3%."
Humanoid Loco-Manipulations Using Combined Fast Dense 3D Tracking and SLAM with Wide-Angle Depth-Images,"Kevin Chappellet, Masaki Murooka, Guillaume Caron, Fumio Kanehiro, Abderrahmane Kheddar","CNRS,AIST,National Inst. of AIST,CNRS-AIST",Visual Tracking,"To efficiently achieve complex humanoid loco-manipulation tasks in industrial contexts, we propose a combined vision-based tracker-localization interplay integrated as part of a task-space whole-body optimization control. To achieve good perception complementarity between manipulation and localization, a new fast dense 3D model-based tracking using wide-angle depth image is developed and used in conjunction with a simultaneous localization and mapping software. Our approach allows humanoid robots, targeted for industrial manufacturing, to manipulate and assemble large-scale objects while walking. It is assessed with experiments consisting in rolling and assembling in an unwinder a heavy and wide bobbin using bimanual grasping and bipedal locomotion at a time. This experimental use-case is found in some large-scale manufacturing where bobbins are enrolled with various materials (cables, papers, rubbers, etc.). The same experiments are made using two different humanoid robots of the same family."
LiteTrack: Layer Pruning with Asynchronous Feature Extraction for Lightweight and Efficient Visual Tracking,"Qingmao Wei, Bi Zeng, Jianqi Liu, Li He, Guotian Zeng","Guandong University of Technology,Guangdong University of Technology,Southern University of Science and Technology",Visual Tracking,"The recent advancements in transformer-based visual trackers have led to significant progress, attributed to their strong modeling capabilities. However, as performance improves, running latency correspondingly increases, presenting a challenge for real-time robotics applications, especially on edge devices with computational constraints. In response to this, we introduce LiteTrack, an efficient transformer-based tracking model optimized for high-speed operations across various devices. It achieves a more favorable trade-off between accuracy and efficiency than the other lightweight trackers. The main innovations of LiteTrack encompass: 1) asynchronous feature extraction and interaction between the template and search region for better feature fushion and cutting redundant computation, and 2) pruning encoder layers from a heavy tracker to refine the balnace between performance and speed. As an example, our fastest variant, LiteTrack-B4, achieves 65.2% AO on the GOT-10k benchmark, surpassing all preceding efficient trackers, while running over 100 fps with ONNX on the Jetson Orin NX edge device. Moreover, our LiteTrack-B9 reaches competitive 72.2% AO on GOT-10k and 82.4% AUC on TrackingNet, and operates at 171 fps on an NVIDIA 2080Ti GPU. The code and demo materials will be available at https://github.com/TsingWei/LiteTrack."
WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth Estimation under Adverse Weather Conditions,"Jiyuan Wang, Chunyu Lin, Lang Nie, Shujuan Huang, Xing Pan, Rui Ai, Yao Zhao","Beijing JiaoTong University,Beijing Jiaotong University,Haomo Zhixing,Haomo AI Technology Co.,Ltd",RGB-D Sensing and Perception II,"Depth estimation models have shown promising performance on clear scenes but fail to generalize to adverse weather conditions due to illumination variations, weather particles, etc. In this paper, we propose WeatherDepth, a self-supervised robust depth estimation model with curriculum contrastive learning, to tackle performance degradation in complex weather conditions. Concretely, we first present a progressive curriculum learning scheme with three simple-to-complex curricula to gradually adapt the model from clear to relative adverse, and then to adverse weather scenes. It encourages the model to gradually grasp beneficial depth cues against the weather effect, yielding smoother and better domain adaption. Meanwhile, to prevent the model from forgetting previous curricula, we integrate contrastive learning into different curricula. By drawing reference knowledge from the previous course, our strategy establishes a depth consistency constraint between different courses toward robust depth estimation in diverse weather. Besides, to reduce manual intervention and better adapt to different models, we designed an adaptive curriculum scheduler to automatically search for the best timing for course switching. In the experiment, the proposed solution is proven to be easily incorporated into various architectures and demonstrates state-of-the-art (SoTA) performance on both synthetic and real weather datasets. Source code and data are available at url{https://github.com/wangjiyuan9/WeatherDepth}."
Collaborative Decision-Making Using Spatiotemporal Graphs in Connected Autonomy,"Peng Gao, Yu Shen, Ming C. Lin","University of Massachussets Amherst,University of Maryland,University of Maryland at College Park",RGB-D Sensing and Perception II,"Collaborative decision-making is an essential capability for multi-robot systems, such as connected vehicles, to collaboratively control autonomous vehicles in accident-prone scenarios. Under limited communication bandwidth, capturing comprehensive situational awareness by integrating connected agents' observation is very challenging. In this paper, we propose a novel collaborative decision-making method that efficiently and effectively integrates collaborators' representations to control the ego vehicle in accident-prone scenarios. Our approach formulates collaborative decision-making as a classification problem. We first represent sequences of raw observations as spatiotemporal graphs, which significantly reduce the package size to share among connected vehicles. Then we design a novel spatiotemporal graph neural network based on heterogeneous graph learning, which analyzes spatial and temporal connections of objects in a unified way for collaborative decision-making. We evaluate our approach using a high-fidelity simulator that considers realistic traffic, communication bandwidth, and vehicle sensing among connected autonomous vehicles. The experimental results show that our representation achieves over 100x reduction in the shared data size that meets the requirements of communication bandwidth for connected autonomous driving. In addition, our approach achieves over 30% improvements in driving safety."
Multi-Model 3D Registration: Finding Multiple Moving Objects in Cluttered Point Clouds,"David Jin, Sushrut Karmalkar, Harry Zhang, Luca Carlone","MIT,University of Wisconsin-Madison,Massachusetts Institute of Technology",RGB-D Sensing and Perception II,"We investigate a variation of the 3D registration problem, named multi-model 3D registration. In the multi-model registration problem, we are given two point clouds picturing a set of objects at different poses (and possibly including points belonging to the background) and we want to simultaneously reconstruct how all objects moved between the two point clouds. This setup generalizes standard 3D registration where one wants to reconstruct a single pose, e.g., the motion of the sensor picturing a static scene. Moreover, it provides a mathematically grounded formulation for relevant robotics applications, e.g., where a depth sensor onboard a robot perceives a dynamic scene and has the goal of estimating its own motion (from the static portion of the scene) while simultaneously recovering the motion of all dynamic objects. We assume a correspondence-based setup where we have putative matches between the two point clouds and consider the practical case where these correspondences are plagued with outliers. We then propose a simple approach based on Expectation-Maximization (EM) and establish theoretical conditions under which the EM approach converges to the ground truth. We evaluate the approach in simulated and real datasets ranging from table-top scenes to self-driving scenarios and demonstrate its effectiveness when combined with state-of-the-art scene flow methods to establish dense correspondences."
A Robust Deformable Linear Object Perception Pipeline in 3D: From Segmentation to Reconstruction,"Zhaole Sun, Hang Zhou, Nanbo Li, Longfei Chen, Jihong Zhu, Robert Fisher","Tsinghua University, the University of Edinburgh, Intel Lab Chin,Shanghai University,University of Edinburgh,University of York",RGB-D Sensing and Perception II,"3D perception of deformable linear objects (DLOs) is crucial for DLO manipulation. However, perceiving DLOs in 3D from a single RGBD image is challenging. Previous DLO perception methods fail to extract a decent 3D DLO model due to different textures, occlusions, sparse and false depth information. To address these problems and provide a more robust DLO state estimation for downstream tasks like tracking and manipulation, this paper proposes a 3D DLO perception pipeline to first segment a DLO in 2D images and post-process masks to eliminate false positive segmentation, reconstruct the DLO in 3D space to predict the occluded part of the DLO, and physically smooth the reconstructed DLO. By testing on a synthetic DLO dataset and further validating on a real-world dataset with seven different DLOs, we demonstrate that the proposed method is an effective and robust 3D perception pipeline solution with better performance on 2D DLO segmentation and 3D DLO reconstruction compared to State-of-the-Art algorithms."
Unlocking the Performance of Proximity Sensors by Utilizing Transient Histograms,"Carter Sifferman, Yeping Wang, Mohit Gupta, Michael Gleicher","University of Wisconsin-Madison,University of Wisconsin - Madison",RGB-D Sensing and Perception II,"We provide methods which recover planar scene geometry by utilizing the transient histograms captured by a class of close-range time-of-flight (ToF) distance sensor. A transient histogram is a one dimensional temporal waveform which encodes the arrival time of photons incident on the ToF sensor. Typically, a sensor processes the transient histogram using a proprietary algorithm to produce distance estimates, which are commonly used in several robotics applications. Our methods utilize the transient histogram directly to enable recovery of planar geometry more accurately than is possible using only proprietary distance estimates, and consistent recovery of the albedo of the planar surface, which is not possible with proprietary distance estimates alone. This is accomplished via a differentiable rendering pipeline, which simulates the transient imaging process, allowing direct optimization of scene geometry to match observations. To validate our methods, we capture 3,800 measurements of eight planar surfaces from a wide range of viewpoints, and show that our method outperforms the proprietary-distance-estimate baseline by an order of magnitude in most scenarios. We demonstrate a simple robotics application which uses our method to sense the distance to and slope of a planar surface from a sensor mounted on the end effector of a robot arm."
VG4D: Vision-Language Model Goes 4D Video Recognition,"Zhichao Deng, Xiangtai Li, Xia Li, Yunhai Tong, Shen Zhao, Mengyuan Liu","Sun Yat-sen university,Peking University,ETH Zurich,Sun Yat-sen University",RGB-D Sensing and Perception II,"Understanding the real world through point cloud video is a crucial aspect of robotics and autonomous driving systems. However, prevailing methods for 4D point cloud recognition have limitations due to sensor resolution, which leads to a lack of detailed information. Recent advances have shown that Vision-Language Models (VLM) pre-trained on web-scale text-image datasets can learn fine-grained visual concepts that can be transferred to various downstream tasks. However, effectively integrating VLM into the domain of 4D point clouds remains an unresolved problem. In this work, we propose the Vision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from visual-text pre-trained models to a 4D point cloud network. Our approach involves aligning the 4D encoder's representation with a VLM learning a shared visual and text space from training on large-scale image-text pairs. By transferring the knowledge of the VLM to the 4D encoder and combining the VLM, our VG4D achieves improved recognition performance. To enhance the 4D encoder, we modernize the classic dynamic point cloud backbone and propose an improved version of PSTNet, im-PSTNet, which can efficiently model point cloud videos. Experiments demonstrate that our method achieves state-of-the-art performance for action recognition on both NTU RGB+D 60 dataset and NTU RGB+D 120 dataset."
ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning,"Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, Chuang Gan, Celso De Melo, Joshua Tenenbaum, Antonio Torralba, Florian Shkurti, Liam Paull","University of Toronto,Université de Montréal, Mila,MIT,International Institute of Information Technology,IIIT Hyderabad,Johns Hopkins University Applied Physics Lab,Mila, Université de Montréal,Johns Hopkins University,IBM,,CCDC US Army Research Laboratory,,Massachusetts Institute of Technology,,Université de Montréal",RGB-D Sensing and Perception II,"For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. To explore the full scope of our experiments and results, we encourage readers to visit our project webpage."
TrackDLO: Tracking Deformable Linear Objects under Occlusion with Motion Coherence,"Jingyi Xiang, Holly Dinkel, Harry Zhao, Naixiang Gao, Brian Coltin, Trey Smith, Timothy Bretl","University of Illinois at Urbana-Champaign,Carnegie Mellon University,NASA Ames Research Center",RGB-D Sensing and Perception II,"The TrackDLO algorithm estimates the shape of a Deformable Linear Object (DLO) under occlusion from a sequence of RGB-D images. TrackDLO is vision-only and runs in real-time. It requires no external state information from physics modeling, simulation, visual markers, or contact as input. The algorithm improves on previous approaches by addressing three common scenarios which cause tracking failure: tip occlusion, mid-section occlusion, and self-occlusion. This is achieved through the application of Motion Coherence Theory to impute the spatial velocity of occluded nodes, the use of the topological geodesic distance to track self-occluding DLOs, and the introduction of a non-Gaussian kernel that only penalizes lower-order spatial displacement derivatives to reflect DLO physics. Improved real-time DLO tracking under mid-section occlusion, tip occlusion, and self-occlusion is demonstrated experimentally. The source code and demonstration data are publicly released."
Long-Tailed 3D Semantic Segmentation with Adaptive Weight Constraint and Sampling,"Jean Lahoud, Fahad Khan, Hisham Cholakkal, Rao Anwer, Salman Khan","MBZUAI,Linkoping University,CSIRO",RGB-D Sensing and Perception II,"Existing 3D understanding datasets typically provide annotations for a limited number of object classes, with sufficient examples per class. However, real-world object classes are not equally represented in practical settings, leading to poor performance on rarely-occurring categories if the class imbalance is neglected. In this work, we address the challenge of 3D semantic segmentation with a long-tail distribution of classes. Common methods to reduce class imbalance during training include data re-sampling, loss re-weighting, and transfer learning. In contrast, our work proposes to effectively utilize network classifier weights in 3D models to balance the training on long-tail class distributions. While previous work in the 2D domain has studied imposing constraints on the classifier weights to regularize the training, it is sensitive to hyper-parameter choices and has not been yet explored for the 3D domain. To address these challenges, our work proposes adaptive regularization for frequent classes and sampling-based regularization for rare classes that alleviate the need to manually select thresholds and can dynamically focus training on the hard classes. Our experiments on the large-scale ScanNet200 benchmark show that our method achieves improved performance, surpassing methods that rely on re-sampling, re-weighting, and pre-training."
BioSLAM: A Bio-Inspired Lifelong Memory System for General Place Recognition,"Abulikemu Abuduweili, Shiqi Zhao, Peng Yin, Changliu Liu, Sebastian Scherer","Carnegie Mellon University,University of California San Diego",Learning in Localization and Navigation,"We present BioSLAM, a lifelong SLAM framework for learning various new appearances incrementally and maintaining accurate place recognition for previously visited areas. Unlike humans, artificial neural networks suffer from catastrophic forgetting and may forget the previously visited areas when trained with new arrivals. For humans, researchers discover that there exists a memory replay mechanism in the brain to keep the neuron active for previous events. Inspired by this discovery, BioSLAM designs a gated generative replay to control the robotâ€™s learning behavior based on the feedback rewards. Specifically, BioSLAM provides a novel dual-memory mechanism for maintenance: 1) a dynamic memory to efficiently learn new observations and 2) a static memory to balance new-old knowledge. When the agent is encountered with different appearances under new domains, the complete processing pipeline can help to incrementally update the place recognition ability, robust to the increasing complexity of long-term place recognition. We demonstrate BioSLAM in three incremental SLAM scenarios: 1) a 120km city-scale trajectories with LiDAR-based inputs, 2) a multi-visited 4.5"
Efficient Hierarchical Reinforcement Learning for Mapless Navigation with Predictive Neighbouring Space Scoring,"Yan Gao, Jing Wu, Xintong Yang, Ze Ji","Cardiff University,CARDIFF UNIVERSITY",Learning in Localization and Navigation,"Solving reinforcement learning-based mapless navigation tasks is challenging due to their sparse reward and long decision horizon nature. Hierarchical reinforcement learning has the ability to leverage knowledge at different abstract levels and is thus preferred in complex mapless navigation tasks. However, it is computationally expensive and inefficient to learn navigation end-to-end from raw high-dimensional sensor data, such as Lidar or RGB cameras. The use of subgoals based on a compact intermediate representation is therefore preferred for dimension reduction. This work proposes an efficient HRL-based framework to achieve this with a novel scoring method, named Predictive Neighbouring Space Scoring(PNSS). The PNSS model estimates the explorable space for a given position of interest based on the current robot observation. The PNSS values for a few candidate positions around the robot provide a compact and informative state representation for subgoal selection. We study the effects of different candidate position layouts and demonstrate that our layout design facilitates higher performances in longer-range tasks. Moreover, a penalty term is introduced in the reward function for the high-level policy, so that the subgoal selection process takes the performance of the low-level policy into consideration. Comprehensive evaluations demonstrate that using the proposed PNSS module consistently improves performances over the use of Lidar only or Lidar and encoded RGB features."
Learning Diverse Skills for Local Navigation under Multi-Constraint Optimality,"Jin Cheng, Marin Vlastelica, Pavel Kolev, Chenhao Li, Georg Martius","ETH Zurich,Max Planck Institute for Intelligent Systems",Learning in Localization and Navigation,"Despite many successful applications of data-driven control in robotics, extracting meaningful diverse behaviors remains a challenge. Typically, task performance needs to be compromised in order to achieve diversity. In many scenarios, task requirements are specified as a multitude of reward terms, each requiring a different trade-off. In this work, we take a constrained optimization viewpoint on the quality-diversity trade-off and show that we can obtain diverse policies while imposing multiple constraints on the reward terms. In line with previous work, further control of the diversity level can be achieved through an attract-repel reward term motivated by the Van der Waals force. We demonstrate the effectiveness of our method on a local navigation task where a quadruped robot needs to reach the target within a finite horizon. Finally, our trained policies transfer well to the real 12-DoF quadruped robot, Solo12, and exhibit diverse agile behaviors with successful obstacle traversal."
Snake Robot with Tactile Perception Navigates on Large-Scale Challenging Terrain,"Shuo Jiang, Adarsh Salagame, Alireza Ramezani, Lawson L.S. Wong",Northeastern University,Learning in Localization and Navigation,"Along with the advancement of robot skin technology, there has been notable progress in the development of snake robots featuring body-surface tactile perception. In this study, we proposed a locomotion control framework for snake robots that integrates tactile perception to augment their adaptability to various terrains. Our approach embraces a hierarchical reinforcement learning (HRL) architecture, wherein the high-level orchestrates global navigation strategies while the low-level uses curriculum learning for local navigation maneuvers. Due to the significant computational demands of collision detection in whole-body tactile sensing, the efficiency of the simulator is severely compromised. Thus a distributed training pattern to mitigate the efficiency reduction was adopted. We evaluated the navigation performance of the snake robot in complex large-scale cave exploration with challenging terrains to exhibit improvements in motion efficiency, evidencing the efficacy of tactile perception in terrain-adaptive locomotion of snake robots."
RaLF: Flow-Based Global and Metric Radar Localization in LiDAR Maps,"Abhijeet Nayak, Daniele Cattaneo, Abhinav Valada",University of Freiburg,Learning in Localization and Navigation,"Localization is paramount for autoatnomous robots. While camera and LiDAR-based approaches have been exten- sively investigated, they are affected by adverse illumination and weather conditions. Therefore, radar sensors have recently gained attention due to their intrinsic robustness to such conditions. In this paper, we propose RaLF, a novel deep neural network-based approach for localizing radar scans in a LiDAR map of the environment, by jointly learning to address both place recognition and metric localization. RaLF is composed of radar and LiDAR feature encoders, a place recognition head that generates global descriptors, and a metric localization head that predicts the 3-DoF transformation between the radar scan and the map. We tackle the place recognition task by learning a shared embedding space between the two modalities via cross-modal metric learning. Additionally, we perform metric localization by predicting pixel-level flow vectors that align the query radar scan with the LiDAR map. We extensively evaluate our approach on multiple real-world driving datasets and show that RaLF achieves state-of-the-art performance for both place recognition and metric localization. Moreover, we demonstrate that our approach can effectively generalize to different cities and sensor setups than the ones used during training. We make the code and trained models publicly available at http://ralf.cs.uni-freiburg.de."
VPE-SLAM: Neural Implicit Voxel-Permutohedral Encoding for SLAM,"Zhiyao Zhang, Yunzhou Zhang, You Shen, Lei Rong, Sizhan Wang, Xin Ouyang, Yulong Li",Northeastern University,Learning in Localization and Navigation,"NeRF can reconstruct incredibly realistic environmental maps in dense simultaneous localization and mapping, providing robots with more comprehensive scene map information. However, NeRF often struggles with geometric distortions in indoor reconstructions. To correct geometric distortions, we develop VPE-SLAM, based on the proposed voxel-permutohedral encoding, which can incrementally reconstruct maps of unknown scenes. Specifically, voxel-permutohedral encoding combines a sparse voxel feature grid created by an octree and multi-resolution permutohedral tetrahedral feature grids to represent the scene effectively. Especially when dealing with object edges, our method can effectively encode the geometry and texture of edges by the hybrid structural grid. We propose a novel local bundle adjustment module that utilizes a sliding window mechanism to manage adjacent keyframes requiring optimization. Furthermore, the proposed method establishes local map consistency by repeatedly optimizing keyframes that were initially under-optimized through a compensation strategy. The consistency of the local map can enhance the adaptability of our method to challenging scenes. Extensive experiments demonstrate that our method can achieve accurate camera tracking and produce high-quality reconstruction results on the Replica and ScanNet datasets. The source code will be available at https://github.com/NeuCV-IRMI/VPE-SLAM."
Zero-Shot Wireless Indoor Navigation through Physics-Informed Reinforcement Learning,"Mingsheng Yin, Tao Li, Haozhe Lei, Yaqi Hu, Sundeep Rangan, Quanyan Zhu","New York University,NYU",Learning in Localization and Navigation,"The growing focus on indoor robot navigation utilizing wireless signals has stemmed from the capability of these signals to capture high-resolution angular and temporal measurements. Prior heuristic-based methods, based on radio frequency (RF) propagation, are intuitive and generalizable across simple scenarios, yet fail to navigate in complex environments. On the other hand, end-to-end (e2e) deep reinforcement learning (RL) can explore a rich class of policies, delivering surprising performance when facing complex wireless environments. However, the price to pay is the astronomical amount of training samples, and the resulting policy, without fine-tuning (zero-shot), is unable to navigate efficiently in new scenarios unseen in the training phase. To equip the navigation agent with sample-efficient learning and {zero-shot} generalization, this work proposes a novel physics-informed RL (PIRL) where a distance-to-target-based cost (standard in e2e) is augmented with physics-informed reward shaping. The key intuition is that wireless environments vary, but physics laws persist. After learning to utilize the physics information, the agent can transfer this knowledge across different tasks and navigate in an unknown environment without fine-tuning. The proposed PIRL is evaluated using a wireless digital twin (WDT) built upon simulations of a large class of indoor environments from the AI Habitat dataset augmented with electromagnetic radiation simulation for wireless signals. It is shown that the PIRL significantly outperforms both e2e RL and heuristic-based solutions in terms of generalization and performance. Source code is available at url{https://github.com/Panshark/PIRL-WIN}."
An Environmental-Complexity-Based Navigation Method Based on Hierarchical Deep Reinforcement Learning,"Pengbin Chen, Qi Liu, Yanjie Li, Shuaikang Ma","Harbin Institute of Technology, Shenzhen,Harbin Institute of Technology,Harbin Institute of Technology (Shenzhen)",Learning in Localization and Navigation,"Navigation methods based on deep reinforcement learning (RL) have recently exhibited superior performance, particularly for navigation in dynamic environments. However, most existing methods solely rely on deep neural network feature encoders to extract features from raw LiDAR data, lacking an explicit representation of environmental structure. This limitation hinders effective environmental representation and interpretability, constraining navigation performance improvement. To solve this problem, we propose two quantitative metrics based on laser scans, which explicitly represent environmental complexity and show great interpretability. Furthermore, we propose an environmental-complexity-based navigation method based on hierarchical deep RL with the proposed metrics. Experimental results show that the proposed method achieves better navigation performance than baselines, especially in challenging scenarios with corners and dynamic obstacles."
Pre-Trained Masked Image Model for Mobile Robot Navigation,"Vishnu Sharma, Anukriti Singh, Pratap Tokekar",University of Maryland,Learning in Localization and Navigation,"2D top-down maps are commonly used for the navigation and exploration of mobile robots through unknown areas. Typically, the robot builds the navigation maps incrementally from local observations using onboard sensors. Recent works have shown that predicting the structural patterns in the environment through learning-based approaches can greatly enhance task efficiency. While many such works build task-specific networks using limited datasets, we show that the existing foundational vision networks can accomplish the same without any fine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street images, to present novel applications for field-of-view expansion, single-agent topological exploration, and multi-agent exploration for indoor mapping, across different input modalities. Our work motivates the use of foundational vision models for generalized structure prediction-driven applications, especially in the dearth of training data. We share more qualitative results at https://raaslab.org/projects/MIM4Robots."
Active Automotive Augmented Reality Displays Using Reinforcement Learning,"Ju-hyeok Ryu, Chan Kim, Seong-woo Kim",Seoul National University,Reinforcement Learning II,"Demand for home-like spatial functions is increasing in the traditional role of automobiles, which had value as a means of transportation in the past. Advances in technology are making this possible, and as a result, the total area of the display in a vehicle is increasing. This is leading to increased demand for augmented reality displays in vehicles. In order to enhance driving convenience and safety, automotive Augmented Reality displays, e.g.,head-up displays, have garnered attention and are gradually being deployed. However, when vehicles encounter uneven roads, vertical vibrations lead to mismatches between external physical objects and augmented reality overlay images, adversely affecting the AR displayâ€™s visibility. Resolving the problem is quite challenging because the optical system operates on magnification design, and is highly sensitive due to its multifunctional nature involving reflection and refraction through an intermediate medium. This paper aims to address the newly emerging problem of vertical mismatches in automotive AR displays. To tackle this issue, we begin by defining the problem and then examine the effectiveness of traditional control methods,on-policy and off-policy reinforcement learning as potential solutions. Finally,we validate our approach through experiments, demonstrating a significant reduction in vertical mismatches and an improvement in the overall visibility of automotive AR displays. Our findings provide valuable insightsfor enhancing driving convenience and safety in real-world conditions."
Extremum-Seeking Action Selection for Accelerating Policy Optimization,"Ya-Chien Chang, Sicun Gao","University of California San Diego,UCSD",Reinforcement Learning II,"Reinforcement learning for control over continuous spaces typically uses high-entropy stochastic policies, such as Gaussian distributions, for local exploration and estimating policy gradient to optimize performance. Many robotic control problems deal with complex unstable dynamics, where applying actions that are off the feasible control manifolds can quickly lead to undesirable divergence. In such cases, most samples taken from the ambient action space generate low-value trajectories that hardly contribute to policy improvement, resulting in slow or failed learning. We propose to improve action selection in this model-free RL setting by introducing additional adaptive control steps based on Extremum-Seeking Control (ESC). On each action sampled from stochastic policies, we apply sinusoidal perturbations and query for estimated Q-values as the response signal. Based on ESC, we then dynamically improve the sampled actions to be closer to nearby optima before applying them to the environment. Our methods can be easily added in standard policy optimization to improve learning efficiency, which we demonstrate in various control learning environments."
Privacy Risks in Reinforcement Learning for Household Robots,"Miao Li, Wenhao Ding, Ding Zhao","Carnegie Mellon University,Carnegie mellon university",Reinforcement Learning II,"The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advances in computer vision and large language models. Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot accesses substantial personal information. However, the issue of privacy leakage in embodied AI tasks, particularly concerning reinforcement learning algorithms, has not received adequate consideration in research. This paper aims to address this gap by proposing an attack on the training process of the value-based algorithm and the gradient-based algorithm, utilizing gradient inversion to reconstruct states, actions, and supervisory signals. The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or transmitting the data to public servers. Nevertheless, these gradients contain sufficient information to potentially expose private data. To validate our approach, we conducted experiments on the AI2THOR simulator and evaluated our algorithm on active perception, a prevalent task in embodied AI. The experimental results demonstrate the effectiveness of our method in successfully reconstructing all information from the data in 120 room layouts. Check our website for videos."
MAexp: A Generic Platform for RL-Based Multi-Agent Exploration,"Shaohao Zhu, Jiacheng Zhou, Anjun Chen, Mingming Bai, Jiming Chen, Jinming Xu","Zhejiang University,College of Control Science and Engineering, Zhejiang University",Reinforcement Learning II,"The sim-to-real gap poses a significant challenge in RL-based multi-agent exploration due to scene quantization and action discretization. Existing platforms suffer from the inefficiency in sampling and the lack of diversity in Multi-Agent Reinforcement Learning (MARL) algorithms across different scenarios, restraining their widespread applications. To fill these gaps, we propose MAexp, a generic platform for multi-agent exploration that integrates a broad range of state-of-the-art MARL algorithms and representative scenarios. Moreover, we employ point clouds to represent our exploration scenarios, leading to high-fidelity environment mapping and a sampling speed approximately 40 times faster than existing platforms. Furthermore, equipped with an attention-based Multi-Agent Target Generator and a Single-Agent Motion Planner, MAexp can work with arbitrary numbers of agents and accommodate various types of robots. Extensive experiments are conducted to establish the first benchmark featuring several high-performance MARL algorithms across typical scenarios for robots with continuous actions, which highlights the distinct strengths of each algorithm in different scenarios."
Improving Offline Reinforcement Learning with Inaccurate Simulators,"Yiwen Hou, Haoyuan Sun, Jinming Ma, Feng Wu",University of Science and Technology of China,Reinforcement Learning II,"Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment in robotics. However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process. In many robotic applications, an inaccurate simulator is often available. However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment. To address this, we propose a novel approach to better combine the offline dataset and the inaccurate simulation data. Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset. Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator. Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods."
REFORMA: Robust REinFORceMent Learning Via Adaptive Adversary for Drones Flying under Disturbances,"Hao-Lun Hsu, Haocheng Meng, Shaocheng Luo, Juncheng Dong, Vahid Tarokh, Miroslav Pajic","Duke University,Harvard University",Reinforcement Learning II,"In this work, we introduce REFORMA, a novel robust reinforcement learning (RL) approach to design controllers for unmanned aerial vehicles (UAVs) robust to unknown disturbances during flights. These disturbances, typically due to wind turbulence, electromagnetic interference, temperature extremes and many other external physical interference, are highly dynamic and difficult to model. REFORMA can perform a real-time online adaptation to these disturbances and generate appropriate velocity actions as countermeasures to stabilize the drone. REFORMA consists of two components: a base policy trained completely in simulation using model-free RL and an adaptation module trained via supervised learning with on-policy datasets. By varying the disturbance strength in an adaptation module, i.e., adopting adaptive adversary, the policy is then able to handle extreme cases when the velocity of the drone is immediately affected by disturbances. Finally, we demonstrate the effectiveness of our method through extensive simulated experiments. To the best of our knowledge, REFORMA is the first robust RL approach that uses adaptive adversaries to tackle uncertain disturbances in drone tasks."
Brain-Inspired Hyperdimensional Computing in the Wild: Lightweight Symbolic Learning for Sensorimotor Controls of Wheeled Robots,"Hyukjun Kwon, Kangwon Kim, Junyoung Lee, Hyunsei Lee, Jiseung Kim, Jinhyung Kim, Taehyeong Kim, Yongnyeon Kim, Yang Ni, Mohsen Imani, Il Hong Suh, Yeseong Kim","DGIST,HYU(Hanyang University),coga-robotics,Hanyang university,University of California, Irvine,University of California Irvine,Hanyang University",Reinforcement Learning II,"Efficiency and performance are significant challenges in applying Machine Learning (ML) to robotics, especially in energy-constrained real-world scenarios. In this context, Hyperdimensional Computing offers an energy-efficient alternative but has been underexplored in robotics. We introduce ReactHD, an HDC-based framework tailored for perception-action-based learning for sensorimotor controls of robot tasks. ReactHD employs hypervectors to encode sensory inputs and learn the suitable high-dimensional pattern for robot actions. It also integrates two HD-based lightweight symbolic learning paradigms: HDC-based supervised learning by demonstration (HDC-IL) and HD-Reinforcement Learning (HDC-RL). It renders robots to show precisely situated reactive behaviors in complex environments. Our empirical evaluations show that ReactHD achieves robust and accurate learning outcomes comparable to state-of-the-art deep learning while substantially improving the performance and energy consumption efficiency by 14.2Ã— and 15.3Ã—. To the best of our knowledge, ReactHD is the first HDC-based framework deployed in real-world settings."
Learning for Deformable Linear Object Insertion Leveraging Flexibility Estimation from Visual Cues,"Mingen Li, Changhyun Choi","University of California, San Diego,University of Minnesota, Twin Cities",Reinforcement Learning II,"Manipulation of deformable Linear objects (DLOs), including iron wire, rubber, silk, and nylon rope, is ubiquitous in daily life. These objects exhibit diverse physical properties, such as Young's modulus and bending stiffness. Such diversity poses challenges for developing generalized manipulation policies. However, previous research limited their scope to single-material DLOs and engaged in time-consuming data collection for the state estimation. In this paper, we propose a two-stage manipulation approach consisting of a material property (e.g., flexibility) estimation and policy learning for DLO insertion with reinforcement learning. Firstly, we design a flexibility estimation scheme that characterizes the properties of different types of DLOs. The ground truth flexibility data is collected in simulation to train our flexibility estimation module. During the manipulation, the robot interacts with the DLOs to estimate flexibility by analyzing their visual configurations. Secondly, we train a policy conditioned on the estimated flexibility to perform challenging DLO insertion tasks. Our pipeline trained with diverse insertion scenarios achieves an 85.6% success rate in simulation and 66.67% in real robot experiments. Please refer to our project page: https://lmeee.github.io/DLOInsert/"
Reinforcement Learning of Action and Query Policies with LTL Instructions under Uncertain Event Detector,"Wataru Hatanaka, Ryota Yamashina, Takamitsu Matsubara","Ricoh Company, Ltd.,RICOH COMPANY, LTD.,Nara Institute of Science and Technology",Reinforcement Learning II,"Reinforcement learning (RL) with linear temporal logic (LTL) objectives can allow robots to carry out symbolic event plans in unknown environments. Most existing methods assume that the event detector can accurately map environmental states to symbolic events; however, uncertainty is inevitable for real-world event detectors. Such uncertainty in an event detector generates multiple branching possibilities on LTL instructions, confusing action decisions. Moreover, the queries to the uncertain event detector, necessary for the task's progress, may increase the uncertainty further. To cope with those issues, we propose an RL framework, Learning Action and Query over Belief LTL (LAQBL), to learn an agent that can consider the diversity of LTL instructions due to uncertain event detection while avoiding task failure due to the unnecessary event-detection query. Our framework simultaneously learns 1) an embedding of belief LTL, which is multiple branching possibilities on LTL instructions using a graph neural network, 2) an action policy, and 3) a query policy which decides whether or not to query for the event detector. Simulations in a 2D grid world and image-input robotic inspection environments show that our method successfully learns actions to follow LTL instructions even with uncertain event detectors."
Guided by the Way: The Role of On-The-Route Objects and Scene Text in Enhancing Outdoor Navigation,"Yanjun Sun, Yue Qiu, Yoshimitsu Aoki, Hirokatsu Kataoka","Keio University,National Institute of Advanced Industrial Science and Technology",Vision-Based Navigation and Learning,"In outdoor environments, Vision-and-Language Navigation (VLN) requires an agent to rely on multi-modal cues from real-world urban environments and natural language instructions. While existing outdoor VLN models predict actions using a combination of panorama and instruction features, this approach ignores objects in the environment and learns data bias to fail navigation. According to our preliminary findings, most instances of navigation failure in previous models were due to turning or stopping at the wrong place. In contrast, humans intuitively frequently use identifiable objects or store names as reference landmarks, ensuring accurate turns and stops, especially in unfamiliar places. To address this insight gap, we propose an Object-Attention VLN (OAVLN) model that helps the agent focus on relevant objects during training and understand the environment better. Our model outperforms previous methods in all evaluation metrics under both seen and unseen scenarios on two existing benchmark datasets, Touchdown and map2seq."
PlaceNav: Topological Navigation through Place Recognition,"Lauri Aleksanteri Suomela, Jussi Oskari Kalliola, Harry Edelman, Joni Kamarainen","Tampere University,Tampere University of Technology",Vision-Based Navigation and Learning,"Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by robots of different types. However, the navigation methods' performance is still limited by the scarcity of suitable training data and they suffer from poor computational scaling. In this work, we present PlaceNav, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayesian filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new method obtains a 76% higher success rate in indoor and 23% higher in outdoor navigation tasks with higher computational efficiency."
Aligning Knowledge Graph with Visual Perception for Object-Goal Navigation,"Nuo Xu, Wen Wang, Rong Yang, Mengjie Qin, Zheyuan Lin, Wei Song, Chunlong Zhang, Jason Gu, Chao Li","Zhejiang Lab,Dalhousie University",Vision-Based Navigation and Learning,"Object-goal navigation is a challenging task that requires guiding an agent to specific objects based on first-person visual observations. The ability of agent to comprehend its surroundings plays a crucial role in achieving successful object finding. However, existing knowledge-graph-based navigators often rely on discrete categorical one-hot vectors and vote counting strategy to construct graph representation of the scenes, which results in misalignment with visual images. To provide more accurate and coherent scene descriptions and address this misalignment issue, we propose the Aligning Knowledge Graph with Visual Perception (AKGVP) method for object-goal navigation. Technically, our approach introduces continuous modeling of the hierarchical scene architecture and leverages visual-language pre-training to align natural language description with visual perception. The integration of a continuous knowledge graph architecture and multimodal feature alignment empowers the navigator with a remarkable zero-shot navigation capability. We extensively evaluate our method using the AI2-THOR simulator and conduct a series of experiments to demonstrate the effectiveness and efficiency of our navigator. Code available: https://github.com/nuoxu/AKGVP."
Probable Object Location (POLo) Score Estimation for Efficient Object Goal Navigation,"Jiaming Wang, Harold Soh",National University of Singapore,Vision-Based Navigation and Learning,"In this work, we focus on object search tasks within unexplored environments. We introduce a framework centered around the Probable Object Location (POLo) score. Utilizing a 3D object probability map, the POLo score allows the agent to make data-driven decisions for efficient object search. We further enhance the framework's practicality by introducing POLoNet, a neural network trained to approximate the computationally-intensive POLo score. Our approach addresses critical limitations of both end-to-end reinforcement learning methods, which suffer from memory decay over long-horizon tasks, and traditional map-based methods that neglect visibility constraints. Our experiments, involving the first phase of the Open-Vocabulary Mobile Manipulation (OVMM) 2023 challenge, demonstrate that an agent equipped with POLoNet significantly outperforms a range of baseline methods, including end-to-end RL techniques and prior map-based strategies. To provide a comprehensive evaluation, we introduce new performance metrics that offer insights into the efficiency and effectiveness of various agents in object goal navigation."
Bridging Zero-Shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill,"Wenzhe Cai, Siyuan Huang, Guangran Cheng, Yuxing Long, Peng Gao, Changyin Sun, Hao Dong","Southeast University,Shanghai Jiao Tong University,Peking University,Shanghai AI lab",Vision-Based Navigation and Learning,"Zero-shot object navigation is a challenging task for home-assistance robots. This task emphasizes visual grounding, commonsense inference and locomotion abilities, where the first two are inherent in foundation models. But for the locomotion part, most works still depend on map-based planning approaches. The gap between RGB space and map space makes it difficult to directly transfer the knowledge from foundation models to navigation tasks. In this work, we propose a Pixel-guided Navigation skill (PixNav), which bridges the gap between the foundation models and the embodied navigation task. It is straightforward for recent foundation models to indicate an object by pixels, and with pixels as the goal specification, our method becomes a versatile navigation policy towards all different kinds of objects. Besides, our PixNav is a pure RGB-based policy that can reduce the cost of home- assistance robots. Experiments demonstrate the robustness of the PixNav which achieves 80+% success rate in the local path-planning task. To perform long-horizon object navigation, we design an LLM-based planner to utilize the commonsense knowledge between objects and rooms to select the best waypoint. Evaluations across both photorealistic indoor simulators and real- world environments validate the effectiveness of our proposed navigation strategy. More details are accessible via our project website https://sites.google.com/view/pixnav/."
GeoAdapt: Self-Supervised Test-Time Adaptation in LiDAR Place Recognition Using Geometric Priors,"Joshua Barton Knights, Stephen Hausler, Sridha Sridharan, Clinton Fookes, Peyman Moghadam","Queensland University of Technology,CSIRO",Vision-Based Navigation and Learning,"LiDAR place recognition approaches based on deep learning suffer from significant performance degradation when there is a shift between the distribution of training and test datasets, often requiring re-training the networks to achieve peak performance. However, obtaining accurate ground truth data for new training data can be prohibitively expensive, especially in complex or GPS-deprived environments. To address this issue we propose GeoAdapt, which introduces a novel auxiliary classification head to generate pseudo-labels for re-training on unseen environments in a self-supervised manner. GeoAdapt uses geometric consistency as a prior to improve the robustness of our generated pseudo-labels against domain shift, improving the performance and reliability of our Test-Time Adaptation approach. Comprehensive experiments show that GeoAdapt significantly boosts place recognition performance across moderate to severe domain shifts, and is competitive with fully supervised test-time adaptation approaches. Our code is available at https://github.com/csiro-robotics/GeoAdapt."
ViPlanner: Visual Semantic Imperative Learning for Local Navigation,"Pascal Roth, Julian Nubert, Fan Yang, Mayank Mittal, Marco Hutter","ETH Zurich,ETH Zürich",Vision-Based Navigation and Learning,"Real-time path planning in outdoor environments still challenges modern robotic systems due to differences in terrain traversability, diverse obstacles, and the necessity for fast decision-making. Established approaches have primarily focused on geometric navigation solutions, which work well for structured geometric obstacles but have limitations regarding the semantic interpretation of different terrain types and their affordances. Moreover, these methods fail to identify traversable geometric occurrences, such as stairs. To overcome these issues, we introduce ViPlanner, a learned local path planning approach that generates local plans based on geometric and semantic information. The system is trained using the Imperative Learning paradigm, for which the network weights are optimized end-to-end based on the planning task objective. This optimization uses a differentiable formulation of a semantic costmap, which enables the planner to distinguish between the traversability of different terrains and accurately identify obstacles. The semantic information is represented in 30 classes using an RGB colorspace that can effectively encode the multiple levels of traversability. We show that the planner can adapt to diverse real-world environments without requiring any real-world training. In fact, the planner is trained purely in simulation, enabling a highly scalable training data generation. Experimental results demonstrate resistance to noise, zero-shot sim-to-real transfer, and a decrease of 38.02% in terms of traversability cost compared to purely geometric-based approaches. Code and models are made publicly available: https://github.com/leggedrobotics/viplanner."
UIVNAV: Underwater Information-Driven Vision-Based Navigation Via Imitation Learning,"Xiaomin Lin, Nare Karapetyan, Kaustubh Joshi, Tianchen Liu, Nikhil Chopra, Miao Yu, Pratap Tokekar, Yiannis Aloimonos","University of Maryland,Woods Hole Oceanographic Institution,University of Maryland College Park,University of Maryland, College Park",Vision-Based Navigation and Learning,"Autonomous navigation in the underwater environment is challenging due to limited visibility, dynamic changes, and the lack of a cost-efficient, accurate localization system. We introduce UIVNav, a novel end-to-end underwater navigation solution designed to navigate robots over Objects of Interest (OOI) while avoiding obstacles, all without relying on localization. UIVNav utilizes imitation learning and draws inspiration from the navigation strategies employed by human divers, who do not rely on localization. UIVNav consists of the following phases: (1) generating an intermediate representation (IR) and (2) training the navigation policy based on human-labeled IR. By training the navigation policy on IR instead of raw data, the second phase is domain-invariant --- the navigation policy does not need to be retrained if the domain or the OOI changes. We demonstrate this within simulation by deploying the same navigation policy to survey two distinct Objects of Interest (OOIs): oyster and rock reefs. We compared our method with complete coverage and random walk methods, showing that our approach is more efficient in gathering information for OOIs while avoiding obstacles. The results show that UIVNav chooses to visit the areas with larger area sizes of oysters or rocks with no prior information about the environment or localization. Moreover, a robot using UIVNav compared to complete coverage method surveys on average 36% more oysters when traveling the same distances. We also demonstrate the feasibility of real-time deployment of UIVNav in pool experiments with BlueROV underwater robot for surveying a bed of oyster shells."
"Wait, That Feels Familiar: Learning to Extrapolate Human Preferences for Preference-Aligned Path Planning","Haresh Karnan, Elvin Yang, Garrett Warnell, Peter Stone, Joydeep Biswas","The University of Texas at Austin,University of Michigan, Ann Arbor,U.S. Army Research Laboratory,University of Texas at Austin",Vision-Based Navigation and Learning,"Autonomous mobility tasks such as last-mile delivery require reasoning about operator-indicated preferences over terrains on which the robot should navigate to ensure both robot safety and mission success. However, coping with out-of-distribution data from novel terrains or appearance changes due to lighting variations remains a fundamental problem in visual terrain-adaptive navigation. Existing solutions either require labor-intensive manual data re-collection and labeling or use hand-coded reward functions that may not align with operator preferences. In this work, we posit that operator preferences for visually novel terrains, which the robot should adhere to, can often be extrapolated from established terrain preferences within the inertial-proprioceptive-tactile domain. Leveraging this insight, we introduce Preference extrApolation for Terrain-awarE Robot Navigation (PATERN), a novel framework for extrapolating operator terrain preferences for visual navigation. PATERN learns to map inertial-proprioceptive-tactile measurements from the robotâ€™s observations to a representation space and performs a nearest-neighbor search in this space to estimate operator preferences over novel terrains. Through physical robot experiments in outdoor environments, we assess PATERNâ€™s capability to extrapolate preferences and generalize to novel terrains and challenging lighting conditions. Compared to baseline approaches, our findings indicate that PATERN robustly generalizes to diverse terrains and varied lighting conditions, while navigating in a preference-aligned manner."
A Biomorphic Whisker Sensor for Aerial Tactile Applications,"Chaoxiang Ye, Guido De Croon, Salua Hamaza","Delft University of Technology,TU Delft",Soft Sensors and Actuators I,"Unmanned air vehicles (UAVs) have traditionally been considered as â€œeyes in the skyâ€, that can move in three dimensions and need to avoid any contact with their environment. On the contrary, contact should not be considered as a problem, but as an opportunity to expand the range of UAVs applications. In this paper, we designed, fabricated, and characterized a whisker sensor unit based on MEMS barometers suitable for tactile localization on UAVs, featuring lightweight, low stiffness, high sensitivity, a broad sensing range, and scalability. Then, for the challenging task of contact point localization, we propose a Recurrent Multi output Network (RMN) for predicting 3D contact points under continuous contact conditions to address the problems of non-linearity, hysteresis, and non-injective mapping between signals and contact points by considering time series. In addition, we propose an azimuth prediction loss function which reduces the RMSE by 3.24â—¦ compared to L1 loss. Finally, we conduct experiments on a linear stage to validate the 3D contact point localization capability of the proposed whisker system and model. The results show that our localization can achieve excellent performance, with an inference time of 1.4 ms and a mean error of only 9.18 mm in Euclidean distance within 3D space, laying a robust foundation for future implementation of tactile localization on UAVs. The design files, dataset, and source code are available on: https: //github.com/BioMorphic-Intelligence-Lab/Whisker-3D-Localiz ation."
Embedded Air Channels Transform Soft Lattices into Sensorized Grippers,"Annan Zhang, Lillian Chin, Daniel L Tong, Daniela Rus","Massachusetts Institute of Technology,UT Austin,MIT",Soft Sensors and Actuators I,"Sensing plays a pivotal role in robotic manipulation, dictating the accuracy and versatility with which objects are handled. Vision-based sensing methods often suffer from fabrication complexity and low durability, while approaches that rely on direct measurements on the gripper often have limited resolution and are difficult to scale. Here, we present a soft robotic gripper made out of two cubic lattices that are sensorized by embedding air channels within the structure. The lattices are 3D printed from a single build material, simplifying the fabrication process. The flexibility of this approach offers significant control over sensor and lattice design, while the pressure-based internal sensing provides measurements with minimal disruption to the grasping surface. With only 12 sensors, 6 per lattice, this gripper can estimate an object's weight and location and offer new insights into grasp parameters like friction coefficients and grasp force."
Towards Automatic Design of Soft Pneumatic Actuators: Inner Structure Design Using CNN Model and BÃ©zier Curve-Based Genetic Algorithm,"Loïc Mosser, Laurent Barbé, Lennart Rubbert, Pierre Renaud","Icube - Université de strasbourg,University of Strasbourg, ICube CNRS,INSA - Strasbourg,ICube",Soft Sensors and Actuators I,"In this paper, the development of a method for the design of soft pneumatic actuators is described. The focus is given on the interest of using a deep learning model to explore the design space with a genetic algorithm. In particular, we propose to perform the automatic synthesis of the inner structure of pneumatic actuators using BÃ©zier curves and Gaussian Mixture Points, to have a simple representation of the actuator genotype. This makes it possible to represent a wide variety of structures and to take into account the presence of the actuator pneumatic supply. It is shown a CNN model can interestingly be used in conjunction with FEM. FEM is being used to train initially the CNN model and for the control of accuracy, while the CNN model reduces the computational cost, offering a sufficient accuracy during the synthesis thanks to transfer learning. Through two case studies, the capacity of generating geometrically complex designs such as a double-helix network for a twisting actuator is outlined. Its possible extension and further use are also discussed."
Design of a Rigid-Soft Hybrid Robotic Glove with Force Sensing Function,"Hexin Li, Jiang Li, Ruichen Zhen, Ming Cheng, Kehan Ding","Harbin Institute of Technology,Harbin institute of technology",Soft Sensors and Actuators I,"Soft robotic gloves can not only provide timely, effective, safe and cheap rehabilitation training for patients with impaired movement function of hand, but also assist in completing daily grasping activities. However, most soft robotic gloves are completely composed of flexible structures. Although they have high flexibility and safety, there are problems such as poor fit and low output force. In order to solve these problems, this paper refers to the structure of the human hand and designs an articulated rigid-soft hybrid robotic glove, which combines the advantages of rigid robotic gloves and soft robotic gloves, and has high flexibility, high output force and good fit. In addition, soft robotic gloves generally lack the ability to sense the force between the human hand and the glove. Therefore, this paper designed an array flexible force sensor, and studied the structure, signal acquisition and preparation process of the sensor. Finally, a complete test platform was built to test the performance of the rigid-soft hybrid robotic glove with force sensing function. The test results show that the robotic glove has good fit and high output force, can effectively assist training and assist grasping, and can perceive the contact force."
SoftER: A Spiral Soft Robotic Ejector for Sorting Applications,"Ilias Zournatzis, Sotiris Kalaitzakis, Panagiotis Polygerinos",Hellenic Mediterranean University,Soft Sensors and Actuators I,"For over thirty years optical belt drive configurations are being used in food industries to automatically sort produce in high operating speeds. Despite their benefits, these multi-component assemblies are prone to faults, difficult to clean, and require frequent maintenance that halts the production lines for considerable amount of time. In this paper, we adopt the abundantly occurring spiral motions encountered in nature and translate them to the proof-of concept design and development of a soft pneumatic actuator (SPA), the SoftER. This novel actuator has the ability to rapidly unwind when pressurized to deliver impact forces. We explore this inherently low-cost and simple design and its potential to replace current systems based on the results of an application case study presented in this paper. Simulation driven optimization methods are leveraged, utilizing quasi-static and dynamic finite element methods models, to create a scalable framework for selecting the best performing design parameters for each application. Using rapid manufacturing processes the optimized actuator is constructed and physical testing validates its high-speed and impact force delivering capabilities."
Design and Validation of Soft Sliding Structure with Adjustable Stiffness for Ankle Sprain Prevention,"Seoyeon Ham, Soe Lin Paing, Brian Byunghyun Kang, Hyunglae Lee, Wansoo Kim","Hanyang University,Arizona State University,Sejong University,Hanyang University ERICA",Soft Sensors and Actuators I,"This study presents the design and validation of a soft sliding stiffness structure with a soft-rigid layer sliding mechanism. It aims to mitigate ankle sprains and address the progression of chronic ankle instability by providing stiffness support. The soft-rigid layer sliding mechanism of the structure is designed to achieve a wide range of stiffness while maintaining a compact form factor. The structure incorporates rigid retainer pieces within each layer, which allows for sliding within a hollow cuboid structure and enables modulation of stiffness. An analytical model is presented to investigate the variations in stiffness resulting from the different sliding states. The stiffness characteristics of the structure were validated through both bench tests and human subject tests. The gradual sliding of the structureâ€™s layer resulted in an increase in stiffness, aligning with the analytical modelâ€™s predictions. At the most rigid stage (0% alignment), the stiffness exhibited a significant increase of 111.1% compared to the most flexible stage (100% alignment). Additionally, the human subject testing demonstrated a stiffness increase of up to 93.8%. These results underscore the potential applicability of the soft sliding structure in ankle support applications."
"Capacitive Origami Sensing Modules for Measuring Force in a Neurosurgical, Soft Robotic Retractor","Daniel Van Lewen, Catherine Wang, Hun Chan Lee, Anand Devaiah, Urvashi Upadhyay, Sheila Russo",Boston University,Soft Sensors and Actuators I,"In neurosurgery, soft robots have the potential to introduce significant benefits over traditional metal tools for their ability to safely interact with delicate tissues. In this paper, we introduce a proof-of-concept soft, capacitive origami sensing module (OSM) that can measure forces during neurosurgical retraction. Using origami-inspired design and fabrication principles, the OSM is easily folded and integrated within a soft robotic retractor that interacts with brain tissue to generate a surgical workspace upon actuation. We demonstrate the individual OSM signal response to forces and folding. We further characterize the OSM response within a fully-assembled soft robotic retractor to both folding and the application of forces over 0-5 N showing a 0.38 N average prediction error and resolution of 0.25 N. The sensing capability of the retractor is validated on an in-vitro model to demonstrate prediction errors of 0.06 N and the proposed operation during neurosurgery."
A Soft Miniaturized Continuum Robot with 3D Shape Sensing Via Functionalized Soft Optical Waveguides,"Viola Del Bono, Max Mccandless, Frank Juliá Wise, Sheila Russo","Boston University,Boston Children's Hospital",Soft Sensors and Actuators I,"In this paper, we present a fully soft miniaturized continuum robot that integrates 3D optical shape sensing through functionalized tubing used as soft optical waveguides. The sensor is fabricated by laser patterning an off-the-shelf medical tubing, allowing for bidirectional responses to large curvatures in two bending directions, enabling 3D shape sensing and tip tracking of the continuum robot. The robot is able to bend and sense its own shape up to a curvature of 44.7 m-1, corresponding to a bending angle of 102â—¦, having high accuracy tracking capabilities, resulting in an average tracking error of 3.08 mm, that is 7.7 % of the robot length. The robotâ€™s functionality was shown in validation experiments, including a real-time shape prediction through a graphical user interface."
Continuously Estimate and Control Prosthetic Grip Force by an Optical Waveguide Sensor,"Linhang Ju, Hanze Jia, Yanjun Shi, Xilun Ding, Yanggang Feng, Wuxiang Zhang","Beihang University,Beijing Univerisity of Aeronautics and Astronautics",Soft Sensors and Actuators I,"The emergence of intelligent prostheses has facilitated the life and work of disabled patients. The interaction aspect of prostheses has become a highlight research topic in the field of rehabilitation robotics. However, most of the existing prosthetic interaction methods focus on the use of myoelectricity to classify finite gestures, rather than continuous (infinite) force detection, which greatly limits the use of prosthetic scenarios. In this study, a novel optical waveguide sensor was used to collect muscle deformation information from the human arm for continuous control of the prosthetic grip force. The optical waveguide sensor was embedded with carbon fiber to limit the stretching of the waveguide, which led to the optical waveguide sensor being sensitive to bending deformation. Compared with EMGs, the accuracy of continuous grip force control based on the optical waveguide sensor is higher. The R-Square for prosthetic grip force and hand grip force were 0.867 and 0.9724 in the periodic and sustaining grip force experiments, respectively. The results suggested that the proposed method could provide a new approach to the interaction of prostheses."
UAV-Sim: NeRF-Based Synthetic Data Generation for UAV-Based Perception,"Christopher Maxey, Jaehoon Choi, Hyungtae Lee, Dinesh Manocha, Heesung Kwon","University of Maryland, Army Research Laboratory,University of Maryland, College Park,US Army Research Laboratory,University of Maryland,DEVCOM Army Research Laboratory",Deep Learning for Visual Perception III,"Tremendous variations coupled with large degrees of freedom in UAV-based imaging conditions lead to a significant lack of data in adequately learning UAV-based perception models. Using various synthetic renderers in conjunction with perception models is prevalent to create synthetic data to augment the learning in the ground-based imaging domain. However, severe challenges in the austere UAV-based domain require distinctive solutions to image synthesis for data aug- mentation. In this work, we leverage recent advancements in neural rendering to improve static and dynamic novel- view UAV-based image synthesis, especially from high altitudes, capturing salient scene attributes. Finally, we demonstrate a considerable performance boost is achieved when a state-of- the-art detection model is optimized primarily on hybrid sets of real and synthetic data instead of the real or synthetic data separately."
Contrastive Learning for Enhancing Robust Scene Transfer in Vision-Based Agile Flight,"Jiaxu Xing, Leonard Bauersfeld, Yunlong Song, Chunwei Xing, Davide Scaramuzza","University of Zurich,University of Zurich (UZH),ETH Zurich",Deep Learning for Visual Perception III,"Scene transfer for vision-based mobile robotics applications is a highly relevant and challenging problem. The utility of a robot greatly depends on its ability to perform a task in the real world, outside of a well-controlled lab environment. Existing scene transfer end-to-end policy learning approaches often suffer from poor sample efficiency or limited generalization capabilities, making them unsuitable for mobile robotics applications. This work proposes an adaptive multi-pair contrastive learning strategy for visual representation learning that enables zero-shot scene transfer and real-world deployment. Control policies relying on the embedding are able to operate in unseen environments without the need for finetuning in the deployment environment. We demonstrate the performance of our approach on the task of agile, vision-based quadrotor flight. Extensive simulation and real-world experiments demonstrate that our approach successfully generalizes beyond the training domain and outperforms all baselines."
Watching the Air Rise: Learning-Based Single-Frame Schlieren Detection,"Florian Achermann, Julian Andreas Haug, Tobias Zumsteg, Nicholas Lawrance, Jen Jen Chung, Andrey Kolobov, Roland Siegwart","ETH Zurich, ASL,ETH Zurich,ETH Zürich,CSIRO Data,,,The University of Queensland,Microsoft Research",Deep Learning for Visual Perception III,"Detecting air flows caused by phenomena such as heat convection is valuable in multiple scenarios, including leak identification and locating thermal updrafts for extending UAV flight duration. Unfortunately, the heat signature of these flows is often too subtle to be seen by a thermal camera. While convection also leads to fluctuations in air density and hence causes so-called schlieren â€“ intensity and color variations in images â€“ existing techniques such as Background-oriented schlieren (BOS) allow detecting them only against a known background and from a static camera, making these approaches unsuitable for moving vehicles. In this work we demonstrate the feasibility of visualizing air movement by predicting the corresponding schlieren-induced optical flow from a single greyscale image captured by a moving camera against an unfamiliar background. We first record and label a set of optical flows in an indoor setup using standard BOS techniques. We then train a convolutional neural network (CNN) by applying the previously collected optical flow distortions to a dataset containing a mixture of real and synthetically generated images to predict the two-dimensional optical flow from a single image. Finally, we evaluate our approach on the task of extracting the optical flow caused by schlieren from both a static and moving camera on previously unseen flow patterns and background images."
High-Throughput Visual Nano-Drone to Nano-Drone Relative Localization Using Onboard Fully Convolutional Networks,"Luca Crupi, Alessandro Giusti, Daniele Palossi","IDSIA USI-SUPSI,ETH Zurich",Deep Learning for Visual Perception III,"Relative drone-to-drone localization is a fundamental building block for any swarm operations. We address this task in the context of miniaturized nano-drones, i.e., 10cm in diameter, which show an ever-growing interest due to novel use cases enabled by their reduced form factor. The price for their versatility comes with limited onboard resources, i.e., sensors, processing units, and memory, which limits the complexity of the onboard algorithms. A traditional solution to overcome these limitations is represented by lightweight deep learning models directly deployed aboard nano-drones. This work tackles the challenging relative pose estimation between nano-drones using only a gray-scale low-resolution camera and an ultra-low-power System-on-Chip (SoC) hosted onboard. We present a vertically integrated system based on a novel vision-based fully convolutional neural network (FCNN), which runs at 39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8 SoC. We compare our FCNN against three State-of-the-Art (SoA) systems. Considering the best-performing SoA approach, our model results in a R^2 improvement from 32 to 47% on the horizontal image coordinate and from 18 to 55% on the vertical image coordinate, on a real-world dataset of 30k images. Finally, our in-field tests show a reduction of the average tracking error of 37% compared to a previous SoA work and an endurance performance up to the entire battery lifetime of 4min."
End-To-End Semi-Supervised 3D Instance Segmentation with PCTeacher,"Linfeng Li, Na Zhao","Nanyang Technological University,SUTD",Deep Learning for Visual Perception III,"3D instance segmentation is a fundamental and critical task for enabling robots to operate effectively in unstructured 3D environments. In order to address the challenges posed by the high demand for large-scale annotated data and the limited availability of such data in the context of 3D instance segmentation, we study semi-supervised 3D instance segmentation problem and propose a novel end-to-end framework based on the mean teacher paradigm, named PCTeacher. Our PCTeacher generates both point-level and cluster-level pseudo labels to harness knowledge from unlabeled data. It notably enhances the training stability through end-to-end training and improves pseudo-label quality. Specifically, for point-level pseudo labels, PCTeacher employs a multi-view fusion strategy to achieve higher precision and recall. Regarding cluster-level pseudo labels, it introduces a hybrid grouping strategy to generate more potential proposals and utilizes a point-cluster agreement-based thresholding (PCAT) mechanism to fully exploit cluster-level pseudo labels. By combining and strengthening both point-level and cluster-level pseudo labels, our PCTeacher achieves state-of-the-art performance on two benchmark datasets across multiple labeled data ratios with a more compact network compared to the existing method."
PAg-NeRF: Towards Fast and Efficient End-To-End Panoptic 3D Representations for Agricultural Robotics,"Claus Smitt, Michael Halstead, Patrick Zimmer, Thomas Läbe, Esra Guclu, Cyrill Stachniss, Chris McCool","University of Bonn,Bonn University",Deep Learning for Visual Perception III,"Precise scene understanding is key for most robot monitoring and intervention tasks in agriculture. In this work we present PAg-NeRF which is a novel NeRF-based system that enables 3D panoptic scene understanding. Our representation is trained using an image sequence with noisy robot odometry poses and automatic panoptic predictions with inconsistent IDs between frames. Despite this noisy input, our system is able to output scene geometry, photo-realistic renders and 3D consistent panoptic representations with consistent instance IDs. We evaluate this novel system in a very challenging horticultural scenario and in doing so demonstrate an end-to-end trainable system that can make use of noisy robot poses rather than precise poses that have to be pre-calculated. Compared to a baseline approach the peak signal to noise ratio is improved from 21.34dB to 23.37dB while the panoptic quality improves from 56.65% to 70.08%. Furthermore, our approach is faster and can be tuned to improve inference time by more than a factor of 2 while being memory efficient with approximately 12 times fewer parameters. Code, data and interactive results are available at https://claussmitt.com/pagnerf"
GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints,"Anqi Cheng, Zhiyuan Yang, Haiyue Zhu, Kezhi Mao","Nanyang Technological University (NTU),Agency for Science, Technology and Research (A*STAR)",Deep Learning for Visual Perception III,"Self-supervised depth estimation has evolved into an image reconstruction task that minimizes a photometric loss. While recent methods have made strides in indoor depth estimation, they often produce inconsistent depth estimation in textureless areas and unsatisfactory depth discrepancies at object boundaries. To address these issues, in this work, we propose GAM-Depth, developed upon two novel components: gradient-aware mask and semantic constraints. The gradient-aware mask enables adaptive and robust supervision for both key areas and textureless regions by allocating weights based on gradient magnitudes.The incorporation of semantic constraints for indoor self-supervised depth estimation improves depth discrepancies at object boundaries, leveraging a co-optimization network and proxy semantic labels derived from a pretrained segmentation model. Experimental studies on three indoor datasets, including NYUv2, ScanNet, and InteriorNet, show that GAM-Depth outperforms existing methods and achieves state-of-the-art performance, signifying a meaningful step forward in indoor depth estimation. Our code will be available at https://github.com/AnqiCheng1234/GAM-Depth."
RoboKeyGen: Robot Pose and Joint Angles Estimation Via Diffusion-Based 3D Keypoint Generation,"Yang Tian, Jiyao Zhang, Guowei Huang, Bin Wang, Ping Wang, Jiangmiao Pang, Hao Dong","Peking University,Huawei Technologies Co., Ltd.,Noah's Ark Lab, Huawei,Shanghai AI Laboratory",Deep Learning for Visual Perception III,"Estimating robot pose and joint angles is pivotal in advanced robotics, underpinning applications like robot collaboration and online hand-eye calibration. However, the introduction of unknown joint angles makes prediction more complex than simple robot pose estimation, due to its higher dimensionality. Previous methods either regress 3D keypoints directly or utilise a render&compare strategy. These approaches often falter in terms of performance or efficiency and grapple with the cross-camera gap problem. This paper presents a novel framework that bifurcates the high-dimensional prediction challenge into two manageable subtasks: detecting 2D keypoints and lifting 2D keypoints to 3D. This separation promises enhanced performance without sacrificing the efficiency innate to keypoint-based techniques. A vital component of our method is the lifting of 2D projections to 3D keypoints. Common deterministic regression methods may falter when faced with uncertainties from 2D detection errors or self-occlusions. Leveraging the robust modeling potential of diffusion models, we reframe this issue as a conditional 3D keypoints generation task. To bolster cross-camera adaptability, we introduce the Normalized Camera Coordinate Space (NCCS), ensuring alignment of estimated 2D keypoints across varying camera intrinsics. Experimental results demonstrate that the proposed method outperforms the state-of-the-art render&compare method RoboPose and achieves higher inference speed. Furthermore, the tests accentuate our method's robust cross-camera generalisation capabilities. We intend to release both the dataset and code."
Advancements in 3D Lane Detection Using LiDAR Point Clouds: From Data Collection to Model Development,"Runkai Zhao, Yuwen Heng, Heng Wang, Yuanda Gao, Shilei Liu, Changhao Yao, Jiawen Chen, Weidong Cai","University of Sydney,Baidu ACG,Shanghai Jiao Tong University",Deep Learning for Visual Perception III,"Advanced Driver-Assistance Systems (ADAS) have successfully integrated learning-based techniques into vehicle perception and decision-making. However, their application in 3D lane detection for effective driving environment perception is hindered by the lack of comprehensive LiDAR datasets. The sparse nature of LiDAR point cloud data prevents an efficient manual annotation process. To solve this problem, we present LiSV-3DLane, a large-scale 3D lane dataset that comprises 20k frames of surround-view LiDAR point clouds with enriched semantic annotation. Unlike existing datasets confined to a frontal perspective, LiSV-3DLane provides a full 360-degree spatial panorama around the ego vehicle, capturing complex lane patterns in both urban and highway environments. We leverage the geometric traits of lane lines and the intrinsic spatial attributes of LiDAR data to design a simple yet effective automatic annotation pipeline for generating finer lane labels. To propel future research, we propose a novel LiDAR-based 3D lane detection model, LiLaDet, incorporating the spatial geometry learning of the LiDAR point cloud into Birdâ€™s Eye View (BEV) based lane identification. Experimental results indicate that LiLaDet outperforms existing camera- and LiDAR-based approaches in the 3D lane detection task on the K-Lane dataset and our LiSV-3DLane."
STOPNet: Multiview-Based 6-DoF Suction Detection for Transparent Objects on Production Lines,"Yuxuan Kuang, Qin Han, Danshi Li, Qiyu Dai, Lian Ding, Dong Sun, Hanlin Zhao, He Wang","Peking University,New York University,Huawei Cloud Computing Technologies Co., Ltd.",Deep Learning in Grasping and Manipulation III,"In this work, we present STOPNet, a framework for 6-DoF object suction detection on production lines, with a focus on but not limited to transparent objects, which is an important and challenging problem in robotic systems and modern industry. Current methods requiring depth input fail on transparent objects due to depth camerasâ€™ deficiency in sensing their geometry, while we proposed a novel framework to reconstruct the scene on the production line depending only on RGB input, based on multiview stereo. Compared to existing works, our method not only reconstructs the whole 3D scene in order to obtain high-quality 6-DoF suction poses in real time but also generalizes to novel environments, novel arrangements and novel objects, including challenging transparent objects, both in simulation and the real world. Extensive experiments in simulation and the real world show that our method significantly surpasses the baselines and has better generalizability, which caters to practical industrial needs."
RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation,"Mel Vecerik, Carl Doersch, Yi Yang, Todor Bozhinov Davchev, Yusuf Aytar, Guangyao Zhou, Raia Hadsell, Lourdes Agapito, Jonathan Scholz","DeepMind,Google DeepMind,University College London,Google Deepmind",Deep Learning in Grasping and Manipulation III,"For robots to be useful outside labs and specialized factories we need a way to teach them new useful behaviors quickly. Current approaches lack either the generality to onboard new tasks without task-specific engineering, or else lack the data-efficiency to do so in an amount of time that enables practical use. In this work we explore dense tracking as a representational vehicle to allow faster and more general learning from demonstration. Our approach utilizes Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration, and parameterize a low-level controller to reproduce this motion across changes in the scene configuration.We show this results in robust robot policies that can solve complex object-arrangement tasks such as shape-matching,stacking, and even full path-following tasks such as applying glue and sticking objects together, all from demonstrations that can be collected in minutes."
Learning Extrinsic Dexterity with Parameterized Manipulation Primitives,"Shih-Min Yang, Martin Magnusson, Johannes A. Stork, Todor Stoyanov","Örebro University,Orebro University",Deep Learning in Grasping and Manipulation III,"Many practically relevant robot grasping problems feature a target object for which all grasps are occluded, e.g., by the environment. Single-shot grasp planning invariably fails in such scenarios. Instead, it is necessary to first manipulate the object into a configuration that affords a grasp. We solve this problem by learning a sequence of actions that utilize the environment to change the object's pose. Concretely, we employ hierarchical reinforcement learning to combine a sequence of learned parameterized manipulation primitives. By learning the low-level manipulation policies, our approach can control the object's state through exploiting interactions between the object, the gripper, and the environment. Designing such a complex behavior analytically would be infeasible under uncontrolled conditions, as an analytic approach requires accurate physical modeling of the interaction and contact dynamics. In contrast, we learn a hierarchical policy model that operates directly on depth perception data, without the need for object detection, pose estimation, or manual design of controllers. We evaluate our approach on picking box-shaped objects of various weight, shape, and friction properties from a constrained table-top workspace. Our method transfers to a real robot and is able to successfully complete the object picking task in 98% of experimental trials."
"Learning Active Manipulation to Target Shapes with Model-Free, Long-Horizon Deep Reinforcement Learning","Matias Sivertsvik, Kirill Sumskiy, Ekrem Misimi","Norwegian University of Science and Technology,SINTEF Ocean",Deep Learning in Grasping and Manipulation III,"We investigate the active manipulation of objects using model-free and long-horizon DRL (Deep Reinforcement Learning) to achieve target shapes. Our proposed approach uses visual observations consisting of segmented images, to mitigate the sim-to-real gap. We address a long-horizon manipulation task requiring a sequence of accurate actions to achieve the target shapes using a robot arm with an RGB-D camera in eye-in-hand configuration, and an elongated, volumetric, elastoplastic object. We find similar objects in food, marine, and manufacturing domains. The aim is to actively manipulate the object into an arbitrary target shape using image observations. We trained a DRL agent using PPO (Proximal Policy Optimization) by running 768 parallel actors in simulation, for a total of 1,2M environment interactions, and tested this on 200 unseen target deformations. In three attempts, 82% of the trials achieved a greater than 90% overlap with the 200 target shapes. By relying on segmentation images as a visual observation space, we successfully transferred the agent to the real world without supplementary training. Our approach does not need any real-world manipulation examples nor fine-tuning in the real world. The robustness of our approach was demonstrated in simulation, and experimentally validated in the real world for specific manipulation tasks, achieving a 94.2% mean zero-shot overlap success rate on previously unseen target shapes."
GAMMA: Generalizable Articulation Modeling and Manipulation for Articulated Objects,"Qiaojun Yu, Junbo Wang, Wenhai Liu, Ce Hao, Liu Liu, Lin Shao, Weiming Wang, Cewu Lu","SHANGHAI JIAO TONG UNIVERSITY,Shanghai Jiao Tong University,University of California, Berkeley,Hefei University of Technology,National University of Singapore,ShangHai Jiao Tong University",Deep Learning in Grasping and Manipulation III,"Articulated objects like cabinets and doors are widespread in daily life. However, directly manipulating 3D articulated objects is challenging because they have diverse geometrical shapes, semantic categories, and kinetic constraints. Prior works mostly focused on recognizing and manipulating articulated objects with specific joint types. They can either estimate the joint parameters or distinguish suitable grasp poses to facilitate trajectory planning. Although these approaches have succeeded in certain types of articulated objects, they lack generalizability to unseen objects, which significantly impedes their application in broader scenarios. In this paper, we propose a novel framework of Generalizable Articulation Modeling and Manipulating for Articulated Objects (GAMMA), which learns both articulation modeling and grasp pose affordance from diverse articulated objects with different categories. In addition, GAMMA adopts adaptive manipulation to iteratively reduce the modeling errors and enhance manipulation performance. We train GAMMA with the PartNet-Mobility dataset and evaluate with comprehensive experiments in SAPIEN simulation and real-world Franka robot. Results show that GAMMA significantly outperforms SOTA articulation modeling and manipulation algorithms in unseen and cross-category articulated objects. Images, videos and codes are published on the project website at: http://sites.google.com/view/gamma-articulation"
Efficient End-To-End Detection of 6-DoF Grasps for Robotic Bin Picking,"Yushi Liu, Alexander Qualmann, Zehao Yu, Miroslav Gabriel, Philipp Schillinger, Markus Spies, Ngo Anh Vien, Andreas Geiger","University Tübingen,Robert Bosch GmbH, Corporate Sector Research and Advance Enginee,University of Tübingen,Bosch Center for Artificial Intelligence,Bosch GmbH,Max Planck Institute for Intelligent Systems, Tübingen",Deep Learning in Grasping and Manipulation III,"Bin picking is an important building block for many robotic systems, in logistics, production or in household use-cases. In recent years, machine learning methods for the prediction of 6-DoF grasps on diverse and unknown objects have shown promising progress. However, existing approaches only consider a single ground-truth grasp orientation at a grasp location during training and therefore can only predict limited grasp orientations which leads to a reduced number of feasible grasps in bin picking with restricted reachability. In this paper, we propose a novel approach for learning dense and diverse 6-DoF grasps for parallel-jaw grippers in robotic bin picking. We introduce a parameterized grasp distribution model based on Power-Spherical distributions that enables a training based on all possible ground-truth samples. Thereby, we also consider the grasp uncertainty enhancing the modelâ€™s robustness to noisy inputs. As a result, given a single top-down view depth image, our model can generate diverse grasps with multiple collision-free grasp orientations. Experimental evaluations in simulation and on a real robotic bin picking setup demonstrate the modelâ€™s ability to generalize across various object categories achieving an object clearing rate of around 90% in simulation and real-world experiments. We also outperform state of the art approaches. Moreover, the proposed approach exhibits its usability in real robot experiments without any refinement steps, even when only trained on a synthetic dataset, due to the probabilistic grasp distribution modeling."
Contact Energy Based Hindsight Experience Prioritization,"Erdi Sayar, Zhenshan Bing, Carlo D’Eramo, Ozgur S. Oguz, Zhenshan Bing","Technical University of Munich,University of Würzburg,Bilkent University,Tech. Univ. Muenchen TUM",Deep Learning in Grasping and Manipulation III,"Multi-goal robot manipulation tasks with sparse rewards are difficult for reinforcement learning (RL) algorithms due to the inefficiency in collecting successful experiences. Recent algorithms such as Hindsight Experience Replay (HER) expedite learning by taking advantage of failed trajectories and replacing the desired goal with one of the achieved states so that any failed trajectory can be utilized as a contribution to learning. However, HER uniformly chooses failed trajectories, without taking into account which ones might be the most valuable for learning. In this paper, we address this problem and propose a novel approach Contact Energy Based Prioritization~(CEBP) to select the samples from the replay buffer based on rich information due to contact, leveraging the touch sensors in the gripper of the robot and object displacement. Our prioritization scheme favors sampling of contact-rich experiences, which are arguably the ones providing the largest amount of information. We evaluate our proposed approach on various sparse reward robotic tasks and compare it with the state-of-the-art methods. We show that our method surpasses or performs on par with those methods on robot manipulation tasks. Finally, we deploy the trained policy from our method to a real Franka robot for a pick-and-place task. We observe that the robot can solve the task successfully. The videos and code are publicly available at: https://erdiphd.github.io/HER_force/"
ASGrasp: Generalizable Transparent Object Reconstruction and 6-DoF Grasp Detection from RGB-D Active Stereo Camera,"Jun Shi, Yong A, Yixiang Jin, Dingzhe Li, Haoyu Niu, Zhezhu Jin, He Wang","Samsung Research China – Beijing (SRC-B),Beihang University,University of Chinese Academy of Sciences,Samsung Research Institute China – Beijing (SRC-B),Peking University",Deep Learning in Grasping and Manipulation III,"In this paper, we tackle the problem of grasping transparent and specular objects. This issue holds importance, yet it remains unsolved within the field of robotics due to failure of recover their accurate geometry by depth cameras. For the first time, we propose ASGrasp, a 6-DoF grasp detection network that uses an RGB-D active stereo camera. ASGrasp utilizes a two-layer learning-based stereo network for the purpose of transparent object reconstruction, enabling material-agnostic object grasping in cluttered environments. In contrast to existing RGB-D based grasp detection methods, which heavily depend on depth restoration networks and the quality of depth maps generated by depth cameras, our system distinguishes itself by its ability to directly utilize raw IR and RGB images for transparent object geometry reconstruction. We create an extensive synthetic dataset through domain randomization, which is based on GraspNet-1Billion. Our experiments demonstrate that ASGrasp can achieve over 90% success rate for generalizable transparent object grasping in both simulation and the real via seamless sim-to-real transfer. Our method significantly outperforms SOTA networks and even surpasses the performance upper bound set by perfect visible point cloud inputs. Project page: https://pku-epic.github.io/ASGrasp"
An Offline Learning of Behavior Correction Policy for Vision-Based Robotic Manipulation,"Qingxiuxiong Dong, Toshimitsu Kaneko, Masashi Sugiyama","Toshiba Corporation,The University of Tokyo",Deep Learning in Grasping and Manipulation III,"Offline learning usually requires a large dataset for training. In this paper, we focus on vision-based robotic manipulation tasks and utilize certain task properties to achieve offline learning with a small dataset. We propose a two-stage agent consisting of a tentative decision stage and a correction stage, where the tentative decision stage determines a tentative action from the original camera image,and the correction stage determines a correction to the tentative action based on the cropped image according to the tentative action. The correction stage utilizes task properties to obtain the cropped image with task-relevant features, enabling efficient correction. In particular, the training of the two stages can be performed individually, which enables a straightforward application of general offline learning algorithms. We conduct experiments by combining the two-stage agent with conventional offline reinforcement learning and imitation learning algorithms. In both cases, we benchmark the proposed method using RLBench and demonstrate that the task performance is significantly improved by the correction stage."
Training a Non-Cooperator to Identify Vulnerabilities and Improve Robustness for Robot Navigation,"Quecheng Qiu, Xuyang Zhang, Shunyi Yao, Yu'an Chen, Guangda Chen, Bei Hua, Jianmin Ji","School of Data Science, USTC, Hefei ,,,,,,, China,University of Science and Technology of China,NetEase,University of science and technology of China",Social HRI,"Autonomous mobile robots have become popular in various applications coexisting with humans, which requires robots to navigate efficiently and safely in crowd environments with diverse pedestrians. Pedestrians may cooperate with the robot by avoiding it actively or ignoring the robot during their walking, while some pedestrians, denoted as non-cooperators, may try to block the robot. It is also challenging to identify potential vulnerabilities of a navigation policy, i.e., situations that the robot may cause a collision, in various crowd environments, which reduces the reliability and the safety of the robot. In this paper, we propose a deep reinforcement learning (DRL) approach to train a policy simulating the behavior of non-cooperators, which can effectively identify vulnerabilities of a navigation policy. We evaluate the approach both on the ROS navigation stack with DWA and a DRL-based navigation policy, which identifies useful vulnerabilities of both navigation policies for further improvements. Moreover, these non-cooperators play a game with the DRL-based navigation policy, then we can improve the robustness of such navigation policy by retraining it in the sense of asymmetric self-play. We evaluate the retrained navigation policy in various crowd environments with diverse pedestrians. The experimental results show that the approach can improve the robustness of the navigation policy. The source code for the training and the simulation platform is released online at h"
Toward Grounded Commonsense Reasoning,"Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan, Dorsa Sadigh","Stanford University,Facebook,UC Berkeley,University of California Berkeley",Social HRI,"Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not appropriate to disassemble the sports car and put it away as part of the â€œtidying.â€ How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and actively gather information from the environment that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded commonsense reasoning. To evaluate our framework at scale, we release the MESSYSURFACES dataset which contains images of 70 real-world surfaces that need to be cleaned. We additionally illustrate our approach with a robot on 2 carefully designed surfaces. We find an average 12.9% improvement on the MESSYSURFACES benchmark and an average 15% improvement on the robot experiments over baselines that do not use active perception. The dataset, code, and videos of our approach can be found at https://minaek.github.io/grounded_commonsense_reasoning/."
The Effect of Rejection Strategy on Trust and Shopping Choices in Robot-Assisted Shopping,"Matthias Rehm, Antonia Krummheuer, Carlos Gomez Cubero",Aalborg University,Social HRI,"In this paper, we investigate how a customer-facing service robot can support decision making in shopping interactions. In this role, a robot needs sometimes to reject a customer's choice. Thus, we investigate different rejection strategies with the goal of changing customer behavior. The implemented strategies have been developed based on an ethnographic study on assisted shopping and tested in a lab experiment with 31 participants. The experiment showed significant differences in trust ratings and decision-making depending on the employed strategy."
Planning of Explanations for Robot Navigation,"Amar Halilovic, Senka Krivic","Ulm University,University of Sarajevo",Social HRI,"The choices made by autonomous robots in social settings bear consequences for humans and their presumptions of robot behavior. Explanations can serve to alleviate detrimental impacts on humans and amplify their comprehension of robot decisions. We model the process of explanation generation for robot navigation as an automated planning problem considering different possible explanation attributes. Our visual and textual explanations of a robot's navigation are influenced by the robot's personality. Moreover, they account for different contextual, environmental, and spatial characteristics. We present the results of a user study demonstrating that users are more satisfied with multimodal than unimodal explanations. Additionally, our findings reveal low user satisfaction with explanations of a robot with extreme personality traits. In conclusion, we deliberate on potential future research directions and the associated constraints. Our work advocates for fostering socially adept and safe autonomous robot navigation."
Learning Crowd Behaviors in Navigation with Attention-Based Spatial-Temporal Graphs,"Yanying Zhou, Jochen Garcke","University of Bonn,Universität Bonn",Social HRI,"Safe and efficient navigation in dynamic environments shared with humans remains an open and challenging task for mobile robots. Previous works have shown the efficacy of using reinforcement learning frameworks to train policies for efficient navigation. However, their performance deteriorates when crowd configurations change, i.e. become larger or more complex. Thus, it is crucial to fully understand the complex, dynamic, and sophisticated interactions of the crowd resulting in proactive and foresighted behaviors for robot navigation. In this paper, a novel deep graph learning architecture based on attention mechanisms is proposed, which leverages the spatial-temporal graph to enhance robot navigation. We employ spatial graphs to capture the current spatial interactions, and through the integration with RNN, the temporal graphs utilize past trajectory information to infer the future intentions of each agent. The spatial-temporal graph reasoning ability allows the robot to better understand and interpret the relationships between agents over time and space, thereby making more informed decisions. Compared to previous state-of-the-art methods, our method demonstrates superior robustness in terms of safety, efficiency, and generalization in various challenging scenarios."
Grounding Conversational Robots on Vision through Dense Captioning and Large Language Models,"Lucrezia Grassi, Zhouyang Hong, Carmine Tommaso Recchiuto, Antonio Sgorbissa","University of Genova,University of Genoa",Social HRI,"This work explores a novel approach to empowering robots with visual perception capabilities using textual descriptions. Our approach involves the integration of GPT-4 with dense captioning, enabling robots to perceive and interpret the visual world through detailed text-based descriptions. To assess both user experience and the technical feasibility of this approach, experiments were conducted with human participants interacting with a Pepper robot equipped with visual capabilities. The results affirm the viability of the proposed approach, allowing to perform vision-based conversations effectively, despite processing time limitations."
Exploring the Impact of Narrator Type on Response Latency and Utterance Length During Interactive Storytelling,"Iman Bakhoda, Pourya Shahverdi, Katelyn Rousso, Justin Klotz, Wing-yue Geoffrey Louie","Intelligent Robotics Laboratory, Oakland University, Michigan,Oakland University, Michigan, USA,Intelligent Robotics Lab, Oakland University, Michigan,Oakland University",Social HRI,"The inexorable progress of technology brought forth an era where robots increasingly integrate into human life which necessitates the understanding of human-robot interactions (HRI). This study unravels the details of HRI within interactive storytelling contexts. Through a between-subject experiment with 28 participants, we assessed response latency and utterance lengths to interactive story narrations delivered by either a human or a robot. Findings indicated that participants displayed longer response latency interacting with the robot narrator while articulating shorter utterances compared to the human condition where participants displayed longer utterances and shorter response latency. These observations suggest significant differences in cognitive and communicative strategies in human-human versus human-robot interactions. The results underscore the challenges and potential of designing social robots that are time-sensitive in interacting with humans. Future explorations should focus on the cognitive and emotional drivers behind these interactions."
Design of Embodied Mediator Haru for Remote Cross Cultural Communication,"Randy Gomez, Deborah Szapiro, Sara Cooper, Nabil Bougria Sanchez, Guillermo Pérez, Eric Nichols, Javier Giménez-figueroa, Jose Manuel Perez-moleron, Matthew Peavy, Daniel Serrano, Luis Merino","Honda Research Institute Japan Co., Ltd.,University of Technology Sydney,Honda Research Institute Japan,Universidad Pablo de Olavide,,i Intelligent Insights,Eurecat",Social HRI,"Social robots for children have focused mainly on conventional education domains such as teaching language, science, and math, while applications focusing on the enhancement of cultural competency are quite scarce. In this paper, we present a prototype of a robot-mediation framework for cross-cultural communication. This framework paves the way for a social robot to act as a mediator between groups of schoolchildren from different countries. First, we conducted a participatory design activity with an interdisciplinary team, resulting in the extraction of the design, robot roles, and technical requirements. Based on these requirements, we built the robot-mediation system prototype. We conducted a pilot study using the system with groups of high school children in Japan and Australia and our results show the potential of the system to drive children's interest in communicating, sharing, and discussing cultural themes with their remote peers through the social robot."
ChatAdp: ChatGPT-Powered Adaptation System for Human-Robot Interaction,"Zhidong Su, Weihua Sheng",Oklahoma State University,Social HRI,"Different people have different preferences when it comes to human-robot interaction. Therefore, it is desirable for the robot to adapt its actions to fit users' preferences. Human feedback is essential to facilitating robot adaptation. However, when the task is complex or the robot action space is large, it requires a large amount of user feedback. ChatGPT is a powerful generative AI tool based on large language models (LLMs), which possesses a significant corpus of information obtained from human society, and exhibits robust proficiency in the comprehension and acquisition of natural language. Therefore, in this paper, we proposed a ChatGPT-powered adaptation system (ChatAdp) for human-robot interaction which requires less user feedback to achieve a good adaptation result. In the proposed ChatAdp, we use ChatGPT as a user simulator to provide feedback. We evaluated ChatAdp in a case study for context-aware conversation adaptation. The results are very promising. Our proposed method can achieve a mean success rate of 92% on the user's natural language-described preferences after receiving 33 rounds of feedback from a user on average, which is only 2% of the number of states covered by the user preferences and outperforms the two baseline methods."
The Impact of Evolutionary Computation on Robotic Design: A Case Study with an Underactuated Hand Exoskeleton,"Baris Akbas, Aleyna Soylemez, Huseyin Taner Yuksel, Mazhar Eid Zyada, Mine Sarac, Fabio Stroppa",Kadir Has University,Rehabilitation Robotics,"Robotic exoskeletons can enhance human strength and aid people with physical disabilities. However, designing them to ensure safety and optimal performance presents significant challenges. Developing exoskeletons should incorporate specific optimization algorithms to find the best design. This study investigates the potential of Evolutionary Computation (EC) methods in robotic design optimization, with an underactuated hand exoskeleton (U-HEx) used as a case study. We propose improving the performance and usability of the UHEx design, which was initially optimized using a naive bruteforce approach, by integrating EC techniques such as Genetic Algorithm and Big Bang-Big Crunch Algorithm. Comparative analysis revealed that EC methods consistently yield more precise and optimal solutions than brute force in a significantly shorter time. This allowed us to improve the optimization by increasing the number of variables in the design, which was impossible with naive methods. The results show significant improvements in terms of the torque magnitude the device transfers to the user, enhancing its efficiency. These findings underline the importance of performing proper optimization while designing exoskeletons, as well as providing a significant improvement to this specific robotic design."
Design & Systematic Evaluation of Power Transmission Efficiency of an Ankle Exoskeleton for Walking Post-Stroke,"Myles Cooper, Santiago Canete, Asa Eckert-Erdheim, Aidan Kimberley, Christopher Siviy, Teresa Baker, Terry Ellis, Patrick Slade, Conor James Walsh","Harvard University,Harvard University School of Engineering and Applied Sciences,Boston University",Rehabilitation Robotics,"Community-based locomotor training post-stroke has shown improvements in independent ambulation by increasing dose, intensity, and specificity of walking practice. Robotic ankle exoskeletons hold the potential to facilitate continued rehabilitation at home, but understanding what aspects of the design are most relevant for successful translation to the community presents a challenge. Here, we design a portable rigid ankle exoskeleton to use as a research platform for investigating the effect of assistance on post-stroke gait during overground, community-based walking. We first test our device with stroke survivors and validate its potential for future community use. We then present a systematic method for quantifying power transmission losses at each transmission stage from the battery to the wearer, using data gathered from walking trials with healthy participants. Our evaluation method revealed inefficiencies in power transfer at the interface level, likely resulting from the compliance in the structural components of the system, which motivates future redesign considerations. Overall, our method provides a framework to identify and characterize the components that must be redesigned to lower exoskeleton weight and maximize performance."
Achieving Mechanical Transparency Using Fusion Hybrid Linear Actuator for Shoulder Flexion and Extension in Exoskeleton Robot,"Takuma Shimoyama, Tomoyuki Noda, Tatsuya Teramae, Yoshihiro Nakata","The University of Electro-Communications,ATR Computational Neuroscience Laboratories",Rehabilitation Robotics,"Recently, the importance of mechanical transparency in human-assistive robots has grown. Traditionally, its primary goal was minimizing interaction forces during assistance. However, under this conventional definition, mechanical transparency was not considered when an interaction force was required during assistance. This research focuses on achieving mechanical transparency within the context of shoulder motion in upper extremity exoskeletons for rehabilitation. Our primary goal is maintaining interaction forces at target values, even with motion disturbances. To this end, we developed a shoulder actuation testbed for exoskeletons, incorporating a fusion hybrid linear actuator distinguished by high back-drivability, robust torque generation capability, and safety features. To attain mechanical transparency, we created a model for calculating the required joint torque, accounting for gravitational dynamics, and subsequently determined the necessary actuator output. The system characteristics were evaluated based on the joint torque generated by the actuator. The actuator utilized pneumatic pressure to generate force and compensated for kinetic friction using electromagnetic forces. The results showed that the compensation by the electromagnetic force reduced the root mean square error of the torque to less than 60% in relation to pneumatic pressure alone. This demonstrated the ability to generate consistent torque with high robustness to motion disturbances."
Imitation Learning-Based System for the Execution of Self-Paced Robotic-Assisted Passive Rehabilitation Exercises,"Rafael J. Escarabajal, José Luis Pulloquinga, Pau Zamora-ortiz, Angel Valera, Vicente Mata, Marina Valles","Universidad Politécnica de Valencia,Universitat Politècnica de València",Rehabilitation Robotics,"The development of robotic-assisted rehabilitation exercises involving physical human-robot interaction requires extreme care since an injured limb may be in physical contact with the robot, so compliant behavior is imperative for these tasks. Typical approaches involve force control schemes like admittance controllers that allow humans to adapt the motion. However, when the patient's limb has limited mobility or is potentially injured, unintentional forces may occur during the robot's trajectory that could be incompatible with these controllers. This paper addresses a new way of generating compliant trajectories for passive rehabilitation exercises, considering that previous positions of the trajectory are attainable for the patient, so reversing the trajectory is a safe operation. Since there is no clear way to optimize such a goal due to the physiological variability among patients, the condition of reversal is based on imitation learning by taking the analogous healthy limb of the patient as a reference and encoding the forces using Gaussian Mixture Regression, and reversibility is accomplished by means of Reversible Dynamic Movement Primitives. The system allows for self-paced rehabilitation exercises by back-and-forth movements along the trajectory according to the patient's reaction, and it has been successfully applied to a 4-DOF parallel robot for lower-limb rehabilitation."
An Adaptable Ankle Trajectory Generation Method for Lower-Limb Exoskeletons by Means of Safety Constraints Computation and Minimum Jerk Planning,"Raffaele Giannattasio, Stefano Maludrottu, Gaia Zinni, Elena De Momi, Matteo Laffranchi, Lorenzo De Michieli","Italian Institute of Technology,Istituto Italiano di Tecnologia,Politecnico di Milano",Rehabilitation Robotics,"This paper presents a method to compute smooth ankle trajectories for lower limb exoskeletons with powered ankle joints. The proposed approach defines ankle trajectories using four polynomial functions, each representing one of the four primary phases of gait. These polynomials are computed according to different safety constraints. During the single support phase, ground contact constraints are enforced. In the swing phase, an optimization problem is solved to achieve minimum jerk planning while respecting a set of equality and inequality constraints designed to minimize the risk of stumbling. The used approach focuses on making the ankle joint able to smoothly adapt in real-time to different walking styles defined by user-selected gait parameters such as step length and clearance. The primary aim is to improve the user experience by producing a secure and comfortable walking pattern. To validate the effectiveness of the proposed method, the new ankle trajectories were tested on a group of healthy volunteers using the TWIN lower limb exoskeleton."
Controlling FES of Arm Movements Using Physics-Informed Reinforcement Learning Via Co-Kriging Adjustment,"Nat Wannawas, Clara Diaz-pintado, Jyotindra Narayan, Aldo Faisal","Imperial College London,Imperial College / University of Bayreuth",Rehabilitation Robotics,"Upper limb paralysis affects the quality of life. Functional Electrical Stimulation (FES) offers a solution to restore lost motor functions. Yet, there remain challenges in controlling FES to induce arbitrary arm movements. Reinforcement learning (RL) emerges as a promising method for controlling arm movement with success in simulation. However, challenges remain in translating the successes into real-world settings. One dominant challenge is the sample efficiency of RL. This study presents a practical RL setup to control FES for arm movements. We also present a flexible method, called co-kriging adjustment (CKA), which combines a biomechanical simulator and real data to build an accurate model of the real system. We demonstrate our RL-based control on a 2-DoF planar setting where the subject's arm, placed on a frictionless supporter, is stimulated to perform point-to-point reaching. By using 90 seconds of real interaction data, our RL-based control can perform the reaching with the average error over the workspace of 5.5 cm. Beyond the application of FES, our method can be extended to other control systems, propelling RL towards general uses in the real world."
Adaptive Control for Triadic Human-Robot-FES Collaboration in Gait Rehabilitation: A Pilot Study,"Andreas Christou, Antonio J. Del-ama, Juan C. Moreno, Sethu Vijayakumar","The University of Edinburgh,Rey Juan Carlos University,Cajal Institute, CSIC,University of Edinburgh",Rehabilitation Robotics,"The hybridisation of robot-assisted gait training and functional electrical stimulation (FES) can provide numerous physiological benefits to neurological patients. However, the design of an effective hybrid controller poses significant challenges. In this over-actuated system, it is extremely difficult to find the right balance between robotic assistance and FES that will provide personalised assistance, prevent muscle fatigue and encourage the patientâ€™s active participation in order to accelerate recovery. In this paper, we present an adaptive hybrid robot-FES controller to do this and enable the triadic collaboration between the patient, the robot and FES. A patient-driven controller is designed where the voluntary movement of the patient is prioritised and assistance is provided using FES and the robot in a hierarchical order depending on the patientâ€™s performance and their musclesâ€™ fitness. The performance of this hybrid adaptive controller is tested in simulation and on one healthy subject. Our results indicate an increase in tracking performance with lower overall assistance, and less muscle fatigue when the hybrid adaptive controller is used, compared to its non adaptive equivalent. This suggests that our hybrid adaptive controller may be able to adapt to the behaviour of the user to provide assistance as needed and prevent the early termination of physical therapy due to muscle fatigue."
Stretch with Stretch: Physical Therapy Exercise Games Led by a Mobile Manipulator,"Matthew Lamsey, Meredith Wells, You Liang Tan, Madeline Beatty, Zexuan Liu, Arjun Majumdar, Kendra Washington, Jerry Feldman, Naveen Kuppuswamy, Elizabeth Nguyen, Arielle Wallenstein, Charlie Kemp, Madeleine Eve Hackney","Georgia Institute of Technology,Emory University School of Medicine,University of Michigan,The Georgia Institute of Technology,Parkinson's Foundation,Toyota Research Institute,Long School of Medicine,Emory University,Hello Robot Inc.",Rehabilitation Robotics,"Physical therapy (PT) is a key component of many rehabilitation regimens, such as treatments for Parkinson's disease (PD). However, there are shortages of physical therapists and adherence to self-guided PT is low. Robots have the potential to support physical therapists and increase adherence to self-guided PT, but prior robotic systems have been large and immobile, which can be a barrier to use in homes and clinics. We present Stretch with Stretch (SWS), a novel robotic system for leading stretching exercise games for older adults with PD. SWS consists of a compact and lightweight mobile manipulator (Hello Robot Stretch RE1) that visually and verbally guides users through PT exercises. The robot's soft end effector serves as a target that users repetitively reach towards and press with a hand, foot, or knee. For each exercise, target locations are customized for the individual via a visually estimated kinematic model, a haptically estimated range of motion, and the person's exercise performance. The system includes sound effects and verbal feedback from the robot to keep users engaged throughout a session and augment physical exercise with cognitive exercise. We conducted a user study for which people with PD (n=10) performed 6 exercises with the system. Participants perceived the SWS to be useful and easy to use. They also reported mild to moderate perceived exertion (RPE)."
Subject-Independent Estimation of Continuous Movements Using CNN-LSTM for a Home-Based Upper Limb Rehabilitation System,"He Li, Shuxiang Guo, Dongdong Bu, Hanze Wang, Masahiko Kawanishi","Beijing Institute of Technology,Kagawa University",Intention Recognition,"Exoskeleton-assisted home-based rehabilitation plays a vital role in upper limb rehabilitation of stroke patients in the early stage. The surface electromyography (sEMG)-based control can facilitate friendly interactions between individuals and rehabilitation exoskeletons. The exoskeleton can also meet the requirements of home-based rehabilitation, including affordability, portability, safety, and active participation. Although various systems have been proposed to enhance upper limb training, few studies addressed the inter-subject variability of sEMG signals, which limits the generalization capability of the intention estimation model. In this paper, a subject-independent continuous motion estimation method combining convolutional neural networks (CNN) and long and short term memory (LSTM) is proposed, which is applied to a home-based bilateral training system. The sEMG-driven CNN-LSTM model builds the relationship between sEMG signals and continuous movements. To verify the effectiveness of the CNN-LSTM model in achieving subject-independent estimation, the offline estimation under the backpropagation neural network, CNN, and CNN-LSTM are compared. Moreover, the online intention estimation and the real-time control are performed, and the estimation angle error and time delay are controlled at approximately 10Â°and 300 ms, proving the feasibility of the subject-independent estimation method and its availability in the upper-limb rehabilitation system."
Robot Trajectron: Trajectory Prediction-Based Shared Control for Robot Manipulation,"Pinhao Song, Pengteng Li, Erwin Aertbelien, Renaud Detry","KU Leuven,Shenzhen University",Intention Recognition,"We address the problem of (a) predicting the trajectory of an arm reaching motion, based on a few seconds of the motion's onset, and (b) leveraging this predictor to facilitate shared-control manipulation tasks, easing the cognitive load of the operator by assisting them in their anticipated direction of motion. Our novel intent estimator, dubbed the Robot Trajectron (RT), produces a probabilistic representation of the robot's anticipated trajectory based on its recent position, velocity and acceleration history. Taking arm dynamics into account allows RT to capture the operator's intent better than other SOTA models that only use the arm's position, making it particularly well-suited to assist in tasks where the operator's intent is susceptible to change. We derive a novel shared-control solution that combines RT's predictive capacity to a representation of the locations of potential reaching targets. Our experiments demonstrate RT's effectiveness in both intent estimation and shared-control tasks. We will make the code and data supporting our experiments publicly available at https://gitlab.kuleuven.be/detry-lab/public/robot-trajectron."
On the Feasibility of EEG-Based Motor Intention Detection for Real-Time Robot Assistive Control,"Ho Jin Choi, Satyajeet Das, Shaoting Peng, Ruzena Bajcsy, Nadia Figueroa","University of Pennsylvania,Univ of California, Berkeley",Intention Recognition,"This paper explores the feasibility of employing EEG-based intention detection for real-time robot assistive control. We focus on predicting and distinguishing motor intentions of left/right arm movements by presenting: i) an offline data collection and training pipeline, used to train a classifier for left/right motion intention prediction, and ii) an online realtime prediction pipeline leveraging the trained classifier and integrated with an assistive robot. Central to our approach is a rich feature representation composed of the tangent space projection of time-windowed sample covariance matrices from EEG filtered signals and derivatives; allowing for a simple SVM classifier to achieve unprecedented accuracy and realtime performance. In pre-recorded real-time settings (160 Hz), a peak accuracy of 86.88% is achieved, surpassing prior works. In robot-in-the-loop settings, our system successfully detects intended motion solely from EEG data with 70% accuracy, triggering a robot to execute an assistive task. We provide a comprehensive evaluation of the proposed classifier."
Microexpression to Macroexpression: Facial Expression Magnification by Single Input,"Yaqi Song, Tong Chen, Shigang Li, Jianfeng Li","Southwest University,Hiroshima City University,Southwest university",Intention Recognition,"Microexpressions are expressions that people inadvertently express, and therefore often represent a person's true emotion. However, because it has a low intensity and a short duration, it is hard to be recognized correctly. In this paper, we propose a deep learning magnification method to generate macroexpressions from a single microexpression image. In the first stage, we extract the expression information from a single microexpression image. Then, We combine the idea of cyclegan and optical flow consistency to model the extracted expression features as the optical flow field between the neutral face and microexpressions. To extract a reliable optical flow field from the expression information, we design an optical flow refiner. In the second stage, we adopt an encoder-decoder network and let it learn to magnify the optical flow. Finally, the magnified optical flow guided the microexpression images to generate macroexpression images. We compare our single input based network with current two-frames-input based networks. The results show that our method performs better, even in wild images. We fed our magnified images directly into a simple ResNet18 network for recognition, achieving a competitive score under the MEGC2019 standard, compared with recent complex recognition networks."
Looking Inside Out: Anticipating Driver Intent from Videos,"Yung-chi Kung, Arthur King Zhang, Junmin Wang, Joydeep Biswas","The University of Texas at Austin,University of Texas at Austin",Intention Recognition,"Anticipating driver intention is an important task when vehicles of mixed and varying levels of human/machine autonomy share roadways. Driver intention can be leveraged to improve road safety, such as warning surrounding vehicles in the event the driver is attempting a dangerous maneuver. In this work, we propose a novel method of utilizing both in-cabin and external camera data to improve state-of-the-art performance in predicting future driver actions. Compared to existing methods, our approach explicitly extracts object and road-level features from external camera data, which we demonstrate are important features for predicting driver intention. Using our handcrafted features as inputs for both a transformer and a long-short-term-memory-based architecture, we empirically show that jointly utilizing in-cabin and external features improves performance compared to using in-cabin features alone. Furthermore, our models predict driver maneuvers more accurately and sooner than existing approaches, with an accuracy of 87.5% and an average prediction time of 4.35 seconds before the maneuver takes place. We release our model configurations and training scripts on https://github.com/ykung83/Driver-Intent-Prediction."
CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots,"Dmitriy Rivkin, Nikhil Rajiv Kakodkar, Francois Hogan, Bobak Hamed Baghi, Gregory Dudek","None,McGill University,Massachusetts Institute of Technology,unaffiliated",Intention Recognition,"This work explores the capacity of large language models (LLMs) to address problems at the intersection of spatial planning and natural language interfaces for navigation. We focus on following complex instructions that are more akin to natural conversation than traditional explicit procedural directives typically seen in robotics. Unlike most prior work where navigation directives are provided as simple imperative commands (e.g., ""go to the fridge""), we examine implicit directives obtained through conversational interactions.We leverage the 3D simulator AI2Thor to create household query scenarios at scale, and augment it by adding complex language queries for 40 object types. We demonstrate that a robot using our method CARTIER (Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots) can parse descriptive language queries up to 42% more reliably than existing LLM-enabled methods by exploiting the ability of LLMs to interpret the user interaction in the context of the objects in the scenario."
A Novel Hybrid Unsupervised Domain Adaptation Method for Cross-Subject Joint Angle Estimation from Surface Electromyography,"Long Wang, Xiaoling Li, Zhangyi Chen, Zhipeng Sun, Jingyi Xue, Shiwen Zhang, Wei Sun, Guimin Chen, Jiajia Sun","Xi'an Jiaotong University,Xi'an Jiaotong University,Xi’an Jiaotong University",Intention Recognition,
A Novel Benchmarking Paradigm and a Scale and Motion-Aware Model for Egocentric Pedestrian Trajectory Prediction,Amir Rasouli,Huawei Technologies Canada,Intention Recognition,"In this paper, we present a new paradigm for evaluating egocentric pedestrian trajectory prediction algorithms. Based on various contextual information, we extract driving scenarios for a meaningful and systematic approach to identifying challenges for prediction models. In this regard, we also propose a new metric for more effective ranking within the scenario-based evaluation. We conduct extensive empirical studies of existing models on these scenarios to expose shortcomings and strengths of different approaches. The scenario-based analysis highlights the importance of using multimodal sources of information and challenges caused by inadequate modeling of ego-motion and scale of pedestrians. To this end, we propose a novel egocentric trajectory prediction model that benefits from multimodal sources of data fused in an effective and efficient step-wise hierarchical fashion and two auxiliary tasks designed to learn more robust representation of scene dynamics. We conduct empirical evaluation on common benchmark datasets and show that our model not only achieves state-of-the-art performance, but also significantly improves performance by up to 39% in challenging scenarios, such as high ego-speed, compared to the past arts."
A 3D Vector Field and Gaze Data Fusion Framework for Hand Motion Intention Prediction in Human-Robot Collaboration,"Maleen Jayasuriya, Gibson Hu, Dinh Dang Khoa Le, Karyne Ang, Shankar Sankaran, Dikai Liu","University of Technology Sydney,University of Technology, Sydney,The University of Technology Sydney,UTS Robotics Institute",Intention Recognition,"In human-robot collaboration (HRC) settings, hand motion intention prediction (HMIP) plays a pivotal role in ensuring prompt decision-making, safety, and an intuitive collaboration experience. Precise and robust HMIP with low computational resources remains a challenge due to the stochastic nature of hand motion and the diversity of HRC tasks. This paper proposes a framework that combines hand trajectories and gaze data to foster robust, real-time HMIP with minimal to no training. A novel 3D vector field method is introduced for hand trajectory representation, leveraging minimum jerk trajectory predictions to discern potential hand motion endpoints. This is statistically combined with gaze fixation data using a weighted Naive Bayes Classifier (NBC). Acknowledging the potential variances in saccadic eye motion due to factors like fatigue or inattentiveness, we incorporate stationary gaze entropy to gauge visual concentration, thereby adjusting the contribution of gaze fixation to the HMIP. Empirical experiments substantiate that the proposed framework robustly predicts intended endpoints of hand motion before at least 50% of the trajectory is completed. It also successfully exploits gaze fixations when the human operator is attentive and mitigates its influence when the operator loses focus. A real-time implementation in a construction HRC scenario (collaborative tiling) showcases the intuitive nature and potential efficiency gains to be leveraged by introducing the proposed HMIP into HRC contexts. The opensource implementation of the framework is made available at https://github.com/maleenj/hmip_ros.git."
Contrastive Learning-Based Attribute Extraction Method for Enhanced Terrain Classification,"Xiao Liu, Hongjin Chen, Haoyao Chen","Harbin Institute of Technology, Shenzhen,Harbin Institute of Technology Shenzhen",Force and Tactile Sensing III,"The outdoor environment has many uneven surfaces that put the robot at risk of sinking or tipping over. Recognizing the type of terrain can help robot avoid risks and choose an appropriate gait. One of the critical problems is how to extract the terrain-related knowledge from sensor data collected as the robot traversed the ground. Many existing vision-based approaches are limited in directly perceiving the intrinsic properties of various terrains. The intuitive approach entails directly analyzing data recorded by the robot's proprioceptive sensors. However, it faces challenges in being specific to certain robot leg configurations or in the lack of interpretability of the extracted features. In this paper, a terrain attribute extraction algorithm is proposed based on contrastive learning. It leverages the haptic data generated from the interaction between the robot's legs and terrain to automatically extract terrain attributes. The results demonstrate that the attributes extracted using this method strongly correlate with the actual softness of the terrain. Furthermore, these attributes played an important role in achieving high accuracy in terrain classification tasks."
Enhancing Generalizable 6D Pose Tracking of an In-Hand Object with Tactile Sensing,"Yun Liu, Xiaomeng Xu, Weihang Chen, Haocheng Yuan, He Wang, Jing Xu, Rui Chen, Li Yi","Tsinghua University,Stanford University,Northwestern Polytechnical University,Peking University",Force and Tactile Sensing III,"When manipulating an object to accomplish complex tasks, humans rely on both vision and touch to keep track of the object's 6D pose. However, most existing object pose tracking systems in robotics rely exclusively on visual signals, which hinder a robot's ability to manipulate objects effectively. To address this limitation, we introduce TEG-Track, a tactile-enhanced 6D pose tracking system that can track previously unseen objects held in hand. From consecutive tactile signals, TEG-Track optimizes object velocities from marker flows when slippage does not occur, or regresses velocities using a slippage estimation network when slippage is detected. The estimated object velocities are integrated into a geometric-kinematic optimization scheme to enhance existing visual pose trackers. To evaluate our method and to facilitate future research, we construct a real-world dataset for visual-tactile in-hand object pose tracking. Experimental results demonstrate that TEG-Track consistently enhances state-of-the-art generalizable 6D pose trackers in synthetic and real-world scenarios. Our code and dataset are available at https://github.com/leolyliu/TEG-Track."
UnfoldIR: Tactile Robotic Unfolding of Cloth,"Remko Proesmans, Andreas Verleysen, Francis wyffels",Ghent University,Force and Tactile Sensing III,"Robotic unfolding of cloth is challenging due to the wide range of textile materials and their ability to deform in unpredictable ways. Previous work has focused almost exclusively on visual feedback to solve this task. We present UnfoldIR (""unfolder""), a dual-arm robotic system relying on infrared (IR) tactile sensing and cloth manipulation heuristics to achieve in-air unfolding of randomly crumpled rectangular textiles by means of edge tracing. The system achieves >85% coverage on multiple textiles of different sizes and textures. After unfolding, at least three corners are visible in 83.3 up to 94.7% of cases. Given these strong ""tactile-only"" results, we argue that the fusion of both tactile and visual sensing can bring cloth unfolding to a new level of performance."
3D Force and Contact Estimation for a Soft-Bubble Visuotactile Sensor Using FEM,"Jing-chen Peng, Shaoxiong Yao, Kris Hauser","University of Illinois @ Urbana-Champaign,University of Illinois Urbana-Champaign,University of Illinois at Urbana-Champaign",Force and Tactile Sensing III,"Soft-bubble tactile sensors have the potential to capture dense contact and force information across a large contact surface. However, it is difficult to extract contact forces directly from observing the bubble surface because local contacts change the global surface shape significantly due to membrane mechanics and air pressure. This paper presents a model-based method of reconstructing dense contact forces from the bubble sensor's internal RGBD camera and air pressure sensor. We present a finite element model of the force response of the bubble sensor that uses a linear plane stress approximation that only requires calibrating 3 variables. Our method is shown to reconstruct normal and shear forces significantly more accurately than the state-of-the-art, with comparable accuracy for detecting the contact patch, and with very little calibration data."
A Detachable FBG-Based Contact Force Sensor for Capturing Gripper-Vegetable Interactions,"Wenjie Lai, Jiajun Liu, Bing Rui Sim, Ming Rui Joel Tan, Chidanand Hegde, Shlomo Magdassi, Louis Phee","Nanyang Technological University,Nanyang Technological University, Singapore,Hebrew University of Jerusalem",Force and Tactile Sensing III,"Vertical farming, a sustainable key for urban agriculture, has garnered attention for its land use optimization and enhanced food production capabilities. The adoption of automation in vertical farming is a pivotal response to labor shortages, addressing the need for increased efficiency, particularly in labor-intensive tasks like harvesting. Although soft robotic grippers offer a significant promise for delicately handling fragile objects, the absence of sensors has hindered their full potential to execute precise and secure grasping. To address this challenge, we present a new solution: a detachable Fiber Bragg Grating-based flexible contact force sensor to capture gripper-vegetable interactions. The sensing module was 3D printed using soft material, and the FBG fiber was attached to the module using epoxy. From evaluation tests, this lightweight sensor demonstrated a wide measurement range of up to 9.87 N, with a high sensitivity of 141.7 pm/N, good repeatability, and a hysteresis of 7.96%. Compared to commercial load cells, our sensor achieves a small measurement RMSE of 0.41 N and a percentage error of 4.15%. The sensor was integrated into two robotic 3D-printed soft grippers to enable real-time monitoring of dynamic contact force during vegetable harvesting in vertical farming scenarios. By reflecting contact status, this sensor provides a promising glimpse into the future of agricultural automation, enhancing operational efficiency and strengthening situation awareness and decision-making capabilities in vertical farms. Beyond agriculture, the versatility of this sensor extends to application in areas such as warehousing, logistics, and the food and beverage industry."
"SATac: A Thermoluminescence Enabled Tactile Sensor for Concurrent Perception of Temperature, Pressure, and Shear","Ziwu Song, Ran Yu, Xuan Zhang, Kit Wa Sou, Shilong Mu, Dengfeng Peng, Xiao-Ping (Steven) Zhang, Wenbo Ding","Tsinghua University,Shenzhen University,Ryerson University",Force and Tactile Sensing III,"Most vision-based tactile sensors use elastomer deformation to infer tactile information, which can not sense some modalities, like temperature. As an important part of human tactile perception, temperature sensing can help robots better interact with the environment. In this work, we propose a novel multi-modal vision-based tactile sensor, SATac, which can simultaneously perceive information on temperature, pressure, and shear. SATac utilizes the thermoluminescence of strontium aluminate to sense a wide range of temperatures with exceptional resolution. Additionally, the pressure and shear can also be perceived by analyzing the Voronoi diagram. A series of experiments are conducted to verify the performance of our proposed sensor. We also discuss the possible application scenarios and demonstrate how SATac could benefit robot perception capabilities."
Optimizing Multi-Touch Textile and Tactile Skin Sensing through Circuit Parameter Estimation,"Bo Ying Su, Yuchen Wu, Chengtao Wen, Changliu Liu","Carnegie Mellon University,Siemens",Force and Tactile Sensing III,"Tactile and textile skin technologies have become increasingly important for enhancing human-robot interaction and allowing robots to adapt to different environments. Despite notable advancements, there are ongoing challenges in skin signal processing, particularly in achieving both accuracy and speed in dynamic touch sensing. This paper introduces a new framework that poses the touch sensing problem as an estimation problem of resistive sensory arrays. Utilizing a Regularized Least Squares objective functionâ€”which estimates the resistance distribution of the skinâ€”we enhance the touch sensing accuracy and mitigate the ghosting effects, where false or misleading touches may be registered. Furthermore, our study presents a streamlined skin design that simplifies manufacturing processes without sacrificing performance. Experimental outcomes substantiate the effectiveness of our method, showing 26.9% improvement in multi-touch force-sensing accuracy for the tactile skin."
"CushSense: Soft, Stretchable, and Comfortable Tactile Sensing Skin for Physical Human-Robot Interaction","Boxin Xu, Luoyan Zhong, Grace Zhang, Xiaoyu Liang, Diego Virtue, Rishabh Madan, Tapomayukh Bhattacharjee",Cornell University,Force and Tactile Sensing III,"Whole-arm tactile feedback is crucial for robots to ensure safe physical interaction with their surroundings. In this paper, we introduce CushSense, a fabric-based soft and stretchable tactile-sensing skin designed for physical human-robot interaction (pHRI) tasks such as robotic caregiving. Utilizing a combination of stretchable fabric and hyper-elastic polymer, CushSense identifies contacts by monitoring capacitive changes due to skin deformation. CushSense is cost-effective and easy to fabricate. We detail the sensor design and fabrication process, provide characterization results showcasing its sensing proficiency, and present a user study underscoring its perceived safety and comfort for the assistive task of limb manipulation. We open source all sensor-related resources on our project website."
Augmenting Tactile Simulators with Real-Like and Zero-Shot Capabilities,"Osher Azulay, Alon Mizrahi, Nimrod Curtis, Avishai Sintov","Tel Aviv University,Tel-Aviv University",Force and Tactile Sensing III,"Simulating tactile perception could potentially leverage the learning capabilities of robotic systems in manipulation tasks. However, the reality gap of simulators for high-resolution tactile sensors remains large. Models trained on simulated data often fail in zero-shot inference and require fine-tuning with real data. In addition, work on high-resolution sensors commonly focus on ones with flat surfaces while 3D round sensors are essential for dexterous manipulation. In this paper, we propose a bi-directional Generative Adversarial Network (GAN) termed SightGAN. SightGAN relies on the early CycleGAN while including two additional loss components aimed to accurately reconstruct background and contact patterns including small contact traces. The proposed SightGAN learns real-to-sim and sim-to-real processes over difference images. It is shown to generate real-like synthetic images while maintaining accurate contact positioning. The generated images can be used to train zero-shot models for newly fabricated sensors. Consequently, the resulted sim-to-real generator could be built on top of the tactile simulator to provide a real-world framework. Potentially, the framework can be used to train, for instance, reinforcement learning policies of manipulation tasks. The proposed model is verified in extensive experiments with test data collected from real sensors and also shown to maintain embedded force information within the tactile images."
Investigating Stability Outcomes across Diverse Gait Patterns in Quadruped Robots: A Comparative Analysis,"Zhongjin Ju, Ke Wei, Lei Jin, Yundou Xu","Yanshan University,YanShanUniversity,Parallel Robot and Mechatronic System Laboratory of Hebei Provin",Legged Robots III,"Quadruped robots have gained attention for their potential to navigate various terrains. However, the stability of these robots in different gait sequences remains an open question. This study investigates the relationship between different gait sequences and the motion stability of quadruped robots, assuming a flat terrain for the purpose of the analysis. Utilizing mathematical models based on spiral theory, we examine the stability margins associated with different leg movement sequences. Notably, our findings confirm that the most commonly observed sequence in both natural and robotic contexts indeed offers optimal stability. The study also scrutinizes the influence of the robot's structural parameters and gait configuration on its motion stability. These results provide a theoretical foundation for the design and stability control of quadruped robots, setting the stage for future work on more complex terrains."
Pedipulate: Enabling Manipulation Skills Using a Quadruped Robot's Leg,"Philip Arm, Mayank Mittal, Hendrik Kolvenbach, Marco Hutter",ETH Zurich,Legged Robots III,"Legged robots have the potential to become vital in maintenance, home support, and exploration scenarios. In order to interact with and manipulate their environments, most legged robots are equipped with a dedicated robot arm, which means additional mass and mechanical complexity compared to standard legged robots. In this work, we explore pedipulation - using the legs of a legged robot for manipulation. By training a reinforcement learning policy that tracks position targets for one foot, we enable a dedicated pedipulation controller that is robust to disturbances, has a large workspace through whole-body behaviors, and can reach far-away targets with gait emergence, enabling loco-pedipulation. By deploying our controller on a quadrupedal robot using teleoperation, we demonstrate various real-world tasks such as door opening, sample collection, and pushing obstacles. We demonstrate load carrying of more than 2.0 kg at the foot. Additionally, the controller is robust to interaction forces at the foot, disturbances at the base, and slippery contact surfaces. Videos of the experiments are available at https://sites.google.com/leggedrobotics.com/pedipulate."
An Efficient Model Based Approach on Learning Agile Motor Skills without Reinforcement,"Shi Haojie, Tingguang Li, Qingxu Zhu, Jiapeng Sheng, Lei Han, Max Qing Hu Meng","Chinese University of Hong Kong,The Chinese University of Hong Kong,Tencent,Shandong University,Tencent Robotics X",Legged Robots III,"Learning-based methods have improved locomotion skills of quadruped robots through deep reinforcement learning. But the sim-to-real gap and low sample efficiency still limits the skill transfer. To address this issue, we propose a model-based supervised learning framework that combines a world model with a policy network. We train a differentiable world model to predict future states and use it to train a Variational Autoencoder (VAE)-based policy network through supervised learning to imitate the natural behavior of real animals. This approach significantly diminishes the requirement for substantial amounts of real interaction data since it solely focuses on training the world model, concurrently allowing for rapid policy updates through supervised learning. We also develop a high-level network to track diverse commands and trajectories. We initially train the policy within a simulation environment and subsequently fine-tune it using a physical robot. Simulated results show a tenfold sample efficiency increase compared to reinforcement learning methods such as PPO. Transitioning to real-world testing, our policy achieves proficient command-following performance with only a two-minute data collection period, and generalizes well to new speeds and paths."
Adaptive Model Predictive Control with Data-Driven Error Model for Quadrupedal Locomotion,"Xuanqi Zeng, Hongbo Zhang, Linzhu Yue, Zhitao Song, Lingwei Zhang, Yunhui Liu","Chinese University of Hong Kong,The Chinese University of Hong Kong,Hong Kong Centre For Logistics Robotics",Legged Robots III,"Model Predictive Control (MPC) relies heavily on the robot model for its control law. However, a gap always exists between the reduced-order control model with uncertainties and the real robot, which degrades its performance. To address this issue, we propose the controller of integrating a data-driven error model into traditional MPC for quadruped robots. Our approach leverages real-world data from sensors to compensate for defects in the control model. Specifically, we employ the Autoregressive Moving Average Vector (ARMAV) model to construct the state error model of the quadruped robot using data. The predicted state errors are then used to adjust the predicted future robot states generated by MPC. By such an approach, our proposed controller can provide more accurate inputs to the system, enabling it to achieve desired states even in the presence of inaccuracies in the model parameters or disturbances. The proposed controller exhibits the capability to partially eliminate the disparity between the model and the real-world robot, thereby enhancing the locomotion performance of quadruped robots. We validate our proposed method through simulations and real-world experimental trials on a large-size quadruped robot that involves carrying a 20 kg un-modeled payload (84% of body weight)."
Layered Control for Cooperative Locomotion of Two Quadrupedal Robots: Centralized and Distributed Approaches,"Jeeseop Kim, Randall Fawcett, Vinaykarthik Kamidi, Aaron Ames, Kaveh Akbari Hamed","Caltech,Virginia Polytechnic Institute and State University,Virginia Tech",Legged Robots III,"This paper presents a layered control approach for real-time trajectory planning and control of robust cooperative locomotion by two holonomically constrained quadrupedal robots. A novel interconnected network of reduced-order models, based on the single rigid body (SRB) dynamics, is developed for trajectory planning purposes. At the higher level of the control architecture, two different model predictive control (MPC) algorithms are proposed to address the optimal control problem of the interconnected SRB dynamics: centralized and distributed MPCs. The distributed MPC assumes two local quadratic programs that share their optimal solutions according to a one-step communication delay and an agreement protocol. At the lower level of the control scheme, distributed nonlinear controllers are developed to impose the full-order dynamics to track the prescribed reduced-order trajectories generated by MPCs. The effectiveness of the control approach is verified with extensive numerical simulations and experiments for the robust and cooperative locomotion of two holonomically constrained A1 robots with different payloads on variable terrains and in the presence of disturbances. It is shown that the distributed MPC has a performance similar to that of the centralized MPC, while the computation time is reduced significantly."
RL + Model-Based Control: Using On-Demand Optimal Control to Learn Versatile Legged Locomotion,"Dongho Kang, Jin Cheng, Miguel Zamora, Fatemeh Zargarbashi, Stelian Coros",ETH Zurich,Legged Robots III,"This letter presents a control framework that combines model-based optimal control and reinforcement learning (RL) to achieve versatile and robust legged locomotion. Our approach enhances the RL training process by incorporating on-demand reference motions generated through finite-horizon optimal control, covering a broad range of velocities and gaits. These reference motions serve as targets for the RL policy to imitate, leading to the development of robust control policies that can be learned with reliability. Furthermore, by utilizing realistic simulation data that captures whole-body dynamics, RL effectively overcomes the inherent limitations in reference motions imposed by modeling simplifications. We validate the robustness and controllability of the RL training process within our framework through a series of experiments. In these experiments, our method showcases its capability to generalize reference motions and effectively handle more complex locomotion tasks that may pose challenges for the simplified model, thanks to RLâ€™s flexibility. Additionally, our framework effortlessly supports the training of control policies for robots with diverse dimensions, eliminating the necessity for robot-specific adjustments in the reward function and hyperparameters."
Generation of Steady Wheel Gait for Planar X-Shaped Walker with Reaction Wheel,"Fumihiko Asano, Taiki Sedoguchi, Cong Yan","Japan Advanced Institute of Science and Technology,Ritsumeikan University",Legged Robots III,"This paper addresses the problem of realizing a novel robotic bipedal locomotion called wheel gait, which is achieved by rotating the stance and swing legs in the same direction. First, a model of a planar 3-DOF X-shaped walker with a reaction wheel is introduced, and the mathematical equations are described. Second, the condition for stabilizing zero dynamics is formulated as the time integral value of control input to the reaction wheel for one step becomes zero, and the control system for achieving this is designed based on the method of continuous-time output deadbeat control. Third, a typical steady wheel gait of the linearized model is numerically generated, and its extension to the nonlinear model is discussed. Although the nonlinear model has only one nonlinear term in the gravity term, numerical simulations show that there is a big gap between this and the linearized model. Through analysis of the typical nonlinear wheel gaits, the difficulty of achieving the same walking speed as the linearized model is discussed."
"Trajectory Optimization Strategy That Considers Body Tip-Over Stability, Limb Dynamics, and Motion Continuity in Legged Robots","Kuan-lun Lu, I-Chia Chang, Wei-shun Yu, Pei-Chun Lin","National Taiwan University,Purdue University",Legged Robots III,"We propose a limb trajectory planning method that considers both body and limb dynamics in robots, particularly suitable for those with non-trivial limb mass. To simplify the complexity and computation cost of using the full-body dynamics of the limbs, a reduced-order model that can simulate the dynamic characteristics of the original limb is proposed. The performance of the model is experimentally validated using an exemplary single leg-wheel of the leg-wheel transformable robot. The limb trajectory optimization is developed using a genetic algorithm that considers many aspects, including body and limb dynamics, limb workspace, limb motion continuity, body tip-over stability, and power consumption. The performance of the proposed limb trajectory planning strategy is experimentally validated using the same leg-wheel transformable robot, and the results confirm the effectiveness of the strategy."
Hybrid Force-Impedance Control for Fast End-Effector Motions,"Maged Iskandar, Christian Ott, Alin Albu-Schäffer, Bruno Siciliano, Alexander Dietrich","German Aerospace Center - DLR,TU Wien,DLR - German Aerospace Center,Univ. Napoli Federico II,German Aerospace Center (DLR)",Motion Control III,"Controlling the contact force on various surfaces is essential in many robotic applications such as in service tasks or industrial use cases. Mostly, classical impedance and hybrid motion-force control approaches are employed for these kinds of physical interaction scenarios. In this work, an extended Cartesian impedance control algorithm is developed, which includes geometrical constraints and enables explicit force tracking in a hybrid manner. The unified framework features compliant behavior in the free (motion) task directions and explicit force tracking in the constrained directions. Advantageously, the involved force subspace in contact direction is fully dynamically decoupled from dynamics in the motion subspace. The experimental validation with a torque-controlled robotic manipulator on both flat and curved surfaces demonstrates the performance during highly dynamic desired trajectories and confirms the theoretical claims of the approach."
A Reinforcement Learning-Based Control Strategy for Robust Interaction of Robotic Systems with Uncertain Environments,"Diletta Sacerdoti, Federico Benzi, Cristian Secchi","University of Modena and Reggio-Emilia,University of Modena and Reggio Emilia,Univ. of Modena & Reggio Emilia",Motion Control III,"In the context of interaction with unmodelled systems, it becomes imperative for a robot controller to possess the capability to dynamically adjust its actions in real-time, enhancing its resilience in the face of fluctuating environmental conditions. This adaptation process must be performed in a stability-preserving fashion, and resourcefully exploit the knowledge acquired during the interaction process. In this article, we propose a novel control strategy, based on the synergistic usage of state-of-the-art passivity-based control and Deep Reinforcement Learning (DRL). The concept of energy tank is used to provide stability guarantees for the interaction controller with uncertain environments, while an online learning policy allows to properly estimate the requirements of the task and adapt the controller accordingly, thus simultaneously achieving stability and performance. The proposed architecture is successfully validated through simulations and experiments with a collaborative manipulator in a surface polishing task."
Stiffness-Based Hybrid Motion/ Force Control for Cable-Driven Serpentine Manipulator,"Wenshuo Li, Wenfu Xu, Peisheng Huang, Boyang Lin, Bin Liang","Harbin Institute of Technology,Harbin Institute of Technology, Shenzhen,Tsinghua University",Motion Control III,"In recent years, there has been a growing demand for robotic manipulators to perform tasks in various unstructured environments and situations requiring precision and force control. However, traditional robotic arms have limitations in fully leveraging their advantages in such scenarios. To address this demand, we have designed a cable-driven serpentine manipulator (CDSM) that combines force and precision motion control. This control method allows for precise manipulation of forces and torques at the end-effector, particularly in applications like electric vehicle charging and narrow-space exploration. It also enables independent control in multiple configurations. We achieve force-position hybrid control in task space, ensuring accurate control of end-effector force while achieving precise position control in other directions. Additionally, we implement joint angle closed-loop control in joint space to reduce the impact of cable elasticity deformation and friction on joint motion accuracy. Finally, servo control is applied at the lowest motor level. This paper investigates the modeling, sensing, and control of CDSM within a unified framework of hybrid motion/force control. Through experiments and simulations, we demonstrate the high accuracy and practicality of this control method in various scenarios."
Reinforcement Learning for Reduced-Order Models of Legged Robots,"Yu-Ming Chen, Hien Bui, Michael Posa",University of Pennsylvania,Motion Control III,"Model-based approaches for planning and control for bipedal locomotion have a long history of success. It can provide stability and safety guarantees while being effective in accomplishing many locomotion tasks. Model-free reinforcement learning, on the other hand, has gained much popularity in recent years due to computational advancements. It can achieve high performance in specific tasks, but it lacks physical interpretability and flexibility in re-purposing the policy for a different set of tasks. For instance, we can initially train a neural network (NN) policy using velocity commands as inputs. However, to handle new task commands like desired hand or footstep locations at a desired walking velocity, we must retrain a new NN policy. In this work, we attempt to bridge the gap between these two bodies of work on a bipedal platform. We formulate a model-based reinforcement learning problem to learn a reduced-order model (ROM) within a model predictive control (MPC). Results show a 49% improvement in viable task region size and a 21% reduction in motor torque cost. All videos and code are available at https://sites.google.com/view/ymchen/research/rl-for-roms."
K-BMPC: Derivative-Based Koopman Bilinear Model Predictive Control for Tractor-Trailer Trajectory Tracking with Unknown Parameters,"Zehao Wang, Han Zhang, Jingchuan Wang","ShanghaiJiaoTong University,Shanghai Jiao Tong University",Motion Control III,"Nonlinear dynamics bring difficulties to controller design for control-affine systems such as tractor-trailer vehicles, especially when the parameters in the dynamics are unknown. To address this constraint, we propose a derivative-based lifting function construction method, show that the corresponding infinite dimensional Koopman bilinear model over the lifting function is equivalent to the original control-affine system. Further, we analyze the propagation and bounds of state prediction errors caused by the truncation in derivative order. The identified finite dimensional Koopman bilinear model would serve as predictive model in the next step. Koopman Bilinear Model Predictive control (K-BMPC) is proposed to solve the trajectory tracking problem. We linearize the bilinear model around the estimation of the lifted state and control input. Then the bilinear Model Predictive Control problem is approximated by a quadratic programming problem. Further, the estimation is updated at each iteration until the convergence is reached. Moreover, we implement our algorithm on a tractor-trailer system, taking into account the longitudinal and side slip effects. The open-loop simulation shows the proposed Koopman bilinear model captures the dynamics with unknown parameters and has good prediction performance. Closed-loop tracking results show the proposed K-BMPC exhibits elevated tracking precision with the commendable computational efficiency. The experimental results demonstrate the feasibility of K-BMPC."
Learning to Shape by Grinding: Cutting-Surface-Aware Model-Based Reinforcement Learning,"Takumi Hachimine, Jun Morimoto, Takamitsu Matsubara","Nara Institute of Science and Technology,ATR Computational Neuroscience Labs",Motion Control III,"Object shaping by grinding is a crucial industrial process in which a rotating grinding belt removes material. Object-shape transition models are essential to achieving automation by robots; however, learning such a complex model that depends on process conditions is challenging because it requires a significant amount of data, and the irreversible nature of the removal process makes data collection expensive. This paper proposes a cutting-surface-aware Model-Based Reinforcement Learning (MBRL) method for robotic grinding. Our method employs a cutting-surface-aware model as the object's shape transition model, which in turn is composed of a geometric cutting model and a cutting-surface-deviation model, based on the assumption that the robot action can specify the cutting surface made by the tool. Furthermore, according to the grinding resistance theory, the cutting-surface-deviation model does not require raw shape information, making the model's dimensions smaller and easier to learn than a naive shape transition model directly mapping the shapes. Through evaluation and comparison by simulation and real robot experiments, we confirm that our MBRL method can achieve high data efficiency for learning object shaping by grinding and also provide generalization capability for initial and target shapes that differ from the training data."
Adaptive Contact-Implicit Model Predictive Control with Online Residual Learning,"Wei-cheng Huang, Alp Aydinoglu, Wanxin Jin, Michael Posa","University of Pennsylvania, GRASP Lab,University of Pennsylvania,Arizona State University",Motion Control III,"The hybrid nature of multi-contact robotic systems, due to making and breaking contact with the environment, creates significant challenges for high-quality control. Existing model-based methods typically rely on either good prior knowledge of the multi-contact model or require significant offline model tuning effort, thus resulting in low adaptability and robustness. In this paper, we propose a real-time adaptive multi-contact model predictive control framework, which enables online adaption of the hybrid multi-contact model and continuous improvement of the control performance for contact-rich tasks. This framework includes an adaption module, which continuously learns a residual of the hybrid model to minimize the gap between the prior model and reality, and a real-time multi-contact MPC controller. We demonstrated the effectiveness of the framework in synthetic examples, and applied it on hardware to solve contact-rich manipulation tasks, where a robot uses its end-effector to roll different unknown objects on a table to track given paths. The hardware experiments show that with a rough prior model, the multi-contact MPC controller adapts itself on-the-fly with an adaption rate around 20 Hz and successfully manipulates previously unknown objects with non-smooth surface geometries. Accompanying media can be found at: https://sites.google.com/view/adaptive-contact-implicit-mpc /home"
DRIVE: Data-Driven Robot Input Vector Exploration,"Dominic Baril, Simon-pierre Deschênes, Luc Coupal, Cyril Goffin, Julien Lépine, Philippe Giguere, Francois Pomerleau","Université Laval,EPFL",Motion Control III,"An accurate motion model is a fundamental component of most autonomous navigation systems. While much work has been done on improving model formulation, no standard protocol exists for gathering empirical data required to train models. In this work, we address this issue by proposing Data-driven Robot Input Vector Exploration (DRIVE), a protocol that enables characterizing uncrewed ground vehicles (UGVs) input limits and gathering empirical model training data. We also propose a novel learned slip approach outperforming similar acceleration learning approaches. Our contributions are validated through an extensive experimental evaluation, cumulating over 7 km and 1.8 h of driving data over three distinct UGVs and four terrain types. We show that our protocol offers increased predictive performance over common human-driven data-gathering protocols. Furthermore, our protocol converges with 46 s of training data, almost four times less than the shortest human dataset gathering protocol. We show that the operational limit for our model is reached in extreme slip conditions encountered on surfaced ice. DRIVE is an efficient way of characterizing UGV motion in its operational conditions. Our code and dataset are both available online at this link: https://github.com/norlab-ulaval/DRIVE."
Redundancy Resolution at Position Level,"Alin Albu-Schäffer, Arne Sachtler","DLR - German Aerospace Center,Technical University of Munich (TUM)",Motion Control III,"Increasing robotic systems' degrees of freedom (DoFs) makes them more versatile and flexible. This usually renders the system kinematically redundant: the main manipulation or interaction task does not fully determine its joint maneuvers. Additional constraints or objectives are required to solve the underdetermined control and planning problems. The state-of-the-art approaches arrange tasks in a hierarchy and decouple lower priority tasks from higher priority tasks on velocity or torque level using projectors. We develop an approach to redundancy resolution and decoupling on position level by determining subspaces of the configurations space independent of the primary task. We call them orthogonal foliations because they are, in a certain sense, orthogonal to the task self-motion manifolds. The approach provides a better insight into the topological properties of robot kinematics and control problems, allowing a global view. A condition for the existence of orthogonal foliations is derived. If the condition is not satisfied, we will still find approximate solutions by numerical optimization. Coordinates can be defined on these orthogonal foliations and used as additional task variables for control. We show in simulations that we can control the system without the need for projectors using these coordinates, and we validate the approach experimentally on a seven-DoF robot."
A Force-Driven and Vision-Driven Hybrid Control Method of Autonomous Laparoscope-Holding Robot,"Jin Fang, Ling Li, Xiaojian Li, Hangjie Mo, Pengxin Guo, Xilin Xiao, Yanwei Qu","Hefei University of Technology,City University of HongKong",Medical Robots III,"Laparoscope-holding robots significantly enhance the stability and precision of visualization in minimally invasive surgeries. Most existing robots of this kind depend on visual servo systems and struggle with efficient, rapid adjustments in the field-of-view (FOV), especially when identifying organs and needles outside the FOV. This paper presents a laparoscope-holding robot system capable of employing both vision-driven and force-driven mechanisms for continuous and large-scale FOV adjustments, respectively. The system features an integrated tactile handle, enabling the reception of human-robot interaction forces during surgical navigation. We propose a hybrid control method that leverages both force and vision inputs for laparoscopic FOV adjustments. This approach integrates a virtual wrench, generated from visual information, and an interaction wrench, obtained from the tactile handle, into the robot's dynamic model, which complies with remote center of motion constraints. The interaction wrench's gain is adjusted with the gripping force on the integrated tactile handle, ensuring that unintended movements caused by accidental contacts are prevented, thus safeguarding operational safety. The proposed method eliminates the need to switch control modes, enabling simultaneous visual tracking and tactile interaction guidance. Experimental results demonstrate that the proposed method not only allows for FOV adjustments with surgical instrument guiding but also adapts well to large-scale FOV adjustment tasks."
Inconstant Curvature Kinematics of Parallel Continuum Robot without Static Model,"Tao Zhang, Huxin Gao, Hongliang Ren","Chinese University of Hong Kong,National University of Singapore,Chinese Univ Hong Kong (CUHK) & National Univ Singapore(NUS)",Medical Robots III,"In the study of minimally invasive surgical robots, a mini parallel continuum robot has shown motion advantage after passing through a long and winding working channel. However, due to the interaction force between the elastic wires of the parallel robots during motion generation processes, the constant curvature assumption has shown modeling errors. This causes the current geometric kinematic model to become unreliable. Therefore, there is a need for a more accurate kinematic model in the absence of a complicated static model. This paper aims to solve this issue. The simulation in ANSYS is carried out, and the shape of one of the driving wires, when bending, is fitted by a two-segment polynomial curve. Then, the position of the distal wrist tip can be calculated based on the curve shape. To verify the accuracy of the proposed model, bending simulation and experiment are carried out. The accuracy of the proposed model is compared with that of the kinematic model based on constant curvature assumption. The result shows that the proposed model can get more accurate results, especially when the driving wire displacement increases. For a 10 mm parallel robot, when the displacements of the two pairs of wires are both 3.0 mm, the errors of the two models are 0.42 mm and 5.79 mm (4.2% and 57.9%), respectively."
A Novel SEA-Based Haptic Interface for Robot-Assisted Vascular Interventional Surgery,"Yonggan Yan, Shuxiang Guo, Chuqiao Lyu, Jian Guo, Jian Wang, Pengfei Yang, Yongwei Zhang, Yongxin Zhang, Jianmin Liu","Beijing Institute of Technology,Kagawa University,Shenzhen Institute of Advanced Biomedical Robot  Co., Ltd.,Changhai hospital",Medical Robots III,"Robot-assisted vascular interventional surgery can isolate interventionists and X-ray radiation, and improve surgical accuracy. However, the leader side outside the operating room still has problems such as incomplete collection of operating information and unrealistic tactile feedback. The main objective of this paper is to design a haptic interface that can simultaneously capture the force-position information of the interventionists and generate force to assist the interventionists in performing surgeries on the leader side. It can capture the interventionists' delivery displacement, twisting angle, clamping force, and provide real-time force feedback. A leader-follower bidirectional force feedback control strategy was proposed. Based on this strategy, on the one hand, the interventionist perceives the multi-modal information fed back from the follower side, makes judgments, and actively adjusts the surgical operation. On the other hand, the interventionist controls the grasping state of the instruments remotely to control the safety operating force threshold. Finally, the experimental setup was built and a series of evaluation experiments were performed. The experimental results verified the feasibility of the designed haptic interface. It can generate dynamic and accurate force feedback and realize leader-follower grasping force control."
A Miniature 1R1T Precision Manipulator with Remote Center of Motion for Minimally Invasive Surgery,Hiroyuki Suzuki,"Sony Computer Science Laboratories, Inc.",Medical Robots III,"In robotic-assisted minimally invasive surgery, the remote center of motion (RCM) achieves precision and safe manipulation of surgical devices through the insertion point into the patientâ€™s body. One of the RCM configurations, one-rotation and one-translation (1R1T) RCM based on a closed-loop design, enables two-degrees-of-freedom transmission from the proximal end of the robotic arm to the distal end. This feature offers important advantages, particularly in enhancing safety by minimizing physical contact risks with patients or other surgical tools owing to the simplified layout near the surgical field. However, conventional 1R1T RCM robots typically employ complex structures with numerous joints consisting of pin-and-hole mating mechanisms. This complexity can increase the overall size of the robot and compromise motion precision. This study presents a miniature 1R1T precision manipulator with RCM, ORIGANOID (dimensions: W60 x D120 x H30 mm, weight: 12.6 g). The robotic arm features flexure hinges, eliminates clearance issues, and can be fabricated using an origami-inspired robotic approach. Furthermore, using a novel backlash-free coupling method, the robotic arm could be easily attached and detached from the drive units. A prototype was fabricated and experimentally validated. The results demonstrated that high-resolution motion could be achieved within 10 um. Furthermore, a demonstration using an eyeball model confirmed the successful implementation of 1R1T RCM."
A Three-Dimensional Compliant Bowtie-Shaped Mechanical Amplifier to Magnify Coaxial Displacement in a Confined Space,"Jintaek Im, Eunsil Jang, Cheol Song","DGIST,Daegu Gyeongbuk Institute of Science and Technology",Medical Robots III,"This paper proposes a novel form of a three-dimensional coaxial bowtie-shaped mechanical amplifier. The proposed model incorporates a lever mechanism into the Sarrus linkage structure. It allows the target plate to move along one axis with amplified displacement in a parallel manner. The amplifier was assembled after machining the components using a 3-axis computer numerical control machine. A flexible hinge was incorporated into the amplifier design for simplified fabrication and reduced friction in the actuation mechanism. Castiglianoâ€™s theorem is used to build a mathematical model of the proposed mechanical amplifier, and the performance was validated through finite element analysis and prototype fabrication. We achieved the amplification ratio of Ã—8.44, resulting in the axial displacement up to 86 Âµm. The demonstrated amplifier is expected to apply to compact microsurgical robots or biomedical imaging apparatus requiring coaxial displacement amplification in confined spaces."
A Magnetic Continuum Robot with In-Situ Magnetic Reprogramming Capability,"Junnan Xue, Moqiu Zhang, Xurui Liu, Jiaqi Zhu, Yanfei Cao, Li Zhang","The Chinese University of Hong Kong,Huazhong University of Science and Technology",Medical Robots III,"Magnetic continuum robots (MCR) have shown great potential in minimally invasive interventions because they can be actively and remotely navigated through complex in vivo environments. However, the deformation capability of current MCRs is limited by fixed magnetization configurations, preventing them from accessing hard-to-reach areas. This is due to the fact that under a global magnetic field, fixed magnetization configuration causes the magnets on the MCRs exposed to coupled magnetic forces and torques, resulting in a lack of controllable degrees of freedom. Here, we introduce a reprogrammable magnetic continuum robot (RMCR) enabled by magnetic reprogramming modules (MRM). Actuated by shape memory alloys, the magnetic moment direction of MRMs can be selectively reprogrammed in real-time and in-situ. Magnetic reprogramming capabilities enable the RMCR to achieve complex shape transformations. Results show that the range of motion in the tip direction of the RMCR increases by 193% compared with regular MCR. Besides, MRMs on the RMCR can achieve active attraction and separation under simple magnetic fields. The reprogramming process of the RMCR is theoretically investigated. A design methodology for MRMs is then proposed and the fabrication process of RMCR is described in detail. Furthermore, a kinematic model of the RMCR is established, simulated, and experimentally validated."
Design and Control of a Magnetically-Actuated Anti-Interference Microrobot for Targeted Therapeutic Delivery,"Yanding Qin, Zhuocong Cai, Jianda Han",Nankai University,Medical Robots III,"This paper proposes an anti-interference targeted therapeutic delivery microrobot, where the targeted therapeutic delivery to the lesion site in human intestine can be operated by the external magnetic field. The robot is composed of a shell and a targeted delivery mechanism. Under the actuation of the external magnetic field, the spiral structure on the outer surface of the shell can actively moves the robot back and forth in human intestine. The internal embedded targeted delivery mechanism is fixed with a radial magnetized O-type permanent magnet. It not only realizes flexible movement in the fluid environment, but also realizes the intestinal anchoring and targeted therapeutic delivery functions against the constant peristalsis of human intestine. The proposed robot is designed to finish the drug treatment concentration and treatment effect precisely in the lesion site against the intestinal peristalsis. A series of simulations and experiments are conducted to evaluate the feasibility of the developed robot. In experiments, the robot is used to release drugs after anchoring in the lesion site. Finally, ex vivo experiments are carried out on fresh porcine intestines."
Design and Visual Servoing Control of a Hybrid Dual-Segment Flexible Neurosurgical Robot for Intraventricular Biopsy,"Jian Chen, Mingcong Chen, Qing Xiang Zhao, Shuai Wang, Yihe Wang, Ying Xiao, Jian Hu, Tat-Ming Chan, Kam Tong Leo Yeung, David Yuen Chung Chan, Hongbin Liu","University of Chinese Academy of Sciences,City University of Hong Kong,Hong Kong Institute of Science & Innovation, Centre for Artifici,HKPU(The Hong Kong Polytechnic University),Centre for Artificial Intelligence and Robotics,Institute of Automation, Chinese Academy of Sciences,Prince of Wales Hospital,Department of Surgery Faculty of Medicine The Chinese University,Hong Kong Institute of Science & Innovation, Chinese Academy of ",Medical Robots III,"Traditional rigid endoscopes have challenges in flexibly treating tumors located deep in the brain, and low operability and fixed viewing angles limit its development. This study introduces a novel dual-segment flexible robotic endoscope MicroNeuro, designed to to perform biopsies with dexterous surgical manipulation deep in the brain. Taking into account the uncertainty of the control model, an image-based visual servoing with online robot Jacobian estimation has been implemented to enhance motion accuracy. Furthermore, the application of model predictive control with constraints significantly bolsters the flexible robot's ability to adaptively track mobile objects and resist external interference. Experimental results underscore that the proposed control system enhances motion stability and precision. Phantom testing substantiates its considerable potential for deployment in neurosurgery."
Uncertainty-Aware Shape Estimation of a Surgical Continuum Manipulator in Constrained Environments Using Fiber Bragg Grating Sensors,"Alexander Schwarz, Arian Mehrfard, Golchehr Amirkhani, Henry Phalen, Justin Ma, Robert Grupp, Alejandro Martin-gomez, Mehran Armand",Johns Hopkins University,Medical Robots III,"Continuum Dexterous Manipulators (CDMs) are well-suited tools for minimally invasive surgery due to their inherent dexterity and reachability. Nonetheless, their flexible structure and non-linear curvature pose significant challenges for shape-based feedback control. The use of Fiber Bragg Grating (FBG) sensors for shape sensing has shown great potential in estimating the CDM's tip position and subsequently reconstructing the shape using optimization algorithms. This optimization, however, is under-constrained and may be ill-posed for complex shapes, falling into local minima. In this work, we introduce a novel method capable of directly estimating a CDM's shape from FBG sensor wavelengths using a deep neural network. In addition, we propose the integration of uncertainty estimation to address the critical issue of uncertainty in neural network predictions. Neural network predictions are unreliable when the input sample is outside the training distribution or corrupted by noise. Recognizing such deviations is crucial when integrating neural networks within surgical robotics, as inaccurate estimations can pose serious risks to the patient. We present a robust method that not only improves the precision upon existing techniques for FBG-based shape estimation but also incorporates a mechanism to quantify the models' confidence through uncertainty estimation. We validate the uncertainty estimation through extensive experiments, demonstrating its effectiveness and reliability on out-of-distribution (OOD) data, adding an additional layer of safety and precision to minimally invasive surgical robotics."
Autonomous Robotic Re-Alignment for Face-To-Face Underwater Human-Robot Interaction,"Demetrious T. Kutzke, Ashwin Wariar, Junaed Sattar","University of Minnesota - Twin Cities,University of Minnesota Twin-Cities,University of Minnesota",Search & Rescue Robotics in Fields,"The use of autonomous underwater vehicles (AUVs) to accomplish traditionally challenging and dangerous tasks has proliferated thanks to advances in sensing, navigation, manipulation, and on-board computing technologies. Utilizing AUVs in underwater human-robot interaction (UHRI) has witnessed comparatively smaller levels of growth due to limitations in bi-directional communication and significant technical hurdles to bridge the gap between analogies with terrestrial interaction strategies and those that are possible in the underwater domain. A necessary component to support UHRI is establishing a system for safe robotic-diver approach to establish face-to-face communication that considers non-standard human body pose. In this work, we introduce a stereo vision system for enhancing UHRI that utilizes three-dimensional reconstruction from stereo image pairs and machine learning for localizing human joint estimates. We then establish a convention for a coordinate system that encodes the direction the human is facing with respect to the camera coordinate frame. This allows automatic setpoint computation that preserves human body scale and can be used as input to an image-based visual servo control scheme. We show that our setpoint computations tend to agree both quantitatively and qualitatively with experimental setpoint baselines. The methodology introduced shows promise for enhancing UHRI by improving robotic perception of human orientation underwater."
Mechanism Design for New Sensors Field Deployment by LineRanger Powerline Robot,"Pierre-Luc Richard, Bellemare Jonathan, Philippe Hamelin, Camille Hébert, Ghislain Lambert, Samuel Lavoie, Leprohon Sebastien, Matthieu Montfrond, Nourry Marion, Alex Sartor, Nicolas Pouliot","Hydro-Quebec Research Institute,IREQ- Hydro-Québec Research Institute,Hydro-Québec's Research Institute,Hydro-Québec Research Institute",Search & Rescue Robotics in Fields,"Powerline robotics is slowly becoming key tools for electric utilities. Contrary to drones that are usually limited to inspection tasks, wheeled robots like LineRanger can ensure a broader range of applications. In this paper, a suite of mechanical devices is featured, as several new asset management tasks were recently added to LineRangerâ€™s capabilities. While previous applications were focusing on non-contact inspection (visual, electro-magnetic, etc.), the new tasks at hand involved reaching adjacent conductors to probe line components with micro-Ohmmeter, installing and retrieving custom build sensors for multi-day line monitoring, and assessing aging conductors surface properties, to refine their thermal model and optimize the line capacity during heat waves. All three applications were recently field validated onto LineRanger, and mechanical design insights shall be presented for each module."
Translational Disturbance Rejection for Jet-Actuated Flying Continuum Robots on Mobile Bases,"Yukihiro Maezawa, Yuichi Ambe, Yu Yamauchi, Masashi Konyo, Kenjiro Tadakuma, Satoshi Tadokoro","Tohoku University,Osaka University,Akita Prefectural University",Search & Rescue Robotics in Fields,"Although continuum robots have the potential to operate in narrow areas by changing their shapes and propelling their bodies, they easily vibrate under sudden or periodic applications of external forces. Suppressing vibrations is difficult in our jet-actuated continuum robot because the movements of its mobile base cannot be controlled with the same system as the movements of the robot, and mobile base oscillation increases the risk of resonance. In this study, a disturbance rejection was realized for the Dragon Firefighter, a jet-actuated flying continuum robot on a mobile base, for rapid and safe fire extinguishing using a 4-m-long flying fire hose consisting of two nozzle units and flexible hoses. An H-infinity-based disturbance-rejection controller was designed to suppress the vibration of the head nozzle unit posture against the acceleration of the mobile base. Then, the robot parameters were identified from tensile tests and dynamic excitation experiments. Dynamic simulations confirmed that the controller reduced the peak gain of the frequency response by approximately 2 dB for various robot shapes. Robot experiments confirmed that the proposed method reduced the peak gain of the frequency response by approximately 3 dB, which increased the extra injection range of the nozzle by approximately 16 %."
Cellular-Enabled Collaborative Robots Planning and Operations for Search-And-Rescue Scenarios,"Arnau Romero, Carmen Delgado, Lanfranco Zanzi, Raul Suarez, Xavier Costa","i,CAT Foundation,NEC Laboratories,Universitat Politecnica de Catalunya (UPC)",Search & Rescue Robotics in Fields,"Mission-critical operations, particularly in the context of Search-and-Rescue (SAR) and emergency response situations, demand optimal performance and efficiency from every component involved to maximize the success probability of such operations. In these settings, cellular-enabled collaborative robotic systems have emerged as invaluable assets, assisting first responders in several tasks, ranging from victim localization to hazardous area exploration. However, a critical limitation in the deployment of cellular-enabled collaborative robots in SAR missions is their energy budget, primarily supplied by batteries, which directly impacts their task execution and mobility. This paper tackles this problem, and proposes a search-and-rescue framework for cellular-enabled collaborative robots use cases that, taking as input the area size to be explored, the robots fleet size, their energy profile, exploration rate required and target response time, finds the minimum number of robots able to meet the SAR mission goals and the path they should follow to explore the area. Our results, i) show that first responders can rely on a SAR cellular-enabled robotics framework when planning mission-critical operations to take informed decisions with limited resources, and, ii) illustrate the number of robots versus explored area and response time trade-off depending on the type of robot: wheeled vs quadruped."
STAGE: Scalable and Traversability-Aware Graph Based Exploration Planner for Dynamically Varying Environments,"Akash Patel, Mario Alberto Valdes Saucedo, Christoforos Kanellakis, George Nikolakopoulos","Luleå University of Technology,Lulea University of Technology,LTU",Search & Rescue Robotics in Fields,"In this article, we propose a novel navigation framework that leverages a two layered graph representation of the environment for efficient large-scale exploration, while it integrates a novel uncertainty awareness scheme to handle dynamic scene changes in previously explored areas. The framework is structured around a novel goal oriented graph representation, that consists of, i) the local sub-graph and ii) the global graph layer respectively. The local sub-graphs encode local volumetric gain locations as frontiers, based on the direct pointcloud visibility, allowing fast graph building and path planning. Additionally, the global graph is build in an efficient way, using node-edge information exchange only on overlapping regions of sequential sub-graphs. Different from the state-of-the-art graph based exploration methods, the proposed approach efficiently re-uses sub-graphs built in previous iterations to construct the global navigation layer. Another merit of the proposed scheme is the ability to handle scene changes (e.g. blocked pathways), adaptively updating the obstructed part of the global graph from traversable to not-traversable. This operation involved oriented sample space of a path segment in the global graph layer, while removing the respective edges from connected nodes of the global graph in cases of obstructions. As such, the exploration behavior is directing the robot to follow another route in the global re-positioning phase through path-way updates in the global graph. Finally, we showcase the performance of the method both in simulation runs as well as deployed in real-world scene involving a legged robot carrying camera and lidar sensor."
Computation-Aware Multi-Object Search in 3D Space Using Submodular Tree,"Yanshuo Li, Kuo-shih Tseng",National Central University,Search & Rescue Robotics in Fields,"Search for targets in a 3D environment can be formulated as submodular maximization problems with routing constraints. However, it involves solving two NP-hard problems: maximal coverage and traveling salesman problems. Since the time constraint is critical for search problems, this research proposes a Computation-Aware Search for Multiple Objects (CASMO) algorithm to further consider the computational time in the cost constraints. Due to the submdularity, the greedy algorithm achieves $frac{1}{2}(1-frac{1}{e})overline{OPT}$, where $overline{OPT}$ is the approximate optimum. The experiment results show that the proposed algorithms outperform state-of-the-art approaches in multi-object search."
Multi-Robot Search in a 3D Environment with Intersection System Constraints,"Yanshuo Li, Kuo-shih Tseng",National Central University,Search & Rescue Robotics in Fields,The efficient task allocation is a challenge for multi-robot search. The multi-robot search problem is reformulated as submodular maximization subject to intersection system constraints. The objective function is submodular and consists of a coverage function to cover environments and a balancing function to efficiently dispatch robots. The intersection system is composed of routing and clustering constraints. The experiment results show that the proposed approach outperforms state-of-the-art methods in multi-robot search.
Wireless Communication Infrastructure Building for Mobile Robot Search and Inspection Missions,"Martin Zoula, Jan Faigl",Czech Technical University in Prague,Search & Rescue Robotics in Fields,"In the paper, we address wireless communication infrastructure building by relay placement based on approaches utilized in wireless network sensors. The problem is motivated by search and inspection missions with mobile robots, where known sensing ranges may be exploited. We investigate the relay placement, establishing network connectivity to support robust flood-based communication routing. The proposed method decomposes the given area into Open space and Corridor space where specific deployment patterns allow for guaranteed k-connectivity, making the resulting network redundant while keeping channel utilization bounded. In particular, a hexagonal tesselation coverage pattern with 3-connectivity is investigated in Open space and a linear 4-connectivity pattern in Corridor space, respectively. The proposed approach is empirically evaluated in a realistic scenario, and based on the reported results, it is found superior compared to the existing stochastic randomized dual sampling schema."
RB5 Low-Cost Explorer: Implementing Autonomous Long-Term Exploration on Low-Cost Robotic Hardware,"Adam Seewald, Marvin Chancán, Connor Mccann, Seonghoon Noh, Omeed Fallahi, Hector Castillo, Ian Abraham, Aaron Dollar","Yale University,Harvard University",Search & Rescue Robotics in Fields,"This systems paper presents the implementation and design of RB5, a wheeled robot for autonomous long-term exploration with fewer and cheaper sensors. Requiring just an RGB-D camera and low-power computing hardware, the system consists of an experimental platform with rocker-bogie suspension. It operates in unknown and GPS-denied environments and on indoor and outdoor terrains. The exploration consists of a methodology that extends frontier- and sampling-based exploration with a path-following vector field and a state-of-the-art SLAM algorithm. The methodology allows the robot to explore its surroundings at lower update frequencies, enabling the use of lower-performing and lower-cost hardware while still retaining good autonomous performance. The approach further consists of a methodology to interact with a remotely located human operator based on an inexpensive long-range and low-power communication technology from the internet-of-things domain (i.e., LoRa) and a customized communication protocol. The results and the feasibility analysis show the possible applications and limitations of the approach."
Takeoff of a 2.1g Fully Untethered Tailless Flapping-Wing Micro Aerial Vehicle with Integrated Battery,"Takashi Ozaki, Norikazu Ohta, Tomohiko Jimbo, Kanae Hamaguchi","Toyota Central R&D Labs. Inc.,Company,TOYOTA CENTRAL R&D LABS., INC.,Toyota Central R&D Labs., Inc.",Bioinspired Microrobots,"Insect-scale micro-aerial vehicles (MAV) are becoming increasingly important for sensing and mapping spatially constrained environments. However, achieving untethered flight powered by a small and lightweight on-board energy source remains a challenge. In this study, we successfully demonstrated the untethered takeoff of a flapping-wing MAV powered by a commercially available LiPo battery in short duration without stability control. By incorporating a high-efficiency direct-drive piezoelectric actuator, and an optimized control circuit for high-impedance-width modulation and charge recovery, this electronics realizes over 5 min of practical operation with a thrust of 1.5 times its own weight, while offering wireless communication capability and sensors for attitude estimation. Our MAV has a total mass of 2.1 g, which is an eight-fold reduction compared to the lightest battery-powered tailless flapping-wing MAV currently available, making this is the first report of a battery-powered tailless flapping-wing MAV with insect-scale weight."
Design and Optimization of a Miniature Locust-Inspired Stable Jumping Robot,"Yi Xu, Yanzhou Jin, Weitao Zhang, Yunhao Si, Yulai Zhang, Chang Li, Qing Shi",Beijing Institute of Technology,Bioinspired Microrobots,"Jumping is a key locomotion for miniature robots, but it is difficult for a robot to jump a long distance without flipping. To solve this problem, we develop a miniature locust-inspired jumping robot, which has a body length of 10 cm and weight of 60 g. On the basis of the extracted skeletal muscle movement of a locust, we make full use of the Stephenson six-bar mechanism in designing a jumping leg to achieve power amplification. Moreover, we carry out a two-step optimization of the mechanism parameters to achieve high jumping energy (first step) through optimizing the storage and dissipation of energy and then high jumping stability (second step) through optimizing the force characteristics. A series of experimental tests show that the robot can jump to a height three times its body length and a distance seven times its body length. Remarkably, the jumping height and distance relative to the body length of our jumper exceeds that of other robots with stable mechanisms by 30% and 33%, respectively. Meanwhile, our robot has a high degree of stability, which allows it to maintain a proper aerial orientation without flipping."
"Multi-Modal Jumping and Crawling in an Autonomous, Springtail-Inspired Microrobot","Shashwat Singh, Zeynep Temel, Ryan St. Pierre","Carnegie Mellon University,University at Buffalo",Bioinspired Microrobots,"Springtails are tiny arthropods that crawl and jump. They jump by temporarily storing elastic energy in resilin elastic cuticular structures and releasing that energy to accelerate a tail, called a furca, propelling them in the air. This paper presents an autonomous, springtail-inspired microrobot that can crawl and jump. The microrobot has a mass of 980 mg and stands 13 mm tall, and has on-board sensing, computation, and power, enabling autonomy. The microrobot was designed with a super-elastic shape memory alloy (SMA) spring that is manually loaded to store elastic energy. The on-board sensing and computation triggers an actuator at the jump frequency range that unlatches the spring, launching the microrobot into the air at speeds up to 3.171 m/s. At the same time, the microrobot is capable of crawling, when actuated at frequencies lower or higher than the jump frequency range, demonstrating autonomous multi-modal locomotion. This work opens up new pathways toward autonomy in multi-modal microrobots."
High-Speed Interfacial Flight of an Insect-Scale Robot,"Hang Gao, Sunghwan Jung, Farrell E. Helbling",Cornell University,Bioinspired Microrobots,"Several insect species are able to locomote across the air-water interface by leveraging surface tension to remain above the water surface. A subset of these insects, such as the stonefly and waterlily beetle, flap their wings to actively move around the two dimensional surface â€” a locomotion strategy referred to as interfacial flight. Here, we present an insect-scale robot, the gamma-bot, inspired by these interfacial fliers. The robot is comprised of a flapping-wing vehicle that generates a thrust force parallel to the water surface, and three passive legs utilize surface tension to support the body mass and maintain contact with the air-water interface. We developed and validated a simple model to characterize the drag forces acting on the vehicle and estimate the robotâ€™s velocity. This 112 mg robot can reach maximum velocities of 0.9 m/s (corresponding to 15 body lengths per second) and can initiate both left and right turns, demonstrating high maneuverability along the air-water interface. In addition, the robot can carry an additional 419 mg, enabling future sensing, control, and power autonomous operation."
VLEIBot: A New 45-Mg Swimming Microrobot Driven by a Bioinspired Anguilliform Propulsor,"Elijah Blankenship, Conor Trygstad, Francisco Goncalves, Nestor O Perez-Arancibia","Washington State University,Washington State University (WSU)",Bioinspired Microrobots,"This paper presents the VLEIBot* (Very Little Eel-Inspired roBot), a 45-mg/23-mm3 microrobotic swimmer that is propelled by a bioinspired anguilliform propulsor. The propulsor is excited by a single 6-mg high-work-density (HWD) microactuator and undulates periodically due to wave propagation phenomena generated by fluid-structure interaction (FSI) during swimming. The microactuator is composed of a carbon-fiber beam, which functions as a leaf spring, and shape-memory alloy (SMA) wires, which deform cyclically when excited periodically using Joule heating. The VLEIBot can swim at speeds as high as 15.1 mm Â· sâˆ’1 (0.33 Bl Â· sâˆ’1) when driven with a heuristically-optimized propulsor. To improve maneuverability, we evolved the VLEIBot design into the 90-mg/47-mm3 VLEIBot+, which is driven by two propulsors and fully controllable in the two-dimensional (2D) space. The VLEIBot+ can swim at speeds as high as 16.1 mm Â· sâˆ’1 (0.35 Bl Â· sâˆ’1), when driven with heuristically-optimized propulsors, and achieves turning rates as high as 0.28 rad Â· sâˆ’1, when tracking path references. The measured root-mean-square (RMS) values of the tracking errors are as low as 4 mm."
Direct Learning of Home Vector Direction for Insect-Inspired Robot Navigation,"Michiel Vital M Firlefyn, Jesse Hagenaars, Guido De Croon","TU Delft,Delft University of Technology",Bioinspired Microrobots,"Insects have long been recognized for their ability to navigate and return home using visual cues from their nest's environment. However, the precise mechanism underlying this remarkable homing skill remains a subject of ongoing investigation. Drawing inspiration from the learning flights of honey bees and wasps, we propose a robot navigation method that directly learns the home vector direction from visual percepts during a learning flight in the vicinity of the nest. After learning, the robot will travel away from the nest, come back by means of odometry, and eliminate the resultant drift by inferring the home vector orientation from the currently experienced view. Using a compact convolutional neural network, we demonstrate successful learning in both simulated and real forest environments, as well as successful homing control of a simulated quadrotor. The average errors of the inferred home vectors in general stay well below the 90Â° required for successful homing, and below 24Â° if all images contain sufficient texture and illumination. Moreover, we show that the trajectory followed during the initial learning flight has a pronounced impact on the network's performance. A higher density of sample points in proximity to the nest results in a more consistent return. Code and data are available at https://mavlab.tudelft.nl/learning_to_home."
A Dragonfly-Inspired Flapping Wing Robot Mimicking Force Vector Control Approach,"Fangyuan Liu, Song Li, Jinwu Xiang, Daochun Li, Zhan Tu",Beihang University,Bioinspired Microrobots,"Dragonflies show impressive flying skills by achieving both high efficiency and agility. They can perform distinctive flight maneuvers, such as flying backwards, which has proven to be achieved through â€force vectoringâ€ mechanism recently. In this paper, to explore the agile flight ability of dragonflies on man-made flapping wing systems, we designed, optimized and fabricated a dragonfly-inspired flapping wing robot (DFWR) with inclinable stroke plane control degrees. The proposed platform employs a four-wing configuration, each of which integrates an extra servo motor to enable the rotation of the flapping plane and imitate the â€force vectoringâ€ mechanism. Besides, referring to the flapping kinematics of dragonflies, the installation angle and wing pitch angle of the proposed DFWR are optimized considering the total lift and energy consumption through multiobjective optimization based on NSGA-II method. The â€force vectorâ€ produced by the proposed platform has been illustrated through both theoretical method and experimental method. Moreover, the feasibility of the design is further verified through a series of operation validation experiments. Such a robot has the potential to provide a highly biomimetic platform to validate the flight mechanism studying of Odonata as well as the relative on-board applications such as bio-inspired vision."
Microrobotic Flight Enabled by Ultralight Ion Thrusters with High Thrust-To-Weight Ratio and Low Fabrication Cost,"Yang Gu, Xianfa Cai, Khadga Thakuri, Wenyu Yang, Yufeng Guo, Wei Li","Nanjing University of Posts and Telecommunications,University of Vermont,Huazhong University of Science and Technology",Bioinspired Microrobots,"Flying microrobots have garnered growing research interest owing to their technological intricacies and suitability for various applications leveraging miniaturized size. Electrohydrodynamic (EHD) thrust offers advantages by generating propulsion without moving parts, but real-world use is limited by insufficient thrust generation, manufacturing challenges, fragility, and cost. This work presents the design and development of an optimized ion-propelled flying microrobot that excels in low weight, high thrust-to-weight ratio, and cost efficiency. Regarding design, multiphysics simulations guided structural optimization to increase thrust while decreasing weight. For materials, metal-coated polyethylene terephthalate (PET) film was selected to leverage the combined merits of metal conductivity and polymer flexibility, light weight, and low cost, enabling further weight reduction, easy assembly, robustness, and cost-effectiveness. Various experiments, including voltage-current measurements, ionic wind speed, thrust quantification, and airflow visualization, directed design refinements and validated performance. Through structural optimization, the maximum wind speed attained 2.18 m/s. Flight demonstrations with payloads evidenced the microrobot can stably fly at an inherent 16 mg weight while carrying an additional 72 mg load, achieving a record 5.5 thrust-to-weight ratio. These results open possibilities to incorporate microelectronics, enabling autonomous flight functionality."
A Modular Biological Neural Network-Based Neuro-Robotic System Via Local Chemical Stimulation and Calcium Imaging,"Zhe Chen, Xie Chen, Shingo Shimoda, Qiang Huang, Qing Shi, Toshio Fukuda, Tao Sun","Beijing Institute of Technology,RIKEN",Bioinspired Microrobots,"Embodying in vitro biological neural networks (BNNs) with robots to explore the rise of intelligence in these simpler models and to endow robots with biological intelligence has been attracting increasing attention in the fields of neuroscience and robotics. However, current research suffers from unstable sensory-motor mapping due to the random wiring of neurons seeded on multi-electrode arrays (MEAs). Therefore, here we propose a modular BNN (mBNN)-based neuro-robotic system via local chemical stimulation and calcium recording. In this system, reliable evoked sensory-motor mapping (success rate > 89%) from the sensory to the motor area in the mBNN was demonstrated. It is achieved in the mBNNs by combining global chemical modulation (for suppressing spontaneous signal transmission) and local chemical stimulation (for inducing the evoked signal transmission). The neural signals of the motor area of the BNN are recorded by calcium imaging, analyzed, and decoded to control the motion state of the mobile robot in real-time. The sensory signals of the robot are encoded and transmitted to the sensory area of the BNN, closing the loop. This system presents a platform to investigate how information is processed and transmitted in mBNNs, and also to examine the influence of local and global chemical modulation on within-network signal transmission."
An Augmented Catenary Model for Underwater Tethered Robots,"Martin Filliung, Juliette Drupt, Charly Peraud, Claire Dune, Nicolas Boizot, Andrew Ian Comport, Cedric Anthierens, Vincent Hugel","CNRS LIS, COSMER Laboratory, Université de Toulon,Université de Toulon,COSMER Laboratory, Université de Toulon,CNRS-I,S/UNS,Universite de Toulon,University of Toulon",Marine Robotics III,"This paper examines the relevance of using catenary-based curves to model cables in underwater tethered robotic applications in order to take into account the influence of hydrodynamic damping. To this end, an augmented catenary- based model is introduced to deal with the dynamical effects of surge motion, sway motion or a combination of both on a cable. Experimental studies are carried out with eight cables of varying stiffness, weight and buoyancy. One end of the cable is fixed, while the other end is moved by the underwater robot. The obtained results help to determine which cables and which dynamics are compatible with a fair estimation of the cable shape through the proposed models."
A Hybrid Dynamical Model for Robotic Underwater Vehicles When Submerged or Surfaced: Approach and Preliminary Evaluation,"James Hunt, Louis Whitcomb","Johns Hopkins University,The Johns Hopkins University",Marine Robotics III,"This paper reports a numerical method for modeling underwater vehicle (UV) interactions with the free surface using a finite-dimensional dynamical plant model. Although finite-dimensional plant models of fully submerged UV behavior are well-established, they are unable to model the ubiquitous condition of a UV operating at or near the free surface. We report a Monte Carlo-based hybrid model approach for calculating the buoyancy and righting moment of a partially or fully submerged UV in order to model interactions with the free surface. We also report a preliminary evaluation of the hybrid model in numerical simulations, comparing the hybrid model's performance to that of a model for fully submerged UVs and to the experimentally observed behavior of an actual vehicle while fully submerged and while interacting with the free surface. The results of this preliminary study suggest that the proposed hybrid approach may offer a simple and practical method for modeling UV behavior when submerged or interacting with the free surface."
Surfing Algorithm: Agile and Safe Transition Strategy for Hybrid Aerial Underwater Vehicle in Waves,"Yuanbo Bi, Yufei Jin, Hexiong Zhou, Yulin Bai, Chenxin Lyu, Zheng Zeng, Lian Lian","Shanghai jiao tong University,Shanghai Jiao Tong University,Shanghai Jiaotong University",Marine Robotics III,"The agile and safe trans-domain in waves is a promising feature but the primary bottleneck of the hybrid aerial underwater vehicle (HAUV). In this article, the surfing algorithm is proposed to search for the dynamic window facilitating takeoff in waves and avoiding hazardous waves. For the first time, the cross-domain window, i.e. the vehicle is at the wave crest and heading downstream, is characterized and defined. The novel surfing algorithm consists of the gradient perceptron, time-limited momentum gradient search, heading server, and initial conditions. Numerical simulations and experiments in regular and irregular waves reveal the effectiveness of the algorithm. The algorithm ensures a healthy initial attitude and inaccessible wave disturbance during takeoff, thus alleviating the thrust distraction from stability recovery and uncertainty. The average transition time and energy cost are saved by 59.2% and 26.1% compared with random take-off cases, and the locomotion is smooth, graceful, and low-risk. Compared with the adaptive robust controller, this article provides an ingenious and enlightening strategy from the perspective of harnessing waves."
ReefGlider: A Highly Maneuverable Vectored Buoyancy Engine Based Underwater Robot,"Kevin Macauley, Levi Cai, Peter G. Adamczyk, Yogesh Girdhar","University of Wisconsin-Madison,Massachusetts Institute of Technology,University of Wisconsin - Madison,Woods Hole Oceanographic Institution",Marine Robotics III,"There exists a capability gap in the design of currently available autonomous underwater vehicles (AUV). Most AUVs use a set of thrusters, and optionally control surfaces, to control their depth and pose. AUVs utilizing thrusters can be highly maneuverable, making them well-suited to operate in complex environments such as in close-proximity to coral reefs. However, they are inherently power-inefficient and produce significant noise and disturbance. Underwater gliders, on the other hand, use changes in buoyancy and center of mass, in combination with a control surface to move around. They are extremely power efficient but not very maneuverable. Gliders are designed for long-range missions that do not require precision maneuvering. Furthermore, since gliders only activate the buoyancy engine for small time intervals, they do not disturb the environment and can also be used for passive acoustic observations. In this paper we present ReefGlider, a novel AUV that uses only buoyancy for control but is still highly maneuverable from additional buoyancy control devices. ReefGlider bridges the gap between the capabilities of thruster-driven AUVs and gliders. These combined characteristics make ReefGlider ideal for tasks such as long-term visual and acoustic monitoring of coral reefs. We present the overall design and implementation of the system, as well as provide analysis of some of its capabilities."
Robust Model Predictive Control with Control Barrier Functions for Autonomous Surface Vessels,"Wei Wang, Wei Xiao, Alejandro Gonzalez-Garcia, Jan Swevers, Carlo Ratti, Daniela Rus","University of Wisconsin-Madison,MIT,KU Leuven,Massachusetts Institute of Technology",Marine Robotics III,"In autonomous robot navigation, the trajectories from path planners are considered to be safe regions, and deviations could endanger vessels. Model Predictive Control (MPC) stands as a popular choice for trajectory tracking problems as it naturally addresses operational constraints, such as dynamics and control constraints. Nevertheless, achieving robustness in changing environments like oceans and rivers, which are constantly subject to significant external disturbances, remains an ongoing challenge for MPC. It must consistently keep the system within a predefined safe region (such as a reference trajectory) even in the presence of model inaccuracies and perturbations. To address this challenge, we present a robust model predictive control strategy utilizing Control Barrier Functions (CBFs), which increases the disturbance-rejection abilities. We verify our method on an autonomous surface vessel in simulation and natural waters, both with external disturbances. Specifically, compared with the traditional MPC method, our proposed MPC-CBF strategy reduces tracking errors by 17.82% and 40.26% in simulations and field experiments, respectively. Although the control effort slightly increases by 7.78% and 4.20%, respectively, these results clearly demonstrate the enhanced resilience of MPC-CBF to disturbances."
Design and Optimization of a Multimode Amphibious Robot with Propeller-Leg,"Xinmeng Ma, Gang Wang, Liu Kaixin",Harbin Engineering University,Marine Robotics III,"This paper describes a novel multimode motion robot named SHOALBOT,which with multimode operations depends on only one type of propulsiondevice and can work flexibly in the amphibious environment. Robots that work in water need to minimize the number of drive components to improve reliability and reduce communication pressure, our unique design enables the robot to rely on a kind of propulsion device named propeller-leg to have the ability to run rapidly in the seaside and seabed, and swim with multi degrees of freedom in the water (contains only four driving elements). We analyzed and optimized propeller-leg by simulation combined with the open water test, the propeller-leg's thrust in the water before and after optimization differs by 400% according to the test results. We optimized the minimum difference between the forward and reverse thrust of the propeller-leg to improve the stability of the robot movement process, and optimized the difference from 25% to 3%. This paper provides sufficient technical details and completeness, and through a series of experiments validated that the SHOALBOT has excellent movement ability in the amphibious environment."
Underwater Dome-Port Camera Calibration: Modeling of Refraction and Offset through N-Sphere Camera Model,"Monika Roznere, Adithya Pediredla, Samuel Lensgraf, Yogesh Girdhar, Alberto Quattrini Li","Dartmouth College,Woods Hole Oceanographic Institution",Marine Robotics III,"The optical effects that are observed in underwater imagery are more complex than those in-air. This is partially because we enclose most underwater cameras in a watertight enclosure, such as a hemispheric dome window. We then observe optical issues including the distortion effects of the lens, e.g., wide-angle field-of-view (FOV), the refractive effects at the enclosure (water-acrylic and acrylic-air) interfaces, and offset effects of a non-centered camera with respect to the dome. In this paper, we present an N-Sphere (NS) and Shifted N-Sphere (S-NS) camera models, tailored to these cameras and lenses mounted in water-tight dome enclosures. The proposed camera models treat each layer of effects as a â€˜sphereâ€™ that a 3D point will project on. Furthermore, the S-NS model includes additional parameters to address the camera offset variability. The versatility of the NS model makes it applicable to various lenses, as validated with fisheye (FOV >120 deg) and wide-FOV (FOV ~120 deg). We validated our models with different in-water calibration sequences, lenses, and housing setups, as well as with comparisons with other state-of-the-art camera models. Additionally, we demonstrated the performance of our proposed models in an example stereo-based visual odometry application. The low computational load of the proposed models makes it ideal for integrating in real-time visual navigation and reconstruction frameworks. We provide full math derivations of the proposed models as well as example C++ header files for easy incorporation in independent projects."
Benchmarking Classical and Learning-Based Multibeam Point Cloud Registration,"Li Ling, Jun Zhang, Nils Bore, John Folkesson, Anna Wåhlin","KTH Royal Institute of Technology,KTH,University of Gothenburg",Marine Robotics III,"Deep learning has shown promising results for multiple 3D point cloud registration datasets. However, in the underwater domain, most registration of multibeam echo-sounder (MBES) point cloud data are still performed using classical methods in the iterative closest point (ICP) family. In this work, we curate and release DotsonEast, a semi-synthetic MBES registration dataset constructed from an autonomous underwater vehicle in West Antarctica. Using this dataset, we systematically benchmark the performance of 2 classical and 4 learning-based methods. The experimental results show that the learning-based methods work well for coarse alignment, and are better at recovering rough transforms consistently at high overlap (20-50%). In comparison, GICP (a variant of ICP) performs well for fine alignment and is better across all metrics at extremely low overlap (10%). To the best of our knowledge, this is the first work to benchmark both learning-based and classical registration methods on an AUV-based MBES dataset. To facilitate future research, both the code and data are made available online."
Angler: An Autonomy Framework for Intervention Tasks with Lightweight Underwater Vehicle Manipulator Systems,"Evan Palmer, Chris Holm, Geoffrey Hollinger",Oregon State University,Marine Robotics III,"Developing autonomous intervention capabilities for lightweight underwater vehicle manipulator systems (UVMS) has garnered significant attention within recent years because of the opportunity for these systems to reduce intervention operating costs. Developing autonomous UVMS capabilities is challenging, however, because of the lack of available standardized software frameworks and pipelines. Previous works offer simulation environments and deployment pipelines for underwater vehicles, but fall short of providing a complete UVMS software framework. We address this gap by creating Angler: a software framework for developing localization, control, and decision-making algorithms with support for sim-to-real transfer. We validate this framework by implementing a state-of-the-art control architecture and demonstrate the ability to perform station keeping with a mean error below 0.25 m and waypoint tracking with an average final error of 0.398 m."
Hitchhiker: A Quadrotor Aggressively Perching on a Moving Inclined Surface Using Compliant Suction Cup Gripper,"Sensen Liu, Zhaoying Wang, Xinjun Sheng, Wei Dong","ShanghaiJiaotong University,Shanghai Jiao Tong University",Mechanics and Control III,"Perching on the surface of moving objects, like vehicles, could extend the flight time and range of quadrotors. Suction cups are usually adopted for surface attachment due to their durability and large adhesive force. To seal on a surface, suction cups must be aligned with the surface and possess proper relative tangential velocity. However, quadrotors' attitude and relative velocity errors would become significant when the object's surface is moving and inclined. To address this problem, we proposed a real-time trajectory planning algorithm. The time optimal aggressive trajectory is efficiently generated through multimodal search in a dynamic time-domain. The velocity errors relative to the moving surface are alleviated. To further adapt to the residual errors, we design a compliant gripper using self-sealing cups. Multiple cups in different directions are integrated into a wheel-like mechanism to increase the tolerance to attitude errors. The wheel mechanism also eliminates the requirement of matching the attitude and tangential velocity. Extensive tests are conducted to perch on static and moving surfaces at various inclinations. Results demonstrate that our proposed system enables a quadrotor to reliably perch on moving inclined surfaces (up to 1.07m/s and 90â—¦) with a success rate of 70% or higher. The efficacy of the trajectory planner is also validated."
Harnessing the Differential Flatness of Monocopter Dynamics for the Purpose of Trajectory Tracking in a Stable Invertible Coaxial Actuated ROtorcraft (SICARO),"Emmanuel Tang, Wei Jun Ang, Kian Wee Tan, Shaohui Foong","Singapore University of Technology & Design,Singapore University of Technology and Design",Mechanics and Control III,"In this paper, the dynamics of an emerging class of rotating nature-inspired micro aerial vehicles known as the Monocopter is proven and shown to be differentially flat. By exploiting this phenomenon, trajectory tracking can now be implemented on Monocopters via feed-forward terms that are computed per the trajectory. To demonstrate this, a Monocopter in the form of a Stable Invertible Coaxial Actuated ROtorcraft (SICARO) is chosen to harness this approach fully. The SICARO is capable of flying with either side of the wing facing up and this feature determines the craftâ€™s direction of rotation about its body Z axis as well. In addition, it has the unique feature of a coaxial motor configuration that allows for a pitching-up moment regardless of the wing side facing up. The feed-forward terms computed are fused into a cascaded nonlinear controller on the craft to ensure its effectiveness in tracking trajectories. Lastly, the flight experiments extend to both sides of the wing to validate this method as being applicable for trajectory tracking for Monocopters such as the SICARO which has an extended range of flying capabilities."
Passive Aligning Physical Interaction of Fully-Actuated Aerial Vehicles for Pushing Tasks,"Tong Hui, Eugenio Cuniato, Michael Pantic, Marco Tognon, Matteo Fumagalli, Roland Siegwart","Technical University of Denmark,ETH Zurich,ETH Zürich,Inria Rennes,Danish Technical University",Mechanics and Control III,"Recently, the utilization of aerial manipulators for performing pushing tasks in non-destructive testing (NDT) applications has seen significant growth. Such operations entail physical interactions between the aerial robotic system and the environment. End-effectors with multiple contact points are often used for placing NDT sensors in contact with a surface to be inspected. Aligning the NDT sensor and the work surface while preserving contact, requires that all available contact points at the end-effector tip are in contact with the work surface. With a standard full-pose controller, attitude errors often occur due to perturbations caused by modeling uncertainties, sensor noise, and environmental uncertainties. Even small attitude errors can cause a loss of contact points between the end-effector tip and the work surface. To preserve full alignment amidst these uncertainties, we propose a control strategy which selectively deactivates angular motion control and enables direct force control in specific directions. In particular, we derive two essential conditions to be met, such that the robot can passively align with flat work surfaces achieving full alignment through the rotation along non-actively controlled axes. Additionally, these conditions serve as hardware design and control guidelines for effectively integrating the proposed control method for practical usage. Real world experiments are conducted to validate both the control design and the guidelines."
Modeling and Control of PADUAV: A Passively Articulated Dual UAVs Platform for Aerial Manipulation,"Jiali Sun, Kaidi Wang, Chuanbeibei Shi, Xiujia Li, Xiaojian Yi, Yushu Yu, Fuchun Sun, Yiqun Dong","Beijing Institute of Technology,Univeristy of Bristol,Tsinghua University,Nanyang Technological University",Mechanics and Control III,"In this paper, we introduce PADUAV, a novel 5-DOF aerial platform designed to overcome the limitations of traditional tiltrotor vehicles. PADUAV features a unique mechanical design that incorporates two off-the-shelf quadrotors passively articulated to a rigid frame. This innovation enables free pitch rotation without mechanical constraints like cable winding, significantly enhancing its capabilities for various tasks. To control PADUAV's 5 degrees of freedom, we propose a versatile and straightforward 5-DOF geometric tracking control strategy that generates 2D force and 3D torque. A decomposition approach is designed to distribute the output to the torque and thrust commands for each subplane, with no need for complex optimization. We validate our approach through three simulation experiments conducted in the Gazebo environment, leveraging the utilities provided by the RotorS simulator. These experiments not only demonstrate the feasibility of our platform but also provide new perspectives for future aerial platform development, particularly in terms of simulation-based approaches."
Particle Filter with Stable Embedding for State Estimation of the Rigid Body Attitude System on the Set of Unit Quaternions,"Hee-deok Jang, Jae-Hyeon Park, Dong Eui Chang","Korea Advanced Institute of Science Technology,Korea Advanced Institute of Science and Technology (KAIST),KAIST",Mechanics and Control III,"This paper presents a novel method for state estimation of rigid body attitude system evolving on the manifold, S3, which is crucial in robotics and drone applications. We introduce a particle filter with stable embedding that extends the system into Euclidean space while ensuring stability of the manifold. Our particle filter with stable embedding enables accurate state estimation by maintaining estimated state values in close proximity to the manifold, while requiring significantly fewer computational resources than the standard exponential-map-based method that keeps state estimates on the manifold. Furthermore, our method facilitates the application of usual techniques designed for particle filters in Euclidean spaces, to the manifold system, as is, without any modification. The accuracy and the efficiency of our particle filter are confirmed both by simulation and by real drone experiments."
End-To-End Reinforcement Learning for Time Optimal Quadcopter Flight,"Robin Ferede, Christophe De Wagter, Dario Izzo, Guido De Croon","TU Delft,Delft University of Technology,European Space Agency",Mechanics and Control III,"Aggressive time-optimal control of quadcopters poses a significant challenge in the field of robotics. The state-of-the-art approach leverages reinforcement learning (RL) to train optimal neural policies. However, a critical hurdle is the sim-to-real gap, often addressed by employing a robust inner loop controller â€”an abstraction that, in theory, constrains the optimality of the trained controller, necessitating margins to counter potential disturbances. In contrast, our novel approach introduces high-speed quadcopter control using end-to-end RL (E2E) that gives direct motor commands. To bridge the reality gap, we incorporate a learned residual model and an adaptive method that can compensate for modeling errors in thrust and moments. We compare our E2E approach against a state-of-the-art network that commands thrust and body rates to an INDI inner loop controller, both in simulated and real-world flight. E2E showcases a significant 1.39-second advantage in simulation and a 0.17-second edge in real-world testing, highlighting end-to-end reinforcement learning's potential. The performance drop observed from simulation to reality shows potential for further improvement, including refining strategies to address the reality gap or exploring offline reinforcement learning with real flight data."
Quadrolltor: A Reconfigurable Quadrotor with Controlled Rolling and Turning,"Huaiyuan Jia, Runze Ding, Kaixu Dong, Songnan Bai, Pakpong Chirarattananon","City University of Hong Kong,CITY UNIVERSITY OF HONGKONG",Mechanics and Control III,"This letter reports an aerial robot--Quadrolltor, with the ability to roll and turn. Existing bimodal quadrotors feature cylindrical rolling cages that are rotationally decoupled from the robot's main rigid body. In contrast, the proposed robot employs passively reconfigurable structures to enable the second mode of locomotion, tightly coupling the attitude of the robot to the rolling cage. The benefits are precise rolling and turning control as well as improved rolling efficiency. Experiments were conducted to comprehensively validate the hybrid locomotion. The robot leveraged the superior maneuverability in the rolling mode to take photos of the surroundings at different tilting and panning angles to construct a panoramic image. Besides, the results of the power measurements show a significant reduction in the cost of transport brought by rolling, equating to a 15-fold extension in the operational range."
Co-Design Optimisation of Morphing Topology and Control of Winged Drones,"Fabio Bergonti, Gabriele Nava, Valentin Wueest, Antonello Paolino, Giuseppe L'erario, Daniele Pucci, Dario Floreano","Istituto Italiano di Tecnologia,EPFL,Italian Institute of Technology,Ecole Polytechnique Fédérale de Lausanne (EPFL)",Mechanics and Control III,"The design and control of winged aircraft and drones is an iterative process aimed at identifying a compromise of mission-specific costs and constraints. When agility is required, shape-shifting (morphing) drones represent an efficient solution. However, morphing drones require the addition of actuated joints that increase the topology and control coupling, making the design process more complex. We propose a co-design optimisation method that assists the engineers by proposing a morphing droneâ€™s conceptual design that includes topology, actuation, morphing strategy, and controller parameters. The method consists of applying multi-objective constraint-based optimisation to a multi-body winged drone with trajectory optimisation to solve the motion intelligence problem under diverse flight mission requirements, such as energy consumption and mission completion time. We show that co-designed morphing drones outperform fixed-winged drones in terms of energy efficiency and mission time, suggesting that the proposed co-design method could be a useful addition to the aircraft engineering toolbox."
Robust Control for Bidirectional Thrust Quadrotors under Instantaneously Drastic Disturbances,"Zujian Chen, Mo Shaolin, Botao Zhang, Hui Cheng, Jiyu Li","Shenzhen University,Sun Yat-sen University,College of Engineering, South China Agricultural University",Mechanics and Control III,"Quadrotors may crash and cause severe accidents under instantaneously drastic disturbances. To mitigate the effect of such disturbances, these critical issues should be considered: efficient disturbance observation and compensation, full attitude controllability, and instant output power generation of the quadrotor. In this paper, to keep the quadrotor stable even under suddenly drastic disturbances, a novel control framework is presented to by integrating the advantages of active disturbance rejection control (ADRC) as well as geometric control for a quadrotor with bidirectional thrust capabilities. Moreover, to strengthen the adaptability under significant disturbances, a novel switching strategy is introduced into the control framework by virtue of the quadrotorâ€™s bidirectional thrust capabilities. The ADRC scheme is performed when the disturbances are within a range; alternatively, if the disturbances surpass the preset range and the desired control is beyond the ultimate output of the quadrotor, the quadrotor compliantly responds by executing a $180^circ$ flip reverse flight to handle such drastic disturbances. Numerical and real-world experiments demonstrate that the proposed robust control strategy has superior performance adapts to instantaneously drastic disturbances."
Topological Exploration Using Segmented Map with Keyframe Contribution in Subterranean Environments,"Boseong Kim, Hyunki Seong, David Hyunchul Shim","Korea Advanced Institute of Science and Technology (KAIST),KAIST",Field Robot Systems,"Existing exploration algorithms mainly generate frontiers using random sampling or motion primitive methods within a specific sensor range or search space. However, frontiers generated within constrained spaces lead to back-and-forth maneuvers in large-scale environments, thereby diminishing exploration efficiency. To address this issue, we propose a method that utilizes a 3D dense map to generate Segmented Exploration Regions (SERs) and generate frontiers from a global-scale perspective. In particular, this paper presents a novel topological map generation approach that fully utilizes Line-of-Sight (LOS) features of LiDAR sensor points to enhance exploration efficiency inside large-scale subterranean environments. Our topological map contains the contributions of keyframes that generate each SER, enabling rapid exploration through a switch between local path planning and global path planning to each frontier. The proposed method achieved higher explored volume generation than the state-of-the-art algorithm in a large-scale simulation environment and demonstrated a 62% improvement in explored volume increment performance. For validation, we conducted field tests using UAVs in real subterranean environments, demonstrating the efficiency and speed of our method."
"A Powerline Inspection UAV Equipped with Dexterous, Lockable Gripping Mechanisms for Autonomous Perching and Contact Rolling","Angus Lynch, Corey Duguid, Joao Buzzatto, Minas Liarokapis","University of Auckland,The University of Auckland",Field Robot Systems,"Inspection of powerlines is a hard problem that requires humans to operate in remote locations and dangerous conditions. This paper proposes a quadcopter unmanned aerial vehicle (UAV) equipped with rolling-capable perching mechanisms and a depth-vision system for the purpose of autonomous power line inspection. The perching mechanism grips onto the power line, allowing the UAV to withstand external forces such as wind disturbances. Once engaged and applying the desired gripping force, the perching mechanism requires no power through the use of a ratcheting serial elastic transmission, allowing the UAV to perch indefinitely. The depth-vision system automates the perching and unperching procedures by estimating the position and pose of the UAV relative to the powerline. These measurements are sent to a local position controller that guides the UAV to and from the power line. Once perched, rollers in the fingers of the perching mechanism drive the UAV along the powerline, providing a close-up platform for inspection equipment. The proposed system was tested in an outdoor testing environment and shown to autonomously perch and unperch from a steel cable. The grippers force application was analysed and the UAVs powerless robust perch is demonstrated by total disconnect of power while perched. These results suggest that such a system could be a valuable tool for the upkeep of electricity networks."
GIRA: Gaussian Mixture Models for Inference and Robot Autonomy,"Kshitij Goel, Wennie Tabib",Carnegie Mellon University,Field Robot Systems,"This paper introduces the open-source framework, GIRA, which implements fundamental robotics algorithms for reconstruction, pose estimation, and occupancy modeling using compact generative models. Compactness enables perception in the large by ensuring that the perceptual models can be communicated through low-bandwidth channels during large-scale mobile robot deployments. The generative property enables perception in the small by providing high-resolution reconstruction capability. These properties address perception needs for diverse robotic applications, including multi-robot exploration and dexterous manipulation. State-of-the-art perception systems construct perceptual models via multiple disparate pipelines that reuse the same underlying sensor data, which leads to increased computation, redundancy, and complexity. GIRA bridges this gap by providing a uniï¬ed perceptual modeling framework using Gaussian mixture models (GMMs) as well as a novel systems contribution, which consists of GPU-accelerated functions to learn GMMs 10-100x faster compared to existing CPU implementations. Because few GMM-based frameworks are open-sourced, this work seeks to accelerate innovation and broaden adoption of these techniques."
Hybrid Trajectory Optimization for Autonomous Terrain Traversal of Articulated Tracked Robots,"Zhengzhe Xu, Yanbo Chen, Zhuozhu Jian, Junbo Tan, Xueqian Wang, Bin Liang","Harbin Institute of Technology, Shenzhen,Tsinghua University,Center for Artificial Intelligence and Robotics, Graduate School",Field Robot Systems,"Autonomous terrain traversal of articulated tracked robots can reduce operator cognitive load to enhance task efficiency and facilitate extensive deployment. We present a novel hybrid trajectory optimization method aimed at generating efficient, stable, and smooth traversal motions. To achieve this, we develop a planar robot-terrain contact model and divide the robotâ€™s motion into hybrid modes of driving and traversing. By using a generalized coordinate description, the configuration space dimension is reduced, which facilitates real-time planning. The hybrid trajectory optimization is transcribed into a nonlinear programming problem and divided into subproblems to be solved in a receding-horizon planning fashion. Mode switching is facilitated by associating optimized motion durations with a predefined traversal sequence. A multi-objective cost function is formulated to further improve the traversal performance. Additionally, map sampling, terrain simplification, and tracking controller modules are integrated into the autonomous terrain traversal system. Our approach is validated in simulation and real-world scenarios with the Searcher robotic platform. Comparative experiments with expert operator control and state-of-the-art methods show advantages in terms of time and energy efficiency, stability, and smoothness of motion."
X-ICP: Localizability-Aware LiDAR Registration for Robust Localization in Extreme Environments,"Turcan Tuna, Julian Nubert, Yoshua Alfredo Nava Chocrón, Shehryar Khattak, Marco Hutter","ETH Zurich, Robotic Systems Lab,ETH Zürich,ANYbotics AG,ETH Zurich",Field Robot Systems,"LiDAR-based localization methods, such as the Iterative Closest Point(ICP) algorithm, can suffer in geometrically uninformative environments that are known to deteriorate registration performance and push optimization toward divergence along weakly constrained directions. To overcome this issue, this work proposes i) a robust multi-category (non-)localizability detection module, and ii) a localizability-aware constrained ICP optimization module and couples both in a unified manner. The proposed localizability detection is achieved by utilizing the correspondences between the scan and the map to analyze the alignment strength against the principal directions of the optimization as part of its multi-category LiDAR localizability analysis. In the second part, this localizability analysis is then integrated into the scan-to-map point cloud registration to generate drift-free pose updates by enforcing controlled updates or leaving the degenerate directions of the optimization unchanged. The proposed method is thoroughly evaluated and compared to state-of-the-art methods in simulation and during real-world experiments, underlying the gain in performance and reliability."
Seabed Intervention with an Underwater Legged Robot,"Giacomo Picardi, Anna Astolfi, Marcello Calisti","Instituto de Ciencias del Mar (ICM)—Consejo Superior de Investig,Scuola Superiore Sant'Anna,The University of Lincoln",Field Robot Systems,"Efficiently performing intervention tasks underwater is crucial in various commercial and scientific sectors; however, propeller-driven vehicles face limitations due to their floating nature. In Remotely Operated Vehicles (ROVs) operations, this can be compensated by the ability of the operator, but they come with high operational costs. Instead, Autonomous Underwater Vehicles (AUVs) have shown promise, but demonstrated intervention tasks are limited to controlled environments or docked. To address these limitations, we focused on the use of Underwater Legged Robots (ULRs), which offer greater stability and agile seabed mobility thanks to their legged propulsion system. This paper presents the field demonstration of teleoperated pick-and-place tasks using the ULR SILVER2 for which a novel stance control, Graphic User Interface (GUI), and tendon-driven gripper have been developed based on the lessons learned through several hours of field use. The methodology is validated through four field trials, including missions in both shallow water and open sea environments. The trials involve picking and placing various objects, such as plastic bottles, bags, and cans. The results demonstrate successful teleoperated object grasping and manipulation in real-world conditions, with collection times ranging from a few minutes to around ten minutes. Overall, this research contributes to advancing the capabilities of ULRs and lays the foundation for future underwater intervention missions in various scientific and industrial applications, aligning with the goals of the Decade of Ocean Science for Sustainable Development."
Predicting against the Flow: Boosting Source Localization by Means of Field Belief Modeling Using Upstream Source Proximity,"Finn Lukas Busch, Nathalie Bauschmann, Sami Haddadin, Robert Seifried, Daniel Andre Duecker","Hamburg University of Technology,Technical University of Munich,Technical University of Munich (TUM)",Field Robot Systems,"Time-effective and accurate source localization with mobile robots is crucial in safety-critical scenarios, e.g. leakage detection. This becomes particular challenging in realistic cluttered scenarios, i.e. in the presence of complex current flows or wind. Traditional methods often fall short due to simplifications or limited onboard resources. We propose to combine source localization with a Gaussian Markov Random Field (GMRF). This allows to improve source localization hypotheses by building on the GMRF's concentration and flow field belief that are continuously updated by gathered measurements. We introduce the upstream source proximity (USP) as a natural metric that exploits the joint knowledge represented in the field belief's concentration and flow field, i.e. predicting sources upstream. As a result, our method yields a computationally efficient source localization and field belief module providing substantially more stable gradients than conventional concentration gradient-based methods. We demonstrate the suitability of our approach in a series of numerical experiments covering complex source location scenarios. With regard to computational requirements, the method achieves update rates of 10 Hz on a RaspberryPi 4B."
A Turning Radius Prediction Scheme for Sailing Robots under Complex Marine Environment,"Weimin Qi, Qinbo Sun, Huihuan Qian","The Chinese University of Hong Kong, Shenzhen,The Chinese Univeristy of Hong Kong, Shenzhen,The Chinese University of Hong Kong, Shenzhen",Field Robot Systems,"This paper presents a strategy for predicting the turning radius of a sailing robot with consideration of aerodynamic and hydrodynamic interferences from the marine environment. The turning radius is initially obtained based on three consecutive designated points during the turning process, which is regarded as the baseline method. Subsequently, on the basis of our constructed turning datasets, a model is trained using Gaussian process regression (GPR) to achieve radius prediction. The feasibility and effectiveness of the proposed scheme have been validated in both simulation and experiments (conducted with OceanVoy as shown in Fig.1). Under experimental circumstances, the Mean Absolute Error (MAE) of the turning radius produced by the trained prediction model is 0.58m. Furthermore, it has been observed that during long-term sailing covering a distance of 1200km, apart from wind speed and robot velocity, the tidal range also has a significant impact on the navigation of sailing robots."
A Vision-Based Autonomous UAV Inspection Framework for Unknown Tunnel Construction Sites with Dynamic Obstacles,"Zhefan Xu, Baihan Chen, Xiaoyang Zhan, Yumeng Xiu, Christopher Suzuki, Kenji Shimada",Carnegie Mellon University,Field Robot Systems,"Tunnel construction using the drill-and-blast method requires the 3D measurement of the excavation front to evaluate underbreak locations. Considering the inspection and measurement task's safety, cost, and efficiency, deploying lightweight autonomous robots, such as unmanned aerial vehicles (UAV), becomes more necessary and popular. Most of the previous works use a prior map for inspection viewpoint determination and do not consider dynamic obstacles. To maximally increase the level of autonomy, this paper proposes a vision-based UAV inspection framework for dynamic tunnel environments without using a prior map. Our approach utilizes a hierarchical planning scheme, decomposing the inspection problem into different levels. The high-level decision maker first determines the task for the robot and generates the target point. Then, the mid-level path planner finds the waypoint path and optimizes the collision-free static trajectory. Finally, the static trajectory will be fed into the low-level local planner to avoid dynamic obstacles and navigate to the target point. Besides, our framework contains a novel dynamic map module that can simultaneously track dynamic obstacles and represent static obstacles based on an RGB-D camera. After inspection, the Structure-from-Motion (SfM) pipeline is applied to generate the 3D shape of the target. To our best knowledge, this is the first time autonomous inspection has been realized in unknown and dynamic tunnel environments."
WayIL: Image-Based Indoor Localization with Wayfinding Maps,"Obin Kwon, Dongki Jung, Youngji Kim, Soohyun Ryu, Suyong Yeon, Songhwai Oh, Donghwan Lee","Seoul Natl University,NAVER LABS,NAVER Labs,Seoul National University,Naverlabs",Localization III,"This paper tackles a localization problem in large-scale indoor environments with wayfinding maps. A wayfinding map abstractly portrays the environment, and humans can localize themselves based on the map. However, when it comes to using it for robot localization, large geometrical discrepancies between the wayfinding map and the real world make it hard to use conventional localization methods. Our objective is to estimate a robot pose within a wayfinding map, utilizing RGB images from perspective cameras. We introduce two different imagination modules which are inspired by how humans can comprehend and interpret their surroundings for localization purposes. These modules jointly learn how to effectively observe the first-person-view (FPV) world to interpret bird-eye-view (BEV) maps. Providing explicit guidance to the two imagination modules significantly improves the precision of the localization system. We demonstrate the effectiveness of the proposed approach using real-world datasets, which are collected from various large-scale crowded indoor environments. The experimental results show that, in 85% of scenarios, the proposed localization system can estimate its pose within 3m in large indoor spaces. Project Site: https://rllab-snu.github.io/projects/WayIL/"
TransAPR: Absolute Camera Pose Regression with Spatial and Temporal Attention,"Chengyu Qiao, Zhiyu Xiang, Yuangang Fan, Tingming Bai, Xijun Zhao, Jingyun Fu","Zhejiang University,China North Vehicle Research Institute, China North Artificial I",Localization III,"Visual relocalization aims to estimate the absolute camera pose from an image or sequential images. Recent works tackle this problem by exploiting deep neural networks to regress camera poses. However, spatial and temporal clues from sequential images still remain underexplored, resulting in inaccurate poses and large outliers. In this work, we introduce a novel vision Transformer based absolute pose regression model, TransAPR, to tackle this problem. Upon the traditional CNN backbone, we design Transformer based spatial and temporal fusion modules respectively to realize sufficient feature interaction among the neighboring images in the sequence. A hierarchical feature aggregation (HFA) module is further designed to aggregate multi-scale and multi-level features in the pose regressor. Benefiting from these delicate designs, our model is able to generate reliable image representations for absolute pose regression, resulting in more robust localization under challenging environments. We conduct extensive experiments on various indoor and outdoor datasets and show that our method achieves state-of-the-art performance."
Globalizing Local Features: Image Retrieval Using Shared Local Features with Pose Estimation for Faster Visual Localization,"Song Wenzheng, Ran Yan, Boshu Lei, Takayuki Okatani","Tohoku University,Megvii,University of Pennsylvania",Localization III,"Visual localization is an important sub-task in SfM and visual SLAM that involves estimating a 6-DoF camera pose for an input query image relative to a given 3D model of the environment. The most accurate approach is a hierarchical one that splits the task into two stages: image retrieval and camera pose estimation. Each stage requires different image features, with global features compactly encoding holistic image information for the first stage and local features encoding the appearance around salient image points for the second stage. While existing methods use independent networks to extract these features, one for global and one for local, this strategy is suboptimal in terms of computational efficiency. In this paper, we propose a novel approach that achieves state-of-the-art inference accuracy with significantly improved efficiency. Our approachâ€™s core component is SuperGF, a network that aggregates local features optimized for camera pose estimation to create a global feature that enables precise image retrieval. Through extensive experiments on the standard benchmark tests, we demonstrate that the method offers a better trade-off between accuracy and computational cost."
Leveraging Neural Radiance Fields for Uncertainty-Aware Visual Localization,"Le Chen, Weirong Chen, Rui Wang, Marc Pollefeys","Max Planck Institute for Intelligent Systems,ETH Zurich,Technical University of Munich",Localization III,"As a promising fashion for visual localization, scene coordinate regression (SCR) has seen tremendous progress in the past decade. Most recent methods usually adopt neural networks to learn the mapping from image pixels to 3D scene coordinates, which requires a vast amount of annotated training data. We propose to leverage Neural Radiance Fields (NeRF) to generate training samples for SCR. Despite NeRF's efficiency in rendering, many of the rendered data are polluted by artifacts or only contain minimal information gain, which can hinder the regression accuracy or bring unnecessary computational costs with redundant data. These challenges are addressed in three folds in this paper: (1) A NeRF is designed to separately predict uncertainties for the rendered color and depth images, which reveal data reliability at the pixel level. (2) SCR is formulated as deep evidential learning with epistemic uncertainty, which is used to evaluate information gain and scene coordinate quality. (3) Based on the three arts of uncertainties, a novel view selection policy is formed that significantly improves data efficiency. Experiments on public datasets demonstrate that our method could select the samples that bring the most information gain and promote the performance with the highest efficiency."
JIST: Joint Image and Sequence Training for Sequential Visual Place Recognition,"Gabriele Berton, Gabriele Trivigno, Barbara Caputo, Carlo Masone","Politecnico di Torino,Polytechnic of Turin,Sapienza University",Localization III,"Visual Place Recognition aims at recognizing previously visited places by relying on visual clues, and it is used in robotics applications for SLAM and localization. Since typically a mobile robot has access to a continuous stream of frames, this task is naturally cast as a sequence-to-sequence localization problem. Nevertheless, obtaining sequences of labelled data is much more expensive than collecting isolated images, which can be done in an automated way with little supervision. As a mitigation to this problem, we propose a novel Joint Image and Sequence Training protocol (JIST) that leverages large uncurated sets of images through a multi-task learning framework. With JIST we also introduce SeqGeM, an aggregation layer that revisits the popular GeM pooling to produce a single robust and compact embedding from a sequence of single-frame embeddings. We show that our model is able to outperform previous state of the art while being faster, using 8 times smaller descriptors, having a lighter architecture and allowing to process sequences of various lengths. The code is available at https://github.com/ga1i13o/JIST"
CCL: Continual Contrastive Learning for LiDAR Place Recognition,"Jiafeng Cui, Xieyuanli Chen","Tongji University,National University of Defense Technology",Localization III,"Place recognition is an essential and challenging task in loop closing and global localization for robotics and autonomous driving applications. Benefiting from the recent advances in deep learning techniques, the performance of LiDAR place recognition (LPR) has been greatly improved. However, current deep learning-based methods suffer from two major problems: poor generalization ability and catastrophic forgetting. In this paper, we propose a continual contrastive learning method, named CCL, to tackle the catastrophic forgetting problem and generally improve the robustness of LPR approaches. Our CCL constructs a contrastive feature pool and utilizes contrastive loss to train more transferable representations of places. When transferred into new environments, our CCL continuously reviews the contrastive memory bank and applies a distribution-based knowledge distillation to maintain the retrieval ability of the past data while continually learning to recognize new places from the new data. We thoroughly evaluate our approach on Oxford, MulRan, and PNV datasets using three different LPR methods. The experimental results show that our CCL consistently improves the performance of different methods in different environments outperforming the state-of-the-art continual learning method. The implementation of our method has been released at https://github.com/cloudcjf/CCL."
OptiState: State Estimation of Legged Robots Using Gated Networks with Transformer-Based Vision and Kalman Filtering,"Alexander Schperberg, Yusuke Tanaka, Saviz Mowlavi, Feng Xu, Bharathan Balaji, Dennis Hong","University of California Los Angeles,University of California, Los Angeles,Mitsubishi Electric Research Laboratories,UCLA,Amazon",Localization III,"State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot's trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated in hardware using a quadruped robot on various terrains, yielding a 65% improvement on the Root Mean Squared Error compared to our VIO SLAM baseline. Code example: https://github.com/AlexS28/OptiState"
Pose-Graph Attentional Graph Neural Network for Lidar Place Recognition,"Milad Ramezani, Liang Wang, Joshua Barton Knights, Zhibin Li, Pauline Pounds, Peyman Moghadam","CSIRO,University of Queensland,Queensland University of Technology,The University of Queensland",Localization III,"This paper proposes a pose-graph attentional graph neural network, called P-GAT, which compares (key)nodes between sequential and non-sequential sub-graphs for place recognition tasks as opposed to a common frame-to-frame retrieval problem formulation currently implemented in SOTA place recognition methods. P-GAT uses the maximum spatial and temporal information between neighbour cloud descriptors â€” generated by an existing encoderâ€” utilising the concept of pose-graph SLAM. Leveraging intra- and inter-attention and graph neural network, P-GAT relates point clouds captured in nearby locations in Euclidean space and their embeddings in feature space. Experimental results on the large-scale publically available datasets demonstrate the effectiveness of our approach in scenes lacking distinct features and when training and testing environments have different distributions (domain adaptation). Further, an exhaustive comparison with the state-of-the-art shows improvements in performance gains. Code is available at https://github.com/csiro-robotics/P-GAT."
ColonMapper: Topological Mapping and Localization for Colonoscopy,"Javier Morlana, Juan D. Tardos, Jose M M Montiel","Universidad de Zaragoza, CIF: ESU,,,,,,,G, C/ Pedro Cerbuna ,,,Universidad de Zaragoza,I,A. Universidad de Zaragoza",Localization III,"We propose a topological mapping and localization system able to operate on real human colonoscopies, despite significant shape and illumination changes. The map is a graph where each node codes a colon location by a set of real images, while edges represent traversability between nodes. For close-in-time images, where scene changes are minor, place recognition can be successfully managed with the recent transformers-based local feature matching algorithms. However, under long-term changes --such as different colonoscopies of the same patient-- feature-based matching fails. To address this, we train on real colonoscopies a deep global descriptor achieving high recall with significant changes in the scene. The addition of a Bayesian filter boosts the accuracy of long-term place recognition, enabling relocalization in a previously built map. Our experiments show that ColonMapper is able to autonomously build a map and localize against it in two important use cases: localization within the same colonoscopy or within different colonoscopies of the same patient. Code will be available upon acceptance."
Simultaneous Localization and Actuation Using Electromagnetic Navigation Systems,"Denis Von Arx, Cedric Fischer, Harun Torlakcik, Salvador Pané Vidal, Bradley Nelson, Quentin Boehler","ETH Zurich,ETHZ",Localization III,"Remote magnetic navigation provides a promising approach for improving the maneuverability and safety of surgical tools, such as catheters and endoscopes, in complex anatomies. The lack of existing localization systems compatible with this modality, beyond fluoroscopy and its harmful ionizing radiation, impedes its translation to clinical practice. To address this challenge, we propose a localization method that achieves full pose estimation by superimposing oscillating magnetic fields for localization onto actuation fields generated by an electromagnetic navigation system. The resulting magnetic field is measured using a three-axis magnetic field sensor embedded in the magnetic device to be localized. The method is evaluated on a three-coil system, and simultaneous actuation and localization is demonstrated with a magnetic catheter prototype with a Hall-effect sensor embedded at its tip. We demonstrate position estimation with mean accuracy and precision below 1 mm, and orientation estimation with mean errors below 2 deg at 10 Hz in a workspace of 80 x 80 x 60 mm."
Scene Action Maps: Behavioural Maps for Navigation without Metric Information,"Joel Loo, David Hsu",National University of Singapore,Mapping II,"Humans are remarkable in their ability to navigate without metric information. We can read abstract 2D maps, such as floor-plans or hand-drawn sketches, and use them to navigate in unseen rich 3D environments, without requiring prior traversals to map out these scenes in detail. We posit that this is enabled by the ability to represent the environment abstractly as interconnected navigational behaviours, e.g., â€œfollow the corridorâ€ or â€œturn rightâ€, while avoiding detailed, accurate spatial information at the metric level. We introduce the Scene Action Map (SAM), a behavioural topological graph, and propose a learnable map-reading method, which parses a variety of 2D maps into SAMs. Map-reading extracts salient information about navigational behaviours from the overlooked wealth of pre-existing, abstract and inaccurate maps, ranging from floor-plans to sketches. We evaluate the performance of SAMs for navigation, by building and deploying a behavioural navigation stack on a quadrupedal robot. Videos and more information is available at: https://scene-action-maps.github.io"
Continuous Occupancy Mapping in Dynamic Environments Using Particles,"Gang Chen, Wei Dong, Peng Peng, Javier Alonso-Mora, Xiangyang Zhu","Delft University of Technology,Shanghai Jiao Tong University",Mapping II,"Particle-based dynamic occupancy maps were proposed in recent years to model the obstacles in dynamic environments. Current particle-based maps describe the occupancy status in discrete grid form and suffer from the grid size problem, wherein a large grid size is unfavorable for motion planning while a small grid size lowers efficiency and causes gaps and inconsistencies. To tackle this problem, this paper generalizes the particle-based map into continuous space and builds an efficient 3D egocentric local map. A dual-structure subspace division paradigm, composed of a voxel subspace division and a novel pyramid-like subspace division, is proposed to propagate particles and update the map efficiently with the consideration of occlusions. The occupancy status of an arbitrary point in the map space can then be estimated with the particles' weights. To reduce the noise in modeling static and dynamic obstacles simultaneously, an initial velocity estimation approach and a mixture model are utilized. Compared to the grid-form particle-based map, our map enables continuous occupancy estimation and substantially improves the mapping performance at different resolutions."
Building Volumetric Beliefs for Dynamic Environments Exploiting Map-Based Moving Object Segmentation,"Benedikt Mersch, Tiziano Guadagnino, Xieyuanli Chen, Ignacio Vizzo, Jens Behley, Cyrill Stachniss","University of Bonn,National University of Defense Technology,Dexory",Mapping II,"Mobile robots that navigate in unknown environments need to be constantly aware of the dynamic objects in their surroundings for mapping, localization, and planning. It is key to reason about moving objects in the current observation and at the same time to also update the internal model of the static world to ensure safety. In this paper, we address the problem of jointly estimating moving objects in the current 3D LiDAR scan and a local map of the environment. We use sparse 4D convolutions to extract spatio-temporal features from scan and local map and segment all 3D points into moving and non-moving ones. Additionally, we propose to fuse these predictions in a probabilistic representation of the dynamic environment using a Bayes filter. This volumetric belief models, which parts of the environment can be occupied by moving objects. Our experiments show that our approach outperforms existing moving object segmentation baselines and even generalizes to different types of LiDAR sensors. We demonstrate that our volumetric belief fusion can increase the precision and recall of moving object segmentation and even retrieve previously missed moving objects in an online mapping scenario."
Fast and Robust Normal Estimation for Sparse LiDAR Scans,"Igor Bogoslavskyi, Konstantinos Zampogiannis, Raymond Phan",Magic Leap,Mapping II,"Light Detection and Ranging (LiDAR) technology has proven to be an important part of many robotics systems. Surface normals estimated from LiDAR data are commonly used for a variety of tasks in such systems. As most of the today's mechanical LiDAR sensors produce sparse data, estimating normals from a single scan in a robust manner poses difficulties. In this paper, we address the problem of estimating normals for sparse LiDAR data avoiding the typical issues of smoothing out the normals in high curvature areas. Mechanical LiDARs rotate a set of rigidly mounted lasers. One firing of such a set of lasers produces an array of points where each point's neighbor is known due to the known firing pattern of the scanner. We use this knowledge to connect these points to their neighbors and label them using the angles of the lines connecting them. When estimating normals at these points, we only consider points with the same label as neighbors. This allows us to avoid estimating normals in high curvature areas. We evaluate our approach on various data, both self-recorded and publicly available, acquired using various sparse LiDAR sensors. We show that using our method for normal estimation leads to normals that are more robust in areas with high curvature which leads to maps of higher quality. We also show that our method only incurs a linear factor runtime overhead with respect to a lightweight baseline normal estimation procedure and is therefore suited for operation in computationally demanding environments."
OmniColor: A Global Camera Pose Optimization Approach of LiDAR-360Camera Fusion for Colorizing Point Clouds,"Bonan Liu, Guoyang Zhao, Jianhao Jiao, Guang Cai, Chengyang Li, Handi Yin, Yuyang Wang, Ming Liu, Pan Hui","HKUST(GZ),University College London,The Hong Kong University of Science and Technology,The Hong Kong University of Science and Technology (Guangzhou),The Hong Kong University of Science and Technology (Guangzhou), ,Hong Kong University of Science and Technology (Guangzhou),Hong Kong University of Science and Technology",Mapping II,"A Colored point cloud, as a simple and efficient 3D representation, has many advantages in various fields, including robotic navigation and scene reconstruction. This representation is now commonly used in 3D reconstruction tasks relying on cameras and LiDARs. However, fusing data from these two types of sensors is poorly performed in many existing frameworks, leading to unsatisfactory mapping results, mainly due to inaccurate camera poses. This paper presents OmniColor, a novel and efficient algorithm to colorize point clouds using an independent 360-degree camera. Given a LiDAR-based point cloud and a sequence of panorama images with initial coarse camera poses, our objective is to jointly optimize the poses of all frames for mapping images onto geometric reconstructions. Our pipeline works in an off-the-shelf manner that does not require any feature extraction or matching process. Instead, we find optimal poses by directly maximizing the photometric consistency of LiDAR maps. In experiments, we show that our method can overcome the severe visual distortion of omnidirectional images and greatly benefit from the wide field of view (FOV) of 360-degree cameras to reconstruct various scenarios with accuracy and stability. The code will be released at https://github.com/liubonan123/OmniColor/."
Gaussian Process Mapping of Uncertain Building Models with GMM As Prior,"Qianqian Zou, Claus Brenner, Monika Sester","Leibniz University Hannover,Leibniz University Hannover, Institute of Cartography and Geoinf",Mapping II,"Mapping with uncertainty representation is required in many research domains, especially for localization. Although there are many investigations regarding the uncertainty of the pose estimation of an ego-robot with map information, the quality of the reference maps is often neglected. To avoid potential problems caused by the errors of maps and a lack of uncertainty quantification, an adequate uncertainty measure for the maps is required. In this paper, uncertain building models with abstract map surfaces using Gaussian Processes (GPs) are proposed to describe the map uncertainty in a probabilistic way. To reduce the redundant computation for simple planar objects, extracted facets from a Gaussian Mixture Model (GMM) are combined with an implicit GP map, also employing local GP-block techniques. The proposed method is evaluated on LiDAR point clouds of city buildings collected by a mobile mapping system. Compared to the performance of other methods such as Octomap, Gaussian Process Occupancy Map (GPOM) and Bayesian Generalized Kernel Inference (BGKOctomap), our method achieves a higher Precision-Recall AUC for the evaluated buildings."
Occupancy Grid Mapping without Ray-Casting for High-Resolution LiDAR Sensors,"Yixi Cai, Fanze Kong, Yunfan Ren, Fangcheng Zhu, Jiarong Lin, Fu Zhang","University of Hong Kong,The University of Hong Kong",Mapping II,"This article presents an efficient occupancy mapping framework for high-resolution LiDAR sensors, termed D-Map. The framework introduces three main novelties to address the computational efficiency challenges of occupancy mapping. Firstly, we use a depth image to determine the occupancy state of regions instead of the traditional ray casting method. Secondly, we introduce an efficient on-tree update strategy on a tree-based map structure. Thirdly, we remove known grids from the map at each update by leveraging the low false alarm rate of LiDAR sensors. To support our design, we provide theoretical analyses of the accuracy of the depth image projection and time complexity of occupancy updates. Furthermore, we conduct extensive benchmark experiments on various LiDAR sensors in both public and private datasets. Our framework demonstrates superior efficiency in comparison with other state-of-the-art methods while maintaining comparable mapping accuracy and high memory efficiency. We demonstrate two real-world applications of D-Map for real-time occupancy mapping using a high-resolution LiDAR. In addition, we open-source the implementation of D-Map on GitHub to"
RH-Map: Online Map Construction Framework of Dynamic Object Removal Based on 3D Region-Wise Hash Map Structure,"Zihong Yan, Xiaoyi Wu, Zhuozhu Jian, Bin Lan, Xueqian Wang","Tsinghua University,Harbin Institute of Technology, Shenzhen,Center for Artificial Intelligence and Robotics, Graduate School",Mapping II,"Mobile robots navigating in outdoor environments frequently encounter the issue of undesired traces left by dynamic objects and manifested as obstacles on map, impeding robots from achieving accurate localization and effective navigation. To tackle the problem, a novel map construction framework based on 3D region-wise hash map structure (RH-Map) is proposed, consisting of front-end scan refresh and back-end removal modules, which realizes real-time map construction and online dynamic object removal (DOR). First, a two-layer 3D region-wise hash map structure of map management is employed for effective online DOR. Then, in scan refresh, region-wise ground plane estimation (R-GPE) is proposed for incrementally estimating and preserving ground information, and Scan-to-Map Removal (S2M-R) is proposed to discriminate and remove dynamic objects. Moreover, the lightweight back-end removal module maintaining keyframes is proposed for further DOR. As experimentally verified on SemanticKITTI, our proposed framework yields promising performance on online DOR of map construction compared with state-of-the-art methods. We also validate the proposed framework in real-world environments. The source code is released to the community: https://github.com/YZH-bot/RH-Map."
Photometric LiDAR and RGB-D Bundle Adjustment,"Luca Di Giammarino, Emanuele Giacomini, Leonardo Brizi, Omar Ashraf Ahmed Khairy Salem, Giorgio Grisetti",Sapienza University of Rome,Mapping II,"The joint optimization of the sensor trajectory and 3D map is a crucial characteristic of Simultaneous Localization and Mapping (SLAM) systems. To achieve this, the gold standard is Bundle Adjustment (BA). Modern 3D LiDARs now retain higher resolutions that enable the creation of point cloud images resembling those taken by conventional cameras. Nevertheless, the typical effective global refinement techniques employed for RGB-D sensors are not widely applied to LiDARs. This paper presents a novel BA photometric strategy that accounts for both RGB-D and LiDAR in the same way. Our work can be used on top of any SLAM/GNSS estimate to improve and refine the initial trajectory. We conducted different experiments using these two depth sensors on public benchmarks. Our results show that our system performs on par or better compared to other state-of-the-art ad-hoc SLAM/BA strategies, free from data association and without making assumptions about the environment. In addition, we present the benefit of jointly using RGB-D and LiDAR within our unified method. We finally release an open-source CUDA/C++ implementation."
InterRep: A Visual Interaction Representation for Robotic Grasping,"Yu Cui, Qi Ye, Qingtao Liu, Anjun Chen, Gaofeng Li, Jiming Chen",Zhejiang University,Grasping III,"Recently, pre-trained vision models have gained significant attention in motor control, showcasing impressive performance across diverse robotic learning tasks. While previous works predominantly concentrate on the significance of the pre-training phase, the equally important task of extracting more effective representations based on existing pre-trained visual models remains unexplored. To better leverage the representation capabilities of pre-trained models for robotic grasping, we propose InterRep, a novel interaction representation method that possesses not only the strengths of pre-trained models, known for their robustness in noisy environments and their proficiency in recognizing essential features, but also the capacity of capturing dynamic interaction details and local geometric features during the grasping process. Based on the novel representation, we introduce a deep reinforcement learning method to learn generalizable grasping policies. The experimental results demonstrate that our proposed representation outperforms the baselines in terms of both training speed and generalization. For the generalized grasping tasks with dexterous robotic hands, our method boasts a success rate nearly 20% higher than methods using the global features of the entire image from pre-trained models. In addition, our proposed representation method demonstrates promising performance when applied to a different robotic hand and task. It also exhibits excellent performance on real robots with a success rate of 70%."
MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation,"Patrick Lancaster, Nicklas Hansen, Aravind Rajeswaran, Vikash Kumar","Meta AI,University of California San Diego",Grasping III,"Robotic systems that aspire to operate in uninstrumented real-world environments must perceive the world directly via onboard sensing. Vision-based learning systems aim to eliminate the need for environment instrumentation by building an implicit understanding of the world based on raw pixels, but navigating the contact-rich high-dimensional search space from solely sparse visual reward signals significantly exacerbates the challenge of exploration. The applicability of such systems is thus typically restricted to simulated or heavily engineered environments since agent exploration in the real-world without the guidance of explicit state estimation and dense rewards can lead to unsafe behavior and safety faults that are catastrophic. In this study, we isolate the root causes behind these limitations to develop a system, called MoDem-V2, capable of learning contact-rich manipulation directly in the uninstrumented real world. Building on the latest algorithmic advancements in model-based reinforcement learning (MBRL), demo-bootstrapping, and effective exploration, MoDem-V2 can acquire contact-rich dexterous manipulation skills directly in the real world. We identify key ingredients for leveraging demonstrations in model learning while respecting real-world safety considerations -- exploration centering, agency handover, and actor-critic ensembles. We empirically demonstrate the contribution of these ingredients in four complex visuo-motor manipulation problems in both simulation and the real world. To the best of our knowledge, our work presents the first successful system for demonstration-augmented visual MBRL trained directly in the real world. Visit sites.google.com/view/modemv2 for videos and more details."
"Towards Feasible Dynamic Grasping: Leveraging Gaussian Process Distance Field, SE(3) Equivariance, and Riemannian Mixture Models","Ho Jin Choi, Nadia Figueroa",University of Pennsylvania,Grasping III,"This paper introduces a novel approach to improve robotic grasping in dynamic environments by integrating Gaussian Process Distance Fields (GPDF), SE(3) equivariant networks, and Riemannian Mixture Models. The aim is to enable robots to grasp moving objects effectively. Our approach comprises three main components: object shape reconstruction, grasp sampling, and implicit grasp pose selection. GPDF accurately models the shape of objects, which is essential for precise grasp planning. SE(3) equivariance ensures that the sampled grasp poses are equivariant to the object's pose changes, enhancing robustness in dynamic scenarios. Riemannian Gaussian Mixture Models are employed to assess reachability, providing a feasible and adaptable grasping strategies. Feasible grasp poses are targeted by novel task or joint space reactive controllers formulated using Gaussian Mixture Models and Gaussian Processes. This method resolves the challenge of discrete grasp pose selection, enabling smoother grasping execution. Experimental validation confirms the effectiveness of our approach in generating feasible grasp poses and achieving successful grasps in dynamic environments. By integrating these advanced techniques, we present a promising solution for enhancing robotic grasping capabilities in real-world scenarios."
A Surprisingly Efficient Representation for Multi-Finger Grasping,"Hengxu Yan, Hao-shu Fang, Cewu Lu","Shanghai Jiao Tong University,ShangHai Jiao Tong University",Grasping III,"The problem of grasping objects using a multi-finger hand has received significant attention in recent years. However, it remains challenging to handle a large number of unfamiliar objects in real and cluttered environments. In this work, we propose a representation that can be effectively mapped to the multi-finger grasp space. Based on this representation, we develop a simple decision model that generates accurate grasp quality scores for different multi-finger grasp poses using only hundreds to thousands of training samples. We demonstrate that our representation performs well on a real robot and achieves a success rate of 78.64% after training with only 500 real-world grasp attempts and 87% with 4500 grasp attempts. Additionally, we achieve a success rate of 84.51% in a dynamic human-robot handover scenario using a multi-finger hand."
GrainGrasp: Dexterous Grasp Generation with Fine-Grained Contact Guidance,"Fuqiang Zhao, Dzmitry Tsetserukou, Qian Liu","Dalian University of Technology,Skolkovo Institute of Science and Technology",Grasping III,"One goal of dexterous robotic grasping is to allow robots to handle objects with the same level of flexibility and adaptability as humans. However, it remains a challenging task to generate an optimal grasping strategy for dexterous hands, especially when it comes to delicate manipulation and accurate adjustment the desired grasping poses for objects of varying shapes and sizes. In this paper, we propose a novel dexterous grasp generation scheme called textbf{GrainGrasp} that provides fine-grained contact guidance for each fingertip. In particular, we employ a generative model to predict separate contact maps for each fingertip on the object point cloud, effectively capturing the specifics of finger-object interactions. In addition, we develop a new dexterous grasping optimization algorithm that solely relies on the point cloud as input, eliminating the necessity for complete mesh information of the object. By leveraging the contact maps of different fingertips, the proposed optimization algorithm can generate precise and determinable strategies for human-like object grasping. Experimental results confirm the efficiency of the proposed scheme."
Regrasping on Printed Circuit Boards with the Smart Suction Cup,"Jungpyo Lee, Zheng Sun, Zhipeng Dong, Fei Chen, Hannah Stuart","University of California, Berkeley,The Chinese University of Hong Kong,Northeastern University,UC Berkeley",Grasping III,"The disposal of waste electrical and electronic equipment (WEEE) presents a sustainability challenge, particularly for waste printed circuit boards (PCBs). PCBs are challenging to sort out from other waste materials in part because traditional industrial end-effectors struggle to reliably grip these irregularly shaped objects with unmodeled surface-mounted components. Vision-based separators, while effective for object categorization, face challenges with identifying precise grasp points on PCB surfaces. This paper studies regrasping control to enhance suction cup grasping performance on PCBs, addressing issues arising from uneven surfaces and intricate features that interfere with suction sealing. We categorize PCBs into two recycling levels â€“ with large surface features intact or removed â€“ and conduct experiments on both stationary and conveyor belt setups with realistic vision-based grasp planners. Results show that jumping regrasping improves pick-and-place success rate. Haptically driven jumping â€“ using the Smart Suction Cup â€“ is especially useful for unprocessed waste PCBs with large surface mount parts. The proposed method offers a promising solution to enhance the efficiency and reliability of robotic grasping in recycling applications."
Anthropomorphic Grasping with Neural Object Shape Completion,"Diego Xavier Hidalgo Carvajal, Hanzhi Chen, Gemma Carolina Bettelani, Jaesug Jung, Melissa Zavaglia, Laura Busse, Abdeldjallil Naceri, Stefan Leutenegger, Sami Haddadin","Technical University of Munich,Technical University of Munich (TUM),Technische Universität München,Division of Neuroscience, Faculty of Biology, LMU Munich",Grasping III,"The progressive prevalence of robots in human suited environments has given rise to a myriad of object manipulation techniques, where dexterity plays a paramount role. It is well-established that humans exhibit extraordinary dexterity when handling objects. Such dexterity seems to derive from a robust understanding of object properties (such as weight, size, and shape), as well as a remarkable capacity to interact with them. Hand postures commonly demonstrate the influence of specific regions on objects that need to be grasped, especially when objects are partially visible. In this work, we leverage human-like object understanding by reconstructing and completing their full geometry from partial observations, and manipulating them using a 7-DoF anthropomorphic robot hand. Our approach has significantly improved the grasping success rates of baselines with only partial reconstruction by nearly 30% and achieved over 150 successful grasps with three different object categories. This demonstrates our approachâ€™s consistent ability to predict and execute grasping postures based on the completed object shapes from various directions and positions in real-world scenarios. Our work opens up new possibilities for enhancing robotic applications that require precise grasping and manipulation skills of real-world reconstructed objects."
Statistical Stratification and Benchmarking of Robotic Grasping Performance,"Brice Denoun, Miles Hansard, Beatriz Leon, Lorenzo Jamone","The Shadow Robot Company,Queen Mary University of London,Shadow Robot Company,Queen Mary University London",Grasping III,"Robotic grasping is fundamental to many real-world applications, and new approaches must be systematically evaluated. However, in most cases, the performance of a specific approach is assessed by simply counting the number of successful attempts in a given task, and this success rate is then compared to those of other solutions, without taking into account the random variability across different experiments (e.g. due to sensor noise, or variations in object placement). In order to address this issue, we classify the observed performance into qualitatively ordered outcomes, thereby stratifying the results. We then show how to analyse these results, in a statistical framework which accounts for the variability between experiments. The advantages of our approach are demonstrated in the practical comparison of four grasp planning algorithms. In particular, we show that the proposed approach allows us to carry out several distinct evaluations from a single set of experiments, without having to repeat the data collection process. We demonstrate that differences between the algorithms, which would not be apparent from overall success rates, can be identified and evaluated."
The Hydra Hand: A Mode-Switching Underactuated Gripper with Precision and Power Grasping Modes,"Digby Chappell, Fernando Bello, Petar Kormushev, Nicolas Rojas",Imperial College London,Grasping III,
Quasi-Static Soft Fixture Analysis of Rigid and Deformable Objects,"Yifei Dong, Florian T. Pokorny","KTH,KTH Royal Institute of Technology",In-Hand Manipulation,"We present a sampling-based approach to reasoning about the caging-based manipulation of rigid and a simplified class of deformable 3D objects subject to energy constraints. Towards this end, we propose the notion of soft fixtures extending earlier work on energy-bounded caging to include a broader set of energy function constraints, such as gravitational and elastic potential energy of 3D deformable objects. Previous methods focused on establishing provably correct algorithms to compute lower bounds or analytically exact estimates of escape energy for a very restricted class of known objects with low-dimensional configuration spaces, such as planar polygons. We instead propose a practical sampling-based approach that is applicable in higher-dimensional configuration spaces, but only produces a sequence of upper-bound estimates that, however, appear to converge rapidly to actual escape energy. We present 8 simulation experiments demonstrating the applicability of our approach to various complex quasi-static manipulation scenarios. Quantitative results indicate the effectiveness of our approach in providing upper-bound estimates for escape energy in quasi-static manipulation scenarios. Two real-world experiments also show that the computed normalized escape energy estimates appear to correlate strongly with the probability of escape of an object under randomized pose perturbation."
In-Hand Rolling Manipulation Based on Ball-On-Cloth System,"Hinano Ichikura, Mitsuru Higashimori",Osaka University,In-Hand Manipulation,"This paper presents a novel in-hand rolling manipulation method in which a ball on a cloth attached to fingertips is controlled using flexible and adaptive deformation of the cloth. First, an analytical model of the ball-on-cloth system is introduced. The shape of the cloth is simplified, and the rolling constraint of the ball on the cloth is defined focusing on the lowest point of the ball. Next, the relationship between the input to the cloth anchor point and the position of the lowest point of the ball is expressed by a linear approximation. Then, the input to generate the desired rolling orbit is designed. Next, as an example of utilizing the rolling orbits, a manipulation method to rotate the ball around a vertical axis is developed. Finally, a multi-fingered hand with a piece of cloth attached to the fingertips is developed, and the effectiveness of the proposed system is experimentally verified."
A Linkage-Driven Underactuated Robotic Hand for Adaptive Grasping and In-Hand Manipulation,"Guotao Li, Xu Liang, Guotao Li, Tingting Su, Zhijie Liu, Zeng-Guang Hou","Institute of Automation Chinese Academy of Sciences,Institute of Automation, Chinese Academy of Sciences,North China University of Technology,Beihang University,Chinese Academy of Science",In-Hand Manipulation,"The development of robotic hand that can imitate human movements has always been an important research topic. In this paper, a linkage-driven underactuated three-finger hand is proposed to imitate the flexion/extension (f/e) and abduction/adduction (a/a) motions of human hand. The robotic hand has three identical underactuated fingers, each of which contains an underactuated planar linkage, a spherical four-bar mechanism, and a set of bevel gears. The spherical four-bar mechanism is designed to provide 2-degree-of-freedom actuation, driving the f/e and a/a motions of the proximal joint simultaneously. Based on screw theory, the kinematic model of the spherical mechanism is established, and the maximum available workspace index (MAW) of the spherical mechanism is proposed to evaluate the workspace with the same adduction and abduction angle ranges. The effects of the parameters of the spherical mechanism on the MAW and the transmission efficiency are obtained, and the parameters of the spherical mechanism are optimized. The optimization results show that the MAW of the spherical mechanism can be increased by up to 3.5 times. Finally, experiments are carried out to show the proposed robotic hand can perform simultaneous adaptive grasping and in-hand manipulation."
Curriculum-Based Sensing Reduction in Simulation to Real-World Transfer for In-Hand Manipulation,"Lingfeng Tao, Jiucai Zhang, Qiaojie Zheng, Xiaoli Zhang","Oklahoma State University,Guangzhou Automotive Group R&D Center, Silicon Valley,Colorado School of Mines",In-Hand Manipulation,"Simulation to Real-World Transfer allows affordable and fast training of learning-based robots for manipulation tasks using Deep Reinforcement Learning methods. Currently, Asymmetric Actor-Critic approaches are used for Sim2Real to reduce the rich idealized features in simulation to the accessible ones in the real world. However, the feature reduction from the simulation to the real world is conducted through an empirically defined one-step curtail. Small feature reduction does not sufficiently remove the actorâ€™s features, which may still cause difficulty setting up the physical system, while large feature reduction may cause difficulty and inefficiency in policy training. To address this issue, we proposed Curriculum-based Sensing Reduction to enable the actor to start with the same rich feature space as the critic and then get rid of the hard-to-extract features step-by-step for higher training performance and better adaptation for real-world feature space. The reduced features are replaced with random signals from a Deep Random Generator to remove the dependency between the output and the removed features and avoid creating new dependencies. The methods are evaluated on the Allegro robot hand in a real-world in-hand manipulation experiment. The results show that our methods have faster training and higher task performance than baselines and can solve real-world tasks when selected tactile features are reduced."
Geometric Fabrics: A Safe Guiding Medium for Policy Learning,"Karl Van Wyk, Ankur Handa, Viktor Makoviichuk, Yijie Guo, Arthur Allshire, Nathan Ratliff","NVIDIA,NVidia,University of Michigan, Ann Arbor,University of Toronto",In-Hand Manipulation,"Robotics policies are always subjected to complex, second order dynamics that entangle their actions with resulting states. In reinforcement learning (RL) contexts, policies have the burden of deciphering these complicated interactions over massive amounts of experience and complex reward functions to learn how to accomplish tasks. Moreover, policies typically issue actions directly to controllers like Operational Space Control (OSC) or joint PD control, which induces straightline motion towards these action targets in task or joint space. However, straightline motion in these spaces for the most part do not capture the rich, nonlinear behavior our robots need to exhibit, shifting the burden of discovering these behaviors more completely to the agent. Unlike these simpler controllers, geometric fabrics capture a much richer and desirable set of behaviors via artificial, second order dynamics grounded in nonlinear geometry. These artificial dynamics shift the uncontrolled dynamics of a robot via an appropriate control law to form textit{behavioral dynamics}. Behavioral dynamics unlock a new action space and safe, guiding behavior over which RL policies are trained. Behavioral dynamics enable bang-bang-like RL policy actions that are still safe for real robots, simplify reward engineering, and help sequence real-world, high-performance policies. We describe the framework more generally and create a specific instantiation for the problem of dexterous, in-hand reorientation of a cube by a highly actuated robot hand."
Robust In-Hand Manipulation with Extrinsic Contacts,"Boyuan Liang, Kei Ota, Masayoshi Tomizuka, Devesh Jha","University of California, Berkeley,Tokyo Institute of Technology,University of California,Mitsubishi Electric Research Laboratories",In-Hand Manipulation,"We present in-hand manipulation tasks where a robot moves an object in grasp, maintains its external contact mode with the environment, and adjusts its in-hand pose simultaneously. The proposed manipulation task leads to complex contact interactions which can be very susceptible to uncertainties in kinematic and physical parameters. Therefore, we propose a robust in-hand manipulation method, which consists of two parts. First, an in-gripper mechanics model that computes a naive motion cone assuming all parameters are precise. Then, a robust planning method refines the motion cone to maintain desired contact mode regardless of parametric errors. Real world experiments were conducted to illustrate the accuracy of the mechanics model and the effectiveness of the robust planning framework in the presence of kinematics parameter errors."
Dexterous In-Hand Manipulation by Guiding Exploration with Simple Sub-Skill Controllers,"Gagan Khandate, Cameron Mehlman, Xingsheng Wei, Matei Ciocarlie",Columbia University,In-Hand Manipulation,"Recently, reinforcement learning has led to dexterous manipulation skills of increasing complexity. Nonetheless, learning these skills in simulation still exhibits poor sample-efficiency which stems from the fact these skills are learned from scratch without the benefit of any domain expertise. In this work, we aim to improve the sample efficiency of learning dexterous in-hand manipulation skills using controllers available via domain knowledge. To this end, we design simple sub-skill controllers and demonstrate improved sample efficiency using a framework that guides exploration toward relevant state space by following actions from these controllers. We are the first to demonstrate learning hard-to-explore finger-gaiting in-hand manipulation skills without the use of an exploratory reset distribution."
Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing,"Ying Yuan, Haichuan Che, Yuzhe Qin, Binghao Huang, Zhao-heng Yin, Kang-won Lee, Yi Wu, Soo-chul Lim, Xiaolong Wang","Tsinghua University,University of California San Diego,UC San Diego,University of California, San Diego,University of California, Berkeley,Dongguk University",In-Hand Manipulation,"Executing contact-rich manipulation tasks necessitates the fusion of tactile and visual feedback. However, the distinct nature of these modalities poses significant challenges. In this paper, we introduce a system that leverages visual and tactile sensory inputs to enable dexterous in-hand manipulation. Specifically, we propose Robot Synesthesia, a novel point cloud-based tactile representation inspired by human tactile-visual synesthesia. This approach allows for the simultaneous and seamless integration of both sensory inputs, offering richer spatial information and facilitating better reasoning about robot actions. The method, trained in a simulated environment and then deployed to a real robot, is applicable to various in-hand object rotation tasks. Comprehensive ablations are performed on how the integration of vision and touch can improve reinforcement learning and Sim2Real performance."
Adaptive Fingers Coordination for Robust Grasp and In-Hand Manipulation under Disturbances and Unknown Dynamics,"Farshad Khadivar, Aude G. Billard",EPFL,In-Hand Manipulation,"We present a control framework for achieving a robust object grasp and manipulation in hand. In-hand manipulation remains a demanding task as the object is never stable and task success relies on carefully synchronizing the fingers' dynamics. Indeed, fingers must simultaneously generate motion while maintaining contact with the object and, by staying within the hand's frame, ensuring that the object remains manipulable. These challenges are exacerbated once the hand gets disturbed or when the internal dynamics of the manipulated object are unknown, such as when it is filled with liquid moving during manipulation. We present a control strategy based on coupled dynamical systems, whereby the fingers move in synchronization using an intermediate dynamic responsible for coordinating fingers. To adapt to changes in forces due to model uncertainties and unexpected disturbances, we employ an adaptive torque-controller combined with a joint impedance regulator that guarantees high tracking accuracy while adapting to dynamic changes. We validate the approach in multiple experiments on a 16 degrees-of-freedom robotic hand grasping and manipulating objects with different"
Robust 3D Object Detection from LiDAR-Radar Point Clouds Via Cross-Modal Feature Augmentation,"Jianning Deng, King Wah Gabriel Chan, Hantao Zhong, Chris Xiaoxuan Lu","University of Edinburgh,Connecticut College,University of Cambridge",Object Detection II,"This paper presents a novel framework for robust 3D object detection from point clouds via cross-modal hallucination. Our proposed approach is agnostic to either hallucination direction between LiDAR and 4D radar. We introduce multiple alignments on both spatial and feature levels to achieve simultaneous backbone refinement and hallucination generation. Specifically, spatial alignment is proposed to deal with the geometry discrepancy for better instance matching between LiDAR and radar. The feature alignment step further bridges the intrinsic attribute gap between the sensing modalities and stabilizes the training. The trained object detection models can deal with difficult detection cases better, even though only single-modal data is used as the input during the inference stage. Extensive experiments on the View-of-Delft (VoD) dataset show that our proposed method outperforms the state-of-the-art (SOTA) methods for both radar and LiDAR object detection while maintaining competitive efficiency in runtime."
Predicting Class Distribution Shift for Reliable Domain Adaptive Object Detection,"Nicolas Chapman, Feras Dayoub, Will Browne, Christopher Lehnert","Queensland University of Technology,The University of Adelaide",Object Detection II,"Unsupervised Domain Adaptive Object Detection (UDA-OD) uses unlabelled data to improve the reliability of robotic vision systems in open-world environments. Previous approaches to UDA-OD based on self-training have been effective in overcoming changes in the general appearance of images. However, shifts in a robot's deployment environment can also impact the likelihood that different objects will occur, termed class distribution shift. Motivated by this, we propose a framework for explicitly addressing class distribution shift to improve pseudo-label reliability in self-training. Our approach uses the domain invariance and contextual understanding of a pre-trained joint vision and language model to predict the class distribution of unlabelled data. By aligning the class distribution of pseudo-labels with this prediction, we provide weak supervision of pseudo-label accuracy. To further account for low quality pseudo-labels early in self-training, we propose an approach to dynamically adjust the number of pseudo-labels per image based on model confidence. Our method outperforms state-of-the-art approaches on several benchmarks, including a 4.7 mAP improvement when facing challenging class distribution shift."
LSSAttn: Towards Dense and Accurate View Transformation for Multi-Modal 3D Object Detection,"Qi Jiang, Hao Sun","Shanghai Jiaotong University,National University of Singapore",Object Detection II,"Fusing the camera and LiDAR information in the unified BEV representation serves as the elegant paradigm for the 3D detection tasks. Current multi-modal fusion methods in BEV can be categorized into LSS-based and Transformer-based in terms of their view transformation. The former leverages inaccurate depth prediction and massive pseudo points for perspective-to-BEV transformation while the latter only fetches sparse image features to the BEV representation. To overcome their shortcomings, an optimized view transformation is proposed, which can be easily modulated into the LSS-based methods. The proposed module capitalizes on the LSS mechanism to establish dense associations between perspective pixels and BEV grids. It utilizes the attention mechanism to compute similarity scores for each associated pair during feature aggregation. Starting from the BEVFusion baseline, we further introduce (1) cross-attention within the associated subsets to transfer image features into the BEV, and (2)a multi-scale feature fusion mechanism for LSS-based view transformation. Extensive experiments on nuScenes validate the effectiveness and efficiency of our proposed module, which achieves an increase of 1.3% in mAP compared to the baseline model."
Learning Temporal Cues by Predicting Objects Move for Multi-Camera 3D Object Detection,"Seokha Moon, Hongbeen Park, Jaekoo Lee, Jinkyu Kim","Korea University,Kookmin University",Object Detection II,"In autonomous driving and robotics, there is a growing interest in utilizing short-term historical data to enhance multi-camera 3D object detection, leveraging the continuous and correlated nature of input video streams. Recent work has focused on spatially aligning BEV-based features over timesteps. However, this is often limited as its gain does not scale well with long-term past observations. To address this, we advocate for supervising a model to predict objects' poses given past observations, thus explicitly guiding to learn objects' temporal cues. To this end, we propose a model called DAP (Detection After Prediction), consisting of a two-branch network: (i) a branch responsible for forecasting the current objects' poses given past observations and (ii) another branch that detects objects based on the current and past observations. The features predicting the current objects from branch (i) is fused into branch (ii) to transfer predictive knowledge. We conduct extensive experiments with the large-scale nuScenes datasets, and we observe that utilizing such predictive information significantly improves the overall detection performance. Our model can be used plug-and-play, showing consistent performance gain."
Improved Yolov5: HIC-Yolov5 for Small Object Detection,"Shiyi Tang, Shu Zhang, Yini Fang","Ocean University of China,Hong Kong University of Science and Technology",Object Detection II,"Small object detection has been a challenging problem in the field of object detection. There has been previous work to make improvements specifically for small objects, such as adding several attention blocks or changing the whole structure of feature fusion networks. However, there is still space of improvement and the computation cost is large, which is bad for real-time object detection. An improved Yolov5 model: HIC-Yolov5 is proposed to address the aforementioned problems. Firstly, by adding an additional prediction head for small objects, the higher-resolution feature maps could be directly used to detect small targets. Secondly, an involution block is adopted between the backbone and neck to increase channel information of the feature map. Moreover, an attention mechanism named CBAM is applied at the end of the backbone, thus not only decreasing the computation cost compared with previous works but also emphasizing the important information in both channel and spatial domain. Finally, it is proved that the improved Yolov5 algorithm improved mAP@[.5:.95] by 6.42$%$ and [email protected] by 9.38$%$ on VisDrone-2019-DET dataset."
CLIPUNetr: Assisting Human-Robot Interface for Uncalibrated Visual Servoing Control with CLIP-Driven Referring Expression Segmentation,"Chen Jiang, Yuchen Yang, Martin Jagersand",University of Alberta,Object Detection II,"The classical human-robot interface in uncalibrated image-based visual servoing (UIBVS) relies on either human annotations or semantic segmentation with categorical labels. Both methods fail to match natural human communication and convey rich semantics in manipulation tasks as effectively as natural language expressions. In this paper, we tackle this problem by using referring expression segmentation, which is a prompt-based approach, to provide more in-depth information for robot perception. To generate high-quality segmentation predictions from referring expressions, we propose CLIPUNetr - a new CLIP-driven referring expression segmentation network. CLIPUNetr leverages CLIP's strong vision-language representations to segment regions from referring expressions, while utilizing its ``U-shaped'' encoder-decoder architecture to generate predictions with sharper boundaries and finer structures. Furthermore, we propose a new pipeline to integrate CLIPUNetr into UIBVS and apply it to control robots in real-world environments. In experiments, our method improves boundary and structure measurements by an average of 120% and can successfully assist real-world UIBVS control in an unstructured manipulation environment."
C2FDrone: Coarse-To-Fine Drone-To-Drone Detection Using Vision Transformer Networks,"Sairam Rebbapragada, Pranoy Panda, Vineeth Balasubramanian","IIT Hyderabad,Fujitsu Research of India,Indian Institute of Technology, Hyderabad",Object Detection II,"A vision-based drone-to-drone detection system offers a cost-effective solution for a range of applications, including collision avoidance, countering hostile drones, and enhancing search-and-rescue operations. However, drone-to-drone detection presents a more intricate set of challenges compared to regular object detection. These challenges encompass the need to detect extremely small-sized objects, contend with strong distortion, handle severe occlusion, operate in uncontrolled environments, and execute real-time processing. While current methods attempt to address these issues by integrating multi-scale feature fusion and temporal information, we propose that these techniques may not be sufficiently equipped to handle extreme blur and minuscule objects. Instead, we put forth a novel coarse-to-fine detection strategy based on vision transformers to achieve precise drone detection. We assess the effectiveness of our approach through a series of comprehensive experiments conducted on three challenging drone-to-drone detection datasets. Our results demonstrate notable improvements, with F1 score enhancements of 7%, 3%, and 1% on the FL-Drones, AOT, and NPS-Drones datasets, respectively. Furthermore, we showcase its real-time processing capability by deploying our model on an edge-computing device. We will make our code repository publicly available."
Better Monocular 3D Detectors with LiDAR from the Past,"Yurong You, Cheng Perng Phoo, Carlos Diaz-Ruiz, Katie Luo, Wei-Lun Chao, Mark Campbell, Bharath Hariharan, Kilian Weinberger",Cornell University,Object Detection II,"Accurate 3D object detection is crucial to autonomous driving. Though LiDAR-based detectors have achieved impressive performance, the high cost of LiDAR sensors precludes their widespread adoption in affordable vehicles. Camera-based detectors are cheaper alternatives but often suffer inferior performance compared to their LiDAR-based counterparts due to inherent depth ambiguities in images. In this work, we seek to improve monocular 3D detectors by leveraging unlabeled historical LiDAR data. Specifically, at inference time we assume that the camera-based detectors have access to multiple unlabeled LiDAR scans from past traversals at locations of interest (potentially from other high-end vehicles equipped with LiDAR sensors). Under this setup, we proposed a novel, simple, and end-to-end trainable framework, termed AsyncDepth, to effectively extract relevant features from asynchronous LiDAR traversals of the same location for monocular 3D detectors. We show consistent and significant performance gain (up to 9 AP) across multiple state-of-the-art models and datasets with a negligible additional latency of 9.66 ms and a small storage cost."
A Metacognitive Approach to Out-Of-Distribution Detection for Segmentation,"Meghna Gummadi, Cassandra Kent, Karl Schmeckpeper, Eric Eaton","University of Pennsylvania,University of Pennslyvania",Object Detection II,"Despite outstanding semantic scene segmentation in closed-worlds, deep neural networks segment novel instances poorly, which is required for autonomous agents acting in an open world. To improve out-of-distribution (OOD) detection for segmentation, we introduce a metacognitive approach in the form of a lightweight module that leverages entropy measures, segmentation predictions, and spatial context to characterize the segmentation modelâ€™s uncertainty and detect pixel-wise OOD data in real-time. Additionally, our approach incorporates a novel method of generating synthetic OOD data in context with in-distribution data, which we use to fine-tune existing segmentation models with maximum entropy training. This further improves the metacognitive moduleâ€™s performance without requiring access to OOD data while enabling compatibility with established pre-trained models. Our resulting approach can reliably detect OOD instances in a scene, as shown by state-of-the-art performance on OOD detection for semantic segmentation benchmarks."
When to Replan? an Adaptive Replanning Strategy for Autonomous Navigation Using Deep Reinforcement Learning,"Kohei Honda, Ryo Yonetani, Mai Nishimura, Tadashi Kozuno","Nagoya University,CyberAgent,OMRON SINIC X,Omron Sinic X",AI-Enabled Robotics II,"The hierarchy of global and local planners is one of the most commonly utilized system designs in autonomous robot navigation. While the global planner generates a reference path from the current to goal locations based on the pre-built map, the local planner produces a kinodynamic trajectory to follow the reference path while avoiding perceived obstacles. To account for unforeseen or dynamic obstacles not present on the pre-built map, ``when to replan'' the reference path is critical for the success of safe and efficient navigation. However, determining the ideal timing to execute replanning in such partially unknown environments still remains an open question. In this work, we first conduct an extensive simulation experiment to compare several common replanning strategies and confirm that effective strategies are highly dependent on the environment as well as the global and local planners. Based on this insight, we then derive a new adaptive replanning strategy based on deep reinforcement learning, which can learn from experience to decide appropriate replanning timings in the given environment and planning setups. Our experimental results show that the proposed replanner can perform on par or even better than the current best-performing strategies in multiple situations regarding navigation robustness and efficiency."
Resolving Loop Closure Confusion in Repetitive Environments for Visual SLAM through AI Foundation Models Assistance,"Hongzhou Li, Sijie Yu, Shengkai Zhang, Guang Tan","Sun Yat-sen University,Wuhan University of Technology,SUN YAT-SEN UNIVERSITY",AI-Enabled Robotics II,"In visual SLAM (VSLAM) systems, loop closure plays a crucial role in reducing accumulated errors. However, VSLAM systems relying on low-level visual features often suffer from the problem of perceptual confusion in repetitive environments, where scenes in different locations are incorrectly identified as the same. Existing work has attempted to introduce object-level features or artificial landmarks. The former approach struggles to distinguish visually similar but different objects, while the latter is both time-consuming and labor-intensive. This paper introduces a novel loop closure detection method that leverages pretrained AI foundation models to extract rich semantic information about specific types of objects (e.g., door numbers), referred to as semantic anchors, that help to distinguish similar scenes better. In settings such as office buildings, hotels, and warehouses, this approach helps to improve the robustness of loop closure detection. We validate the effectiveness of our method through experiments conducted in both simulated and real-world environments."
Prepare the Chair for the Bear! Robot Imagination of Sitting Affordance to Reorient Previously Unseen Chairs,"Xin Meng, Hongtao Wu, Sipu Ruan, Gregory Chirikjian","National University of Singapore,Bytedance",AI-Enabled Robotics II,"In this letter, a paradigm for the classification and manipulation of novel objects is established and demonstrated through a real example of chairs. Our approach leverages the robot's understanding of object stability, perceptibility, and affordance to prepare previously unseen and randomly oriented chairs for a teddy bear to sit on. The teddy bear is a proxy for an elderly person, hospital patient, or child. By autonomously reconstructing a complete model of the object and inserting it into a physical simulator (i.e., the robot's ""imagination""), the robot assesses whether the object is a chair and determines how to reorient it properly to be used. Experiment results show that our method achieves a high success rate on the real robot task of chair preparation. Also, it outperforms several baseline methods on the task of upright pose prediction for chairs. The developed system can be easily transferred to a wide variety of application scenarios, and illustrates a broader paradigm in affordance-based reasoning."
Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models,"Pushkal Katara, Zhou Xian, Aikaterini Fragkiadaki",Carnegie Mellon University,AI-Enabled Robotics II,"Generalist robot manipulators need to learn a wide variety of manipulation skills across diverse environments. Current robot training pipelines rely on humans to provide kinesthetic demonstrations or to program simulation environments and to code up reward functions for reinforcement learning. Such human involvement is an important bottleneck in scaling up robot learning across diverse tasks and environments. We propose Generation to Simulation (Gen2Sim), a method for scaling up robot skill learning in simulation by automating the generation of 3D assets, task descriptions, task decompositions, and reward functions using large pre-trained generative models of language and vision. We generate 3D assets for simulation by lifting open-world 2D object-centric images to 3D using image diffusion models and querying LLMs to determine plausible physics parameters. Given URDF files of generated and human-developed assets, we chain-of-thought prompt LLMs to map these to relevant task descriptions, temporal decompositions, and corresponding python reward functions for reinforcement learning. We show Gen2Sim succeeds in learning policies for diverse long-horizon tasks, where reinforcement learning with non-temporally decomposed reward functions fails. Gen2Sim provides a viable path for scaling up reinforcement learning for robot manipulators in simulation, both by diversifying and expanding task and environment development, and by facilitating the discovery of reinforcement-learned behaviors through temporal task decomposition in RL. Our work contributes hundreds of simulated assets, tasks, and demonstrations, taking a step towards fully autonomous robotic manipulation skill acquisition in simulation."
FLTRNN: Faithful Long-Horizon Task Planning for Robotics with Large Language Models,"Jiatao Zhang, Lanling Tang, Yufan Song, Qiwei Meng, Haofu Qian, Jun Shao, Wei Song, Shiqiang Zhu, Jason Gu","Zhejiang University,University of Chinese Academy of Sciences,The Chinese University of Hong Kong,Zhejiang Lab,Dalhousie University",AI-Enabled Robotics II,"Recent planning methods based on Large Language Models typically employ the In-Context Learning paradigm. Complex long-horizon planning tasks require more context(including instructions and demonstrations) to guarantee that the generated plan can be executed correctly. However, in such conditions, LLMs may overlook(unfaithful) the rules in the given context, resulting in the generated plans being invalid or even leading to dangerous actions. In this paper, we investigate the faithfulness of LLMs for complex long-horizon tasks. Inspired by human intelligence, we introduce a novel framework named FLTRNN. FLTRNN employs a language-based RNN structure to integrate task decomposition and memory management into LLM planning inference, which could effectively improve the faithfulness of LLMs and make the planner more reliable. We conducted experiments in VirtualHome household tasks. Results show that our model significantly improves faithfulness and success rates for complex long-horizon tasks."
Drive Anywhere: Generalizable End-To-End Autonomous Driving with Multi-Modal Foundation Models,"Tsun-Hsuan Johnson Wang, Alaa Maalouf, Wei Xiao, Yutong Ban, Alexander Amini, Guy Rosman, Sertac Karaman, Daniela Rus","Massachusetts Institute of Technology,MIT",AI-Enabled Robotics II,"As autonomous driving technology matures, end-to-end methodologies have emerged as a leading strategy, promising seamless integration from perception to control via deep learning. However, existing systems grapple with challenges such as unexpected open set environments and the complexity of black-box models. At the same time, the evolution of deep learning introduces larger, multimodal foundational models, offering multi-modal visual and textual understanding. In this paper, we harness these multimodal foundation models to enhance the robustness and adaptability of autonomous driving systems. We introduce a method to extract nuanced spatial features from transformers and the incorporation of latent space simulation for improved training and policy debugging. We use pixel/patch-aligned feature descriptors to expand foundational model capabilities to create an end-to-end multimodal driving model, demonstrating unparalleled results in diverse tests. Our solution combines language with visual perception and achieves significantly greater robustness on out-of-distribution situations."
AutoTAMP: Autoregressive Task and Motion Planning with LLMs As Translators and Checkers,"Yongchao Chen, Jacob Arkin, Charles Dawson, Yang Zhang, Nicholas Roy, Chuchu Fan","Harvard University,Massachusetts Institute of Technology,MIT,IBM",AI-Enabled Robotics II,"For effective human-robot interaction, robots need to understand, plan, and execute complex, long-horizon tasks described by natural language. Recent advances in large language models (LLMs) have shown promise for translating natural language into robot action sequences for complex tasks. However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal. When complex environmental and temporal constraints are involved, inference over planning tasks must be performed jointly with motion plans using traditional task-and-motion planning (TAMP) algorithms, making factorization into subgoals untenable. Rather than using LLMs to directly plan task sub-goals, we instead perform few-shot translation from natural language task descriptions to an intermediate task representation that can then be consumed by a TAMP algorithm to jointly solve the task and motion plan. To improve translation, we automatically detect and correct both syntactic and semantic errors via autoregressive re-prompting, resulting in significant improvements in task completion. We show that our approach outperforms several methods using LLMs as planners in complex task domains. See our project website for prompts, videos, and code."
ASC: Adaptive Skill Coordination for Robotic Mobile Manipulation,"Naoki Yokoyama, Alexander Clegg, Joanne Truong, Eric Undersander, Tsung-yen Yang, Sergio Arnaud, Sehoon Ha, Dhruv Batra, Akshara Rai","Georgia Institute of Technology,The Georgia Institute of Technology,Facebook AI Research,META,Meta,Georgia Tech / Facebook AI Research",AI-Enabled Robotics II,"We present Adaptive Skill Coordination (ASC) â€“ an approach for accomplishing long-horizon tasks like mobile pick-and-place (i.e., navigating to an object, picking it, navigating to another location, and placing it). ASC consists of three components â€“ (1) a library of basic visuomotor skills (navigation, pick, place), (2) a skill coordination policy that chooses which skill to use when, and (3) a corrective policy that adapts pre-trained skills in out-of-distribution states. All components of ASC rely only on onboard visual and proprioceptive sensing, without requiring detailed maps with obstacle layouts or precise object locations, easing real-world deployment. We train ASC in simulated indoor environments, and deploy it zero-shot (without any real-world experience or fine-tuning) on the Boston Dynamics Spot robot in eight novel real-world environments (one apartment, one lab, two microkitchens, two lounges, one office space, one outdoor courtyard). In rigorous quantitative comparisons in two environments, ASC achieves near-perfect performance (59/60 episodes, or 98%), while sequentially executing skills succeeds in only 44/60 (73%) episodes. Extensive perturbation experiments show that ASC is robust to hand-off errors, changes in the environment layout, dynamic obstacles (e.g., people), and unexpected disturbances. Supplementary videos at adaptiveskillcoordination.github.io."
Forgetting in Robotic Episodic Long-Term Memory,"Joana Plewnia, Fabian Peller-konrad, Tamim Asfour","Karlsruhe Institute of Technology,Karlsruhe Institute of Technology (KIT)",AI-Enabled Robotics II,"Artificial cognitive architectures traditionally rely on complex memory models to encode, store, and retrieve information. However, the conventional practice of transferring all data from working memory (WM) to long-term memory (LTM) leads to high data volumes and challenges in efficient information processing and access. Deciding what information to retain or discard within a robotâ€™s LTM is particularly challenging since knowledge about future data utilization is absent. Drawing inspiration from human forgetting this paper implements and evaluates novel forgetting techniques that allow consolidation in the robotâ€™s LTM only when new information is encountered. The proposed approach combines fast filtering during data transfer to the robotâ€™s LTM with slower yet more precise forgetting mechanisms that are periodically evaluated for offline data deletion inside the LTM. We compare different mechanisms, utilizing metrics such as data similarity, data age, and consolidation frequency. The efficacy of forgetting techniques is evaluated by comparing their performance in a task where two ARMAR robots search through their LTM for past object locations in episodic ego-centric images and robot state data. Experimental results show that our forgetting techniques significantly reduce the space requirements of a robotâ€™s LTM while maintaining its capacity to successfully perform tasks relying on LTM information. Notably, similarity-based forgetting methods outperform frequency- and time-based approaches. The combination of online frequency-based, online similarity-based, offline similarity-based, and time-based decay methods shows superior performance compared to using individual forgetting strategies."
Fixture Calibration with Guaranteed Bounds from a Few Correspondence-Free Surface Points,"Rasmus Haugaard, Yitaek Kim, Thorbjørn Mosekjær Iversen","University of Southern Denmark,The Maersk Mc-Kinney Moller Institute, University of Southern De",Intelligent and Flexible Manufacturing,"Calibration of fixtures in robotic work cells is essential but also time consuming and error-prone, and poor calibration can easily lead to wasted debugging time in downstream tasks. Contact-based calibration methods let the user measure points on the fixture's surface with a tool tip attached to the robot's end effector. Most methods require the user to manually annotate correspondences on the CAD model, however, this is error-prone and a cumbersome user experience. We propose a correspondence-free alternative: The user simply measures a few points from the fixture's surface, and our method provides a tight superset of the poses which could explain the measured points. This naturally detects ambiguities related to symmetry and uninformative points and conveys this uncertainty to the user. Perhaps more importantly, it provides guaranteed bounds on the pose. The computation of such bounds is made tractable by the use of a hierarchical grid on SE(3). Our method is evaluated both in simulation and on a real collaborative robot, showing great potential for easier and less error-prone fixture calibration."
Data-Driven Virtual Sensing for Probabilistic Condition Monitoring of Solenoid Valves,"Victor Vantilborgh, Tom Lefebvre, Guillaume Crevecoeur",Ghent University,Intelligent and Flexible Manufacturing,"There is an emerging industrial demand for predictive maintenance algorithms that exhibit high levels of predictive accuracy. Such condition monitoring tools must estimate dynamic quantities, such as Remaining Useful Lifetime (RUL) and the State of Health (SOH), based on a, typically, restricted set of measurements that can be obtained in an operational setting. These quantities exhibit inherent stochasticity and can only be approximately determined a posteriori to system failure. This paper proposes a generic prognostic tool for probabilistic condition monitoring of mechatronic systems, with the aim to improve the probabilistic prediction of condition metrics, specifically RUL and SOH. Therefore we propose to identify a Hidden Markov Model (HMM) from a fully instrumented measurement set, that is only available for a restricted set of run-to-failure experiments, typically gathered in an R&D setting. Although being artificial and retrospectively constructed metrics, we interpret RUL and SOH as physical measurements with the purpose to identify accurate degradation dynamics. Once the degradation model is identified, we practice the mathematical flexibility of the HMM framework to estimate several of the no longer available dynamic quantities of interest in real-time, from the limited set of measurements that are available in an operational setting. This modelling paradigm is known as virtual sensing."
Digital Robot Judge (DR.J): Building a Task-Centric Performance Database of Real-World Manipulation with Electronic Task Boards,"Peter So, Andriy Sarabakha, Fan Wu, Utku Culha, Fares Abu-Dakka, Sami Haddadin","Technical University of Munich,Nanyang Technological University,Mondragon University",Intelligent and Flexible Manufacturing,"Robotics aims to develop manipulation skills approaching human performance. However, skill complexity is often over- or underestimated based on individual experience, and the real-world performance gap is difficult or expensive to measure through in-person competitions. To bridge this gap, we propose a compact, Internet-connected, electronic task board to measure manipulation performance remotely; we call it the digital robot judge, or â€œDR.J.â€ By detecting key events on the board through performance circuitry, DR.J provides an alternative to transporting equipment to in-person competitions and serves as a portable test and data generation system that captures and grades performances, making comparisons less expensive. Data collected are automatically published on a web dashboard (WD) that provides a living performance benchmark. We share the results of a proof-of-concept electronic task board with industry-inspired tasks used in an international competition in 2021 and 2022 to benchmark localization, insertion, and disassembly tasks. We present data from 10 DR.J task boards, describe a method for deriving the relative task complexity (RTC) from timing data, and compare robot solutions with a human performer. In the best case, robots performed 9x faster than humans in specialized tasks but achieved only 16% of human speed across the full set of tasks. Finally, we present the design and software to replicate the electronic task board to promote task-centric benchmarking."
Semi-Analytical Design of PDE Endpoint Controller for Flexible Manipulator with Non-Homogenous Boundary Conditions,"Sadeq Yaqubi, Seyedmohammad Tahamipourzarandi, Jouni Mattila",Tampere University,Intelligent and Flexible Manufacturing,"This study proposes a new semi-analytical design and implementation method for nonlinear partial differential equation (PDE) control of a flexible manipulator. The proposed scheme considers the effects of the boundary input force and gravity on the payload, which results in non-homogenous boundary conditions. This objective is achieved based on a model transformation scheme for homogenizing boundary conditions, obtaining semi-analytical solutions for the corresponding PDE model. Model transformation is assigned as a hybrid exponentialâ€“ polynomial function whose coefficients are conveniently calculable without the need for any additional boundary condition measurements. This eliminates the need to use intensive numerical solversâ€”for example, methods based on finite element analysisâ€” and allows the implementation of sophisticated PDE control schemes considering fully nonlinear PDE models with high computation speed. The presented controller is robust to parametric model uncertainty due to its adaptive design. The precision and efficiency of calculating distributed states using the proposed model transformation are demonstrated based on experimental data for the flexible manipulator with respect to the ground truth camera-based motion capture system. Model transformation is also numerically implemented for the proposed nonlinear endpoint control method based on the original PDE model."
Automated Sewing System Enabled by Machine Vision for Smart Garment Manufacturing,"Subyeong Ku, HyunWoong Choi, Ho-young Kim, Yong-Lae Park",Seoul National University,Intelligent and Flexible Manufacturing,"This paper presents an automated sewing system de- signed for smart garment manufacturing, incorporating machine vision capabilities into a custom-built sewing machine. The vision system captures an image of the fabric pattern placed between two acrylic plates with a small opening, utilizing a deep learning model to detect and segment the opening, which represents the area of interest on the plate. Subsequently, a specialized algorithm detects a narrow seam line within the segmented image and generates a stitching path alongside the seam line, ensuring a consistent distance. The sewing machine then accurately stitches along the generated path automatically. The vision system utilized in this study achieves a spatial resolution of 68 um per pixel. The custom-built sewing machine, controlled by an external computer, exhibits a spatial resolution of 10 um, a translation speed of 60 mm/s, and an adjustable stitching interval ranging from 1 mm to 5 mm. The subsystems and components are interconnected using the Robot Operating System (ROS), enabling seamless communication and integration. The proposed system eliminates the need for human intervention, facilitating automated garment production. This innovative system is expected to play a critical role in realizing the vision of smart garment manufacturing."
Segmentation and Coverage Planning of Freeform Geometries for Robotic Surface Finishing,"Stefan Schneyer, Arne Sachtler, Thomas Eiband, Korbinian Nottensteiner","German Aerospace Center (DLR),Technical University of Munich (TUM)",Intelligent and Flexible Manufacturing,"Surface finishing such as grinding or polishing is a time-consuming task, involves health risks for humans and is still largely performed by hand. Due to the high curvatures of complex geometries, different areas of the surface cannot be optimally reached by a simple strategy using a tool with a relatively large and flat finishing disk. In this paper, a planning method is presented that uses a variable contact point on the finishing disk as an additional degree of freedom. Different strategies for covering the workpiece surface are used to optimize the surface finishing process and ensure the coverage of concave areas. Therefore, an automatic segmentation method is developed to find areas with a uniform machining strategy based on the exact tool and workpiece geometry. Further, a method for planning coverage paths is presented, in which the contact area is modeled to realize an adaptive spacing between path lines. The approach was evaluated in simulation and practical experiments on the DLR SARA robot. The results show high coverage for complex freeform geometry and that adaptive spacing can optimize the overall process by reducing uncovered gaps and overlaps between coverage lines."
Integrating Robot Assignment and Maintenance Management: A Multi-Agent Reinforcement Learning Approach for Holistic Control,"Kshitij Bhatta, Qing Chang",University of Virginia,Intelligent and Flexible Manufacturing,"Modern manufacturing requires effective integration of production control and maintenance scheduling to improve productivity and quality. However, there have been few studies on this integrated control due to a lack of a comprehensive manufacturing system model. In response to this challenge, this paper presents a mathematical model framework for a mobile multi-skilled robot-operated manufacturing system that integrates three essential control aspects: robot assignment, maintenance scheduling, and product quality. To demonstrate the effectiveness of this approach, a control problem is solved in the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) framework. Results show that the proposed integrated model outperforms models that consider only system-level parameters, as well as those that only address maintenance scheduling and quality-related parameters."
Towards Fault-Tolerant Deployment of Mobile Robot Navigation in the Edge: An Experimental Study,"Florian Mirus, Frederik Pasch, Kay-ulrich Scholl","Intel Labs,Intel",Intelligent and Flexible Manufacturing,"Modern algorithms allow robots to reach a greater level of autonomy and fulfill more challenging tasks. However, on-board limitations regarding computational and battery resources are hindering factors regarding the deployment of such algorithms particularly on mobile robots. Although offloading a majority of the algorithmic components to the edge or even cloud offers an attractive option to leverage massive computing power in robotics applications, safety and reliability remain critical issues. This paper presents a minimalistic safety fallback mechanism when offloading mobile robot navigation to the edge, that ensures safe and collision-free navigation even in the presence of failures in the connection between the on-board and edge-devices. We show the effectiveness of our approach through extensive testing in three different relevant scenarios in a simulated warehouse environment. Our experiments demonstrate the effects of different fallback strategies and show how our proposed approach is able to ensure safety while allowing the robot to continue its mission during an interrupted connection and thus avoiding unnecessary downtime."
Prompting Multi-Modal Tokens to Enhance End-To-End AutonomousDriving Imitation Learning with LLMs,"Yiqun Duan, Qiang Zhang, Renjing Xu","University of Technolgoy Sydney,The Hong Kong University of Science and Technology (Guangzhou)",Intelligent Transportation Systems III,"The utilization of Large Language Models (LLMs) within the realm of reinforcement learning, particularly as planners, has garnered a significant degree of attention in recent scholarly literature. However, a substantial proportion of existing research predominantly focuses on planning models for robotics that transmute the outputs derived from perception models into linguistic forms, thus adopting a `pure-language' strategy. In this research, we propose a hybrid End-to-End learning framework for autonomous driving by combining basic driving imitation learning with LLMs based on multi-modality prompt tokens. Instead of simply converting perception results from the separated train model into pure language input, our novelty lies in two aspects. 1) The end-to-end integration of visual and LiDAR sensory input into learnable multi-modality tokens, thereby intrinsically alleviating description bias by separated pre-trained perception models. 2) Instead of directly letting LLMs drive, this paper explores a hybrid setting of letting LLMs help the driving model correct mistakes and complicated scenarios. The results of our experiments suggest that the proposed methodology can attain driving scores of 49.21%, coupled with an impressive route completion rate of 91.34% in the offline evaluation conducted via CARLA. These performance metrics are comparable to the most advanced driving models."
Efficient and Differentiable Joint Conditional Prediction and Cost Evaluation for Tree-Structured Planning in Autonomous Driving,"Zhiyu Huang, Peter Karkus, Boris Ivanovic, Yuxiao Chen, Marco Pavone, Chen Lv","Nanyang Technological University,NVIDIA,Nvidia research,Stanford University",Intelligent Transportation Systems III,"Motion prediction and cost evaluation are vital components in the decision-making system of autonomous ve-hicles. However, existing methods often ignore the importance of cost learning and treat them as separate modules. In this study, we employ a tree-structured policy planner and propose a dif-ferentiable joint training framework for both ego-conditioned prediction and cost models, resulting in a direct improvement of the final planning performance. For conditional prediction, we introduce a query-centric Transformer model that performs efficient ego-conditioned motion prediction. For planning cost, we propose a learnable context-aware cost function with latent interaction features, facilitating differentiable joint learning. We validate our proposed approach using the real-world nuPlan dataset and its associated planning test platform. Our frame-work not only matches state-of-the-art planning methods but outperforms other learning-based methods in planning quality, while operating more efficiently in terms of runtime. We show that joint training delivers significantly better performance than separate training of the two modules. Additionally, we find that tree-structured policy planning outperforms the conventional single-stage planning approach."
SIMMF: Semantics-Aware Interactive Multiagent Motion Forecasting for Autonomous Vehicle Driving,"Vidyaa Krishnan Nivash, Ahmed H. Qureshi",Purdue University,Intelligent Transportation Systems III,"Autonomous vehicles require motion forecasting of their surrounding multi-agents (pedestrians and vehicles) to make optimal decisions for navigation. The existing methods focus on techniques to utilize the positions and velocities of these agents and fail to capture semantic information from the scene. Moreover, to mitigate the increase in computational complexity associated with the number of agents in the scene, some works leverage Euclidean distance to prune far-away agents. However, distance-based metric alone is insufficient to select relevant agents and accurately perform their predictions. To resolve these issues, we propose the Semantics-aware Interactive Multiagent Motion Forecasting (SIMMF) method to capture semantics along with spatial information and optimally select relevant agents for motion prediction. Specifically, we achieve this by implementing a semantic-aware selection of relevant agents from the scene and passing them through an attention mechanism to extract global encodings. These encodings along with agentsâ€™ local information, are passed through an encoder to obtain time-dependent latent variables for a motion policy predicting the future trajectories. Our results show that the proposed approach outperforms state-of-the-art baselines and provides more accurate and scene-consistent predictions. The demonstration video is available at: https://youtu.be/Wjla071BPBA"
CausalAgents: A Robustness Benchmark for Motion Forecasting,"Liting Sun, Rebecca Roelofs, Ben Caine, Khaled Refaat, Benjamin Sapp, Scott Ettinger, Wei Chai","University of California, Berkeley,Google Research, Brain Team,Google Research,Waymo",Intelligent Transportation Systems III,"As machine learning models become increasingly prevalent in motion forecasting for autonomous vehicles (AVs), it is critical to ensure that model predictions are safe and reliable. In this paper, we examine the robustness of motion forecasting to non-causal perturbations. We construct a new benchmark for evaluating and improving model robustness by applying perturbations to existing data. Specifically, we conduct an extensive labeling effort to identify causal agents, or agents whose presence influences human drivers' behavior, in the Waymo Open Motion Dataset (WOMD), and we use these labels to perturb the data by deleting non-causal agents from the scene. We evaluate a diverse set of state-of-the-art deep-learning models on our proposed benchmark and find that all evaluated models exhibit large shifts under non-causal perturbation: we observe a surprising $25$-$38%$ relative change in minADE as compared to the original. In addition, we investigate techniques to improve model robustness, including increasing the training dataset size and using targeted data augmentations that randomly drop non-causal agents throughout training. Finally, we release the causal agent labels as an extension to WOMD and the robustness benchmarks to aid the community in building more reliable and safe deep-learning models for motion forecasting."
Highway-Driving with Safe Velocity Bounds on Occluded Traffic,"Truls Nyberg, Jonne Van Haastregt, Jana Tumova",KTH Royal Institute of Technology,Intelligent Transportation Systems III,"Limited visibility and sensor occlusions pose pressing safety challenges for advanced driver-assistance systems (ADAS) and autonomous vehicles (AVs). In this work, our pursuit was to strike a balance: a method that ensures safety in occluded scenarios while preventing overly cautious behavior. We argue that such approaches are crucial for AVs' future, particularly when navigating alongside human drivers on highways at high speeds. To this end, we used reachability analysis to find safe velocity bounds on occluded traffic participants. Compared to state-of-the-art methods, we achieved velocity increases in more than 60% of the 230 cut-in scenarios from the highD dataset, without sacrificing safety."
Generalizing Cooperative Eco-Driving Via Multi-Residual Task Learning,"Vindula Jayawardana, Sirui Li, Cathy Wu, Yashar Farid, Kentaro Oguchi","Massachusetts Institute of Technology,MIT,Toyota North America,Toyota InfoTechnology Center, USA",Intelligent Transportation Systems III,"Conventional control, such as model-based control, is commonly utilized in autonomous driving due to its efficiency and reliability. However, real-world autonomous driving contends with a multitude of diverse traffic scenarios that are challenging for these planning algorithms. Model-free Deep Reinforcement Learning (DRL) presents a promising avenue in this direction, but learning DRL control policies that generalize to multiple traffic scenarios is still a challenge. To address this, we introduce Multi-residual Task Learning (MRTL), a generic learning framework based on multi-task learning that, for a set of task scenarios, decomposes the control into nominal components that are effectively solved by conventional control methods and residual terms which are solved using learning. We employ MRTL for fleet-level emission reduction in mixed traffic using autonomous vehicles as a means of system control. By analyzing the performance of MRTL across nearly 600 signalized intersections and 1200 traffic scenarios, we demonstrate that it emerges as a promising approach to synergize the strengths of DRL and conventional methods in generalizable control."
Approximate Multiagent Reinforcement Learning for On-Demand Urban Mobility Problem on a Large Map,"Daniel Garces, Sushmita Bhattacharya, Dimitri Bertsekas, Stephanie Gil","Harvard University,MIT",Intelligent Transportation Systems III,"In this paper, we focus on the autonomous multiagent taxi routing problem for a large urban environment where the location and number of future ride requests are unknown a-priori, but can be estimated by an empirical distribution. Recent theory has shown that a rollout algorithm with a stable base policy produces a near-optimal stable policy. In the routing setting, a policy is stable if its execution keeps the number of outstanding requests uniformly bounded over time. Although, rollout-based approaches are well-suited for learning cooperative multiagent policies with considerations for future demand, applying such methods to a large urban environment can be computationally expensive due to the large number of taxis required for stability. In this paper, we aim to address the computational bottleneck of multiagent rollout by proposing an approximate multiagent rollout-based two phase algorithm that reduces computational costs, while still achieving a stable near-optimal policy. Our approach partitions the graph into sectors based on the predicted demand and the maximum number of taxis that can run sequentially given the user's computational resources. The algorithm then applies instantaneous assignment (IA) for re-balancing taxis across sectors and a sector-wide multiagent rollout algorithm that is executed in parallel for each sector. We provide two main theoretical results: 1) characterize the number of taxis m that is sufficient for IA to be stable; 2) derive a necessary condition on m to maintain stability for IA as time goes to infinity. Our numerical results show that our approach achieves stability for an $m$ that satisfies the theoretical conditions. We also empirically demonstrate that our proposed two phase algorithm has equivalent performance to the one-at-a-time rollout over the entire map, but with significantly lower runtimes."
Continual Driving Policy Optimization with Closed-Loop Individualized Curricula,"Haoyi Niu, Yizhou Xu, Xingjian Jiang, Jianming Hu",Tsinghua University,Intelligent Transportation Systems III,"The safety of autonomous vehicles (AV) has been a long-standing top concern, stemming from the absence of rare and safety-critical scenarios in the long-tail naturalistic driving distribution. To tackle this challenge, a surge of research in scenario-based autonomous driving has emerged, with a focus on generating high-risk driving scenarios and applying them to conduct safety-critical testing of AV models. However, limited work has been explored on the reuse of these extensive scenarios to iteratively improve AV models. Moreover, it remains intractable and challenging to filter through gigantic scenario libraries collected from other AV models with distinct behaviors, attempting to extract transferable information for current AV improvement. Therefore, we develop a continual driving policy optimization framework featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into a set of standardized sub-modules for flexible implementation choices: AV Evaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as a collision prediction task, where it estimates the chance of AV failures in these scenarios at each iteration. Subsequently, by re-sampling from historical scenarios based on these failure probabilities, CLIC tailors individualized curricula for downstream training, aligning them with the evaluated capability of AV. Accordingly, CLIC not only maximizes the utilization of the vast pre-collected scenario library for closed-loop driving policy optimization but also facilitates AV improvement by individualizing its training with more challenging cases out of those poorly organized scenarios. Experimental results clearly indicate that CLIC surpasses other curriculum-based training strategies, showing substantial improvement in managing risky scenarios, while still maintaining proficiency in handling simpler cases."
Task-Driven Domain-Agnostic Learning with Information Bottleneck for Autonomous Steering,"Yu Shen, Laura Zheng, Tianyi Zhou, Ming C. Lin","University of Maryland,University of Maryland, College Park,University of Maryland at College Park",Intelligent Transportation Systems III,"Environments for autonomous driving can vary from place to place, leading to challenges in designing a learning model for a new scene. Transfer learning can leverage knowledge from a learned domain to a new domain with limited data. In this work, we focus on end-to-end autonomous driving as the target task, consisting of both perception and control. We first utilize information bottleneck analysis to build a causal graph that defines our framework and the loss function; then we propose a novel domain-agnostic learning method for autonomous steering based on our analysis of training data, network architecture, and training paradigm. Experiments show that our method outperforms other SOTA methods."
Towards Inclusive Dance: A Robotic Approach for Enhancing Movement and Interaction for People with Disabilities,"Zhenyu Liao, Zonghao Dong, Ricardo Manríquez, Ankit Ravankar, Juan Carlos Martinez Rocha, Juliette Lavender, Fumi Seto, Eric Monacelli, Yasuhisa Hirata","Tohoku University,LISV, University of Versailles,Université Paris-Saclay",Video Session,"The focus of this topic is to use robotic technology to promote inclusive dance. Inclusive dance aims to break down physical limitations and barriers, celebrating diversity and self-expression, while fostering social inclusion. Robot technologies are employed to encourage user participation and self-expression, enhance social interaction, and assist users in their movement and performance. The â€œYes We Dance!â€ initiative seeks to make inclusive dance accessible to all, irrespective of age, including individuals with disabilities, able-bodied individuals, children, and adults, with the assistance of AI robots. The core concept of the system involves the utilization of various wearable sensors, such as attachable IMUs and motion capture systems, to practice dance in a virtual reality (VR) environment and to control various personal mobility devices, including powered wheelchairs and standing moving devices to perform dance. This method is applied to real dance scenarios, including an inclusive dance show where dancers encompass individuals with disabilities and able-bodied individuals. Dancers can practice dancing in the VR system and then perform the dance in real-life settings. Different types of mobile robots are utilized by the dancers to achieve their dance performances."
Design of Humanoid Robot Foot to Absorb Ground Reaction Force by Mimicking Longitudinal Arch and Transverse Arch of Human Foot,"Jin-deok Lee, Hyun-min Joe","Kyungpook National University, Humanoid Robotics Lab,Kyungpook National University",Video Session,"Abstractâ€” In this video, we describe a Double Arched Robotic Foot-1(DARFT-1) for a humanoid robot. To protect the Force/Torque(F/T) sensor and achieve the mechanical filter effect, we propose a robot foot that mimics the longitudinal arch and transverse arch of a human foot to absorb Ground Reaction Force(GRF) effectively. Each arch of the proposed foot consists of passive joints and springs and is designed with a 2-Degrees-of-Freedom(DoF) structure. Furthermore, DARFT-1 is designed to prevent external obstacles from entering the sole of the foot, while also being designed for shape adaptation to uneven ground. To verify the effectiveness of the designed foot, GRF measurement experiments were conducted by mounting the DARFT-1 on the humanoid robot DRC-HUBO+. Through the experiments, the DARFT-1 reduced GRF by an average of 9.8 % and 10.02 % in three trials when placing the obstacle on the front and side of the foot, respectively, compared to the previous foot. In addition, the proposed foot performed as a mechanical filter by reducing the rate of change in the GRF. Furthermore, the reduced GRF decreased the ZMP, improving the stability of the humanoid robot's walk."
A Novel Modular Robot Inspired by Rubik's Snake,"Ning Zhao, Jian Qi, Zhiyuan Yang, Sikai Zhao, Xin Sui, Kai Han, Tianjiao Zheng, He Zhang, Jie Zhao, Yanhe Zhu",Harbin Institute of Technology,Video Session,"A novel robotic module has been proposed based on the structure and motion pattern of Rubik's Snake unit. Each module has standardized mechanical structure and electrical interface, enabling rapid connection with any other module. This module possesses a rotational degree of freedom, which allows a chain-like robot composed of these modules to assemble diverse robotic configurations through folding and exhibit various bionic movements, such as snake-like movement and worm-like movement. Based on this module, a modular and reconfigurable supernumerary robotic limbs robot has been developed, which could provide operators with additional limbs to perform tasks beyond their capabilities. Compared to robots with fixed configuration, there are superior advantages of this robot in terms of adaptability, robustness and reparability. A demonstration that assisting the operator in holding wires and delivering objects during the welding process was given to confirm the auxiliary capability of this robot."
A Negative Pressure Adsorption Climbing Robot with Independent Steering Wheel for Machining Purposes on Variable Curvature Surface,"Ying Shi, Xiaoshun Liu, Ke Tan, Zhenfeng Gu, Junhui Huang, Zeyu Gong, Bo Tao",Huazhong University of Science and Technology,Video Session,"This video presents a negative pressure adsorption climbing robot with independent steer wheel which is designed for machining purposes on variable curvature freeform surfaces. The climbing robot demonstrates stable adsorptive property and mobility on the surfaces of large, complex, and variable curved components, particularly for non-magnetic components such as aircraft skins, wind turbine blades, etc. The adsorption module of the climbing robot primarily consists of three active compliance suction units, each of which integrates a three-degree-of-freedom active compliance actuator and a flexible suction chamber. By analyzing the leaked airflow of the climbing robot's suction units, we propose a pose control method for the suction chamber based on an adsorption performance metric which depicts the stretch of the flexible sealing lip. For the 15 kg-weight climbing robot, the adsorption force can be kept over 1000 N on the surface with curvature radii ranging from 1.1 to 14.6 m. Equipped with a small manipulator or a flexible grinder, the robot can perform operation, detection, or machining tasks while adsorption-based moving simultaneously."
Posture Estimation and Motion Planning for Plate Insertion into the Frame,"Katsuhiko Fujiki, Takumi Hachimine, Satoshi Makita","Fukuoka Institute of Technology,Nara Institute of Science and Technology",Video Session,"This video presents an essential robotic motion in assembly tasks, inserting a sash-like plate into the frame. The insertion task is often required in production assembly as a component of cabinets and windows. Our demonstration shows that conventional motion planners can lead such a dexterous task insertion with a robotic manipulator. The plate does not go into the groove inside the frame straightforwardly and needs a complicated motion to enter the groove, transferring with rotating. We can reduce the number of dimensions of motion planning for the plate entering the groove from six to three and adopt the A-star algorithm. With an assumption that the relative posture between the end effector of the manipulator and the grasped plate is known, we can also obtain the motion of the manipulator for inserting the plate into the groove by a conventional motion planner, RRT. Our proposed method accomplished the insertion tasks with the position-controlled manipulator. We considered that some hardware problems caused insertion motion failures and checked the manipulability of the manipulator. Our experiments confirmed that manipulability affects the success of robot demonstration even with accurate object perception in manipulation tasks."
Project YORI: Automated Cooking through Proprioceptive Dual-Arm Manipulation within a Structured and Modular Kitchen Environment,"Donghun Noh, Hyunwoo Nam, Kyle Gillespie, Yeting Liu, Dennis Hong","University of California, Los Angeles,University of California Los Angeles,UCLA",Video Session,"We have developed a dual-arm manipulator integrated into a cooking robot system. This system is a departure from traditional single-task-focused cooking robots, as it is designed to multitask operations and complete entire cooking procedures autonomously. One of the standout features is the use of proprioceptive actuators in the dual-arm manipulator, enabling force controlled, dynamic and dexterous cooking actions. Even actions that require impact mitigation, such as chopping zucchini or tenderizing meat with a mallet, are made possible due to the backdrivability provided by the actuators' low gear ratios. The system features a compact but high-payload tool-changing system, allowing the robot to efficiently use customized tools for a wide range of cooking tasks. Additionally, we designed a modular cooking cell composed of customized appliances, providing flexibility to easily alter the kitchen setup. Our goal was to develop a mechanically intelligent platform with proprioceptive actuators, which is highly capable even in the absence of complex algorithms. The future incorporation of more advanced sensors and technologies will further improve its performance. We believe that this approach not only proves the system's capability to perform a variety of cooking tasks but also sets the stage for the development of more advanced cooking robot systems."
Mono Wi-Fi Scene,"Sravan Chittupalli, Liu Jun, Dong Huang",Carnegie Mellon University,Video Session,"We introduce Mono Wi-Fi Scene, the first wireless sensing technology, and a potential sensor module for smart cities and robots, that fully reconstructs the 3D indoor environment with a single free-moving Wi-Fi device. It is accurate for parsing the human and scene interaction in 3D space. Most previous Wi-Fi sensing technologies require two+ separate devices, for instance, one transmitter and one receiver. These multi-device technologies are specific to their relative device positions and cannot generalize well to a new environment. Moreover, they do NOT allow a person to be away from the Line-of-sight between devices. All these limitations are addressed by our new solution, Mono Wi-Fi Scene, which requires only one Standalone Free-moving Device, is Scene-invariant, and can sense up to a 180-degree field of view. Mono Wi-Fi Scene is ready for customization in many use cases. Because Wi-Fi can penetrate most residential building materials, a single sensing device covers multiple rooms and through non-metal occlusions, which is a clear advantage over cameras, mmWave radar, Infrared sensors, etc. It has a clear privacy-preserving benefit over cameras for scenarios such as health care and rehabilitation. This also clearly appeals as a robot-mounted sensor module to occupancy-aware HVAC energy saving, living service, search& rescue, and law enforcement."
Robot Guardians: The Artificial Immune System of Our Planet,"Anna Adamczyk, Alexander Moortgat-pick, Daniel Andre Duecker, Sami Haddadin","Technical University of Munich (TUM),Technical University of Munich",Video Session,"Our planet struggles to maintain its beauty amid escalating environmental challenges. Environmental pollution and rising CO2 emissions severely impact ecosystems worldwide. Despite significant efforts, humanity has yet to overcome these challenges. We need support. Imagine a future where machines and robots are environmental guardians, forming Earth's artificial immune system. Powered by AI and democratized telepresence, robots deployed by autonomous systems continually monitor and combat environmental degradation. Once deployed, they connect to our Collaborative Robot Interface for Telepresence (Co-RIFT) and begin working autonomously, yet an operator can still take control when needed. Through Co-RIFT, a global volunteer force aids experts in addressing environmental issues by guiding and supervising the robots, accessible anytime and anywhere, via computers or smartphones. This synergy of AI and human knowledge allows for handling not only routine tasks but also critical situations. The cooperative effort between humans and machines ultimately leads to a turning point in environmental conservation, leading to a significant reduction in pollution and the restoration of ecosystems. Our vision for environmental robotics is demonstrated through the successful deployment of our prototypes, including the Synchronous Team-Robot Van (SVan) with air, land, and water guardians and Co-RIFT, a collaborative telepresence interface for multi-domain robotic teams."
Gesture Detection System for Collaborative Robots,"Carlos Del Reguero Valle, Diego Emilio Alvarez Guerrero, Kayro Adrian Aguilar Sanchez, Arturo Mireles Bautista, Cesar Alberto Martinez Salazar, Ivo N. Ayala-garcía, Eloina Del-real, Miriam Alvarado Perez, Alejandro González",Tecnologico de Monterrey,Video Session,"The increasing use of collaborative robots (cobots) has provided effective solutions for various applications; however, significant challenges remain for human-robot communication. This project focuses on integrating technologies to facilitate human-machine interaction for collaborative tasks, particularly in the industrial context. While shown at scale, this works seeks to contribute in addressing the difficulty in interacting with heavy industrial equipment and components which often requires the involvement of multiple individuals. Similarly, handling and transporting heavy objects for machine assembly, and part fixation poses common challenges. This project proposes a vision system which helps users to manipulate objects via hand gestures. Its overarching goal is to demonstrate that improved human-machine communication can positively influence efficiency, productivity, and work experience across various industrial sectors."
Universal Monocular 3D Human Recovery Engine,"Haoye Dong, Liu Jun, Dong Huang",Carnegie Mellon University,Video Session,"We present a universal 3D human recovery real-time framework in moving robots, stationary monitoring, and sports training. Our perception engine only uses one monocular RGB camera, produces accurate 3D human meshes in physical sizes and 3D translations, and enables real-time deployment in both moving and stationary platforms. Many existing 3D human perception systems have limitations. Depth-aware sensors detect only visible body parts, RGB-based solutions rely on complex pose estimation, and most rely on slow online optimization for stability. Unlike conventional 2D or 3D keypoint estimation, 3D human mesh represents closed 3D surface shapes, aligning with pixels in 2D images while having measurable physical sizes in meters. The proposed model enables fine-grained human motion analysis and interactions in mobile and fixed environments. However, recovering 3D human meshes from a monocular camera is intrinsically challenging due to texture and scale ambiguities, especially in real-time and diverse settings. We propose a universal 3D human recovery engine with a well-designed robust monocular image-based multi-person mesh detector. Locally, it preserves body aspect ratios and refines person-wise features. Globally, it learns dense-depth-guided features for accurate depth estimation. The predicted 3D human meshes are then tracked and refined in physical 3D space with real-time capabilities, accelerated using multi-threading and Tensorrt. The experiments demonstrate that the propo"
Collaborative Robot Assistance for Assembly: A Deep Learning Approach,"Oscar Ochoa, Enrico Mendez, David Olivera Guzman, Victor Hugo Soto Herrera, José Alfredo Luna Sánchez, Carolina Lucas, Ivo N. Ayala-garcía, Eloina Del-real, Miriam Alvarado Perez, Alejandro González","Tecnologico de Monterrey,Escuela de Ingenieria y Ciencias, Tecnologico de Monterrey, Av. ,Tecnológico de Monterrey,Instituto Tecnológico y de Estudios Superiores de Monterrey,ITESM",Video Session,"Deep learning, as a prominent research area, has witnessed remarkable research growth, resulting in diverse architectures and applications. Robotics stands out as a field with substantial potential for leveraging deep learning algorithms, particularly for collaborative robots (cobots). Cobots are especially suited for Human-Robot Interaction (HRI) given their sensing capabilities. This video shows the implementation of three Neural Networks designed to augment the cobot with visual, spatial, and auditory inputs to improve active collaboration with human users. To this end, the cobot was equipped with object classification, voice command recognition, and hand tracking systems. The system was tested on a collaborative assembly task, which included fetching and storing parts. The Neural Networks were trained on a dataset obtained from various users and were tested both individually and as a complete integrated system. The complete system showcases its ability to interact with several users. Implementing Deep Learning across fields such as HRI opens novel avenues for achieving heightened efficiency and frictionless processes. This work serves as a proof of concept for integrating three Neural Networks in a human-robot collaborative activity."
Intelligent Dextrous Manipulation: Humanoid Agile Justin Meets Learning AI,"Johannes Tenhumberg, Leon Sievers, Dominik Winkelbauer, Lennart Röstel, Johannes Pitz, Matthias Humt, Ulf Kasolowsky, Ulrich Hillenbrand, Jörg Butterfass, Werner Friedl, Thomas Gumpert, Berthold Bäuml","German Aerospace Center (DLR),DLR - German Aerospace Center,DLR,German Aerospace Center,German Aerospace Center (DLR), Technical University Munich (TUM),Technical University of Munich,German AerospaceCenter (DLR)",Video Session,"DLR's mobile humanoid Agile Justin is an advanced mechatronic system with its beginnings dating back as early as 2007. However, with the advent of modern deep learning (DL) and deep reinforcement learning (DRL) methods we developed, it now has reached a new level of autonomous dexterity never before shown by any humanoid. This is especially true for autonomous dextrous manipulation with its multi-fingered DLR-Hands II: it can perform in-hand manipulation -- purely tactile (w/o cameras) and learned from scratch in only 5h on a single GPU -- as well as grasp unknown objects based on a single depth image by shape completion and grasp prediction (in"
Knowledge-Based Reasoning and Learning in Ad Hoc Teamwork,"Hasra Dodampegama, Mohan Sridharan",University of Edinburgh,Video Session,"We present an architecture for ad hoc teamwork, i.e., to enable an agent to collaborate with other agents ""on the fly"". State of the art frameworks for ad hoc teamwork are often data-driven, using a large dataset of prior observations to model the behaviour of other agents and to determine the ad hoc agent's behaviour. It is often difficult to pursue such an approach in complex domains due to the lack of sufficient training examples and computational resources. In addition, the learned models lack transparency, and it is difficult to revise the existing knowledge in response to previously unseen changes. In a departure from existing work, we propose a novel architecture for ad hoc teamwork that performs non-monotonic logical reasoning with prior commonsense domain knowledge and with models that are learned and revised rapidly from limited examples to predict the behaviour of other agents. In addition, the architecture enables the agent to provide relational descriptions of its decisions and beliefs as on-demand explanations in response to different types of questions."
Inertial-Only Positioning for Human and Car Localization,Han Wang,"Nanyang Technological University, Singapore",Video Session,"Indoor Positioning System (IPS) is an emerging tech- nology that is widely applied in indoor navigation, urgent evacuation, human activity analysis, Autonomous Valet Park- ing (AVP), etc. Existing wireless IPS (e.g., Wi-Fi, blue- tooth, UWB) is limited by the pre-deployment of anchors (or beacons) and signal calibration, which is typically not feasible in many robotic applications. Moreover, the wireless signal is often blocked due to the Non-Line-Of-Sight (NLOS) issue. At the same time, recent active localization methods via on-board sensors such as Visual-SLAM and LiDAR- SLAM is also limited by the sensor property. For example, the lighting condition is often poor in the underground car park, and the LiDAR is not portable for human localization. In this paper, we present an inertial-only IPS that uses an external portable IMU for localization. The system is independent of any external setup or environments due to the advantage of IMU sensor; while the proposed method will automatically analyze the behaviour of the moving target and generate the trajectory. Alternatively, an map (optional) can be integrated to provide long-term localization on a daily basis. To demonstrate the robustness, we evaluate the proposed method on both human and car localization. The results show that the proposed method is able to achieve sub-meter level localization accuracy across both indoor and outdoor environments."
Open Sourse Underwater Robot: Easys,"Michikuni Eguchi, Koki Kato, Tatsuya Oshima, Shunya Hara","University of Tsukuba,Osaka University",Video Session,"We have created an open-source underwater robot named ""Easys"". This video introduces the hardware, vision, and control aspects of Easys. Most parts of the Easys body are made up of readily available products or are 3D printed. This makes it easy to build and modify as an open-source project. Communication with the robot is done using a LAN cable. By connecting this cable between the ground-based PC and the robot, one can control the robot's movement using real-time footage from the camera mounted on the front of the robot. The robot is equipped with four thrusters, which allow it to move freely underwater. We built an underwater robot simulator using Gazebo. By utilizing the simulator, we can efficiently improve the control system."
3D Tracking of a Moving Object by a Multibeam Imaging Sonar-Equipped Autonomous Underwater Vehicle for Continuous Observation of Marine Life,"Sehwa Chun, Chihaya Kawamura, Kenji Ohkuma, Toshihiro Maki","The University of Tokyo,Univ. of Tokyo",Video Session,"Biologging methods have been utilized to study marine life's ecology and behavior. However, this procedure is criticized for its operational inconvenience and stress on marine animals. Recently, the methods with Autonomous Underwater Vehicles (AUVs) have been gaining attention for marine animal tracking, particularly those equipped with multibeam imaging sonar (MBS) unaffected by underwater light and turbidity. In this work, we introduce a novel method for 3D detection and tracking a moving object for marine animal surveys, employing the tilt control of MBS on AUVs. YOLOv3, the object detection algorithm, determines the target's position in sonar images. Combining the particle filter algorithm with the detection, we improve detection accuracy and estimate the target's state, velocity, and pose. The AUV can track the target in 3D space based on the estimated states. This method has been validated through tank experiments with a hovering-type AUV, Tri-TON, and a sea turtle replica, successfully tracking the target moving horizontally and vertically. The AUV succeeded in estimating with an RMSE of 0.23m and velocity and pose within the error of 0.022m/s and 16.59 degrees, respectively."
Cut and Fire Resistant Variable Stiffness Gripper Mechanisms,"Kenjiro Tadakuma, Masahiro Watanabe, Satoshi Tadokoro",Tohoku University,Video Session,"Able to grasp objects of any shape and size, universal grippers using variable stiffness phenomenon such as granular jamming have been developed for disaster robotics application. However, as their contact interface is mainly composed of unrigid and burnable silicone rubber, conventional soft grippers are not applicable to objects with sharp sections such as broken valves and glass fragments, especially on fire. In this research, the authors proposed a new method of variable stiffness mechanism using a string of beads that can be composed of cut-resistant and incombustible metals, arrange the mechanism to form a torus gripper, and conducted experiments to show its effectiveness."
Introduction to the Music Intervention Using Socially Engaging Robotics (MUSE) System for Persons with Dementia,"Tyler Morris, Eric Vaughan, Sydney Walker, Laython Holder, Xiaopeng Zhao","University of Tennessee - Knoxville,University of Tennessee, Knoxville,University of Tennessee at Knoxville,University of Tennessee",Video Session,"In response to the rising number of persons with dementia, requests for novel forms of care that promote the overall well-being of patients are on the rise. One such form of care is music intervention, which has been proven to promote the cognitive, physical, and emotional well-being of dementia patients. However, the number of caregivers available to provide such care is not rising fast enough, so any new forms should require minimal intervention. To provide for both requests, we propose a system that uses a social robot to perform music intervention sessions for groups of dementia patients. This system, named the Music intervention Using Socially Engaging robotics (MUSE), has the social robot, Pepper, guiding persons with dementia through a music intervention session. This includes three activities: a Keep-with-the-Rhythm game, a Sing-Along, and a Dance-Along. In each activity, the robot guides users through the music-based task, in which they can sing/dance/tap along. The system is optimized for persons with dementia through large, easy-to-read/press buttons on a touch screen, spoken instructions, and familiar music. Overall, the MUSE system seeks to bring the benefits of music intervention to those with dementia while also alleviating caregiver burden."
Autonomous Robotics for Transcatheter Delivery System,"Xiu Zhang, Angela Peloso, Emiliano Votta, Elena De Momi",Politecnico di Milano,Video Session,"Cardiovascular diseases (CVD) are a leading cause of impaired quality of life and substantial healthcare costs. Approximate 30% of CVD cases are attributed to structural heart diseases (SHDs), which encompass various heart abnormalities. Patients with SHDs typically require open-chest surgery. Nowadays percutaneous procedures are becoming more popular because of less trauma and shorter hospitalization time, which also provide an alternative solution for patients, who cannot suffer open chest surgery. However, these life-saving procedures are accessible in a few clinics with skilled and experienced operators, because of the complexity of the procedure and the steep learning curve. Moreover, both the patient and operators will be exposed under the radiative fluoroscopy during the procedure. The ARTERY project focuses on the development of a radiation-free percutaneous procedure using autonomous robotic catheters. The project's core concept involves two robots: one for handling the clip delivery system and another for managing the transesophageal echocardiographic probe. Key technological advancements encompass real-time computational model and sensors, echo image processing, augmented reality user interface, novel actuation hardware, and AI based catheter control. The project aims to eliminate X-ray exposure risks for operators, and increase the level of autonomy in percutaneous procedures, potentially enhancing safety in the process."
Roomac: Affordable Open-Source Autonomous Mobile Manipulation Robot - Fetch a Bottle Demo,Maciej Stępień,AGH University of Krakow,Video Session,"Mobile manipulation robots that can be used as service robots are becoming more popular, but they still are expensive and lack autonomous capabilities. An open-source, inexpensive platform for research and development can provide better access to hardware, and thus accelerate advancements in the personal robotics field. In this work, an affordable (parts cost of around 550$) 3D printed robot consisting of a mobile base and 5-degree-of-freedom manipulator was developed. It was used to create a service robot application, in which the robot autonomously navigates to a table, picks up a bottle and delivers it to the user. The software was based on the Robot Operating System and utilized some of its solutions for the navigation problem (Move Base, TEB, RTAB-Map, Robot Localization) and manipulation (MoveIt)."
Human-Robot Skill Transfer with Enhanced Compliance Via Dynamic Movement Primitives,"Jayden Hong, Zengjie Zhang, Amir Mehdi Soufi Enayati, Homayoun Najjaran","University of Victoria,Eindhoven University of Technology",Video Session,"Improving robots' performance relies on efficiently planning their trajectories. One way to do this is by using the Learning from Demonstrations (LfD) method to transfer human-like skills to robots. However, human motion isn't naturally suited for robots due to differences between human biomechanics and robot dynamics. To address this issue with LfD, the Dynamic Movement Primitives (DMP) framework offers a promising solution. However, it requires tuning of second-order dynamic system parameters. Our contribution is a systematic method that extracts dynamic features from human demonstrations to auto-tune the parameters in the DMP framework. This method is also valuable for using in conjunction with Reinforcement Learning (RL) in robot training. Through our approach, these extracted features enable robots to explore potential trajectories more effectively, significantly enhancing robot compliance. We achieve this by optimizing similarity in the parameter space while maintaining human-likeness. Our method was implemented on an actual human-robot setup. We extracted dynamic human features and generated robot trajectories through LfD and RL with DMP. This experiment resulted in a stable robot performance while preserving a high degree of human-likeness, as good as the best heuristic tuning."
Investigation of Turning Strategies for a Buoyancy Assisted Biped,"Sepehr Ghassemi, Dennis Hong",UCLA,Video Session,"Buoyancy Assisted Lightweight Legged Unit (BALLU) is a human size bipedal robot offering intrinsic stability, safety, and affordability. It leverages a buoyant helium-filled upper body paired with two slender carbon fiber legs bearing the majority of weight in the feet. Despite each leg possessing only one active degree of freedom (cable-driven knee joint), BALLU is capable of forward and backward locomotion, jumping, turning, and traversing rough terrain. This study explores three distinct turning methodologies. The first method capitalizes on foot-ground friction, employing repeated steps with one foot while using the other as a pivot. Due to the robot's low weight and small foot size, limitations exist in ground-foot static friction, requiring smaller steps to mitigate foot slippage. The second approach involves adjusting leg posture and utilizing foot mass to lean into a turn. By lifting one foot off the ground and maintaining a bent knee, the foot naturally falls back down while the other foot acts as a supporting pivot, resulting in a sideway lean and subsequent turn. The third method introduces a torque optimization technique reminiscent of flywheel steering in satellites. Unlike high-speed rotating wheels, it relies on precise robot posture control to spin around a pivot foot while the other foot is lifted. This unique dynamic configuration of BALLU allows for energy-efficient turning, where the cost of spinning becomes favorable compared to that of a direct fall."
FiReSpARX â€“ FinTech/RegTech in Space for Trustful Autonomous Robotic Interaction,"Renan Lima Baima, Eduard Hartwich, Loïck Chovet, Miguel Olivares-Mendez, Gilbert Fridgen","University of Luxembourg,Interdisciplinary Centre for Security, Reliability and Trust (Sn,Interdisciplinary Centre for Security, Reliability and Trust - U",Video Session,"In the not-so-distant future, space agencies and private enterprises will embark on resource-harvesting missions in outer space. These missions will likely involve multiple robots from various entities operating in the same celestial vicinity. To enhance operational efficiency, these robotic entities must collaborate, share data, and offer services, such as interplanetary communication. However, the unpredictability of outer space means that pre-planned cooperation is only sometimes feasible. Therefore, these robots must possess the autonomy to make economic decisions aligned with the interests of their respective parent companies. They must be capable of determining the value of services, engaging in negotiation, and executing transactions. The FiReSpARX project is dedicated to designing market mechanisms, incentives, and governance frameworks for these autonomous economic interactions. Furthermore, we implement and evaluate a prototype system within SnT's LunaLab, an environment that replicates lunar conditions. This video introduces the blockchain-based Multi-Robot System (MRS) for distributed mapping in In-Situ Resource Utilization (ISRU) scenarios. The MRS system optimizes global efficiency by fostering coopetitive (cooperative and competitive) interactions among robots, ultimately streamlining the mapping process in ISRU scenariosâ€”a pivotal advancement for the emerging space economy."
Optical Flow Sensor Enables Automatic Grasping Assistance for Prosthetic Hands,"Joseph Berman, John Patterson, He (Helen) Huang","North Carolina State University,North Carolina State University and University of North Carolina",Video Session,"Modern powered prosthetic hands are equipped with advanced algorithms that can predict high-level movement intentions from input electromyography (EMG) signals. However, during grasping tasks, commanding manual adjustments of grip force to prevent dropping or damaging held objects can be a significant physical and mental burden on patients. One potential solution to this, inspired by the reflexive functionality of the biological human hand, is automating grip force adjustments independent of human input. In our novel automated grasping controller, the slipping motion of held objects is detected by an inexpensive optical flow sensor, allowing quick grip force adjustments to stabilize the grip. A full shared control framework was developed by integrating the automatic grasping control with an EMG-based artificial neural network (ANN) to continuously estimate desired hand movements. The shared controller features seamless switching between EMG-based control and automated grasping control by detection of object contact with an embedded load cell and the prediction of a desired open hand movement from the ANN. This proposed method may lead to the further development of advanced prosthetic hands and reduce the burden of continuous grip force modulation on human users."
Digital Twin of a Robot Cell with Projected AR of Dynamic Safety Zones in a Pick'n Place Task Context,Petri Tikka,VTT Techincal research centre of Finland,Video Session,"The project consists of creating a digital twin of a robot cell. The cell includes dynamic safety zones that are projected to the operator as augmented reality. The context of the demo is a pick and place task performed by the UR10e robot. The digital twin of the robot cell is produced in Solidworks but operated in Unity. The kinematic model of the robot provides chain to move the manipulator, and the Unity model updates its joint values according to the physical counterpart. The robot cell populates a pointcloud, which is formulated with a Trimble X7 scanner of the physical room. The Unity environment works as a server and the robot cell as a client. Pose of the virtual cube is updated as the physical robot checks the workspace for a cube with a wrist camera. The Unity environment functions under physics that are managed by AGX Dynamics. The cobot cell includes a safety scanner, which detects motion on a specified warning and detection areas. The dynamic safety configuration of the scanner is parsed in Unity and visualised for the operator with a projector. When areas are clear, the robot continues its task automatically"
EEWOC: Extended-Reach Enhanced Wheeled Orb for Climbing,"Justin Quan, Mingzhang Zhu, Dennis Hong","UCLA,University of California, Los Angeles",Video Session,"EEWOC (Extended-reach Enhanced Wheeled Orb for Climbing) is a small, lightweight, mobile robot designed to climb steel structures. EEWOC can be deployed for close-contact inspection, maintenance, or surveillance tasks in hard-to-reach or hazardous areas in towers, factories, bridges, power stations, and ships. Existing climbing robots have many shortcomings that limit their practical use. Wheeled climbers have low surface adaptability due to their small reachable workspace. Legged climbers tend to be heavy, slow, complex, and expensive due to their ground-based designs, with excessive actuators and structures that mimic animal limbs. EEWOC overcomes these limitations with an unconventional mechanical design, with a highly extendable tape spring limb that allows it to place its magnetic gripper at distant points. This 1.2 m limb greatly increases EEWOC's workspace and the size of obstacles it can traverse, and it can bend its limb to place grippers around obstacles or above ledges. It also has wheels for ground movement and to assist with transitioning over ledges and corners. EEWOC has demonstrated fast and reliable free climbing on large steel structures, as well as dynamic swinging over gaps with its limb. EEWOC measures 26Ã—30Ã—30 cm and weighs 2.1 kg, much smaller and lighter than other limbed climbing robots, and only costs $1250 per unit. It can climb at 7.3 cm/s, making it one of the fastest for its size, and can carry a 3.4 kg payload, making it one of the strongest."
A Day with GARMI: Robot Home Assistance in the Lives of Senior Citizens,"Seongjin Bien, Jon Škerlj, Felix Eberle, Ainoor Teimoorzadeh, Xiao Chen, Moein Forouhar, Daniel Pérez-suay, Mario Troebinger, Simon Wilhelm, Luis Felipe Cruz Figueredo, Hamid Sadeghian, Abdeldjallil Naceri, Sami Haddadin","Technical University of Munich,Sahand University of Technology,Technische Universität München,Technische Universitaet Muenchen,Technical University of Munich (TUM)",Video Session,"The current work is driven by the shifting demographics in Germany and aims to address the challenges faced by the elderly population. The goal is to empower older individuals to lead independent lives in their own homes, achieved through the application of intelligent robotics. To realize this objective, we envision establishing a service humanoid robot named GARMI that can be utilized in various application fields, including healthcare, daily activities, and communication. Additionally, we are developing sensitive hands, a multi-modal robot head, and a Lightweight Exoskeleton to fulfil the aforementioned goals. While the system is designed to run autonomously, it can also be teleoperated through a variety of multi-modal input devices using the Avatar concept via GARMI. The attached video illustrates the proposed concept and vision in various scenarios within the aforementioned application fields. By doing so, we aim to bring these robotic systems to life and demonstrate their performance in real-world scenarios within a pilot home for the elderly. Collaborations with nursing schools and caregiving companies such as Caritas facilitate the integration of these technologies into the community. Ultimately, the robotic systems developed in our lab prove invaluable to senior citizens, enabling them to lead more fulfilling lives."
A Nitinol-Embedded Wearable Soft Robotic Gripper for Deep-Sea Manipulation,"Zonghao Zuo, Haoxuan Wang, Xia He, Zhuyin Shao, Jiaqi Liu, Qiyi Zhang, Fei Pan, Li Wen",Beihang University,Video Session,"Soft robotic gripper systems that can safely collect deep-sea biological samples, nondestructively collect artifact samples and perform various underwater manipulation tasks are constructive and significant in multiple fields ranging from biology and archaeology research to engineering applications. In this video, we present a soft robotic gripper that is to our knowledge the first to integrate the superelastic nitinol wires with soft materials for performing deep-sea underwater sampling tasks. A manned deep-sea submersible, Deep Sea Warrior, serves as the operating platform for the nitinol-embedded soft robotic gripper. In the first part of the video, we not only show that the gripper can perform nondestructive sampling tasks, including fragile porcelain retrieving and precision instrument manipulation at a depth of more than 3600m but also demonstrate its ability to lift heavy objects. This gripper has the potential to make the collection of deep-sea creatures and cultural relics safer and more efficient. In the second part of the video, we demonstrate the combination of the nitinol-embedded soft robotic gripper with resistive bending sensors (Flex Sensor) in a laboratory environment. We use the feedback of LED lights to determine whether the target object was securely gripped. This may be a possible way for human-machine interaction in the deep sea in the future."
Toward Autonomous Tensegrity Robots,"Xiaonan Huang, Will Johnson, Shiyang Lu, Kun Wang, Kostas E. Bekris, Joran Booth, Rebecca Kramer-Bottiglio","University of Michigan,Yale University,Rutgers University,Amazon.com LLC,Rutgers, the State University of New Jersey",Video Session,"Tensegrity robots, combining rigid struts with compliant tendons, excel in navigating complex environments, enduring impacts, and adapting to confined spaces. However, controlling their coupled dynamics has posed challenges in achieving autonomous control, thus limiting their capabilities compared to their traditional rigid counterparts. We present a tensegrity robot comprising three rigid struts, six motor-cable pairs for locomotion, nine stretch sensors (six for motor control, three for structural integrity), IMUs, a microcontroller, and a Bluetooth module. This robot achieves speeds exceeding 10 body lengths per minute on flat terrain, effortlessly maneuvers through diverse terrains, including ice, sand, grass, and rocky surfaces, and climbs inclines of up to 28 degrees. The robot's innate compliance and shape adaptability allow it to navigate spaces up to 70% of its maximum height, seamlessly switch between rolling and crawling, and execute precise clockwise and counter-clockwise turns. Remarkably, it survives a 20-meter freefall without sustaining damage. The incorporation of onboard sensors enables real-time state estimation, facilitating autonomous operation. The robot's advanced control system employs a receding horizon planner, allowing it to autonomously follow prescribed trajectories with precision. This work represents a significant leap in autonomous tensegrity robot capabilities, offering exceptional maneuverability and robustness in challenging environments."
Tip Growing Actuators with Steering or Lift-Up Function,"Hideyuki Tsukagoshi, Nobuyuki Arai, Ichiro Kiryu, Tomoyuki Nakamura, Ato Kitagawa","Tokyo Institute of Technology,Tokyo Institute of technology",Video Session,"This video introduces two types of flexible devices that generate tip growing motion. The operation of these actuators is based on the process of plant root growth. Namely, this mechanism allows not to moves the entire structure but to extend from the tip. As a result, even in a narrow gap, it is expected that movement will be generated with less kinetic friction with the outside world. The world's first device to propose this motion had a structure that generated only straight motion. In this video, two epoch-making devices are introduced based on the above principle. Grow-hose-I, shown in the first half, is the world's first device to achieve tip growing motion while performing active directional steering, presented at ICRA2011. This operation was made possible by the structure of multiple flat tubes and a mechanism that automatically locks the unrolled outer skin. The results of this research stimulated many researchers around the world to study even more advanced tip growing devices. The second half, Slip-in Manipulator, can generate motions to travel under the weight of the human body and can generate bending motions to assist in turning the human body over. This is a device designed to support body weight while reducing wrinkles in clothing. It is expected that this will help prevent bedsores."
Small Phase-Change Buoyancy Regulator for Deep-Sea Robot,"Xia He, Zonghao Zuo, Zhuyin Shao, Haoxuan Wang, Fei Pan, Jiaqi Liu, Li Wen",Beihang University,Video Session,"Sperm whales can control its buoyancy by changing the solid-liquid phase transition state of the spermaceti in its head, enabling it to ascend and descend in the deep-sea. Inspired by this biomimetic principle, we have designed a buoyancy regulator for small deep-sea robots, the regulator consists of a silicone rubber shell encasing paraffin phase-change material. It is heated by Nickel-chromium alloy wire. Heat is transmitted by the thermal conductivity mechanism. causing a change in volume and thus altering buoyancy. The regulator is approximately 5cm in height and 3cm in diameter. A single regulator weighs only 35g. We demonstrated that the buoyancy regulator system can switch from ascent to descent in 20s. At 2V voltage, within 220 seconds, four buoyancy regulators together generated 5g of buoyancy, with a total adjustment of up to 16g, equivalent to 11.4% of their own weight. We also demonstrated that the regulator can change phase and volume as normal under conditions of 126Mpa, and remained undamaged and continued to function normally in the laboratory. Currently, the buoyancy regulator has been applied to small deep-sea robots, helping them achieve buoyancy control in extremely high-pressure environments."
The Efforts of the ILSR Laboratory in the Surgical Robot,"Hao Liu, Chongyang Wang, Zhenxing Wang, Dingjia Li, Yongming Yang","Chinese Academy of Sciences,Shenyang Institute of Automation Chinese Academy of Sciences,Shenyang Institute of Automation",Video Session,"The development of surgical robots is revolutionizing the paradigm of surgery, but many complex procedures remain beyond robots' reach. What restricts the application of robots in intricate surgeries? In this video, we introduce the work of ILSR Lab in the domains of continuum configurations, surgical robot force sensing, visual reconstruction, and autonomous robot control. We hope this can provide valuable insights for researchers in the field of future surgical robot advancements."
A Series-Parallel-Reconfigurable Tendon-Driven Supernumerary Robotic Arms for Human Operation Augmentation,"Qinghua Zhang, Hongwei Jing, Kerui Sun, Yuancheng Li, Lele Li, Xianglong Li, Tianjiao Zheng, Sikai Zhao, Hongzhe Jin, Jie Zhao, Yanhe Zhu",Harbin Institute of Technology,Video Session,"Supernumerary robotic limbs (SRL) are new types of wearable robots used as the third limb to work with humans. The device is designed to provide the wearer with better auxiliary ability. A Series-Parallel-Reconfigurable Tendon-driven Supernumerary Robotic Arms requiring clever end-conversion tools was proposed to better adapt to variable tasks, which is multi-assistive for the wearers. The conversion time between series and parallel configurations was completed within 20 s in the experimental test (see the video), which indicates that the quick switch mechanism makes the idea of the Series-Parallel-Reconfigurable configuration more practical. The transmission adopts the tendon-driven ways with low friction, good back-drivability, which makes the mechanical device more lightweight and compact and have better force interaction performance with wearer and environment. The shoulder and elbow flexion/extension part adopt the method of coupled tendon-driven transmission by changing the winding method of tendon, which make the load for each degree of freedom can be shared by two motors at the same time. It greatly reduces the burden of the motor compared to traditional ways. Finally, the proposed RTSRAs has been successfully applied in multiple typical scenarios including load operation, overhead support, assisted delivery and cable installation. The capability of the proposed SRL to perform successfully various tasks suggests a great potential in future applications."
Effects of Impedance Control Stiffness on Assisted Elbow Flexion,"Yuan Yang, Wen Liang Yeoh, Teerapapa Luecha, Jeewon Choi, Ping Yeap Loh, Satoshi Muraki","Kyushu University,Saga University,Dong-A University,Faculty of Design, Kyushu University",Video Session,"Robotic exoskeletons are a promising technology for improving the physical capabilities of humans and impedance control is one of the most popular control methods to specify the interaction between these devices and its user. However, many challenges remain on how to achieve effective collaboration between the robotic exoskeleton and its user. In particular, the stiffness parameter of impedance control, which determines the deviceâ€™s compliance, can greatly affect overall performance and the physical load experienced by its user. If the forces generated are inappropriate, its user's natural movement trajectory and muscle activation patterns can be disrupted, leading to discomfort and reduced performance. In this study, we investigate how the stiffness parameter in impedance control changes muscle activation and perceived exertion of its user during collaborative elbow flexion. Participants were instructed to lift a load equivalent to 15% of their maximal voluntary contracted force, in collaboration with the robotic arm, from 40 to 135 degrees. Three levels of stiffness were investigated and surface electromyography was used to measure muscle activities of the biceps brachii and triceps brachii, and participants reported their perceived exertion after each trial. The study revealed that medium or high stiffness levels can effectively reduce both muscles' activity."
Human Operation Augmentation through Wearable Robotic Limb Integrated with Mixed Reality Device,"Hongwei Jing, Tianjiao Zheng, Sikai Zhao, Gangfeng Liu, Jizhuang Fan, Jie Zhao, Yanhe Zhu","Harbin Institute of Technology,Robot Research Institute, Harbin Institute of Technology",Video Session,"The supernumerary robotic limb (SRL) is a new type of wearable robotic arm. It can provide the human with an extra robotic arm to enhance its operation ability. The interaction method of SRL and its enhanced performance on the human body are research issues of great concern. In situations where human limbs are heavily occupied with primary tasks, proposing additional means to control and perceive SRL poses a challenge. We apply a mixed reality device (Hololens2) in a wearable robotic arm system. Taking the typical multi-hand task in aircraft manufacturing as an example, We built a task model and interaction method based on FSM. The approach shown in this video attempts to apply mixed reality to wearable robotic limbs, demonstrating initial potential for improving wearer efficiency."
Open X-Embodiment: Robotic Learning Datasets and RT-X Models,"Sergey Levine, Chelsea Finn, Ken Goldberg, Lawrence Yunliang Chen, Gaurav Sukhatme, Shivin Dass, Lerrel Pinto, Yuke Zhu, Yifeng Zhu, Shuran Song, Oier Mees, Deepak Pathak, Hao-shu Fang, Henrik Christensen, Mingyu Ding, Youngwoon Lee, Dorsa Sadigh, Ilija Radosavovic, Jeannette Bohg, Xiaolong Wang, Xuanlin Li, Krishan Rana, Kento Kawaharazuka, Tatsuya Matsushima","UC Berkeley,Stanford University,University of Southern California,UT Austin,New York University,The University of Texas at Austin,Columbia University,University of California, Berkeley,Carnegie Mellon University,,Shanghai Jiao Tong University,,UC San Diego,,Queensland University of Technology,,The University of Tokyo",Robot Manipulation,"Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train ``generalistâ€™â€™ cross-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective cross-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms."
SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust Attention,"Isabel Leal, Krzysztof Choromanski, Deepali Jain, Avinava Dubey, Jacob Varley, Michael S Ryoo, Yao Lu, Frederick Liu, Vikas Sindhwani, Tamas Sarlos, Kenneth Oslund, Karol Hausman, Quan Vuong, Kanishka Rao","Google Deepmind,Google DeepMind Robotics,Robotics at Google,Google,Google, Stony Brook University,Google Brain, NYC,Google Research,Google Brain,UC San Diego",Robot Manipulation,"We present Self-Adaptive Robust Attention for Robotics Transformers (SARA-RT): a new paradigm for addressing the emerging challenge of scaling up Robotics Transformers (RT) for on-robot deployment. SARA-RT relies on the new method of fine-tuning proposed by us, called up-training. It converts pre-trained or already fine-tuned Transformer-based robotic policies of quadratic time complexity (including massive billion-parameter vision-language-action models or VLAs), into their efficient linear-attention counterparts maintaining high quality. We demonstrate the effectiveness of SARA-RT by speeding up: (a) the class of recently introduced RT-2 models, the first VLA robotic policies pre-trained on internet-scale data, as well as (b) Point Cloud Transformer (PCT) robotic policies operating on large point clouds. We complement our results with the rigorous mathematical analysis providing deeper insight into the phenomenon of SARA."
DenseTact-Mini: An Optical Tactile Sensor for Grasping Multi-Scale Objects from Flat Surfaces,"Won Kyung Do, Ankush Dhawan, Mathilda Kitzmann, Monroe Kennedy",Stanford University,Robot Manipulation,"Dexterous manipulation, especially of small daily objects, continues to pose complex challenges in robotics. This paper introduces the DenseTact-Mini, an optical tactile sensor with a soft, rounded, smooth gel surface and compact design equipped with a synthetic fingernail. We propose three distinct grasping strategies: tap grasping using adhesion forces such as electrostatic and van der Waals, fingernail grasping leveraging rolling/sliding contact between the object and fingernail, and fingertip grasping with two soft fingertips. Through comprehensive evaluations, the DenseTact-Mini demonstrates a lifting success rate exceeding 90.2% when grasping various objects, including items such as 1mm basil seeds, thin paperclips, and items larger than 15mm such as bearings. This work demonstrates the potential of soft optical tactile sensors for dexterous manipulation and grasping."
Hearing Touch: Audio-Visual Pretraining for Contact-Rich Manipulation,"Jared Mejia, Victoria Dean, Tess Hellebrekers, Abhinav Gupta","Carnegie Mellon University,Meta AI Research",Robot Manipulation,"Although pre-training on a large amount of data is beneficial for robot learning, current paradigms only perform large-scale pretraining for visual representations, whereas representations for other modalities are trained from scratch. In contrast to the abundance of visual data, it is unclear what relevant internet-scale data may be used for pretraining other modalities such as tactile sensing. Such pretraining becomes increasingly crucial in the low-data regimes common in robotics applications. In this paper, we address this gap by using contact microphones as an alternative tactile sensor. Our key insight is that contact microphones capture inherently audio-based information, allowing us to leverage large-scale audio-visual pretraining to obtain representations that boost the performance of robotic manipulation. To the best of our knowledge, our method is the first approach leveraging large-scale multisensory pre-training for robotic manipulation."
Towards Generalizable Zero-Shot Manipulation Via Translating Human Interaction Plans,"Homanga Bharadhwaj, Abhinav Gupta, Vikash Kumar, Shubham Tulsiani","Carnegie Mellon University,Meta AI",Robot Manipulation,"We pursue the goal of developing robots that can interact zero-shot with generic unseen objects via a diverse repertoire of manipulation skills and show howpassive human videos can serve as a rich source of data for learning such generalist robots. Unlike typical robot learning approaches which directly learn how a robot should act from interaction data, we adopt a factorized approach that can leverage large-scale human videos to learn how a human would accomplish a desired task (a human `plan'), followed by `translatingâ€™ this plan to the robotâ€™s embodiment. Specifically, we learn a human `plan predictorâ€™ that, given a current image of a scene and a goal image, predicts the future hand and object configurations. We combine this with a `translationâ€™ module that learns a plan-conditioned robot manipulation policy, and allows following humans plans for generic manipulation tasks in a zero-shot manner with no deployment-time training. Importantly, while the plan predictor can leverage large-scale human videos for learning, the translation module only requires a small amount of in-domain data, and can generalize to tasks not seen during training. We show that our learned system can perform over 16 manipulation skills that generalize to 40 objects, encompassing 100 real-world tasks for table-top manipulation and diverse in-the-wild manipulation. https://homangab.github.io/hopman/"
Constrained Bimanual Planning with Analytic Inverse Kinematics,"Thomas Cohn, Seiji Shaw, Max Simchowitz, Russ Tedrake","Massachusetts Institute of Technology,MIT",Robot Manipulation,"In order for a bimanual robot to manipulate an object that is held by both hands, it must construct motion plans such that the transformation between its end effectors remains fixed. This amounts to complicated nonlinear equality constraints in the configuration space, which are difficult for trajectory optimizers. In addition, the set of feasible configurations becomes a measure zero set, which presents a challenge to sampling-based motion planners. We leverage an analytic solution to the inverse kinematics problem to parametrize the configuration space, resulting in a lower-dimensional representation where the set of valid configurations has positive measure. We describe how to use this parametrization with existing motion planning algorithms, including sampling-based approaches, trajectory optimizers, and techniques that plan through convex inner-approximations of collision-free space."
HEGN: Hierarchical Equivariant Graph Neural Network for 9DoF Point Cloud Registration,"Adam Misik, Driton Salihu, Xin Su, Heike Brock, Eckehard Steinbach","Siemens Technology, Technical University Munich,Technical University Munich,Technical University of Munich,Siemens AG",Robot Vision,"Given its wide application in robotics, point cloud registration is a widely researched topic. Conventional methods aim to find a rotation and translation that align two point clouds in 6 degrees of freedom (DoF). However, certain tasks in robotics, such as category-level pose estimation, involve non-uniformly scaled point clouds, requiring a 9DoF transform for accurate alignment. We propose HEGN, a novel equivariant graph neural network for 9DoF point cloud registration. HEGN utilizes equivariance to rotation, translation, and scaling to estimate the transformation without relying on point correspondences. Based on graph representations for both point clouds, we extract equivariant node features aggregated in their local, cross-, and global context. In addition, we introduce a novel node pooling mechanism that leverages the cross-context importance of nodes to pool the graph representation. By repeating the feature extraction and node pooling, we obtain a graph hierarchy. Finally, we determine rotation and translation by aligning equivariant features aggregated over the graph hierarchy. To estimate scaling, we leverage scale information contained in the vector norm of the equivariant features. We evaluate the effectiveness of HEGN through experiments with the synthetic ModelNet40 dataset and the real-world ScanObjectNN dataset. The results show the superior performance of HEGN in 9DoF point cloud registration and its competitive performance in conventional 6DoF point cloud registration."
Deep Evidential Uncertainty Estimation for Semantic Segmentation under Out-Of-Distribution Obstacles,"Siddharth Ancha, Philip Osteen, Nicholas Roy","Massachusetts Institute of Technology,U.S. Army Research Laboratory",Robot Vision,"In order to navigate safely and reliably in novel environments, robots must estimate perceptual uncertainty when confronted with out-of-distribution (OOD) obstacles not seen in training data. We present a method to accurately estimate pixel-wise uncertainty in semantic segmentation without requiring real or synthetic OOD examples at training time. From a shared per-pixel latent feature representation, a classification network predicts a categorical distribution over semantic labels, while a normalizing flow estimates the probability density of features under the training distribution. The label distribution and density estimates are combined in a Dirichlet-based evidential uncertainty framework that efficiently computes epistemic and aleatoric uncertainty in a single neural network forward pass. Our method is enabled by three key contributions. First, we simplify the problem of learning a transformation to the training data density by starting from a fitted Gaussian mixture model instead of the conventional standard normal distribution. Second, we learn a richer and more expressive latent pixel representation to aid OOD detection by training a decoder to reconstruct input image patches. Third, we perform theoretical analysis of the loss function used in the evidential uncertainty framework and propose a principled objective that more accurately balances training the classification and density estimation networks. We demonstrate the accuracy of our uncertainty estimation approach under long-tail OOD obstacle classes for semantic segmentation in both off-road and urban driving environments."
Ultrafast Square-Root Filter-Based VINS,"Yuxiang Peng, Chuchu Chen, Guoquan Huang",University of Delaware,Robot Vision,"In this paper, we strongly advocate square-root covariance (instead of information) filtering for visual-inertial navigation, in particular on resource-constrained edge devices, because of its superior efficiency and numerical stability. Although Visual-Inertial Navigation Systems (VINS) have made tremendous progress in recent years, they still face resource stringency and numerical instability on embedded systems when imposing limited word length. To overcome these challenges, we develop an ultrafast and numerically-stable square-root filter (SRF)-based VINS algorithm (i.e., SR-VINS). The numerical stability of the proposed SR-VINS is inherited from the adoption of square-root covariance while the never-before-seen efficiency is largely enabled by the novel SRF update method that is based on our new permuted-QR (P-QR), which fully utilizes and properly maintains the upper triangular structure of the square-root covariance matrix. Furthermore, we choose a special ordering of the state variables which is amenable for (P-)QR operations in the SRF propagation and update and prevents unnecessary computation. The proposed SR-VINS is validated extensively through numerical studies, demonstrating that when the state-of-the-art (SOTA) filters have numerical difficulties, our SR-VINS has superior numerical stability, and remarkably, achieves efficient and robust performance on 32-bit single-precision float at a speed nearly twice as fast as the SOTA methods. We also conduct comprehensive real-world experiments to validate the efficiency, accuracy, and robustness of the proposed SR-VINS."
SeqTrack3D: Exploring Sequence Information for Robust 3D Point Cloud Tracking,"Yu Lin, Zhiheng Li, Yubo Cui, Zheng Fang",Northeastern University,Robot Vision,"3D single object tracking (SOT) is an important and challenging task for the autonomous driving and mobile robotics. Most existing methods perform tracking between two consecutive frames while ignoring the motion patterns of the target over a series of frames, which would cause performance degradation in scenes with sparse points. To break through this limitation, we introduce Sequence-to-Sequence tracking paradigm and a tracker named SeqTrack3D to capture target motion across continuous frames. Unlike previous methods that primarily adopted three strategies: matching two consecutive point clouds, predicting relative motion, or utilizing sequential point clouds to address feature degradation, our SeqTrack3D combines both historical point clouds and bounding box sequences. This novel approach ensures robust tracking by leveraging location priors from historical boxes, even in scenes with sparse points. Extensive experiments conducted on large-scale datasets show that SeqTrack3D achieves new state-of-the-art performances, improving by 6.00% on NuScenes and 14.13% on Waymo dataset."
NGEL-SLAM: Neural Implicit Representation-Based Global Consistent Low-Latency SLAM System,"Yunxuan Mao, Xuan Yu, Zhuqing Zhang, Kai Wang, Yue Wang, Rong Xiong, Yiyi Liao","Zhejiang University,HuaWei",Robot Vision,"Neural implicit representations have emerged as a promising solution for addressing the challenges of Simultaneous Localization and Mapping (SLAM) problems in indoor scenes. This paper presents NGEL-SLAM, a low-latency global consistent SLAM system that utilizes neural implicit scene representation. To ensure global consistency, our system incorporates loop closure in the tracking module and maintains a global consistent map by representing the scene using multiple neural implicit fields and performing a quick adjustment to the loop closure. The fast convergence and rapid response to loop closure make our system a truly low-latency system that achieves global consistency. The neural implicit representation enables the rendering of high-fidelity RGB-D images with the extraction of explicit, dense, and interactive surfaces. Experiments were conducted on both synthetic and real-world datasets to evaluate the effectiveness of the proposed approach. The results demonstrate the achieved tracking and mapping accuracy and low-latency performance of our system."
Universal Visual Decomposer: Long-Horizon Manipulation Made Easy,"Zichen Zhang, Yunshuang Li, Osbert Bastani, Abhishek Gupta, Dinesh Jayaraman, Yecheng Jason Ma, Luca Weihs","Allen Institute for AI,Univeresity of Pennsylvania,University of Pennsylvania,University of Washington",Robot Vision,"Real-world robotic tasks stretch over extended horizons and encompass multiple stages. Learning long-horizon manipulation tasks, however, is a long-standing challenge, and demands decomposing the overarching task into several manageable subtasks to facilitate policy learning and generalization to unseen tasks. Prior task decomposition methods require task-specific knowledge, are computationally intensive, and cannot readily be applied to new tasks. To address these shortcomings, we propose Universal Visual Decomposer (UVD), an off-the-shelf task decomposition method for visual long-horizon manipulation using pre-trained visual representations for robotic control. At a high level, UVD discovers subgoals by detecting phase shifts in the embedding space of the pre-trained representation. Operating purely on visual demonstrations without auxiliary information, UVD can effectively extract visual subgoals embedded in the videos, while incurring zero additional training cost on top of standard visuomotor policy training. Goal-conditioned policies learned with UVD-discovered subgoals exhibit significantly improved compositional generalization at test time to unseen tasks. Furthermore, UVD-discovered subgoals can be used to construct goal-based reward shaping that jump-starts temporally extended exploration for reinforcement learning. We extensively evaluate UVD on both simulation and real-world tasks, and in all cases, UVD substantially outperforms baselines across imitation and reinforcement learning settings on in-domain and out-of-domain task sequences alike, validating the clear advantage of automated visual task decomposition within the simple, compact UVD framework."
Autonomous Navigation with Online Replanning and Recovery Behaviors for Wheeled-Legged Robots Using Behavior Trees,"Alessio De Luca, Luca Muratore, Nikos Tsagarakis",Istituto Italiano di Tecnologia,Motion and Path Planning I,"Performing autonomous navigation in cluttered and unstructured terrains still remains a challenging task for legged and wheeled mobile robots. To accomplish such a task, online planners shall incorporate new terrain information perceived while the robot is moving within its environment. While hybrid mobility robots offer high flexibility in traversing challenging terrains by leveraging the advantages of both wheeled and legged locomotion, the effective hybrid planning of the mobility actions that transparently combine both modes of locomotion has not been extensively explored. In this work, we present a hierarchical online hybrid primitive-based planner for autonomous navigation with wheeled-legged robots. The framework is handled by a Behavior Tree (BT) and it takes into account recovery methods to deal with possible failures during the execution of the navigation/mobility plan. The framework was evaluated in multiple irregular and heavily cluttered simulated environments randomly generated and in real-world trials, using the CENTAURO robot platform. With these experiments, we demonstrated autonomous capabilities without any human intervention, even in case of collision or planner failures."
Signal Temporal Logic Neural Predictive Control,"Yue Meng, Chuchu Fan",Massachusetts Institute of Technology,Motion and Path Planning I,"Ensuring safety and meeting temporal specifications are critical challenges for long-term robotic tasks. Signal temporal logic (STL) has been widely used to systematically and rigorously specify these requirements. However, traditional methods of finding the control policy under those STL requirements are computationally complex and not scalable to high-dimensional or systems with complex nonlinear dynamics. Reinforcement learning (RL) methods can learn the policy to satisfy the STL specifications via hand-crafted or STL-inspired rewards, but might encounter unexpected behaviors due to ambiguity and sparsity in the reward. In this paper, we propose a method to directly learn a neural network controller to satisfy the requirements specified in STL. Our controller learns to roll out trajectories to maximize the STL robustness score in training. In testing, similar to Model Predictive Control (MPC), the learned controller predicts a trajectory within a planning horizon to ensure the satisfaction of the STL requirement in deployment. A backup policy is designed to ensure safety when our controller fails. Our approach can adapt to various initial conditions and environmental parameters. We conduct experiments on six tasks, where our method with the backup policy outperforms the classical methods (MPC, STL-solver), model-free and model-based RL methods in STL satisfaction rate, especially on tasks with complex STL specifications while being 10X-100X faster than the classical method"
Multi-Query TDSP for Path Planning in Time-Varying Flow Fields,"Ju Heon Lee, Chanyeol Yoo, Stuart Anstee, Robert Fitch","University of Technology Sydney,Defence Science and Technology Group",Motion and Path Planning I,"Many applications of path planning in time-varying flow fields, particularly in areas such as marine robotics and ship routing, can be modelled as instances of the time-varying shortest path (TDSP) problem. Although there are no known polynomial-time solutions to TDSP in general, our recent work has identified a tractable case where the flow is modelled as piecewise constant. Extending this method to allow for computational reuse in larger multi-query problems, however, requires additional thought. This paper shows that the piecewise-linear form of the cost function employed in previously work can be used to build an analogy of a shortest path tree, thereby enabling optimal concatenation of sub-problem solutions in the absence of an optimal substructure, and without uniform time discretisation. We present a framework for multi-query TDSP that finds an optimal path that passes through a defined sequence of waypoints and is computationally efficient. Performance comparison is provided in simulation that shows large (up to 100x) speedup compared to a naive approach. This result is significant for applications such as ship routing, where route evaluation is a desirable capability."
CTopPRM: Clustering Topological PRM for Planning Multiple Distinct Paths in 3D Environments,"Matej Novosad, Robert Pěnička, Vojtech Vonasek","Faculty of Electrical Engineering, Czech Technical University in,Czech Technical University in Prague",Motion and Path Planning I,"We propose a new method called Clustering Topological PRM (CTopPRM) for finding multiple distinct paths in 3D cluttered environments. Finding such distinct paths is useful in many applications. Among others, using multiple distinct paths is necessary for optimization-based trajectory planners where found trajectories are restricted to only a single homotopy class of a given path. Distinct paths can also be used to guide sampling-based motion planning and thus increase the effectiveness of planning in environments with narrow passages. Graph-based representation called roadmap is a common representation for path planning and also for finding multiple distinct paths. Yet, challenging environments with multiple narrow passages require a densely sampled roadmap to capture the connectivity of the environment. Searching such a dense roadmap for multiple paths is computationally too expensive. The majority of existing methods construct only a sparse roadmap which, however, struggles to find all distinct paths in challenging environments. To this end, we propose the CTopPRM which creates a sparse graph by clustering an initially sampled dense roadmap. Such a reduced roadmap allows fast identification of homotopically distinct paths captured in the dense roadmap. We show, that compared to the existing methods the CTopPRM improves the probability of finding all distinct paths by almost 20%, during same run-time. The source code of our method is released as an open-source package."
Stein Variational Guided Model Predictive Path Integral Control: Proposal and Experiments with Fast Maneuvering Vehicles,"Kohei Honda, Naoki Akai, Kosuke Suzuki, Mizuho Aoki, Hirotaka Hosogaya, Hiroyuki Okuda, Tatsuya Suzuki",Nagoya University,Motion and Path Planning I,"This paper presents a novel Stochastic Optimal Control (SOC) method based on Model Predictive Path Integral control (MPPI), named Stein Variational Guided MPPI (SVG-MPPI), designed to handle rapidly shifting multimodal optimal action distributions. While MPPI can find a Gaussian-approximated optimal action distribution in closed form, i.e., without iterative solution updates, it struggles with the multimodality of the optimal distributions. This is due to the less representative nature of the Gaussian. To overcome this limitation, our method aims to identify a target mode of the optimal distribution and guide the solution to converge to fit it. In the proposed method, the target mode is roughly estimated using a modified Stein Variational Gradient Descent (SVGD) method and embedded into the MPPI algorithm to find a closed-form ``mode-seeking'' solution that covers only the target mode, thus preserving the fast convergence property of MPPI. Our simulation and real-world experimental results demonstrate that SVG-MPPI outperforms both the original MPPI and other state-of-the-art sampling-based SOC algorithms in terms of path-tracking and obstacle-avoidance capabilities."
An Efficient Solution to the 2D Visibility Problem in Cartesian Grid Maps and Its Application in Heuristic Path Planning,"Ibrahim Ibrahim, Joris Gillis, Wilm Decré, Jan Swevers","KU Leuven,Katholieke Universiteit Leuven",Motion and Path Planning I,"This paper introduces a novel, lightweight method to solve the visibility problem for 2D grids. The proposed method evaluates the existence of lines-of-sight from a source point to all other grid cells in a single pass with no preprocessing and independently of the number and shape of obstacles. It has a compute and memory complexity of $mathcal{O}(n)$, where $n = n_{x}times n_{y}$ is the size of the grid, and requires at most ten arithmetic operations per grid cell. In the proposed approach, we use a linear first-order hyperbolic partial differential equation to transport the visibility quantity in all directions. In order to accomplish that, we use an entropy-satisfying upwind scheme that converges to the true visibility polygon as the step size goes to zero. This dynamic-programming approach allows the evaluation of visibility for an entire grid much faster than typical algorithms. We provide a practical application of our proposed algorithm by posing the visibility quantity as a heuristic and implementing a deterministic, local-minima-free path planner, setting apart the proposed planner from traditional methods. Lastly, we provide necessary algorithms and an open-source implementation of the proposed methods."
Efficient Clothoid Tree-Based Local Path Planning for Self-Driving Robots,"Minhyeong Lee, Dongjun Lee",Seoul National University,Motion and Path Planning I,"In this paper, we propose a real-time clothoid tree-based path planning for self-driving robots. Clothoids, curves that exhibit linear curvature profiles, play an important role in road design and path planning due to their appealing properties. Nevertheless, their real-time applications face considerable challenges, primarily stemming from the lack of a closed-form clothoid expression. To address these challenges, we introduce two innovative techniques: 1) an efficient and precise clothoid approximation using the Gauss-Legendre quadrature; and 2) a data-efficient decoder for interpolating clothoid splines that leverages the symmetry and similarity of clothoids. These techniques are demonstrated with numerical examples. The clothoid approximation ensures an accurate and smooth representation of the curve, and the clothoid spline decoder effectively accelerates the clothoid tree exploration by relaxing the problem constraints and reducing the problem size. Both techniques are integrated into our path planning algorithm and evaluated in various driving scenarios."
Decentralized Lifelong Path Planning for Multiple Ackerman Car-Like Robots,"Teng Guo, Jingjin Yu",Rutgers University,Motion and Path Planning I,"Path planning for multiple non-holonomic robots in continuous domains constitutes a difficult robotics challenge with many applications. Despite significant recent progress on the topic, computationally efficient and high-quality solutions are lacking, especially in lifelong settings where robots must continuously take on new tasks. In this work, we make it possible to extend key ideas enabling state-of-the-art (SOTA) methods for multi-robot planning in discrete domains to the motion planning of multiple Ackerman (car-like) robots in lifelong settings, yielding high-performance centralized and decentralized planners. Our planners compute trajectories that allow the robots to reach precise SE(2) goal poses. The effectiveness of our methods is thoroughly evaluated and confirmed using both simulation and real-world experiments."
Energy-Aware Ergodic Search: Continuous Exploration for Multi-Agent Systems with Battery Constraints,"Adam Seewald, Cameron Lerch, Marvin Chancán, Aaron Dollar, Ian Abraham",Yale University,Motion and Path Planning I,"Continuous exploration without interruption is important in scenarios such as search and rescue and precision agriculture, where consistent presence is needed to detect events over large areas. Ergodic search already derives continuous trajectories in these scenarios so that a robot spends more time in areas with high information density. However, existing literature on ergodic search does not consider the robot's energy constraints, limiting how long a robot can explore. In fact, if the robots are battery-powered, it is physically not possible to continuously explore on a single battery charge. Our paper tackles this challenge, integrating ergodic search methods with energy-aware coverage. We trade off battery usage and coverage quality, maintaining uninterrupted exploration by at least one agent. Our approach derives an abstract battery model for future state-of-charge estimation and extends canonical ergodic search to ergodic search under battery constraints. Empirical data from simulations and real-world experiments demonstrate the effectiveness of our energy-aware ergodic search, which ensures continuous exploration and guarantees spatial coverage."
Development of Variable Transmission Series Elastic Actuator for Hip Exoskeletons,"Tianci Wang, Hao Wen, Zaixin Song, Zhiping Dong, Chunhua Liu",City University of Hong Kong,Actuation,"Series Elastic Actuator-based exoskeleton can offer precise torque control and transparency when interacting with human wearers. Accurate control of SEA-produced torques ensures the wearerâ€™s voluntary motion and supports the implementation of multiple assistive paradigms. In this paper, a novel variable transmission series elastic actuator (VTSEA) is developed to meet torque-speed requirements in different exoskeleton-assisted locomotion modes, such as running, walking, sit-to-stand, and stand-to-sit. The VTSEA features a SEA-coupled variable transmission ratio adjusting mechanism and works between three discrete levels of transmission ratio depending on the userâ€™s initiative. The proposed prototype can also improve transparency in human-robot interaction. Also, an accurate torque controller with inertial compensation is developed for the VTSEA via the singular perturbation theory, and its stability is proved. The feasibility of the proposed VTSEA prototype and the precise output torque performance of VTSEA are verified by experiments."
Optimization of Mono and Bi-Articular Parallel Elastic Elements for a Robotic Arm Performing a Pick-And-Place Task,"Maxime Marchal, Raphaël Furnémont, Bram Vanderborght, Ghiles Mostafaoui, Tom Verstraten","Vrije Universiteit Brussel,CNRS, University of CergyPontoise, ENSEA",Actuation,"Actuation concepts such as Series Elastic Actuation (SEA), Parallel Elastic Actuation (PEA), and Biarticular Actuation (BA), which introduce elastic elements into the structure, have the potential to reduce the electrical energy consumption of a robot. This letter presents an optimization of the arrangement of springs for a 3 degrees of freedom robotic arm, with the aim of decreasing the electrical energy consumption for a given pick-and-place task. Through simulations and experimental validation, we show that the optimal configuration in terms of electrical energy consumption and complexity consists of rigid actuation on joint 1 and PEAs on joints 2 and 3. With this configuration, root mean square (RMS) and peak load torques for a specific pick-and-place task can be reduced respectively by up to 43% and 44% for joint 2, and by 15% and 21% for joint 3 compared to the configuration without springs."
A Novel Compact Design of a Lever-Cam-Based Variable Stiffness Actuator: LC-VSA,"Hongxi Zhu, Ulrike Thomas",Chemnitz University of Technology,Actuation,"The safer interaction between humans and robots is one of the challenges in robotics. To protect humans and robots from impact, researchers have developed many different soft robots, which incorporate mechanical springs into their joints. The forthcoming generation of soft robots necessitates adaptable joint stiffness to accommodate various tasks. Consequently, the development of variable stiffness joints (VSA) has become crucial. Among the prevalent approaches for stiffness adjustment, lever mechanisms have been implemented in numerous variable stiffness joints. Nonetheless, the integration of the lever technology into VSA often faces challenges in achieving a compact design. This paper introduces a mechanically compact design for a novel lever-cam-based variable stiffness joint."
Design and Modeling of a Compact Serial Variable Stiffness Actuator (SVSA-III) with Linear Stiffness Profile,"Shuowen Yi, Siyu Liu, Junbei Liao, Zhao Guo","Wuhan university,the School of Power and Mechanical Engineering, Wuhan University,Wuhan University",Actuation,"Variable stiffness actuator (VSA) can imitate natural muscles in their compliance capbility, which can provide flexible adaptability for robots, improving the safety of robots interacting with the environment or human. This paper presents a new compact serial variable stiffness actuator ((SVSA-III)) with linear stiffness profile based on symmetrical variable lever arm mechanism. The stiffness motor is used to regulate the position of the pivot located on the Archimedean Spiral Relocation Mechanism (ASRM), so that the stiffness of the actuator can be adjusted (softening or hardening). By designing the lever length, the range of stiffness adjustment can change from 0.3Nm/degree to therotical infinity. Moreover, the continuous linear stiffness profile of the actuator can be customized by solving the transcendental equation of the relationship between the actuator stiffness and the rotation angle of the stiffness motor. SVSA-III has the advantages of compact structure, wide-range stiffness regulation, reduced control difficulty, and linear stiffness profile. Two experiments of step response and stiffness tracking have proved the high accuracy and fast response for both theoretical stiffness and position adjustment."
Optimally Controlling the Timing of Energy Transfer in Elastic Joints: Experimental Validation of the Bi-Stiffness Actuation Concept,"Edmundo Pozo FortuniÄ‡, Mehmet Can Yildirim, Dennis Ossadnik, Abdalla Swikir, Saeed Abdolshah, Sami Haddadin","Technical University of Munich,KUKA Deutschland GmbH",Actuation,"Elastic actuation taps into elastic elements' energy storage for dynamic motions beyond rigid actuation. While Series Elastic Actuators (SEA) and Variable Stiffness Actuators (VSA) are highly sophisticated, they do not fully provide control over energy transfer timing. To overcome this problem on the basic system level, the Bi-Stiffness Actuation (BSA) concept was recently proposed. Theoretically, it allows for full link decoupling, while simultaneously being able to lock the spring in the drive train via a switch-and-hold mechanism. Thus, the user would be in full control of the potential energy storage and release timing. In this work, we introduce an initial proof-of-concept of Bi-Stiffness-Actuation in the form of a 1-DoF physical prototype, which is implemented using a modular testbed. We present a hybrid system model, as well as the mechatronic implementation of the actuator. We corroborate the feasibility of the concept by conducting a series of hardware experiments using an open-loop control signal obtained by trajectory optimization. Here, we compare the performance of the prototype with a comparable SEA implementation. We show that BSA outperforms SEA 1) in terms of maximum velocity at low final times and 2) in terms of the movement strategy itself: The clutch mechanism allows the BSA to generate consistent launch sequences while the SEA has to rely on lengthy and possibly dangerous oscillatory swing-up motions."
Experimental Comparison of Pinwheel and Non-Pinwheel Designs of 3D-Printed Cycloidal Gearing for Robotics,"Wesley Roozing, Glenn Roozing","University of Twente,Auto Elect B.V.",Actuation,"Recent trends in robotic actuation have highlighted the need for low-cost, high performance, and efficient gearing. We present an experimental study comparing pinwheel and non-pinwheel designs of cycloidal gearing. The open source designs are 3D-printable combined with off-the-shelf components, achieving a high performance-to-cost ratio. Extensive experimental data is presented, that compares two prototypes on run-in behaviour and a number of quantitative metrics including transmission error, play, friction, and stiffness. Furthermore, we assess overall actuator performance through position control experiments, and a 10-hour endurance test. The results show strong performance characteristics, and crucially, suggest that non-pinwheel designs of cycloidal gearing can be a lower complexity and cost alternative to classical pinwheel designs, while offering similar performance."
Design and Optimization of an Origami-Inspired Foldable Pneumatic Actuator,"Huaiyuan Chen, Yiyuan Ma, Weidong Chen",Shanghai Jiao Tong University,Actuation,"A novel origami-inspired foldable pneumatic actuator is proposed in this letter to satisfy the comprehensive requirements in wearable assistive application. The pneumatic actuator combines the origami structure of designed Quadrangular-Expand pattern and the foldable pneumatic bellow. Integrated origami structure regulates the motion of actuator with high contraction ratio and enables accurate modeling. The origami framework also improves the strength of bearing negative pressure, and thus can provide bidirectional actuation. The workflow including design, fabrication and mathematic modeling of the pneumatic actuator is presented in detail. Based on the actuator modeling, the multi-objective optimization for parameters using Genetic Algorithm is then conducted to obtain the trade-off design. The verifications for static characteristics of output torque, as well as the dynamic characteristics of power density, mechanical efficiency and frequency response, have been conducted. In summary, the proposed actuator is powerful and energy-efficient."
A Non-Magnetic Dual-Mode Linear Pneumatic Actuator: Initial Design and Assessment,"Timothée Portha, Laurent Barbé, Francois Geiskopf, Jonathan Vappou, Pierre Renaud","University of Strasbourg,University of Strasbourg, ICube CNRS,INSA de Strasbourg,CNRS, Universite de Strasbourg,ICube",Actuation,"A pneumatic linear actuator is presented and evaluated. Designed to operate in demanding environments such as MRI, it is developed to be used with two motion control modes: 1) a step-by-step mode with tooth-based gripping to ensure precision, 2) a continuous mode available locally for fine positioning. The actuator can also be disengaged to enable direct handling by an operator, for example for comanipulation. The design is presented. A prototype, developed in the medical context, is implemented and characterized. A specific step-by-step control sequence is then elaborated based on its characterization. Testing of the dual-mode actuation is finally described. The complementarity between the two motion modes and possible adaptations of the original design are discussed."
Variable Stiffness Floating Spring Leg: Performing Net-Zero Energy Cost Tasks Not Achievable Using Fixed Stiffness Springs,"Sung Kim, David Braun",Vanderbilt University,Actuation,"Sitting down and standing up from a chair and, similarly, moving heavy objects up and down between factory lines are examples of cyclic tasks that require large forces but little to no net mechanical energy. Motor-driven artificial limbs and industrial robots can help humans do these tasks, but motors require energy to provide force even if they supply no net mechanical energy. Springs are energetically conservative mechanical elements useful for building robots that require no energy when performing cyclic tasks. However, conventional springs can be limited by their non-customizable force-deflection behavior -- for example, when they cannot meet the force demand despite storing enough energy to perform a cyclic task. Variable stiffness springs are a special type of spring with customizable force-deflection behavior, but most typical variable stiffness springs require energy to amplify force similar to motors. In this paper, we introduce a new type of variable stiffness spring design which is energetically conservative despite having a customizable force-deflection behavior. We present the theory of these springs and demonstrate their utility in performing a net-zero mechanical energy cost lifting task that requires force amplification and as such is not realizable using conventional springs."
Accurate Kinematic Modeling Using Autoencoders on Differentiable Joints,"Nikolas Jakob Wilhelm, Sami Haddadin, Rainer Burgkart, Patrick Van Der Smagt, Maximilian Karl","Technical University of Munich,Technische Universität München,Volkswagen Group,Volkswagen AG",Kinematics,"In robotics and biomechanics, accurately determining joint parameters and computing the corresponding forward and inverse kinematics are critical yet often challenging tasks, especially when dealing with highly individualized and partly unknown systems. This paper unveils a cutting-edge kinematic optimizer, underpinned by an autoencoder-based architecture, to address these challenges. Utilizing a neural network, our approach simulates inverse kinematics, converting measurement data into joint-specific parameters during encoding, enabling a stable optimization process. These parameters are subsequently processed through a predefined, differentiable forward kinematics model, resulting in a decoded representation of the original data. Beyond offering a comprehensive solution to kinematics challenges, our method also unveils previously unidentified joint parameters. Real experimental data from knee and hand joints validate the optimizer's efficacy. Additionally, our optimizer is multifunctional: it streamlines the modeling and automation of kinematics and enables a nuanced evaluation of diverse modeling techniques. By assessing the differences in reconstruction losses, we illuminate the merits of each approach. Collectively, this preliminary study signifies advancements in kinematic optimization, with potential applications spanning both biomechanics and robotics."
A Miniature Water Jumping Robot Based on Accurate Interaction Force Analysis,"Jihong Yan, Xin Zhang, Kai Yang, Jie Zhao",Harbin Institute of Technology,Kinematics,"Water jumping motion extends the robot's movement space and flexibility. However, the jumping performance is influenced by multiple factors such as driving force, rowing trajectory and robot structure. The interaction force between the robot and water surface is complicated due to water deformation, and the difficulty of the water jumping increases with the robot's scale. This paper designs a miniature water jumping robot with rowing driving legs. The hydrodynamic model between driving legs and water is established based on the modified Wagner theory with consideration of water surface deformation. Particularly, the dynamic model of the robot for the whole jumping process is also developed relate to multiple factors. Then the jumping performance is improved by optimizing the energy storage modality, rowing trajectory and supporting leg shapes through the theoretical analysis and experiments. The fabricated robot weights 91 g, and its length, width and height are 220 mm, 410 mm and 95 mm respectively. The maximum water jumping height and distance are 241 and 965 mm."
Jerk-Limited Traversal of One-Dimensional Paths and Its Application to Multi-Dimensional Path Tracking,"Jonas Kiemel, Torsten Kroeger","Karlsruhe Institute of Technology,Karlsruher Institut für Technologie (KIT)",Kinematics,"In this paper, we present an iterative method to quickly traverse multi-dimensional paths considering jerk constraints. As a first step, we analyze the traversal of each individual path dimension. We derive a range of feasible target accelerations for each intermediate waypoint of a one-dimensional path using a binary search algorithm. Computing a trajectory from waypoint to waypoint leads to the fastest progress on the path when selecting the highest feasible target acceleration. Similarly, it is possible to calculate a trajectory that leads to minimum progress along the path. This insight allows us to control the traversal of a one-dimensional path in such a way that a reference path length of a multi-dimensional path is approximately tracked over time. In order to improve the tracking accuracy, we propose an iterative scheme to adjust the temporal course of the selected reference path length. More precisely, the temporal region causing the largest position deviation is identified and updated at each iteration. In our evaluation, we thoroughly analyze the performance of our method using seven-dimensional reference paths with different path characteristics. We show that our method manages to quickly traverse the reference paths and compare the required traversing time and the resulting path accuracy with other state-of-the-art approaches."
The Kinematics of Constant Curvature Continuum Robots through Three Segments,"Yucheng Li, David H. Myszka, Andrew Murray",University of Dayton,Kinematics,"This letter presents an investigation into the mathematical relationships between the positions and orientations at the segment tips of a piecewise constant curvature (PCC) continuum robot with up to three segments. For one-segment, a reachability criterion is proposed, which simplifies the calculation of the neighboring orientation. For two-segments, a reachability criterion is proposed and the redundancy of its inverse kinematics solution is found, establishing a circle of tip locations. For three-segments, the redundancy of the inverse kinematics includes tips that lie on a sphere providing a closed-form solution to the inverse kinematics problem. These relationships are derived from the unique characteristics of the bisecting plane of a single segment. The degenerate cases for the solutions are also addressed. These outcomes stem from a specific PCC parametrization, with implications extending to the general PCC model. Note that this study is grounded solely in simulation."
An Analytic Solution to the 3D CSC Dubins Path Problem,"Victor Montano, Nikhil V. Navkar, Aaron T. Becker","University of Houston,Hamad Medical Corporation",Kinematics,"We present an analytic solution to the 3D Dubins path problem for paths composed of an initial circular arc, a straight component, and a final circular arc. These are commonly called CSC paths. By modeling the start and goal configurations of the path as the base frame and final frame of an RRPRR manipulator, we treat this as an inverse kinematics problem. The kinematic features of the 3D Dubins path are built into the constraints of our manipulator model. Furthermore, we show that the number of solutions is not constant, with up to seven valid CSC path solutions even in non-singular regions. An implementation of solution is available at https://github.com/aabecker/dubins3D"
Polytope-Based Continuous Scalar Performance Measure with Analytical Gradient for Effective Robot Manipulation,"Keerthi Sagar Somenedi Nageswara Rao, Stephane Caro, Taskin Padir, Philip Long","Irish Manufacturing Research Limited, Ireland,CNRS/LS,N,Northeastern University,Atlantic Technological University",Kinematics,"Performance measures are essential to characterize a robot's ability to carry out manipulation tasks. Generally, these measures examine the systemâ€™s kinematic transformations from configuration to task space, but the Capacity margin, a polytope based kinetostatic index, provides additionally, both an accurate evaluation of the twist and wrench capacities of a robotic manipulator. However, this index is the minimum of a discontinuous scalar function leading to difficulties when computing gradients thereby rendering it unsuitable for online numerical optimization. In this letter, we propose a novel performance index using an approximation of the capacity margin. The proposed index is continuous and differentiable, characteristics that are essential for modelling smooth and predictable system behavior. We demonstrate the effectiveness both as a constraint and objective function for inverse kinematics optimization. Moreover, to show its practical use, two opposing robot architectures are chosen: (i) Serial robot - Universal Robot- UR5 (6-dof); Rethink Robotics- Sawyer Robot (7-dof) and (ii) Parallel manipulator - Cable Driven Parallel Robot to validate the results through both simulation and experiments. A visual representation of the performance index is also presented."
Kinematic Optimization of a Robotic Arm for Automation Tasks with Human Demonstration,"Inbar Meir, Avital Bechar, Avishai Sintov","Tel Aviv University,Agricultural Research Organization,Tel-Aviv University",Kinematics,"Robotic arms are highly common in various automation processes such as manufacturing lines. However, these highly capable robots are usually degraded to simple repetitive tasks such as pick-and-place. On the other hand, designing an optimal robot for one specific task consumes large resources of engineering time and costs. In this paper, we propose a novel concept for optimizing the fitness of a robotic arm to perform a specific task based on human demonstration. Fitness of a robot arm is a measure of its ability to follow recorded human arm and hand paths. The optimization is conducted using a modified variant of the Particle Swarm Optimization for the robot design problem. In the proposed approach, we generate an optimal robot design along with the required path to complete the task. The approach could reduce the time-to-market of robotic arms and enable the standardization of modular robotic parts. Novice users could easily apply a minimal robot arm to various tasks. Two test cases of common manufacturing tasks are presented yielding optimal designs and reduced computational effort by up to 92%."
Enhancing Motion Trajectory Segmentation of Rigid Bodies Using a Novel Screw-Based Trajectory-Shape Representation,"Arno Verduyn, Maxim Vochten, Joris De Schutter",KU Leuven,Kinematics,"Trajectory segmentation refers to dividing a trajectory into meaningful consecutive sub-trajectories. This paper focuses on trajectory segmentation for 3D rigid-body motions. Most segmentation approaches in the literature represent the bodyâ€™s trajectory as a point trajectory, considering only its translation and neglecting its rotation. We propose a novel trajectory representation for rigid-body motions that incorporates both translation and rotation, and additionally exhibits several invariant properties. This representation consists of a geometric progress rate and a third-order trajectory-shape descriptor. Concepts from screw theory were used to make this representation time-invariant and also invariant to the choice of body reference point. This new representation is validated for a self-supervised segmentation approach, both in simulation and using real recordings of human-demonstrated pouring motions. The results show a more robust detection of consecutive sub-motions with distinct features and a more consistent segmentation compared to conventional representations. We believe that other existing segmentation methods may benefit from using this trajectory representation to improve their invariance."
Model Reduction in Soft Robotics Using Locally Volume-Preserving Primitives,"Yi Xu, Gregory Chirikjian",National University of Singapore,Kinematics,"A new, and extremely efficient, computational modeling paradigm is introduced here for specific finite elasticity problems that arise in the context of soft robotics. Whereas continuum mechanics is a very classical area of study that is broadly applicable throughout engineering, and significant effort has been devoted to the development of intricate constitutive models for finite elasticity, we show that for the most part, the isochoric (locally volume-preserving) constraint dominates behavior, and this can be built into closed-form kinematic deformation fields before even considering other aspects of constitutive modeling. We therefore focus on developing and applying primitive deformations that each observe this constraint. By composing a wide enough variety of such deformations, many of the most common behaviors observed in soft robots can be replicated. Case studies include isotropic objects subjected to different boundary conditions, a non-isotropic helically-reinforced tube, and a not-purely-kinematic scenario with gravity loading. We show that this method is at least 50 times faster than the ABAQUS implementation of the finite element method (FEM), and has speed comparable with the real-time FEM framework SOFA. Experiments show that both our method and ABAQUS have approximately 10% error relative to experimentally measured displacements, as well as to each other. And our method outperforms SOFA when the deformation is highly nonlinear."
Automatic Configuration of Multi-Agent Model Predictive Controllers Based on Semantic Graph World Models,"Koen de Vos, Elena Torta, Herman Bruyninckx, Cesar Lopez, Marinus Jacobus Gerardus Van De Molengraft","Eindhoven University of Technology,KU Leuven,University of Technology Eindhoven",Multi-Robot Systems IV,"We propose a shared semantic map architecture to construct and configure Model Predictive Controllers (MPC) dynamically, that solve navigation problems for multiple robotic agents sharing parts of the same environment. The navigation task is represented as a sequence of semantically labeled areas in the map, that must be traversed sequentially, i.e. a route. Each semantic label represents one or more constraints on the robotsâ€™ motion behaviour in that area. The advantages of this approach are: (i) an MPC-based motion controller in each individual robot can be (re-)configured, at runtime, with the locally and temporally relevant parameters; (ii) the application can influence, also at runtime, the navigation behaviour of the robots, just by adapting the semantic labels; and (iii) the robots can reason about their need for coordination, through analyzing over which horizon in time and space their routes overlap. The paper provides simulations of various representative situations, showing that the approach of runtime configuration of the MPC drastically decreases computation time, while retaining task execution performance similar to an approach in which each robot always includes all other robots in its MPC computations."
Meta-Reinforcement Learning Based Cooperative Surface Inspection of 3D Uncertain Structures Using Multi-Robot Systems,"Junfeng Chen, Yuan Gao, Junjie Hu, Fuqin Deng, Tin Lun Lam","Peking University,Shenzhen Institute of Artificial Intelligence and Robotics for S,The Chinese University of Hong Kong, Shenzhen",Multi-Robot Systems IV,"This paper presents a decentralized cooperative motion planning approach for surface inspection of 3D structures which includes uncertainties like size, number, shape, position, using multi-robot systems (MRS). Given that most of existing works mainly focus on surface inspection of single and fully known 3D structures, our motivation is two-fold: first, 3D structures separately distributed in 3D environments are complex, therefore the use of MRS intuitively can facilitate an inspection by fully taking advantage of sensors with different capabilities. Second, performing the aforementioned tasks when considering uncertainties is a complicated and time-consuming process because we need to explore, figure out the size and shape of 3D structures and then plan surface-inspection path. To overcome these challenges, we present a meta-learning approach that provides a decentralized planner for each robot to improve the exploration and surface inspection capabilities. The experimental results demonstrate our method can outperform other methods by approximately 10.5%-27% on success rate and 70%-75% on inspection speed."
Decentralized Multi-Agent Trajectory Planning in Dynamic Environments with Spatiotemporal Occupancy Grid Maps,"Siyuan Wu, Gang Chen, Moji Shi, Javier Alonso-Mora",Delft University of Technology,Multi-Robot Systems IV,"This paper proposes a decentralized trajectory planning framework for the collision avoidance problem of multiple micro aerial vehicles (MAVs) in environments with static and dynamic obstacles. The framework utilizes spatiotemporal occupancy grid maps (SOGM), which forecast the occupancy status of neighboring space in the near future, as the environment representation. Based on this representation, we extend the kinodynamic A* and the corridor-constrained trajectory optimization algorithms to efficiently tackle static and dynamic obstacles with arbitrary shapes. Collision avoidance between communicating robots is integrated by sharing planned trajectories and projecting them onto the SOGM. The simulation results show that our method achieves competitive performance against state-of-the-art methods in dynamic environments with different numbers and shapes of obstacles. Finally, the proposed method is validated in real experiments."
Communicating Intent As Behaviour Trees for Decentralised Multi-Robot Coordination,"Rhett Hull, Diluka Prasanjith Moratuwage, Emily Scheide, Robert Fitch, Graeme Best","University of Technology Sydney,Oregon State University",Multi-Robot Systems IV,"We propose a decentralised multi-robot coordination algorithm that features a rich representation for encoding and communicating each robotâ€™s intent. This representation for â€œintent messagesâ€ enables improved coordination behaviour and communication efficiency in difficult scenarios, such as those where there are unknown points of contention that require negotiation between robots. Each intent message is an adaptive policy that conditions on identified points of contention that conflict with the intentions of other robots. These policies are concisely expressed as behaviour trees via algebraic logic simplification, and are interpretable by robot teammates and human operators. We propose this intent representation in the context of the Dec-MCTS online planning algorithm for decentralised coordination. We present results for a generalised multi-robot orienteering domain that show improved plan convergence and coordination performance over standard Dec-MCTS enabled by the intent representationâ€™s ability to encode and facilitate negotiation over points of contention."
Partial Belief Space Planning for Scaling Stochastic Dynamic Games,"Kamran Vakil, Mela Coffey, Alyssa Pierson",Boston University,Multi-Robot Systems IV,"This paper presents a method to reduce computations for stochastic dynamic games with game-theoretic belief space planning through partially propagating beliefs. Complex interactions in scenarios such as surveillance, herding, and racing can be modeled using game-theoretic frameworks in the belief space. Stochastic dynamic games can be solved to a local Nash Equilibrium using a game-theoretic belief space variant of an iterative Linear Quadratic Gaussian (iLQG). However, the scalability of this method suffers due to the large dimensionality of beliefs which the iLQG must propagate. We examine the utility of partial belief space propagation, which allows polynomial runtime to decrease. We validate our findings through simulations and hardware implementation."
Decentralized Multi-Agent Active Search and Tracking When Targets Outnumber Agents,"Arundhati Banerjee, Jeff Schneider",Carnegie Mellon University,Multi-Robot Systems IV,"Multi-agent multi-target tracking has a wide range of applications, including wildlife patrolling, security surveillance or environment monitoring. Such algorithms often make restrictive assumptions: the number of targets and/or their initial locations may be assumed known, or agents may be pre-assigned to monitor disjoint partitions of the environment, reducing the burden of exploration. This also limits applicability when there are fewer agents than targets, since agents are unable to continuously follow the targets in their fields of view. Multi-agent tracking algorithms additionally assume inter-agent synchronization of observations, or the presence of a central controller to coordinate joint actions. Instead, we focus on the setting of decentralized multi-agent, multi-target, simultaneous active search-and-tracking with asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a sequential monte carlo implementation of the Probability Hypothesis Density filter for posterior inference combined with Thompson sampling for decentralized multi-agent decision making. We compare different action selection policies, focusing on scenarios where targets outnumber agents. In simulation, we demonstrate that DecSTER is robust to unreliable inter-agent communication and outperforms information-greedy baselines in terms of the Optimal Sub-Pattern Assignment (OSPA) metric for different numbers of targets and varying teamsizes."
Multi-Robot Autonomous Exploration and Mapping under Localization Uncertainty with Expectation-Maximization,"Yewei Huang, Xi Lin, Brendan Englot",Stevens Institute of Technology,Multi-Robot Systems IV,"We propose an autonomous exploration algorithm designed for decentralized multi-robot teams, which takes into account map and localization uncertainties of range-sensing mobile robots. Virtual landmarks are used to quantify the combined impact of process noise and sensor noise on map uncertainty. Additionally, we employ an iterative expectation-maximization inspired algorithm to assess the potential outcomes of both a local robotâ€™s and its neighborsâ€™ next-step actions. To evaluate the effectiveness of our framework, we conduct a comparative analysis with state-of-the-art algorithms. The results of our experiments show the proposed algorithmâ€™s capacity to strike a balance between curbing map uncertainty and achieving efficient task allocation among robots."
Optimal Task Allocation for Heterogeneous Multi-Robot Teams with Battery Constraints,"Álvaro Calvo, Jesus Capitan",University of Seville,Multi-Robot Systems IV,"This paper presents a novel approach to optimal multi-robot task allocation in heterogeneous teams of robots. When robots have heterogeneous capabilities and there are diverse objectives and constraints to comply with, computing optimal plans can become especially hard. Moreover, we increase the problem complexity by: 1) considering battery-limited robots that need to schedule recharges; 2) tasks that can be decomposed into multiple fragments; and 3) multi-robot tasks that need to be executed by a coalition synchronously. We define a new problem for heterogeneous multi-robot task allocation and formulate it as a Mixed-Integer Linear Program that includes all the aforementioned features. Then we use an off-the-shelf solver to show the type of optimal solutions that our planner can produce and assess its performance in random scenarios. Our method, which is released as open-source code, represents a first step to formalize and analyze a complex problem that has not been solved in the state of the art."
Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation,"Steve Paul, Nathan Maurer, Souma Chowdhury","University of Connecticut,University at Buffalo,University at Buffalo, State University of New York",Multi-Robot Systems IV,"Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and efficient decision-making, which is often achieved using heuristics-aided methods such as genetic algorithms, auction-based methods, and bipartite graph matching methods. These methods often assume a form that lends better explainability compared to an end-to-end (learnt) neural network based policy for MRTA. However, deriving suitable heuristics can be tedious, risky and in some cases impractical if problems are too complex. This raises the question: can these heuristics be learned? To this end, this paper particularly develops a Graph Reinforcement Learning (GRL) framework to learn the heuristics or incentives for a bipartite graph matching approach to MRTA. Specifically a Capsule Attention policy model is used to learn how to weight task/robot pairings (edges) in the bipartite graph that connects the set of tasks to the set of robots. The original capsule attention network architecture is fundamentally modified by adding encoding of robots' state graph, and two Multihead Attention based decoders whose output are used to construct a LogNormal distribution matrix from which positive bigraph weights can be drawn. The performance of this new bigraph matching approach augmented with a GRL-derived incentive is found to be at par with the original bigraph matching approach that used expert-specified heuristics, with the former offering notable robustness benefits. During training, the learned incentive policy is found to get initially closer to the expert-specified incentive and then slightly deviate from its trend."
Bag of Views: An Appearance-Based Approach to Next-Best-View Planning for 3D Reconstruction,"Sara Hatami Gazani, Matthew Tucsok, Iraj Mantegh, Homayoun Najjaran","University of Victoria,University of British Columbia,National Research Council Canada",Visual Perception and Learning I,"UAV-based intelligent data acquisition for 3D reconstruction and monitoring of infrastructure has experienced an increasing surge of interest due to recent advancements in image processing and deep learning-based techniques. View planning is an essential part of this task that dictates the information capture strategy and heavily impacts the quality of the 3D model generated from the captured data. Recent methods have used prior knowledge or partial reconstruction of the target to accomplish view planning for active reconstruction; the former approach poses a challenge for complex or newly identified targets while the latter is computationally expensive. In this work, we present Bag-of-Views (BoV), a fully appearance-based model used to assign utility to the captured views for both offline dataset refinement and online next-best-view (NBV) planning applications targeting the task of 3D reconstruction. With this contribution, we also developed the View Planning Toolbox (VPT), a lightweight package for training and testing machine learning-based view planning frameworks, custom view dataset generation of arbitrary 3D scenes, and 3D reconstruction. Through experiments which pair a BoV-based reinforcement learning model with VPT, we demonstrate the efficacy of our model in reducing the number of required views for high-quality reconstructions in dataset refinement and NBV planning."
CopperTag: A Real-Time Occlusion-Resilient Fiducial Marker,"Xu Bian, Wenzhao Chen, Xiaoyu Tian, Donglai Ran","Xi’an Jiaotong University,Youibot Robotics Co., Ltd.,Carnegie Mellon University",Visual Perception and Learning I,"Fiducial markers, like AprilTag and ArUco, are extensively utilized in robotics applications within industrial environments, encompassing navigation, docking, and object grasping tasks. However, in contrast to controlled laboratory conditions, markers installed in factory grounds or equipment surfaces, often face challenges like damage or contamination. These issues can lead to compromised marker integrity, resulting in reduced detection reliability. To address this challenge, we propose a novel fiducial marker called CopperTag, which incorporates circular and square elements to create a robust occlusion-resistant pattern. The CopperTag detection process relies on three fundamental steps: firstly, extracting all lines from the image; secondly, identifying corners; and lastly, searching for quadrilateral candidate regions using ellipses and nearby corners. The Reed-Solomon (RS) algorithm is utilized for both encoding and decoding the information content. This algorithm possesses the ability to recover corrupted messages in situations where CopperTag data is incomplete. The experimental results illustrate that CopperTag exhibits superior robustness and accuracy in detection when compared to other state-of-the-art fiducial markers, even in scenarios with heavy occlusion. Moreover, CopperTag maintains an average processing time of 10ms per frame on a standard laptop, effectively meeting the real-time demands of robotics applications."
Robust Collaborative Perception without External Localization and Clock Devices,"Zixing Lei, Zhenyang Ni, Ruize Han, Shuo Tang, Chen Feng, Siheng Chen, Yanfeng Wang","Shanghai Jiao Tong University,Chinese Academy of Sciences,New York University",Visual Perception and Learning I,"A consistent spatial-temporal coordination across multiple agents is fundamental for collaborative perception, which seeks to improve perception abilities through information exchange among agents. To achieve this spatial-temporal alignment, traditional methods depend on external devices to provide localization and clock signals. However, hardware-generated signals could be vulnerable to noise and potentially malicious attack, jeopardizing the precision of spatial-temporal alignment. Rather than relying on external hardwares, this work proposes a novel approach: aligning by recognizing the inherent geometric patterns within the perceptual data of various agents. Following this spirit, we propose a robust collaborative perception system that operates independently of external localization and clock devices. The key module of our system,~emph{FreeAlign}, constructs a salient object graph for each agent based on its detected boxes and uses a graph neural network to identify common subgraphs between agents, leading to accurate relative pose and time. We validate emph{FreeAlign} on both real-world and simulated datasets. The results show that, the ~emph{FreeAlign} empowered robust collaborative perception system perform comparably to systems relying on precise localization and clock devices. We will release code related to this work."
DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal,"Yunhao Li, Jing Wu, Lingzhe Zhao, Peidong Liu",Westlake University,Visual Perception and Learning I,"When capturing images through the glass during rainy or snowy weather conditions, the resulting images often contain waterdrops adhered on the glass surface, and these waterdrops significantly degrade the image quality and perfor- mance of many computer vision algorithms. To tackle these limitations, we propose a method to reconstruct the clear 3D scene implicitly from multi-view images degraded by water- drops. Our method exploits an attention network to predict the location of waterdrops and then train a Neural Radiance Fields to recover the 3D scene implicitly. By leveraging the strong scene representation capabilities of NeRF, our method can render high-quality novel-view images with waterdrops removed. Extensive experimental results on both synthetic and real datasets show that our method is able to generate clear 3D scenes and outperforms existing state-of-the-art (SOTA) image adhesive waterdrop removal methods."
Learning Interaction Regions and Motion Trajectories Simultaneously from Egocentric Demonstration Videos,"Jianjia Xin, Lichun Wang, Kai Xu, Chao Yang, Baocai Yin","Beijing University of technology,Beijing University of Technology",Visual Perception and Learning I,
Marrying NeRF with Feature Matching for One-Step Pose Estimation,"Ronghan Chen, Yang Cong, Yu Ren","Sheyang Institute of Automation, Chinese Academy of Sciences,Chinese Academy of Science, China,Shenyang Institute of Automation Chinese Academy of Sciences",Visual Perception and Learning I,"Given the image collection of an object, we aim at building a real-time image-based pose estimation method, which requires neither its CAD model nor hours of object-specific training. Recent NeRF-based methods provide a promising solution by directly optimizing the pose from pixel loss between rendered and target images. However, during inference, they require long converging time, and suffer from local minima, making them impractical for real-time robot applications. We aim at solving this problem by marrying image matching with NeRF. With 2D matches and depth rendered by NeRF, we directly solve the pose in one step by building 2D-3D correspondences between target and initial view, thus allowing for real-time prediction. Moreover, to improve the accuracy of 2D-3D correspondences, we propose a 3D consistent point mining strategy, which effectively discards unfaithful points reconstruted by NeRF. Moreover, current NeRF-based methods naively optimizing pixel loss fail at occluded images. Thus, we further propose a 2D matches based sampling strategy to preclude the occluded area. Experimental results on representative datasets prove that our method outperforms state-of-the-art methods, and improves inference efficiency by 90x, achieving real-time prediction at 6 FPS."
Occluded Part-Aware Graph Convolutional Networks for Skeleton-Based Action Recognition,"Min Hyuk Kim, Min Ju Kim, Seok Bong Yoo",Chonnam National University,Visual Perception and Learning I,"Recognizing human action is one of the most critical factors in the visual perception of robots. Specifically, skeleton-based action recognition has been actively researched to enhance recognition performance at a lower cost. However, action recognition in occlusion situations, where body parts are not visible, is still challenging. We propose an occluded part-aware graph convolutional network (OP-GCN) to address this challenge using the optimal occluded body parts. The proposed model uses an occluded part detector to identify occluded body parts within a human skeleton. It is based on an autoencoder trained on a nonoccluded human skeleton and exploits the symmetry and angular information of the skeleton. Then, we select an optimal group constructed considering the occluded body parts. Each group comprises five sets of joint nodes, focusing on the body parts, excluding the occluded ones. Finally, to enhance interaction within the selected groups, we apply an interpart association module, considering the fusion of global and local elements. The experimental results reveal that the proposed model outperforms others on the occluded datasets. These comparative experiments demonstrate the effectiveness of the study in addressing the challenge of action recognition in occlusion situations. Our code is publicly available at https://github.com/MJ-Kor/OP-GCN."
MAL: Motion-Aware Loss with Temporal and Distillation Hints for Self-Supervised Depth Estimation,"Yuejiang Dong, Fang-lue Zhang, Song-hai Zhang","Tsinghua University,Victoria University of Wellington",Visual Perception and Learning I,"Depth perception is crucial for a wide range of robotic applications. Multi-frame self-supervised depth estimation methods have gained research interest due to their ability to leverage large-scale, unlabeled real-world data. However, the self-supervised methods often rely on the assumption of a static scene and their performance tends to degrade in dynamic environments. To address this issue, we present Motion-Aware Loss, which leverages the temporal relation among consecutive input frames and a novel distillation scheme between the teacher and student networks in the multi-frame self-supervised depth estimation methods. Specifically, we associate the spatial locations of moving objects with the temporal order of input frames to eliminate errors induced by object motion. Meanwhile, we enhance the original distillation scheme in multi-frame methods to better exploit the knowledge from a teacher network. MAL is a novel, plug-and-play module designed for seamless integration into multi-frame self-supervised monocular depth estimation methods. Adding MAL into previous state-of-the-art methods leads to a reduction in depth estimation errors by up to 4.2% and 10.8% on KITTI and CityScapes benchmarks, respectively."
Stereo Image-Based Visual Servoing towards Feature-Based Grasping,"Albert Enyedy, Ashay Aswale, Berk Calli, Michael Gennert",Worcester Polytechnic Institute,Visual Servoing,"This paper presents an image-based visual servoing scheme that can control robotic manipulators in 3D space using 2D stereo images without needing to perform stereo reconstruction. We use a stereo camera in an eye-to-hand configuration for controlling the robot to reach target positions by directly mapping image space errors to joint space actuation. We achieve convergence without a-priori knowledge of the target object, a reference 2D image, or 3D data. By doing so, we can reach targets in unstructured environments using high-resolution RGB images instead of utilizing relatively noisy depth data. We conduct several experiments on two different physical robots. The Panda 7DOF arm grasps a static target in 3D space, grasps a pitcher handle, and picks and places a box by determining the approach angle using 2D image features, demonstrating that this algorithm can be used for grasping practical objects in 3D space using only 2D image features for feedback. Our second platform, the Atlas humanoid robot, reaches a target from an unknown starting configuration, demonstrating that this controller achieves convergence to a target, even with the uncertainties introduced by walking to a new location. We believe that this algorithm is a step towards enabling intuitive interfaces that allow a user to initiate a grasp on an object by specifying a grasping point in a 2D image."
Visual Feedback Control of an Underactuated Hand for Grasping Brittle and Soft Foods,"Ryogo Kai, Yuzuka Isobe, Sarthak Mahesh Pathak, Kazunori Umeda",Chuo University,Visual Servoing,"This paper presents a novel method to control an underactuated hand by using only a monocular camera, not using any internal sensors. In food factories, robots are required to handle a wide variety of foods without damaging them. To accomplish this, the use of underactuated hands is effective because they can adapt to various food shapes. However, if internal sensors such as tactile sensors and force sensors are used in the underactuated hands, it may cause a problem with hygiene and require complicated calibration. Moreover, if external sensors such as cameras are used, it is necessary to grasp foods without damaging them by using external information such as images. In our method, to tackle these problems, a camera is used as an external sensor. First, contact between the hand and the object is detected by using the contours of both, obtained from a camera image. Then, to avoid damaging the object, the following information is extracted from camera images and observed: the centroid of both the hand and object, the deformation of the object, and the occlusion rate of the hand. Furthermore, to prevent the object from dropping while the robotic arm is in motion, the distance between the centroid of the hand and the object is calculated. The experiments were conducted using twelve different food items."
Compositional Servoing by Recombining Demonstrations,"Maximilian Argus, Abhijeet Nayak, Martin Büchner, Silvio Galesso, Abhinav Valada, Thomas Brox",University of Freiburg,Visual Servoing,"Learning-based manipulation policies from image inputs often show weak task transfer capabilities. In contrast, visual servoing methods allow efficient task transfer in high-precision scenarios while requiring only a few demonstrations. In this work, we present a framework that formulates the visual servoing task as graph traversal. Our method not only extends the robustness of visual servoing, but also enables multitask capability based on a few task-specific demonstrations. We construct demonstration graphs by splitting existing demonstrations and recombining them. In order to traverse the demonstration graph in the inference case, we utilize a similarity function that helps select the best demonstration for a specific task. This enables us to compute the shortest path through the graph. Ultimately, we show that recombining demonstrations leads to higher task-respective success. We present extensive simulation and real-world experimental results that demonstrate the efficacy of our approach."
Second-Order Position-Based Visual Servoing of a Robot Manipulator,"Eduardo Godinho Ribeiro, Raul De Queiroz Mendes, Marco Henrique Terra, Valdir Grassi Junior","University of São Paulo,Eindhoven University of Technology (TU/e),University of Sao Paulo",Visual Servoing,"Visual Servoing is an established approach for controlling robots using visual feedback. Most controllers in this domain generate velocity control signals to guide the cameras to desired positions and orientations. However, the dynamic characteristics of conventional visual servoing controllers may be unsatisfactory, and the velocity signal itself hinders the connection between the feature velocity model and the robot's dynamics. Consequently, research has explored models incorporating the second-order derivative of features and the robot's acceleration. The current state-of-the-art techniques mainly focus on image-based visual servoing, which deals with feature errors in the image domain. In this work, we propose an acceleration-based controller for the position-based visual servoing framework, which models the error in Cartesian space. Our approach involves extracting an acceleration control signal from the traditional velocity-based controller. To achieve this, we redefine the camera orientation using quaternions, generate new interaction matrices, and conduct comprehensive comparative experiments in simulated and real robot scenarios. We show that our method provides better dynamic properties in both image and Cartesian spaces, superior tracking performance, and less sensitivity to noise compared to velocity controllers."
Event-Triggered Image Moments Predictive Control for Tracking Evolving Features Using UAVs,"Sotiris Aspragkathos, George Karras, Kostas Kyriakopoulos","NTUA,University of Thessaly,New York University - Abu Dhabi",Visual Servoing,"This paper presents a novel approach for tracking deformable contour targets using Unmanned Aerial Vehicles (UAVs). The proposed scheme combines image moments descriptor and event-triggered Nonlinear Model Predictive Control (NMPC) for efficient and accurate tracking. The deformable contour model allows adaptation to the evolving target's shape, while the proposed event-triggered scheme achieves improved computational efficiency and extended flight duration while generating new control sequences for the UAV. Real-world experiments validate the scheme, showcasing its robustness in handling complex scenarios. This approach holds promise for various applications, such as surveillance and autonomous navigation."
Lattice-Based Shape Tracking and Servoing of Elastic Objects,"Mohammadreza Shetab-bushehri, Miguel Aranda, Youcef Mezouar, Erol Ozgur","Université Clermont Auvergne, Institut Pascal,Universidad de Zaragoza,Clermont Auvergne INP - SIGMA Clermont,SIGMA-Clermont / Institut Pascal",Visual Servoing,"In this paper, we propose a general unified tracking-servoing approach for controlling the shape of elastic deformable objects using robotic arms. Our approach works by forming a lattice around the object, binding the object to the lattice, and tracking and servoing the lattice instead of the object. This makes our approach have full control over the deformation of elastic deformable objects of any general form (linear, thin-shell, volumetric) in 3D space. Furthermore, it decouples the runtime complexity of the approach from the objectsâ€™ geometric complexity. Our approach is based on the As-Rigid-As-Possible (ARAP) deformation model. It requires no mechanical parameter of the object to be known and can drive the object toward desired shapes through large deformations. The inputs to our approach are the point cloud of the objectâ€™s surface in its rest shape and the point cloud captured by a 3D camera in each frame. Overall, our approach is more broadly applicable than existing approaches. We validate the efficiency of our approach through numerous experiments with elastic deformable objects of various shapes and materials (paper, rubber, plastic, foam)."
DCPT: Darkness Clue-Prompted Tracking in Nighttime UAVs,"Jiawen Zhu, Huayi Tang, Zhi-qi Cheng, Jun-yan He, Bin Luo, Shihao Qiu, Shengming Li, Huchuan Lu","Dalian University of Technology,Carnegie Mellon University,Alibaba Group",Visual Servoing,"Existing nighttime unmanned aerial vehicle (UAV) trackers follow an â€œEnhance-then-Trackâ€ architecture - first using a light enhancer to brighten the nighttime video, then employing a daytime tracker to locate the object. This separate enhancement and tracking fails to build an end-to-end trainable vision system. To address this, we propose a novel architecture called Darkness Clue-Prompted Tracking (DCPT) that achieves robust UAV tracking at night by efficiently learning to generate darkness clue prompts. Without a separate enhancer, DCPT directly encodes anti-dark capabilities into prompts using a darkness clue prompter (DCP). Specifically, DCP iteratively learns emphasizing and undermining projections for darkness clues. It then injects these learned visual prompts into a daytime tracker with fixed parameters across transformer layers. Moreover, a gated feature aggregation mechanism enables adaptive fusion between prompts and between prompts and the base model. Extensive experiments show state-of-the-art performance for DCPT on multiple dark scenario benchmarks. The unified end-to-end learning of enhancement and tracking in DCPT enables a more trainable system. The darkness clue prompting efficiently injects anti-dark knowledge without extra modules. Code is available at https://github.com/bearyi26/DCPT."
Unifying Foundation Models with Quadrotor Control for Visual Tracking Beyond Object Categories,"Alessandro Saviolo, Pratyaksh Prabhav Rao, Vivek Radhakrishnan, Jiuhong Xiao, Giuseppe Loianno","New York University,Technology Innovation Institute, New York University",Visual Servoing,"Visual control enables quadrotors to adaptively navigate using real-time sensory data, bridging perception with action. Yet, challenges persist, including generalization across scenarios, maintaining reliability, and ensuring real-time responsiveness. This paper introduces a perception framework grounded in foundational models for universal object detection and tracking, moving beyond specific training categories. Integral to our approach is a multi-layered tracker integrated with the foundational detector, ensuring continuous target visibility, even when faced with motion blur, abrupt light shifts, and occlusions. Complementing this, we introduce a model-free controller tailored for resilient quadrotor visual tracking. Our system operates efficiently on limited hardware, relying solely on an onboard camera and an inertial measurement unit. Through extensive validation in diverse challenging indoor and outdoor environments, we demonstrate our system's effectiveness and adaptability. In conclusion, our research represents a step forward in quadrotor visual tracking, moving from task-specific methods to more versatile and adaptable operations."
DroneMOT: Drone-Based Multi-Object Tracking Considering Detection Difficulties and Simultaneous Moving of Drones and Objects,"Peng Wang, Yongcai Wang, Deying Li",Renmin University of China,Visual Servoing,"Multi-object tracking (MOT) on static platforms, such as by surveillance cameras, has achieved significant progress, with various paradigms providing attractive performances. However, the effectiveness of traditional MOT methods is significantly reduced when it comes to dynamic platforms like drones. This decrease is attributed to the distinctive challenges in the MOT-on-drone scenario: (1) objects are generally small in the image plane, often blurred, and frequently occluded, making them challenging to detect and recognize; (2) drones move and see objects from different angles, causing the unreliability of the predicted positions and feature embeddings of the objects. This paper proposes DroneMOT, which firstly proposes a Dual-Domain integrated Attention (DIA) module that considers the fast movements of drones to enhance the drone-based object detection and feature embedding for small-sized, blurred, and occluded objects. Then, an innovative Motion-Driven Association (MDA) scheme is introduced, considering the concurrent movements of both the drone and the objects. Within MDA, an Adaptive Feature Synchronization (AFS) technique is presented to update the object features seen from different angles. Additionally, a Dual Motion-based Prediction (DMP) method is employed to forecast the object positions. Finally, both the refined feature embeddings and the predicted positions are integrated to enhance the object association. Comprehensive evaluations on VisDrone2019-MOT and UAVDT datasets show that DroneMOT provides substantial performance improvements over the state-of-the-art in the domain of MOT on drones."
Human-Robot Gym: Benchmarking Reinforcement Learning in Human-Robot Collaboration,"Jakob Thumm, Felix Trost, Matthias Althoff","Technical University of Munich,Technische Universität München",Learning in Planning,"Deep reinforcement learning (RL) has shown promising results in robot motion planning with first attempts in human-robot collaboration (HRC). However, a fair comparison of RL approaches in HRC under the constraint of guaranteed safety is yet to be made. We, therefore, present human-robot gym, a benchmark suite for safe RL in HRC. We provide challenging, realistic HRC tasks in a modular simulation framework. Most importantly, human-robot gym is the first benchmark suite that includes a safety shield to provably guarantee human safety. This bridges a critical gap between theoretic RL research and its real-world deployment. Our evaluation of six tasks led to three key results: (a) the diverse nature of the tasks offered by human-robot gym creates a challenging benchmark for state-of-the-art RL methods, (b) by leveraging expert knowledge in form of an action imitation reward, the RL agent can outperform the expert, and (c) our agents negligibly overfit to training data."
Improving the Generalization of Unseen Crowd Behaviors for Reinforcement Learning Based Local Motion Planners,"Wen Zheng Terence Ng, Jianda Chen, Sinno Jialin Pan, Tianwei Zhang","nanyang technological university,The Chinese University of Hong Kong,Nanyang Technological University",Learning in Planning,"Deploying a safe mobile robot policy in scenarios with human pedestrians is challenging due to their unpredictable movements. Current Reinforcement Learning-based motion planners rely on a single policy to simulate pedestrian movements and could suffer from the over-fitting issue. Alternatively, framing the collision avoidance problem as a multi-agent framework, where agents generate dynamic movements while learning to reach their goals, can lead to conflicts with human pedestrians due to their homogeneity. To tackle this problem, we introduce an efficient method that enhances agent diversity within a single policy by maximizing an information-theoretic objective. This diversity enriches each agent's experiences, improving its adaptability to unseen crowd behaviors. In assessing an agent's robustness against unseen crowds, we propose diverse scenarios inspired by pedestrian crowd behaviors. Our behavior-conditioned policies outperform existing works in these challenging scenes, reducing potential collisions without additional time or travel."
Human-Aligned Longitudinal Control for Occluded Pedestrian Crossing with Visual Attention,"Vinal Asodia, Zhenhua Feng, Saber Fallah",University of Surrey,Learning in Planning,"Reinforcement Learning (RL) has been widely used to create generalizable autonomous vehicles. However, they rely on fixed reward functions that struggle to balance values like safety and efficiency. How can autonomous vehicles balance different driving objectives and human values in a constantly changing environment? To bridge this gap, we propose an adaptive reward function that utilizes visual attention maps to detect pedestrians in the driving scene and dynamically switch between prioritizing safety or efficiency depending on the current observation. The visual attention map is used to provide spatial attention to the RL agent to boost the training efficiency of the pipeline. We evaluate the pipeline against variants of an occluded pedestrian crossing scenario in the CARLA Urban Driving simulator. Specifically, the proposed pipeline is compared against a modular setup that combines the well-established object detection model, YOLO, with a Proximal Policy Optimization (PPO) agent. The results indicate that the proposed approach can compete with the modular setup while yielding greater training efficiency. The trajectories collected with the approach confirm the effectiveness of the proposed adaptive reward function."
Projection-Based Fast and Safe Policy Optimization for Reinforcement Learning,"Shijun Lin, Hao Wang, Ziyang Chen, Zhen Kan",University of Science and Technology of China,Learning in Planning,"While reinforcement learning (RL) attracts increasing research attention, maximizing the return while keeping the agent safe at the same time remains an open problem. Motivated to address this challenge, this work proposes a new Fast and Safe Policy Optimization (FSPO) algorithm, which consists of three steps: the first step involves reward improvement update, the second step projects the policy to the neighborhood of the baseline policy to accelerate the optimization process, and the third step addresses the constraint violation by projecting the policy back onto the constraint set. Such a projection-based optimization can improve the convergence and learning performance. Unlike many existing works that require convex approximations for the objectives and constraints, this work exploits a first- order method to avoid expensive computations and high dimensional issues, enabling fast and safe policy optimization, especially for challenging tasks. Numerical simulation and physical experiments demonstrate that FSPO outperforms existing methods in terms of safety guarantees and task completion rate."
Symmetry Considerations for Learning Task Symmetric Robot Policies,"Mayank Mittal, Nikita Rudin, Victor Klemm, Arthur Allshire, Marco Hutter","ETH Zurich,ETH Zurich, NVIDIA,University of Toronto",Learning in Planning,"Symmetry is a fundamental aspect of many real-world robotic tasks. However, current deep reinforcement learning (DRL) approaches can seldom harness and exploit symmetry effectively. Often, the learned behaviors fail to achieve the desired transformation invariances and suffer from motion artifacts. For instance, a quadruped may exhibit different gaits when commanded to move forward or backward, even though it is symmetrical about its torso. This issue becomes further pronounced in high-dimensional or complex environments, where DRL methods are prone to local optima and fail to explore regions of the state space equally. Past methods on encouraging symmetry for robotic tasks have studied this topic mainly in a single-task setting, where symmetry usually refers to symmetry in the motion, such as the gait patterns. In this paper, we revisit this topic for goal-conditioned tasks in robotics, where symmetry lies mainly in task execution and not necessarily in the learned motions themselves. In particular, we investigate two approaches to incorporate symmetry invariance into DRL â€“ data augmentation and mirror loss function. We provide a theoretical foundation for using augmented samples in an on-policy setting. Based on this, we show that the corresponding approach achieves faster convergence and improves the learned behaviors in various challenging robotic tasks, from climbing boxes with a quadruped to dexterous manipulation."
Learning Dual-Arm Object Rearrangement for Cartesian Robots,"Shishun Zhang, Qijin She, Wenhao Li, Chenyang Zhu, Yongjun Wang, Ruizhen Hu, Kai Xu","National University of Defense Technology,Shenzhen University",Learning in Planning,"This work focuses on the dual-arm object rearrangement problem abstracted from a realistic industrial scenario of Cartesian robots. The goal of this problem is to transfer all the objects from sources to targets with the minimum total completion time. To achieve the goal, the core idea is to develop an effective object-to-arm task assignment strategy for minimizing the cumulative task execution time and maximizing the dual-arm cooperation efficiency. One of the difficulties in the task assignment is the scalability problem. As the number of objects increases, the computation time of traditional offline-search-based methods grows strongly for computational complexity. Encouraged by the adaptability of reinforcement learning (RL) in long-sequence task decisions, we propose an online task assignment decision method based on RL, and the computation time of our method only increases linearly with the number of objects. Further, we design an attention-based network to model the dependencies between the input states during the whole task execution process to help find the most reasonable object-to-arm correspondence in each task assignment round. In the experimental part, we adapt some search-based methods to this specific setting and compare our method with them. Experimental result shows that our approach achieves outperformance over search-based methods in total execution time and computational efficiency, and also verifies the generalization of our method to different numbers of objects. In addition, we show the effectiveness of our method deployed on the real robot in the supplementary video."
Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration,"Jinning Li, Xinyi Liu, Banghua Zhu, Jiantao Jiao, Masayoshi Tomizuka, Chen Tang, Wei Zhan","University of California, Berkeley,University of Michigan,University of California,University of California Berkeley,Univeristy of California, Berkeley",Learning in Planning,"Safe Reinforcement Learning (RL) aims to find a policy that achieves high rewards while satisfying cost constraints. When learning from scratch, safe RL agents tend to be overly conservative, which impedes exploration and restrains the overall performance. In many realistic tasks, e.g. autonomous driving, large-scale expert demonstration data are available. We argue that extracting expert policy from offline data to guide online exploration is a promising solution to mitigate the conserveness issue. Large-capacity models, e.g. decision transformers (DT), have been proven to be competent in offline policy learning. However, data collected in real-world scenarios rarely contain dangerous cases (e.g., collisions), which makes it prohibitive for the policies to learn safety concepts. Besides, these bulk policy networks cannot meet the computation speed requirements at inference time on real-world tasks such as autonomous driving. To this end, we propose Guided Online Distillation (GOLD), an offline-to-online safe RL framework. GOLD distills an offline DT policy into a lightweight policy network through guided online safe RL training, which outperforms both the offline DT policy and online safe RL algorithms. Experiments in both benchmark safe RL tasks and real-world driving tasks based on the Waymo Open Motion Dataset (WOMD) demonstrate that GOLD can successfully distill lightweight policies and solve decision-making problems in challenging safety-critical scenarios."
Sample-Efficient Learning to Solve a Real-World Labyrinth Game Using Data-Augmented Model-Based Reinforcement Learning,"Thomas Bi, Raffaello D'Andrea","ETH Zurich,ETHZ",Learning in Planning,"Motivated by the challenge of achieving rapid learning in physical environments, this paper presents the development and training of a robotic system designed to navigate and solve a labyrinth game using model-based reinforcement learning techniques. The method involves extracting low-dimensional observations from camera images, along with a cropped and rectified image patch centered on the current position within the labyrinth, providing valuable information about the labyrinth layout. The learning of a control policy is performed purely on the physical system using model-based reinforcement learning, where the progress along the labyrinth's path serves as a reward signal. Additionally, we exploit the system's inherent symmetries to augment the training data. Consequently, our approach learns to successfully solve a popular real-world labyrinth game in record time, with only 5 hours of real-world training data."
Active Neural Topological Mapping for Multi-Agent Exploration,"Xinyi Yang, Yuxiang Yang, Chao Yu, Jiayu Chen, Yu Jincheng, Haibing Ren, Huazhong Yang, Yu Wang","Tsinghua University,Meituan Inc.",Learning in Planning,"This paper investigates the multi-agent cooperative exploration problem, which requires multiple agents to explore an unseen environment via sensory signals in a limited time. A popular approach to exploration tasks is to combine active mapping with planning. Metric maps capture the details of the spatial representation, but are memory intensive and may vary significantly between scenarios, resulting in inferior generalization. Topological maps are a promising alternative as they consist only of nodes and edges with abstract but essential information and are less influenced by the scene structures. However, most existing topology-based exploration tasks utilize classical methods for planning, which are time-consuming and sub-optimal due to their handcrafted design. Deep reinforcement learning (DRL) has shown great potential for learning (near) optimal policies through fast end-to-end inference. In this paper, we propose Multi-Agent Neural Topological Mapping (MANTM) to improve exploration efficiency and generalization for multi-agent exploration tasks. MANTM mainly comprises a Topological Mapper and a novel RL-based Hierarchical Topological Planner (HTP). The Topological Mapper employs a visual encoder and distance-based heuristics to construct a graph containing main nodes and their corresponding ghost nodes. The HTP leverages graph neural networks to capture correlations between agents and graph nodes in a coarse-to-fine manner for effective global goal selection. Extensi"
Efficient Multi-Task and Transfer Reinforcement Learning with Parameter-Compositional Framework,"Lingfeng Sun, Haichao Zhang, Wei Xu, Masayoshi Tomizuka","University of California, Berkeley,Horizon Robotics,University of California",Learning in Grasping and Manipulation I,"In this work, we investigate the potential of improving multi-task training and leveraging it for transferring in the reinforcement learning setting. We identify several challenges towards this goal and propose a transferring approach with a parameter-compositional formulation. We investigate ways to improve the training of multi-task reinforcement learning, which serves as the foundation for transferring. Then we conduct a number of transferring experiments on various manipulation tasks. Experimental results demonstrate that the proposed approach can have improved performance in the multi-task training stage, and further show effective transferring in terms of both sample efficiency and performance."
Goal-Conditioned Reinforcement Learning with Disentanglement-Based Reachability Planning,"Zhifeng Qian, You Mingyu, Zhou Hongjun, Xuanhui Xu, Bin He","Tongji University,Tongji,TongJi University",Learning in Grasping and Manipulation I,"Goal-Conditioned Reinforcement Learning (GCRL) can enable agents to spontaneously set diverse goals to learn a set of skills. Despite the excellent works proposed in various fields, reaching distant goals in temporally extended tasks remains a challenge for GCRL. Current works tackled this problem by leveraging planning algorithms to plan intermediate subgoals to augment GCRL. Their methods need two crucial requirements: (i) a state representation space to search valid subgoals, and (ii) a distance function to measure the reachability of subgoals. However, they struggle to scale to high-dimensional state space due to their non-compact representations. Moreover, they cannot collect high-quality training data through standard GC policies, which results in an inaccurate distance function. Both affect the efficiency and performance of planning and policy learning. In the paper, we propose a goal-conditioned RL algorithm combined with Disentanglement-based Reachability Planning (REPlan) to solve temporally extended tasks. In REPlan, a Disentangled Representation Module (DRM) is proposed to learn compact representations which disentangle robot poses and object positions from high-dimensional observations in a self-supervised manner. A simple REachability discrimination Module (REM) is also designed to determine the temporal distance of subgoals. Moreover, REM computes intrinsic bonuses to encourage the collection of novel states for training. We evaluate our REPlan in three vision-"
KINet: Unsupervised Forward Models for Robotic Pushing Manipulation,"Alireza Rezazadeh, Changhyun Choi","University of Minnesota,University of Minnesota, Twin Cities",Learning in Grasping and Manipulation I,"Object-centric representation is an essential abstraction for forward prediction. Most existing forward models learn this representation through extensive supervision (e.g., object class and bounding box) although such ground-truth information is not readily accessible in reality. To address this, we introduce KINet (Keypoint Interaction Network) ---an end-to-end unsupervised framework to reason about object interactions based on a keypoint representation. Using visual observations, our model learns to associate objects with keypoint coordinates and discovers a graph representation of the system as a set of keypoint embeddings and their relations. It then learns an action-conditioned forward model using contrastive estimation to predict future keypoint states. By learning to perform physical reasoning in the keypoint space, our model automatically generalizes to scenarios with a different number of objects, novel backgrounds, and unseen object geometries. Experiments demonstrate the effectiveness of our model in accurately performing forward prediction and learning plannable object-centric representations for downstream robotic pushing manipulation tasks."
Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks,"Eleftherios Triantafyllidis, Filippos Christianos, Zhibin Li","The University of Edinburgh,University of Edinburgh,University College London",Learning in Grasping and Manipulation I,"Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and (iv) maintain robustness against increased levels of uncertainty and horizons."
Touch-Based Manipulation with Multi-Fingered Robot Using Off-Policy RL and Temporal Contrastive Learning,"Naoki Morihira, Pranav Deo, Manoj Bhadu, Akinobu Hayashi, Tadaaki Hasegawa, Satoshi Otsubo, Takayuki Osa","Honda R&D, Ltd.,Honda R&D Co. Ltd.,Honda R&D Co., Ltd.,Honda R&D Co.,Ltd.,Honda R&D,University of Tokyo",Learning in Grasping and Manipulation I,"Tactile information holds promise for enhancing the manipulation capabilities of multi-fingered robots. In tasks such as in-hand manipulation, where robots frequently switch between contact and non-contact states, it is important to address the partial observability of tactile sensors and to properly consider the history of observations and actions. Previous studies have shown that Recurrent Neural Network (RNN) can be used to learn latent representations for handling observation and action histories. However, this approach is usually combined with on-policy reinforcement learning (RL) and suffers from low sample efficiency. Integrating RNN with off-policy RL could enhance sample efficiency, but this often compromises stability and robustness, especially as the dimensions of observation and action increase. This paper presents a time-contrastive learning approach tailored for off-policy RL. Our method incorporates a temporal contrastive model and introduces a surrogate loss to extract task-related latent representations, enhancing the pursuit of the optimal policy. Simulations and real robot experiments demonstrate that our proposed method outperforms RNN-based approaches."
Learning Language-Conditioned Deformable Object Manipulation with Graph Dynamics,"Yuhong Deng, Kai Mo, Chongkun Xia, Xueqian Wang","National University of Singapore,Tsinghua University, Shenzhen International Graduate School,Tsinghua University,Center for Artificial Intelligence and Robotics, Graduate School",Learning in Grasping and Manipulation I,"Multi-task learning of deformable object manipulation is a challenging problem in robot manipulation. Most previous works address this problem in a goal-conditioned way and adapt goal images to specify different tasks, which limits the multi-task learning performance and can not generalize to new tasks. Thus, we adapt language instruction to specify deformable object manipulation tasks and propose a learning framework. We first design a unified Transformer-based architecture to understand multi-modal data and output picking and placing action. Besides, we have applied the visible connectivity graph to tackle nonlinear dynamics and complex configuration of the deformable object. Both simulated and real experiments have demonstrated that the proposed method is effective and can generalize to unseen instructions and tasks. Compared with the state-of-the-art method, our method achieves higher success rates (87.2% on average) and has a 75.6% shorter inference time. We also demonstrate that our method performs well in real-world experiments. Supplementary videos can be found at https://sites.google.com/view/language-deformable."
Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning with Instance Segmentation to Grasp Arbitrary Objects,"Malte Mosbach, Sven Behnke",University of Bonn,Learning in Grasping and Manipulation I,"Interactive grasping from clutter, akin to human dexterity, is one of the longest-standing problems in robot learning. Challenges stem from the intricacies of visual perception, the demand for precise motor skills, and the complex interplay between the two. In this work, we present Teacher-Augmented Policy Gradient (TAPG), a novel two-stage learning framework that synergizes reinforcement learning and policy distillation. After training a teacher policy to master the motor control based on object pose information, TAPG facilitates guided, yet adaptive, learning of a sensorimotor policy, based on object segmentation. We zero-shot transfer from simulation to a real robot by using Segment Anything Model for promptable object segmentation. Our trained policies adeptly grasp a wide variety of objects from cluttered scenarios in simulation and the real world based on human-understandable prompts. Furthermore, we show robust zero-shot transfer to novel objects. Videos of our experiments are available at url{https://maltemosbach.github.io/grasp_anything}."
Composable Interaction Primitives: A Structured Policy Class for Efficiently Learning Sustained-Contact Manipulation Skills,"Ben Abbatematteo, Eric Rosen, Rachael Thompson, Mete Tuluhan Akbulut, Sreehari Rammohan, George Konidaris","University of Texas at Austin,Brown University,MIT,Bogazici University",Learning in Grasping and Manipulation I,"We propose a new policy class, Composable Interaction Primitives (CIPs), specialized for learning sustained-contact manipulation skills like opening a drawer, pulling a lever, turning a wheel, or shifting gears. CIPs have two primary design goals: to minimize what must be learned by exploiting structure present in the world and the robot, and to support sequential composition by construction, so that learned skills can be used by a task-level planner. Using an ablation experiment in four simulated manipulation tasks, we show that the structure included in CIPs substantially improves the efficiency of motor skill learning. We then show that CIPs can be used for plan execution in a zero-shot fashion by sequencing learned skills. We validate our approach on real robot hardware by learning and sequencing two manipulation skills."
CollisionGP: Gaussian Process-Based Collision Checking for Robot Motion Planning,"Javier Muñoz Mendi, Peter Lehner, Luis Moreno, Alin Albu-Schäffer, Maximo A. Roa","Universidad Carlos III de Madrid,German Aerospace Center (DLR),Carlos III University,DLR - German Aerospace Center",Collision Avoidance I,"Collision checking is the primitive operation of motion planning that consumes most time. Machine learning algorithms have proven to accelerate collision checking. We propose CollisionGP, a Gaussian process-based algorithm for modeling a robot's configuration space and query collision checks. CollisionGP introduces a PÃ²lya-Gamma auxiliary variable for each data point in the training set to allow classification inference to be done exactly with a closed form expression. Gaussian processes provide a distribution as the output, obtaining a mean and variance for the collision check. The obtained variance is processed to reduce false negatives (FN). We demonstrate that CollisionGP can use GPU acceleration to process collision checks for thousands of configurations much faster than traditional collision detection libraries. Furthermore, we obtain better accuracy, TPR and TNR results than state-of-the-art learning-based algorithms using less support points, thus making our proposed method more sparse."
Probabilistic Motion Planning and Prediction Via Partitioned Scenario Replay,"Oscar De Groot, Anish Sridharan, Javier Alonso-Mora, Laura Ferranti","Delft University of Technology,Starnus Technologiy",Collision Avoidance I,"Autonomous mobile robots require predictions of human motion to plan a safe trajectory that avoids them. Because human motion cannot be predicted exactly, future trajectories are typically inferred from real-world data via learning-based approximations. These approximations provide useful information on the pedestrianâ€™s behavior, but may deviate from the data, which can lead to collisions during planning. In this work, we introduce a joint prediction and planning framework, Partitioned Scenario Replay (PSR), that stores and partitions previously observed human trajectories, referred to as scenarios. During planning, scenarios observed in similar situations are reintroduced (or replayed) as motion predictions. By sampling real data and by building on scenario optimization and predictive control, the planner provides probabilistic collision avoidance guarantees in the real-world. Relying on this guarantee to remain safe, PSR can incrementally improve its prediction and planning performance online. We demonstrate our approach on a mobile robot navigating around pedestrians."
Prescient Collision-Free Navigation of Mobile Robots with Iterative Multimodal Motion Prediction of Dynamic Obstacles,"Ze Zhang, Hadi Hajieghrary, Emmanuel Dean-Leon, Knut Akesson","Chalmers University of Technology,Magna International",Collision Avoidance I,"To explore safe interactions between a mobile robot and dynamic obstacles, this paper presents a comprehensive approach to collision-free navigation in dynamic indoor environments. The approach integrates multimodal motion predictions of dynamic obstacles with predictive control for obstacle avoidance. Multimodal Motion Prediction (MMP) is achieved by a deep-learning method that predicts multiple plausible future positions. By repeating the MMP for each time offset in the future, multi-time-step multimodal motion predictions are obtained. A nonlinear Model Predictive Control (MPC) solver utilizes the prediction outcomes to achieve collision-free trajectory tracking for the mobile robot. The proposed integration of multimodal motion prediction and trajectory tracking outperforms other non-deep-learning methods in complex scenarios. The approach enables safe interaction between the mobile robot and stochastic dynamic obstacles."
GPU-Accelerated Optimization-Based Collision Avoidance,"Zeming Wu, Zhuping Wang, Hao Zhang",Tongji University,Collision Avoidance I,"This paper proposes a GPU-accelerated optimization framework for collision avoidance problems where the controlled objects and the obstacles can be modeled as the finite union of convex polyhedra. A novel collision avoidance constraint is proposed based on scale-based collision detection and the strong duality of convex optimization. Under this constraint, the high-dimensional non-convex optimization problems of collision avoidance can be decomposed into several low-dimensional quadratic programmings (QPs) following the paradigm of alternating direction method of multipliers (ADMM). Furthermore, these low-dimensional QPs can be solved parallel with GPUs, significantly reducing computational time. High-fidelity simulations are conducted to validate the proposed method's effectiveness and practicality."
Learn to Navigate in Dynamic Environments with Normalized LiDAR Scans,"Wei Zhu, Mitsuhiro Hayashibe",Tohoku University,Collision Avoidance I,"The latest robot navigation methods for dynamic environments assume that the states of obstacles, including their geometries and trajectories, are fully observable. While it's easy to obtain these states accurately in simulations, it's exceedingly challenging in the real world. Therefore, a viable alternative is to directly map raw sensor observations into robot actions. However, acquiring skills from high-dimensional raw observations demands massive neural networks and extended training periods. Furthermore, there are discrepancies between simulated and real environments that impede real-world implementations. To overcome these limitations, we propose a Learning framework for robot Navigation in Dynamic environments that uses sequential Normalized LiDAR (LNDNL) scans. We employ long-short-term memory (LSTM) to propagate historical environmental information from the sequential LiDAR observations. Additionally, we customize a LiDAR-integrated simulator to speed up sampling and normalize the geometry of real-world obstacles to match that of simulated objects, thereby bridging the sim-to-real gap. Our extensive comparisons with state-of-the-art baselines and real-world implementations demonstrate the potential of learning to navigate in dynamic environments using raw sensor observations and sim-to-real transfer."
Learning Terminal State of the Trajectory Planner: Application for Collision Scenarios of Autonomous Vehicles,"Joonhee Lim, Kibeom Lee, Dongsuk Kum","KAIST,Gachon University",Collision Avoidance I,"Collision Avoidance/Mitigation System (CAMS) for autonomous vehicles is a crucial technology that ensures the safety and reliability of autonomous driving systems. Conventional collision avoidance approaches struggle in complex and various scenarios by avoiding collisions based on rules for specific collision scenarios. This has led to learning-based methods using neural networks for adaptive collision avoidance. However, the approaches directly outputting control inputs through neural networks have drawbacks in interpretability and stability. To address these limitations, we propose a trajectory planning method for CAMS that combines deep reinforcement learning (DRL) and quintic polynomial (QP) trajectory planning. The proposed method determines the terminal state and confidence of the trajectory using DRL and plans a QP trajectory based on them. By utilizing the terminal state and confidence of the trajectory rather than direct control inputs as the output of the neural network, it generates a more realistic and continuous path. Moreover, this approach considers collision avoidance and mitigation in an integrated manner through the reward function of RL. Our experimental results demonstrate that the proposed method not only improves interpretability and stability compared to existing learning-based methods but also upholds performance in complex and various collision scenarios."
Informed Steiner Trees: Sampling and Pruning for Multi-Goal Path Finding in High Dimensions,"Nikhil Chandak, Kenny Chour, Sivakumar Rathinam, R Ravi","IIIT Hyderabad,Texas A&M University,TAMU,Carnegie Mellon University",Collision Avoidance I,"We interleave sampling based motion planning methods with pruning ideas from minimum spanning tree algorithms to develop a new approach for solving a Multi-Goal Path Finding (MGPF) problem in high dimensional spaces. The approach alternates between sampling points from selected regions in the search space and de-emphasizing regions that may not lead to good solutions for MGPF. Our approach provides an asymptotic, 2-approximation guarantee for MGPF. We also present extensive numerical results to illustrate the advantages of our proposed approach over uniform sampling in terms of the quality of the solutions found and computation speed."
History-Aware Planning for Risk-Free Autonomous Navigation on Unknown Uneven Terrain,"Yinchuan Wang, Nianfei Du, Yongsen Qin, Xiang Zhang, Rui Song, Chaoqun Wang","Shandong University,School of Control Science and Engineering,Shandong University,shandong university",Collision Avoidance I,"It is challenging for the mobile robot to achieve autonomous and mapless navigation in the unknown environment with uneven terrain. In this study, we present a layered and systematic pipeline. At the local level, we maintain a tree structure that is dynamically extended with the navigation. This structure unifies the planning with the terrain identification. Besides, it contributes to explicitly identifying the hazardous areas on uneven terrain. In particular, certain nodes of the tree are consistently kept to form a sparse graph at the global level, which records the history of the exploration. A series of subgoals that can be obtained in the tree and the graph are utilized for leading the navigation. To determine a subgoal, we develop an evaluation method whose input elements can be efficiently obtained on the layered structure. We conduct both simulation and real-world experiments to evaluate the developed method and its key modules. The experimental results demonstrate the effectiveness and efficiency of our method. The robot can travel through the unknown uneven region safely and reach the target rapidly without a preconstructed map."
Multi-Tap Resistive Sensing and FEM Modeling Enables Shape and Force Estimation in Soft Robots,"Barnabas Gavin Cangan, Sizhe Tian, Stefan Escaida Navarro, Artem Beger, Christian Duriez, Robert Kevin Katzschmann","ETH Zurich,Inria, Université de lille,Universidad de O'Higgins,Festo SE & Co. KG,INRIA",Soft Sensors and Actuators II,"We tackle the problem of proprioception in soft robots, specifically soft grippers with tight packaging constraints, relying only on intrinsic sensors. While various sensing ap- proaches have been studied towards curvature estimation, we look into being able to sense local deformations. To accomplish this, we use a widely available, off-the-shelf resistive sensor and multi-tap this sensor, i.e. make multiple electrical connections onto the resistive layer of the sensor. This allows us to measure changes in resistance at multiple segments throughout the length of the sensor, providing improved resolution of local deformations in the soft body. These measurements inform a finite-element- method (FEM) based model to then estimate the shape of the soft body and the magnitude of an external force acting at a known arbitrary location. Our model-based approach estimates soft body deformation with approximately 3% average relative error and taking into account internal fluidic actuation, our estimate of external force disturbance has 11% relative error within a 5 N range. The combined sensing and modeling approach can be integrated into soft manipulation platforms to enable features such as identifying shape and material properties of an object being grasped. Such manipulators can benefit from the softness and compliance while being proprioceptive relying only on embedded sensing and not on external systems such as motion capture, which is essential for deployment in real-world scena"
Learning Motion Reconstruction from Demonstration Via Multi-Modal Soft Tactile Sensing,"Cheng Pan, Kieran Gilday, Emily Sologuren, Kai Junge, Josie Hughes","Swiss Federal Institute of Technology Lausanne (EPFL),EPFL,MIT,École polytechnique fédérale de Lausanne",Soft Sensors and Actuators II,"Learning manipulation from demonstration is a key way for humans to teach complex tasks. However, this domain mainly focuses on kinetic teaching, and does not consider imitation of interaction forces which is essential for more contact rich tasks. We propose a framework that enables robotic imitation of contact from human demonstration using a wearable finger-tip sensor. By developing a multi-modal sensor (providing both force and contact location) and robotic collection of simple training data of different motion primitives (tapping, rotation and translation), an LSTM-based model can be used to replicate motion from tactile demonstration only. To evaluate this approach, we explore the performance on increasingly complex testing data generated by a robot, and also demonstrate the full pipeline from human demonstration via the sensor used as a wearable device. This approach of using tactile sensing as a means of inferring the required robot motion paves the way for imitation of more contact-rich tasks, and enables imitation of tasks where the demonstration and imitation is performed with different body-schema."
"A Generalized Motion Control Framework of Dielectric Elastomer Actuators: Dynamic Modeling, Sliding-Mode Control and Experimental Evaluation","Jiang Zou, Shakiru Olajide Kassim, Jieji Ren, Vahid Vaziri, Sumeet S. Aphale, Guo-Ying Gu","Shanghai Jiao Tong University,SCHOOL OF ENGINEERING, UNIVERSITY OF ABERDEEN, SCOTLAND,University of Aberdeen",Soft Sensors and Actuators II,"The continuous electromechanical deformation of dielectric elastomer actuators (DEAs) suffers from rate-dependent viscoelasticity, mechanical vibration and configuration dependency, making the generalized dynamic modeling and precise control elusive. In this work, we present a generalized motion control framework for DEAs capable of accommodating different configurations, materials and degrees of freedom (DOFs). First, a generalized, control-enabling dynamic model is developed for DEAs by taking both nonlinear electromechanical coupling, mechanical vibration and rate-dependent viscoelasticity into consideration. Further, a state observer is introduced to predict the unobservable viscoelasticity. Then, an Enhanced Exponential Reaching Law based Sliding-Mode Controller (EERLSMC) is proposed to minimize the viscoelasticity of DEAs. Its stability is also proved mathematically. The experimental results obtained for different DEAs (four configurations, two materials and multi-DOFs) demonstrate that our dynamic model can precisely describe their complex dynamic responses and the EERLSMC can achieve precise tracking control; verifying the generality of our framework."
Vision-Based Tip Force Estimation on a Soft Continuum Robot,"Xingyu Chen, Jialei Shi, Helge Wurdemann, Thomas George Thuruthel",University College London,Soft Sensors and Actuators II,"Soft continuum robots, fabricated from elastomeric materials, offer unparalleled flexibility and adaptability, making them ideal for applications such as minimally invasive surgery and inspections in constrained environments. With the miniaturization of imaging technologies and the development of novel control algorithms, these devices provide exceptional opportunities to visualize the internal structures of the human body. However, there are still challenges in accurately estimating external forces applied to these systems using current technologies. Adding additional sensors is challenging without compromising the softness of the device. This work presents a visual deformation-based force sensing framework for soft continuum robots. The core idea behind this work is that point loads lead to unique deformation profiles in an actuated soft-bodied robot. We introduce a Convolutional Neural Network-based tip force estimation method that utilizes arbitrarily placed camera images and actuation inputs to predict applied tip forces. Experimental validation was performed using the STIFF-FLOP robot, a pneumatically actuated soft robot developed for minimally invasive surgery. Our vision-based force estimation model demonstrated a sensing precision of 0.05 N in the XY plane during testing, with data collection and training taking only 70 minutes."
Soft Bending Actuator with Fiber-Jamming Variable Stiffness and Fiber-Optic Proprioception,"Joonwon Kang, Sudong Lee, Yong-Lae Park","Seoul National University,EPFL (École Polytechnique Fédérale de Lausanne)",Soft Sensors and Actuators II,"Soft actuators with a function of variable stiffness are beneficial to the improvement of the adaptability of robots, expanding the application areas and environments. We propose a tendon-driven soft bending actuator that can change its stiffness using fiber jamming. The actuator is made of an elastomer tube filled with different types of fiber. The three types of fibers play different roles in maintaining the structure, variable stiffness by jamming, and fiber-optic shape sensing while sharing the same structure and materials, realizing a compact form factor of the entire structure. The stiffness of the actuator can be increased to higher than three times its original stiffness by jamming. In addition to jamming, the proposed actuator has a special function of shape sensing that estimates the tip location of the actuator based on image sensing from optical fibers packaged with the jamming fibers. The tip position sensing shows accuracies with errors of 3.1%, 3.0%, and 6.7% for the x, y, and z axes, respectively, using feature extraction and a deep neural network. The proposed actuator has two degrees of freedom (i.e., bending on two orthogonal planes) and is controlled by two tendons. When connected in series, multiple actuators form a soft robotic manipulator (i.e., arm), physically compliant or capable of delivering a relatively high force to the target objects."
A Light and Heat-Seeking Vine-Inspired Robot with Material-Level Responsiveness,"Shivani Deglurkar, Charles Xiao, Luke Gockowski, Megan Valentine, Elliot Hawkes","University of California, San Diego,University of California, Santa Barbara,University of California Santa Barbara",Soft Sensors and Actuators II,"The fields of soft and bio-inspired robotics promise to imbue synthetic systems with capabilities found in the natural world. However, many of these biological capabilities are yet to be realized. For example, vines in nature direct growth via localized responses embedded in the cells of vine body, allowing an organism without a central brain to successfully search for resources (e.g., light). Yet to date, vine-inspired robots have yet to show such localized embedded responsiveness. Here we present a vine-inspired robotic device with material-level responses embedded in its skin and capable of â€œgrowingâ€ and steering toward either a light or heat stimulus. We present basic modeling of the concept, design details, and experimental results showing its behavior in response to infrared (IR) and visible light. Our simple design concept advances the capabilities of bio-inspired robots and lays the foundation for future â€œgrowingâ€ robots that are capable of seeking light or heat, yet are extremely simple and low-cost. Potential applications include solar tracking, and in the future, firefighting smoldering fires. We envision using similar robots to find hot spots in hard- to-access environments, allowing us to put out potentially long- burning fires faster."
Morphological Design for Pneumatic Soft Actuators and Robots with Desired Deformation Behavior,"Feifei Chen, Zenan Song, Shitong Chen, Guo-Ying Gu, Xiangyang Zhu","Shanghai Jiao Tong University,Shanghai Jiaotong University",Soft Sensors and Actuators II,"A homogeneous pneumatic soft robot may generate complex output motions using a simple input pressure, resulting from its morphological shape that locally deforms the soft material to different degrees by simultaneously tailoring the structural characteristics and orienting the input pressure. To date, design of the morphological shape (inverse problem) has not been fully addressed. This article outlines a geometry-mechanics-optimization integrated approach to automatically shaping a pneumatic soft actuator or robot that achieves the desired deformation behavior. Instead of constraining the robot's geometry within any predefined regular shape, we employ B-splines to allow generation of freeform boundary surfaces, and use nonlinear mechanical modelling and shape derivative based optimization to navigate the high-dimensional design space. Our design framework can readily regulate the surface quality during the morphological evolution, by imposing the geometric constraints in terms of the principal curvatures and the minimal distance between surfaces as penalty functions. The effect of external forces including the gravity and the interaction force at the end-effector is also taken int"
Thermally-Activated Biochemically-Sustained Reactor for Soft Fluidic Actuation,"Jialun Liu, Mennaallah Soliman, Dana Damian","The University of Sheffield,University of Sheffield",Soft Sensors and Actuators II,"Soft robots have shown remarkable distinct capabilities due to their high deformation. Recently increasing attention has been dedicated to developing fully soft robots to exploit their full potential, with a recognition that electronic powering may limit this achievement. Alternative powering sources compatible with soft robots have been identified such as combustion and chemical reactions. A further milestone to such systems would be to increase the controllability and responsiveness of their underlying reactions in order to achieve more complex behaviors for soft robots. In this paper, we present a thermally-activated reactor incorporating a biocompatible hydrogel valve that enables control of the biochemical reaction of sugar and yeast. The biochemical reaction is utilized to generate contained pressure, which in turn powers a fluidic soft actuator. Experiments were conducted to evaluate the response time of the hydrogel valves with three different crosslinker concentrations. Among the tested concentrations, we found that the lowest crosslinker concentration yielded the fastest response time of the valve at an ambient temperature of 50Â°C. We also evaluated the pressure generation capacity of the reactor, which can reach up to 0.22 bar, and demonstrated the thermo-responsive behavior of the reactor to trigger a biochemical reaction for powering a fluidic soft actuator. This work opens up the possibility to power and control tetherless and fully soft robots."
"Pulsating Fluidic Sensor for Sensing of Location, Pressure and Contact Area","Joanna Jones, Marco Pontin, Dana Damian",University of Sheffield,Soft Sensors and Actuators II,"Designing information-rich and space-efficient sensors is a key challenge for soft robotics, and crucial for the development of safe soft robots. Sensing and understanding the environmental interactions with a minimal footprint is especially important in the medical context, where portability and unhindered patient/user movement is a priority, to move towards personalized and decentralized healthcare solutions. In this work, a pulsating fluidic soft sensor (PFS) capable of determining location, pressure and contact area of press events is shown. The sensor relies on spatio-temporal resistance changes driven by a pulsating conductive fluid. The sensor demonstrates good repeatability and distinction of single and multiple press events, detecting single indents of sizes greater than 1 cm, forces larger than 2 N, and various locations across the sensor, as well as multiple indents spaced 2 cm apart. Furthermore, the sensor is demonstrated in two applications to detect foot placement and grip location. Overall, the sensor represents an improvement towards minimizing electronic hardware, and cost of the sensing solution, without sacrificing the richness of the sensing information in the field of soft fluidic sensors."
Perception through Cognitive Emulation: â€œA Second Iteration of NaivPhys4RP for Learningless and Safe Recognition and 6D-Pose Estimation of (Transparent) Objectsâ€,"Franklin Kenghagho Kenfack, Michael Neumann, Patrick Mania, Michael Beetz","University of Bremen,Uni Bremen",Semantic Scene Understanding I,"In our previous work, we designed a human-like white-box and causal generative model of perception NaivPhys4RP, essentially based on cognitive emulation to understand the past, the present and the future of the state of complex worlds from poor observations. In this paper, as recommended in that previous work, we first refine the theoretical model of NaivPhys4RP in terms of integration of variables as well as perceptual inference tasks to solve. Intuitively, the system is closed under the injection, update and dependency of variables. Then, we present a first implementation of NaivPhys4RP that demonstrates the learningless and safe recognition and 6D-Pose estimation of objects from poor sensor data (e.g., occlusion, transparency, poor-depth, in-hand). This does not only make a substantial step forward comparatively to classical perception systems in perceiving objects in these scenarios, but escape the burden of data-intensive learning and operate safely (transparency and causality â€” we fit sensor data into mentally constructed meaningful worlds). With respect to ChatGPTâ€™s ambitions, it can imagine physico-realistic socio-physical scenes from texts, demonstrate understanding of these texts, and all these with no data- and resource-intensive learning."
Mapping High-Level Semantic Regions in Indoor Environments without Object Recognition,"Roberto Bigazzi, Lorenzo Baraldi, Shreyas Kousik, Rita Cucchiara, Marco Pavone","University of Modena and Reggio Emilia,Università degli Studi di Modena e Reggio Emilia,Georgia Institute of Technology,Stanford University",Semantic Scene Understanding I,"Robots require a semantic understanding of their surroundings to operate in an efficient and explainable way in human environments. In the literature, there has been an extensive focus on object labeling and exhaustive scene graph generation; less effort has been focused on the task of purely identifying and mapping large semantic regions. The present work proposes a method for semantic region mapping via embodied navigation in indoor environments, generating a high-level representation of the knowledge of the agent. To enable region identification, the method uses a vision-to-language model to provide scene information for mapping. By projecting egocentric scene understanding into the global frame, the proposed method generates a semantic map as a distribution over possible region labels at each location. This mapping procedure is paired with a trained navigation policy to enable autonomous map generation. The proposed method significantly outperforms a variety of baselines, including an object-based system and a pretrained scene classifier, in experiments in a photorealistic simulator."
LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model As an Agent,"Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David Fouhey, Joyce Chai","University of Michigan,Bloomberg",Semantic Scene Understanding I,"3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs significantly improve the grounding capability, especially for complex language queries, making LLM-Grounder an effective approach for 3D vision-language tasks in robotics."
Learning Off-Road Terrain Traversability with Self-Supervisions Only,"Junwon Seo, Sungdae Sim, Inwook Shim","Agency for Defense Development,Inha University",Semantic Scene Understanding I,"Estimating the traversability of terrain should be reliable and accurate in diverse conditions for autonomous driving in off-road environments. However, learning-based ap- proaches often yield unreliable results when confronted with unfamiliar contexts, and it is challenging to obtain manual annotations frequently for new circumstances. In this paper, we introduce a method for learning traversability from images that utilizes only self-supervision and no manual labels, enabling it to easily learn traversability in new circumstances. To this end, we first generate self-supervised traversability labels from past driving trajectories by labeling regions traversed by the vehicle as highly traversable. Using the self-supervised labels, we then train a neural network that identifies terrains that are safe to traverse from an image using a one-class classification algorithm. Additionally, we supplement the limitations of self- supervised labels by incorporating methods of self-supervised learning of visual representations. To conduct a comprehensive evaluation, we collect data in a variety of driving environments and perceptual conditions and show that our method produces reliable estimations in various environments. In addition, the experimental results validate that our method outperforms other self-supervised traversability estimation methods and achieves comparable performances with supervised learning methods trained on manually labeled data."
Improving Radial Imbalances with Hybrid Voxelization and RadialMix for LiDAR 3D Semantic Segmentation,"Jiale Li, Hang Dai, Yu Wang, Guangzhi Cao, Chun Luo, Yong Ding","Zhejiang University,University of Glasgow,YUNJI Technology Co. Ltd.,Pegasus Technology",Semantic Scene Understanding I,"Huge progress has been made in LiDAR 3D semantic segmentation, but there are still two under-explored imbalances on the radial axis: points are unevenly concentrated on the near side, and the distribution of foreground object instances is skewed to the near side. This leads the training of the model to favor semantics at the near side with the majority of points and object instances. Both the popular cylindrical and the neglected spherical voxelizations aim to address the problem of imbalanced point distribution by increasing the volume of voxels along the radial distance to include fewer near-side points in a smaller voxel and more far-side points in a bigger voxel. However, this causes a problem of the receptive field enlarging along the radial distance, which is not desirable in LiDAR point clouds since the size of an object is distance-independent. This can be addressed in cubic voxelization which has a fixed volume of voxels. Thus, we propose a new LiDAR 3D semantic segmentation network (Hi-VoxelNet) with Hybrid Voxelization that leverages the advantages of cubic, cylindrical, and spherical voxelizations for hybrid voxel feature learning. To address the radial imbalance of object instances, we propose a novel data augmentation technique termed as RadialMix that uses radial sample duplication to increase the number of distant foreground object instances and mixes the radial duplication with another point cloud for enriching the training samples. With the joint improvements of the radial imbalances, our method archives state-of-the-art performance on nuScenes and SemanticKITTI datasets, and consistently shows significant improvements along the radial distances. Our code is publicly available at https://github.com/jialeli1/lidarseg3d."
Few-Shot Panoptic Segmentation with Foundation Models,"Markus Käppeler, Kürsat Petek, Niclas Vödisch, Wolfram Burgard, Abhinav Valada","University of Freiburg,University of Technology Nuremberg",Semantic Scene Understanding I,"Current state-of-the-art methods for panoptic segmentation require an immense amount of annotated training data that is both arduous and expensive to obtain posing a significant challenge for their widespread adoption. Concurrently, recent breakthroughs in visual representation learning have sparked a paradigm shift leading to the advent of large foundation models that can be trained with completely unlabeled images. In this work, we propose to leverage such task-agnostic image features to enable few-shot panoptic segmentation by presenting Segmenting Panoptic Information with Nearly 0 labels (SPINO). In detail, our method combines a DINOv2 backbone with lightweight network heads for semantic segmentation and boundary estimation. We show that our approach, albeit being trained with only ten annotated images, predicts high-quality pseudo-labels that can be used with any existing panoptic segmentation method. Notably, we demonstrate that SPINO achieves competitive results compared to fully supervised baselines while using less than 0.3% of the ground truth labels, paving the way for learning complex visual recognition tasks leveraging foundation models. To illustrate its general applicability, we further deploy SPINO on real-world robotic vision systems for both outdoor and indoor environments. To foster future research, we make the code and trained models publicly available at http://spino.cs.uni-freiburg.de."
End-To-End Semantic Segmentation Network for Low-Light Scenes,"Hongmin Mu, Gang Zhang, Mengchu Zhou, Zhengcai Cao","Beijing University of Chemical Technology,New Jersey Institute of Technology,Harbin Institute of Technology",Semantic Scene Understanding I,"In the fields of robotic perception and computer vision, achieving accurate semantic segmentation of low-light or nighttime scenes is challenging. This is primarily due to the limited visibility of objects and the reduced texture and color contrasts among them. To address the issue of limited visibility, we propose a hierarchical gated convolution unit, which simultaneously expands the receptive field and restores edge texture. To address the issue of reduced texture among objects, we propose a dual closed-loop bipartite matching algorithm to establish a total loss function consisting of the unsupervised illumination enhancement loss and supervised intersection-over-union loss, thus enabling the joint minimization of both losses via the Hungarian algorithm. We thus achieve end-to-end training for a semantic segmentation network especially suitable for handling low-light scenes. Experimental results demonstrate that the proposed network surpasses existing methods on the Cityscapes dataset and notably outperforms state-of-the-art methods on both Dark Zurich and Nighttime Driving datasets."
DefFusion: Deformable Multimodal Representation Fusion for 3D Semantic Segmentation,"Rongtao Xu, Changwei Wang, Duzhen Zhang, Man Zhang, Shibiao Xu, Weiliang Meng, Xiaopeng Zhang","Institute of Automation, Chinese Academy of Sciences, Beijing, C,casia,Institute of Automation, Chinese Academy of Sciences,Beijing University of Posts and Telecommunications,National Laboratory of Pattern Recognition, Institute of Automat",Semantic Scene Understanding I,"The complementarity between camera and LiDAR data makes fusion methods a promising approach to improve 3D semantic segmentation performance. Recent transformer-based methods have also demonstrated superiority in segmentation. However, multimodal solutions incorporating transformers are underexplored and face two key inherent difficulties: over-attention and noise from different modal data. To overcome these challenges, we propose a Deformable Multimodal Representation Fusion (DefFusion) framework consisting mainly of a Deformable Representation Fusion Transformer and Dynamic Representation Augmentation Modules. The Deformable Representation Fusion Transformer introduces the deformable mechanism in multimodal fusion, avoiding over-attention and improving efficiency by adaptively modeling a 2D key/value set for a given 3D query, thus enabling multimodal fusion with higher flexibility. To enhance the 2D representation and 3D representation, the Dynamic Representation Enhancement Module is proposed to dynamically remove noise in the input representation via Dynamic Grouped Representation Generation and Dynamic Mask Generation. Extensive experiments validate that our model achieves the best 3D semantic segmentation performance on SemanticKITTI and NuScenes benchmarks."
Lifelong LERF: Local 3D Semantic Inventory Monitoring Using FogROS2,"Adam Rashid, Chung Min Kim, Justin Kerr, Letian Fu, Kush Hari, Ayah Ahmad, Kaiyuan Eric Chen, Huang Huang, Marcus Gualtieri, Michael Wang, Christian Juette, Nan Tian, Liu Ren, Ken Goldberg","UC Berkeley,University of California, Berkeley,University of California at Berkeley,Bosch Research,Bosch,Robert Bosch North America Research Technology Center",Semantic Scene Understanding I,"Inventory monitoring in homes, factories, and retail stores relies on maintaining data despite objects being swapped, added, removed, or moved. We introduce Lifelong LERF, a method that allows a mobile robot with minimal compute to jointly optimize a dense language and geometric representation of its surroundings. Lifelong LERF maintains this representation over time by detecting semantic changes and selectively updating these regions of the environment, avoiding the need to exhaustively remap. Human users can query inventory by providing natural language queries and receiving a 3D heatmap of potential object locations. To manage the computational load, we use Fog-ROS2, a cloud robotics platform, to offload resource-intensive tasks. Lifelong LERF obtains poses from a monocular RGBD SLAM backend, and uses these poses to progressively optimize a Language Embedded Radiance Field (LERF) for semantic monitoring. Experiments with 3-5 objects arranged on a tabletop and a Turtlebot with a RealSense camera suggest that Lifelong LERF can persistently adapt to changes in objects with up to 91% accuracy."
RGBManip: Monocular Image-Based Robotic Manipulation through Active Object Pose Estimation,"Boshi An, Yiran Geng, Kai Chen, Xiaoqi Li, Qi Dou, Hao Dong","Peking University,The Chinese University of Hong Kong",Deep Learning in Grasping and Manipulation IV,"Robotic manipulation requires accurate perception of the environment, which poses a significant challenge due to its inherent complexity and constantly changing nature. In this context, RGB image and point-cloud observations are two commonly used modalities in visual-based robotic manipulation, but each of these modalities have their own limitations. Commercial point-cloud observations often suffer from issues like sparse sampling and noisy output due to the limits of the emission-reception imaging principle. On the other hand, RGB images, while rich in texture information, lack essential depth and 3D information crucial for robotic manipulation. To mitigate these challenges, we propose an image-only robotic manipulation framework that leverages an eye-on-hand monocular camera installed on the robot's parallel gripper. By moving with the robot gripper, this camera gains the ability to actively perceive the object from multiple perspectives during the manipulation process. This enables the estimation of 6D object poses, which can be utilized for manipulation. While, obtaining images from more and diverse viewpoints typically improves pose estimation, it also increases the manipulation time. To address this trade-off, we employ a reinforcement learning policy to synchronize the manipulation strategy with active perception, achieving a balance between 6D pose accuracy and manipulation efficiency. Our experimental results in both simulated and real-world environments showcase the state-of-the-art effectiveness of our approach. We believe that our method will inspire further research on real-world-oriented robotic manipulation."
Part-Guided 3D RL for Sim2Real Articulated Object Manipulation,"Pengwei Xie, Rui Chen, Siang Chen, Yuzhe Qin, Fanbo Xiang, Tianyu Sun, Jing Xu, Guijin Wang, Hao Su","Tsinghua University,UC San Diego,University of California San Diego,UCSD",Deep Learning in Grasping and Manipulation IV,"Manipulating unseen articulated objects through visual feedback is a critical but challenging task for real robots. Existing learning-based solutions mainly focus on visual affordance learning or other pre-trained visual models to guide manipulation policies, which face challenges for novel instances in real-world scenarios. In this paper, we propose a novel part-guided 3D RL framework, which can learn to manipulate articulated objects without demonstrations. We combine the strengths of 2D segmentation and 3D RL to improve the efficiency of RL policy training. To improve the stability of the policy on real robots, we design a Frame-consistent Uncertainty-aware Sampling (FUS) strategy to get a condensed and hierarchical 3D representation. In addition, a single versatile RL policy can be trained on multiple articulated object manipulation tasks simultaneously in simulation and shows great generalizability to novel categories and instances. Experimental results demonstrate the effectiveness of our framework in both simulation and real-world settings."
MORPH: Design Co-Optimization with Reinforcement Learning Via a Differentiable Hardware Model Proxy,"Zhanpeng He, Matei Ciocarlie",Columbia University,Deep Learning in Grasping and Manipulation IV,"We introduce MORPH, a method for co-optimization of hardware design parameters and control policies in simulation using reinforcement learning. Like most co-optimization methods, MORPH relies on a model of the hardware being optimized, usually simulated based on the laws of physics. However, such a model is often difficult to integrate into an effective optimization routine. To address this, we introduce a proxy hardware model, which is always differentiable and enables efficient co-optimization alongside a long-horizon control policy using RL. MORPH is designed to ensure that the optimized hardware proxy remains as close as possible to its realistic counterpart, while still enabling task completion. We demonstrate our approach on simulated 2D reaching and 3D multi-fingered manipulation tasks."
Mastering Stacking of Diverse Shapes with Large-Scale Iterative Reinforcement Learning on Real Robots,"Thomas Lampe, Abbas Abdolmaleki, Sandy H. Huang, Sarah Bechtle, Jost Tobias Springenberg, Michael Blösch, Oliver Groth, Roland Hafner, Tim Hertweck, Michael Neunert, Markus Wulfmeier, Jingwei Zhang, Francesco Nori, Nicolas Heess, Martin Riedmiller","Google UK Ltd.,DeepMind,Google DeepMind,Albert-Ludwigs Universitaet Freiburg,Google,University of Oxford,Google Deepmind",Deep Learning in Grasping and Manipulation IV,"Reinforcement learning solely from an agent's self-generated data is often believed to be infeasible for learning on real robots, due to the amount of data needed. However, if done right, agents learning from real data can be surprisingly efficient through re-using previously collected sub-optimal data. In this paper we demonstrate how the increased understanding of off-policy learning methods and their embedding in an iterative online/offline scheme (""collect and infer"") can drastically improve data-efficiency by using all the collected experience, which empowers learning from real robot experience only. Moreover, the resulting policy improves significantly over the state of the art on a recently proposed real robot manipulation benchmark. Our approach learns end-to-end, directly from pixels, and does not rely on additional human domain knowledge such as a simulator or demonstrations."
Information-Driven Affordance Discovery for Efficient Robotic Manipulation,"Pietro Mazzaglia, Taco Cohen, Daniel Dijkman","University of Gent,Qualcomm AI Research,Qualcomm",Deep Learning in Grasping and Manipulation IV,"Robotic affordances, providing information about what actions can be taken in a given situation, can aid robotic manipulation. However, learning about affordances requires expensive large annotated datasets of interactions or demonstrations. In this work, we argue that well-directed interactions with the environment can mitigate this problem and propose an information-based measure to augment the agent's objective and accelerate the affordance discovery process. We provide a theoretical justification of our approach and we empirically validate the approach both in simulation and real-world tasks. Our method, which we dub IDA, enables the efficient discovery of visual affordances for several action primitives, such as grasping, stacking objects, or opening drawers, strongly improving data efficiency in simulation, and it allows us to learn grasping affordances in a small number of interactions, on a real-world setup with a UFACTORY xArm 6 robot arm."
HybGrasp: A Hybrid Learning-To-Adapt Architecture for Efficient Robot Grasping,"Jungwook Mun, Khang Truong Giang, Yunghee Lee, Nayoung Oh, Sejoon Huh, Min Kim, Sungho Jo","Korea Advanced Institute of Science and Technology,KAIST,Korea Advanced Institute of Science and Technology (KAIST)",Deep Learning in Grasping and Manipulation IV,"Despite the prevalence of robotic manipulation tasks in various real-world applications of different requirements and needs, there has been a lack of focus on enhancing the adaptability of robotic grasping systems. Most of the current literature constructs models around a single gripper, succumbing to a tradeoff between gripper complexity and generalizability. Adapting such models pre-trained on one type of gripper to another to work around the tradeoff is inefficient and not scalable, as it would require tremendous effort and computational cost to generate new datasets and relearn the grasping task. In this letter, we propose a novel hybrid architecture for robot grasping that efficiently learns to adapt to different gripper designs. Our approach involves a three-step process that first obtains a rough grasp pose prediction from a parallel gripper model, then predicts an adaptive action using a convolutional neural network, and finally refines the predicted action with reinforcement learning. The proposed method shows significant improvements in grasping performance compared to existing methods for both generated datasets and real-world scenarios, presenting a promising direction for improving the adaptability and flexibility of robotic manipulation systems."
Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes,"Siang Chen, Wei Tang, Pengwei Xie, Wenming Yang, Guijin Wang",Tsinghua University,Deep Learning in Grasping and Manipulation IV,"Fast and robust object grasping in clutter is a crucial component of robotics. Most current works resort to the whole observed point cloud for 6-Dof grasp generation, ignoring the guidance information excavated from global semantics, thus limiting high-quality grasp generation and real-time performance. In this work, we show that the widely used heatmaps are underestimated in the efficiency of 6-Dof grasp generation. Therefore, we propose an effective local grasp generator combined with grasp heatmaps as guidance, which infers in a global-to-local semantic-to-point way. Specifically, Gaussian encoding and the grid-based strategy are applied to predict grasp heatmaps as guidance to aggregate local points into graspable regions and provide global semantic information. Further, a novel non-uniform anchor sampling mechanism is designed to improve grasp accuracy and diversity. Benefiting from the high-efficiency encoding in the image space and focusing on points in local graspable regions, our framework can perform high-quality grasp detection in real-time and achieve state-of-the-art results. In addition, real robot experiments demonstrate the effectiveness of our method with a success rate of 94% and a clutter completion rate of 100%."
A Hyper-Network Based End-To-End Visual Servoing with Arbitrary Desired Poses,"Hongxiang Yu, Anzhe Chen, Kechun Xu, Zhongxiang Zhou, Wei Jing, Yue Wang, Rong Xiong","Zhejiang University,Alibaba",Deep Learning in Grasping and Manipulation IV,"Recently, several works achieve end-to-end visual servoing (VS) for robotic manipulation by replacing traditional controller with differentiable neural networks, but lose the ability to servo arbitrary desired poses. This letter proposes a differentiable architecture for arbitrary pose servoing: a hyper-network based neural controller (HPN-NC). To achieve this, HPN-NC consists of a hyper net and a low-level controller, where the hyper net learns to generate the parameters of the low-level controller and the controller uses the 2D keypoints error for control like traditional image-based visual servoing (IBVS). HPN-NC can complete 6 degree of freedom visual servoing with large initial offset. Taking advantage of the fully differentiable nature of HPN-NC, we provide a three-stage training procedure to servo real world objects. With self-supervised end-to-end training, the performance of the integrated model can be further improved in unseen scenes and the amount of manual annotations can be significantly reduced."
6-DoF Closed-Loop Grasping with Reinforcement Learning,"Sverre Herland, Kerstin Bach, Ekrem Misimi","Norwegian University of Science and Technology,SINTEF Ocean",Deep Learning in Grasping and Manipulation IV,"We present a novel vision-based, 6-DoF grasping framework based on Deep Reinforcement Learning (DRL) that is capable of directly synthesizing continuous 6-DoF actions in cartesian space. Our proposed approach uses visual observations from an eye-in-hand RGB-D camera, and we mitigate the sim-to-real gap with a combination of domain randomization, image augmentation, and segmentation tools. Our method consists of an off-policy, maximum-entropy, Actor-Critic algorithm that learns a policy from a binary reward and a few simulated example grasps. It does not need any real-world grasping examples, is trained completely in simulation, and is deployed directly to the real world without any fine-tuning. The efficacy of our approach is demonstrated in simulation and experimentally validated in the real world on 6-DoF grasping tasks, achieving state-of-the-art results of an 86% mean zero-shot success rate on previously unseen objects, an 85% mean zero-shot success rate on a class of previously unseen adversarial objects, and a 74.3% mean zero-shot success rate on a class of previously unseen, challenging ""6-DoF"" objects. Raw footage of real-world validation can be found at https://youtu.be/bwPf8Imvook"
Self-Supervised 6-DoF Robot Grasping by Demonstration Via Augmented Reality Teleoperation System,"Xiwen Dengxiong, Xueting Wang, Shi Bai, Yunbo Zhang","Rochester Institute of Technology,Wing",Human-Robot Collaboration I,"Most existing 6-DoF robot grasping solutions depend on strong supervision on grasp pose to ensure satisfactory performance, which could be laborious and impractical when the robot works in some restricted area. To this end, we propose a self-supervised 6-DoF grasp pose detection framework via an Augmented Reality (AR) teleoperation system that can efficiently learn human demonstrations and provide 6-DoF grasp poses without grasp pose annotations. Specifically, the system collects the human demonstration from the AR environment and contrastively learns the grasping strategy from the demonstration. For the real-world experiment, the proposed system leads to satisfactory grasping abilities and learning to grasp unknown objects within three demonstrations."
Trust Recognition in Human-Robot Cooperation Using EEG,"Caiyue Xu, Changming Zhang, Yanmin Zhou, Zhipeng Wang, Ping Lu, Bin He",Tongji University,Human-Robot Collaboration I,"Collaboration between humans and robots is becoming increasingly crucial in our daily life. In order to accomplish efficient cooperation, trust recognition is vital, empowering robots to predict human behaviors and make trust-aware decisions. Consequently, there is an urgent need for a generalized approach to recognize human-robot trust. This study addresses this need by introducing an EEG-based method for trust recognition during human-robot cooperation. A human-robot cooperation game scenario is used to stimulate various human trust levels when working with robots. To enhance recognition performance, the study proposes an EEG Vision Transformer model coupled with a 3-D spatial representation to capture the spatial information of EEG, taking into account the topological relationship among electrodes. To validate this approach, a public EEG-based human trust dataset called EEGTrust is constructed. Experimental results indicate the effectiveness of the proposed approach, achieving an accuracy of 74.99% in slice-wise cross-validation and 62.00% in trial-wise cross-validation. This outperforms baseline models in both recognition accuracy and generalization. Furthermore, an ablation study demonstrates a significant improvement in trust recognition performance of the spatial representation. The source code and EEGTrust dataset are available at https://github.com/CaiyueXu/EEGTrust."
Multimodal Multi-User Surface Recognition with the Kernel Two-Sample Test,"Behnam Khojasteh, Friedrich Solowjow, Sebastian Trimpe, Katherine J. Kuchenbecker","Max Planck Institute for Intelligent Systems,RWTH Aachen University",Human-Robot Collaboration I,"Machine learning and deep learning have been used extensively to classify physical surfaces through images and time-series contact data. However, these methods rely on human expertise and entail the time-consuming processes of data and parameter tuning. To overcome these challenges, we propose an easily implemented framework that can directly handle heterogeneous data sources for classification tasks. Our data-versus-data approach automatically quantifies distinctive differences in distributions in a high-dimensional space via kernel two-sample testing between two sets extracted from multimodal data (e.g., images, sounds, haptic signals). We demonstrate the effectiveness of our technique by benchmarking against expertly engineered classifiers for visual-audio-haptic surface recognition due to the industrial relevance, difficulty, and competitive baselines of this application; ablation studies confirm the utility of key components of our pipeline. As shown in our open-source code, we achieve 97.2% accuracy on a standard multi-user dataset with 108 surface classes, outperforming the state-of-the-art machine-learning algorithm by 6% on a more difficult version of the task. The fact that our classifier obtains this performance with minimal data processing in the standard algorithm setting reinforces the powerful nature of kernel methods for learning to recognize complex patterns."
Learning User Preferences for Complex Cobotic Tasks: Meta-Behaviors and Human Groups,"Elena Vella, Airlie Chapman, Nir Lipovetzky","The University of Melbourne,University of Melbourne",Human-Robot Collaboration I,"In complex tasks (beyond a single targeted controller) requiring robots to collaborate with multiple human users, two challenges arise: complex tasks are often composed of multiple behaviors which can only be evaluated as a collective (a meta-behavior) and user preferences often differ between individuals, yet successful interactions are expected across groups. To address these challenges, we formulate a set-wise preference learning problem, and validate a cost function that captures human group preferences for complex collaborative robotic tasks (cobotics). We develop a sparse optimization formulation to introduce a distinctiveness metric that aggregates individuals with similar preference profiles. Analysis of anonymized unlabelled preferences provides further insight into group preferences. Identification of the mode average most-preferred meta-behavior and minimum covariance bound allows us to analyze group cohesion. A user study with 43 participants is used to validate group preference profiles."
Learning Self-Confidence from Semantic Action Embeddings for Improved Trust in Human-Robot Interaction,"Cedric Goubard, Yiannis Demiris",Imperial College London,Human-Robot Collaboration I,"In Human-Robot Interaction scenarios, human factors like trust can greatly impact task performance and interaction quality. Recent research has confirmed that perceived robot proficiency is a major antecedent of trust. By making robots aware of their capabilities, we can allow them to choose when to perform low-confidence actions, thus actively controlling the risk of trust reduction. In this paper, we propose Self-Confidence through Observed Novel Experiences (SCONE), a policy to learn self-confidence from experience using semantic action embeddings. Using an assistive cooking setting, we show that the semantic aspect allows SCONE to learn self-confidence faster than existing approaches, while also achieving promising performance in simple instructions following. Finally, we share results from a pilot study with 31 participants, showing that such a self-confidence-aware policy increases capability-based human trust."
Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models,"Zhen Zhang, Anran Lin, Chun Wai Wong, Xiangyu Chu, Qi Dou, Samuel Au",The Chinese University of Hong Kong,Human-Robot Collaboration I,"This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like â€œCan you pass through the curtains to deliver medicines to me?â€, to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable objects such as curtains and grasses for verification by instructing the robot to traverse them. Besides, traversing curtains in a medical scenario was tested. All experimental results demonstrated the proposed frameworkâ€™s effectiveness and adaptability to diverse environments."
From Unstable Electrode Contacts to Reliable Control: A Deep Learning Approach for HD-sEMG in Neurorobotics,"Eion Tyacke, Kunal Gupta, Jay Patel, Raghav Katoch, S. Farokh Atashzar","New York University,New York University (NYU), US",Human-Robot Collaboration I,"In the past decade, there has been significant advancement in designing wearable neural interfaces for controlling neurorobotic systems, particularly bionic limbs. These interfaces function by decoding signals captured non-invasively from the skin's surface. Portable high-density surface electromyography (HD-sEMG) modules combined with deep learning decoding have attracted interest by achieving excellent gesture prediction and myoelectric control of prosthetic systems and neurorobots. However, factors like small electrode size and unstable electrode-skin contacts make HD-sEMG susceptible to pixel electrode drops. The sparse electrode-skin disconnections rooted in issues such as low adhesion, sweating, hair blockage, and skin stretch challenge the reliability and scalability of these modules as the perception unit for neurorobotic systems. This paper proposes a novel deep-learning model providing resiliency for HD-sEMG modules, which can be used in the wearable interfaces of neurorobots. The proposed 3D Dilated Efficient CapsNet model trains on an augmented input space to computationally `force' the network to learn channel dropout variations and thus learn robustness to channel dropout. The proposed framework maintained high performance under a sensor dropout reliability study conducted. Results show conventional models' performance significantly degrades with dropout and is recovered using the proposed architecture and the training paradigm."
Enhanced Human-Robot Collaboration with Intent Prediction Using Deep Inverse Reinforcement Learning,"Mukund Mitra, Gyanig Kumar, Partha Pratim Chakrabarti, Pradipta Biswas","IISc Bangalore,Indian Institute of Sciences, India,Indian Institute of Technology, Kharagpur, India,Indian Institute of Science",Human-Robot Collaboration I,"In shared autonomy, human-robot handover for object delivery is crucial. Accurate robot predictions of human hand motion and intentions enhance collaboration efficiency. However, low prediction accuracy increases mental and physical demands on the user. In this work, we propose a system for predicting hand motion and intended target during human-robot handover using Inverse Reinforcement Learning (IRL). A set of feature functions were designed to explicitly capture usersâ€™ preferences during the task. The proposed approach was experimentally validated through user studies. Results indicate that the proposed method outperformed other state-of-the-art methods (PI-IRL, BP-HMT, RNNIK-MKF and CMk=5) with users feeling comfortable reaching upto 60% of the total distance to the target for handover with 90% target prediction accuracy. The target prediction accuracy reaches 99.9% when less than 20% of the task remains."
ToP-ToM: Trust-Aware Robot Policy with Theory of Mind,"Chuang Yu, Baris Serhan, Angelo Cangelosi","University College London,The University of Manchester,University of Manchester",Human-Robot Collaboration I,"Theory of Mind (ToM) is a fundamental cognitive architecture that endows humans with the ability to attribute mental states to others. Humans infer the desires, beliefs, and intentions of others by observing their behavior and, in turn, adjust their actions to facilitate better interpersonal communication and team collaboration. In this paper, we investigated trust-aware robot policy with the theory of mind in a multiagent setting where a human collaborates with a robot against another human opponent. We show that by only focusing on team performance, the robot may resort to the reverse psychology trick, which poses a significant threat to trust maintenance. The human's trust in the robot will collapse when they discover deceptive behavior by the robot. To mitigate this problem, we adopt the robot theory of mind model to infer the human's trust beliefs, including true belief and false belief (an essential element of ToM). We designed a dynamic trust-aware reward function based on different trust beliefs to guide the robot policy learning, which aims to balance between avoiding human trust collapse due to robot reverse psychology and leveraging its potential to boost team performance. The experimental results demonstrate the importance of the ToM-based robot policy for human-robot trust and the effectiveness of our robot ToM-based robot policy in multiagent interaction settings."
VIDAR: Data Quality Improvement for Monocular 3D Reconstruction through In-Situ Visual Interaction,"Han Gao, Yating Liu, Fang Cao, Hao Wu, Fengyuan Xu, Sheng Zhong","National Key Lab for Novel Software Technology, Nanjing Universi,Nanjing University",Human Factors and Human-In-The-Loop I,"3D reconstruction based on monocular videos has attracted wide attention, and existing reconstruction methods usually work in a reconstruction-after-scanning manner. However, these methods suffer from insufficient data collection problems due to the lack of effective guidance for users during the scanning process, which affects reconstruction quality. We propose VIDAR, which visually guides users with the streaming incremental reconstructed mesh in data collection for monocular 3D reconstruction. We propose an incremental mesh extraction algorithm to achieve lossless fusion of streaming incremental mesh data via slice-style management for guidance quality. We also design an incremental mesh rendering algorithm to achieve precise memory reallocation by updating the buffer in a fill-in-the-blank pattern for guidance efficiency. Besides, we introduce several optimizations on data transmission and human-computer interaction to improve the overall system performance. The experiment results on real-world scenes show that VIDAR efficiently delivers high-quality visual guidance and outperforms the non-interactive data collection methods for scene reconstruction."
Transparency Control of a 1-DoF Knee Exoskeleton Via Human-In-The-Loop Velocity Optimisation,"Lukas Cha, Annika Guez, Chih-yu Chen, Sion Kim, Zhenhua Yu, Bo Xiao, Ravi Vaidyanathan","Technical University of Munich,Imperial College London,Imperial college London",Human Factors and Human-In-The-Loop I,"Rehabilitative robotics, particularly lower-limb exoskeletons (LLEs), have gained increasing importance in aiding patients regain ambulatory functions. One of the challenges in making these systems effective is the implementation of an assist-as-needed (AAN) control strategy that intervenes only when the patient deviates from the correct movement pattern. Equally crucial is the need for the LLE to exhibit ""transparency"" â€” minimising its interaction forces with the wearer to feel as natural as possible. This paper introduces a novel approach to transparency control based on a human-in-the-loop velocity optimisation framework. The proposed method employs torque data captured from past steps through a Series Elastic Actuator (SEA) to approximate the wearer's intended future movements and computes a corresponding transparent velocity trajectory. The velocity commands are complemented by an Adaptive Frequency Oscillator (AFO) based position controller that leverages the periodic nature of human gait and is modified with a force sensor for increased reactiveness to human gait variations. This approach is experimentally evaluated against a standard zero-torque controller with a stationary single-degree-of-freedom knee exoskeleton test platform in a proof-of-concept study. Preliminary results indicate that combining adaptive oscillators with interaction force sensing can improve transparency compared to the conventional zero-torque controller, using force readings for position control and torque measurements for velocity optimisation and control."
Towards Enhanced Human Activity Recognition for Real-World Human-Robot Collaboration,"Beril Yalçinkaya, Micael Couceiro, Lucas Pina, Salviano Soares, António Valente, Fabio Remondino","Ingeniarius Lda,University of Coimbra,UTAD,University of trás os montes and alto douro,FBK",Human Factors and Human-In-The-Loop I,"This research contributes to the field of Human-Robot Collaboration (HRC) within dynamic and unstructured environments by extending the previously proposed Fuzzy State-Long Short-Term Memory (FS-LSTM) architecture to handle the uncertainty and irregularity inherent in real-world sensor data. Recognising the challenges posed by low-cost sensors, which are highly susceptible to environmental conditions and often fail to provide regular periodic readings, this paper introduces additional pre-processing blocks. These include two indirect Kalman filters and an additional LSTM network, which together enhance the input variables for the fuzzification process. The enhanced FS-LSTM approach is evaluated using real-world data, demonstrating its effectiveness in extracting meaningful information and accurately recognising human activities. This work underscores the potential of robotics in addressing global challenges, particularly in labour-intensive and hazardous tasks. By improving the integration of humans and robots in unstructured environments, this research contributes to the broader exploration of robotics in new societal applications, fostering connections and collaborations across diverse fields."
Self-Supervised Regression of sEMG Signals Combining Non-Negative Matrix Factorization with Deep Neural Networks for Robot Hand Multiple Grasping Motion Control,"Roberto Meattini, Alessio Caporali, Alessandra Bernardini, Gianluca Palli, Claudio Melchiorri",University of Bologna,Human Factors and Human-In-The-Loop I,"Advanced Human-In-The-Loop (HITL) control strategies for robot hands based on surface electromyography (sEMG) are among major research questions in robotics. Due to intrinsic complexity and inaccuracy of labeling procedures, unsupervised regression of sEMG signals has been employed in literature, however showing several limitations in realizing multiple grasping motion control. In this work, we propose a novel Human-Robot interface (HRi) based on self-supervised regression of sEMG signals, combining Non-Negative Matrix Factorization (NMF) with Deep Neural Networks (DNN) in order to both avoid explicit labeling procedures and have powerful nonlinear fitting capabilities. Experiments involving 10 healthy subjects were carried out, consisting of an offline session for systematic evaluations and comparisons with traditional unsupervised approaches, and an online session for assessing realtime control of a wearable anthropomorphic robot hand. The offline results demonstrate that the proposed self-supervised regression approach overcame traditional unsupervised methods, even considering different robot hands with dissimilar kinematic structures. Furthermore, the subjects were able to successfully perform online control of multiple grasping motions of a real wearable robot hand, reporting for high reliability over repeated grasp-transportation-release tasks with different objects. Statistical support is provided along with experimental outcomes."
Maximising Coefficiency of Human-Robot Handovers through Reinforcement Learning,"Marta Lagomarsino, Marta Lorenzini, Merryn Dale Constable, Elena De Momi, Cristina Becchio, Arash Ajoudani","Istituto Italiano di Tecnologia,Northumbria University,Politecnico di Milano,University Medical Center Hamburg-Eppendorf",Human Factors and Human-In-The-Loop I,"Handing objects to humans is an essential capability for collaborative robots. Previous research works on human-robot handovers focus on facilitating the performance of the human partner and possibly minimising the physical effort needed to grasp the object. However, altruistic robot behaviours may result in protracted and awkward robot motions, contributing to unpleasant sensations by the human partner and affecting perceived safety and social acceptance. This paper investigates whether transferring the psychological principle that ""humans act coefficiently as a group"" (i.e. simultaneously maximising the benefits of all agents involved) to human-robot cooperative tasks promotes a more seamless and natural interaction. Human-robot coefficiency is first modelled by identifying implicit indicators of human comfort and discomfort as well as calculating the robot energy consumption in performing the desired trajectory. We then present a reinforcement learning approach that uses the human-robot coefficiency score as reward to adapt and learn online the combination of robot interaction parameters that maximises such coefficiency. Results proved that by acting coefficiently the robot could meet the individual preferences of most subjects involved in the experiments, improve the human perceived comfort, and foster trust in the robotic partner."
Jacquard V2: Refining Datasets Using the Human in the Loop Data Correction Method,"Qiuhao Li, Shenghai Yuan","Northeastern University,NANYANG TECHNOLOGICAL UNIVERSITY",Human Factors and Human-In-The-Loop I,"In the context of rapid advancements in industrial automation, vision-based robotic grasping plays an increasingly crucial role. In order to enhance visual recognition accuracy, the utilization of large-scale datasets is imperative for training models to acquire implicit knowledge related to the handling of various objects. Creating datasets from scratch is a time and labor-intensive process. Moreover, existing datasets often contain errors due to automated annotations aimed at expediency, making the improvement of these datasets a substantial research challenge. Consequently, several issues have been identified in the annotation of grasp bounding boxes within the popular Jacquard Grasp. We propose utilizing a Human-In-The-Loop(HIL) method to enhance dataset quality. This approach relies on backbone deep learning networks to predict object positions and orientations for robotic grasping. Predictions with Intersection over Union (IOU) values below 0.2 undergo an assessment by human operators. After their evaluation, the data is categorized into False Negatives(FN) and True Negatives(TN). FN are then subcategorized into either missing annotations or catastrophic labeling errors. Images lacking labels are augmented with valid grasp bounding box information, whereas images afflicted by catastrophic labeling errors are completely removed. The open-source tool Labelbee was employed for 53,026 iterations of HIL dataset enhancement, leading to the removal of 2,884 images and the incorporation of ground truth information for 30,292 images. The enhanced dataset, named the Jacquard V2 Grasping Dataset, served as the training data for a range of neural networks. We have empirically demonstrated that these dataset improvements significantly enhance the training and prediction performance of the same network, resulting in an increase of 7.1% across most popular detection architectures for ten iterations. This refined dataset will be accessible on Google Drive and Baidu Netdisk,"
Decision Making for Human-In-The-Loop Robotic Agents Via Uncertainty-Aware Reinforcement Learning,"Siddharth Singi, Zhanpeng He, Alvin Pan, Sandipkumar Patel, Gunnar Sigurdsson, Robinson Piramuthu, Shuran Song, Matei Ciocarlie","Columbia University,Amazon",Human Factors and Human-In-The-Loop I,"In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly autonomously in solving a task, but can request help from an external expert when needed. However, knowing when to request such assistance is critical: too few requests can lead to the robot making mistakes, but too many requests can overload the expert. In this paper, we present a Reinforcement learning-based approach to this problem, where a semi-autonomous agent asks for external assistance when it has low confidence in the eventual success of the task. The confidence level is computed by estimating the variance of the return from the current state. We iteratively improve this estimate during training using a Bellman-like recursion. On discrete navigation problems with both fully- and partially-observable state information, we show that our method makes effective use of a limited budget of expert calls at run-time, despite having no access to the expert at training time. To the best of our knowledge, this is the first instance of using the variance of the return computed in an RL framework as a guidance measure for a Human-in-the-Loop agent."
Building User Proficiency in Piloting Small Unmanned Aerial Vehicles (sUAV),"Siya Kunde, Brittany Duncan","University of Nebraska,University of Nebraska, Lincoln",Human Factors and Human-In-The-Loop I,"Assessing proficiency in small unmanned aerial vehicles (sUAVs) pilots is complex and not well understood, but increasingly important to employ these vehicles in serious jobs such as wildland firefighting and infrastructure inspection. The limited prior work with UAVs has focused on user training using modalities like simulators and VR and no performance assessments with line-of-sight UAVs. This paper presents a training methodology for novice pilots of sUAVs. We presented two studies: the Baseline study (21 participants) and the Training study (16 participants). Our work is of interest to sUAV operators, regulators, and companies developing this technologies to produce a more capable workforce capable of consistent, safe operations. We successfully utilized the method developed in cite{kunde2022recognizing} to assess user proficiency in flying UAVs. We presented a UAV pilot training schedule for novice users (in the Training study), and were able to determine the minimum training time necessary to observe performance gains and mitigate damage. Results indicate that task completions noticeably improved and crashes minimized by day 10 of training, with a training plateau observed by day 15."
A Probabilistic Model for Cobot Decision Making to Mitigate Human Fatigue in Repetitive Co-Manipulation Tasks,"Aya Yaacoub, Vincent Thomas, Francis Colas, Pauline Maurice","LORIA-CNRS,LORIA - Universite de Lorraine,Inria Nancy Grand Est,CNRS - LORIA",Human Factors and Human-In-The-Loop I,"Work related musculoskeletal disorders (WMSDs) are very common. Repetitive motion, which is often present in industrial work, is one of the main physical causes of WMSDs. It uses the same set of human joints repeatedly, which leads to localized joint fatigue. In this work, we present a framework to plan a policy of a collaborative robot that reduces the human fatigue in the long term, in highly repetitive co-manipulation tasks, while taking into account the uncertainty in the human postural reaction to the robot motion and the partial observability of the human fatigue state. We model the problem using continuous-state Partially Observable Markov Decision Process (POMDP), and use a physics-based digital human simulator to predict the fatigue cost of the possible robot actions. We then use an online planning algorithm to compute the optimal robot policy. We demonstrate our approach on a simulated experiment in which a robot repeatedly carries an object for the human to work on, and the object Cartesian pose needs to be optimized. We compare the policy generated with our approach with a random, a cyclic and a greedy (short-term optimization) policy, for different user profiles. We show that our approach outperforms the other policies on all tested scenarios."
GelRoller: A Rolling Vision-Based Tactile Sensor for Large Surface Reconstruction Using Self-Supervised Photometric Stereo Method,"Zhiyuan Zhang, Huan Ma, Yulin Zhou, Jingjing Ji, Hua Yang",Huazhong University of Science and Technology,Force and Tactile Sensing IV,"Accurate perception of the surrounding environment stands as a primary objective for robots. Through tactile interaction, vision-based tactile sensors provide the capability to capture high-resolution and multi-modal surface information of objects, thereby facilitating robots in achieving more dexterous manipulations. However, the prevailing GelSight sensors entail intricate calibration procedures, posing challenges in their application on curved surfaces and requiring the maintenance of stable lighting conditions throughout experimentation. Additionally, constrained by shape and structure, current vision-based tactile sensors are predominantly applied to measurements within a limited area. In this study, we design a novel cylindrical vision-based tactile sensor that enables continuous and swift perception of large-scale object surfaces through rolling. To tackle the challenges posed by laborious calibration processes, we propose a self-supervised photometric stereo method based on deep learning, which eliminates pre-calibration requirements and enables the derivation of surface normals from a single image without relying on stable lighting conditions. Finally, we perform surface reconstruction from normal and point cloud registration on the multiple frames of images obtained by rolling the cylindrical sensor, resulting in large surface reconstruction. We compare our method with the representative lookup table method in the GelSight sensors. The results show that the proposed method enhances both reconstruction accuracy and robustness, thereby demonstrating the potential of the proposed sensor in large-scale surface reconstruction. Codes and mechanical structures are available at: https://github.com/ZhangZhiyuanZhang/GelRoller"
Marker-Embedded Tactile Image Generation Via Generative Adversarial Networks,"Won Dong Kim, Sanghoon Yang, Woojong Kim, Jeong-Jung Kim, Chang-Hyun Kim, Jung Kim","Korea Advanced Institute of Science & Technology (KAIST),KAIST,Korea Institute of Machinery & Materials (KIMM),Korea Institute of Machinery and Materials (KIMM)",Force and Tactile Sensing IV,"Data-driven methods have been successfully applied to images from vision-based tactile sensors to fulfill various manipulation tasks. Nevertheless, these methods remain inefficient because of the lack of methods for simulating the sensors. Relevant research on simulating vision-based tactile sensors generally focus on generating images without markers, owing to the challenges in accurately generating marker motions caused by elastomer deformation. This disallows access to tactile information deducible from markers. In this work, we propose a generative adversarial network (GAN)-based method to generate realistic marker-embedded tactile images in GelSight-like vision-based tactile sensors. We trained the proposed GAN model with an aligned real tactile and simulated depth image dataset obtained from deforming the sensor against various objects. This allows the model to translate simulated depth image sequences into RGB tactile images with markers. Furthermore, the generator in the proposed GAN allows the network to integrate the history of deformations from the depth image sequences to generate realistic marker motions during the normal and lateral sensor deformations. We evaluated and compared the positional accuracy of the markers and image similarity metrics of the images generated via our method with those from prior methods. The generated tactile images from the proposed model show a 28.3 % decrease in marker positional error and a 93.5 % decrease in the image similarity m"
TEXterity: Tactile Extrinsic DeXterity,"Antonia Bronars, Sangwoon Kim, Parag Patre, Alberto Rodriguez","MIT,Massachusetts Institute of Technology,Magna International",Force and Tactile Sensing IV,"We introduce a novel approach that combines tactile estimation and control for in-hand object manipulation. By integrating measurements from robot kinematics and an image-based tactile sensor, our framework estimates and tracks object pose while simultaneously generating motion plans in a receding horizon fashion to control the pose of a grasped object. This approach consists of a discrete pose estimator that tracks the most likely sequence of object poses in a coarsely discretized grid, and a continuous pose estimator-controller to refine the pose estimate and accurately manipulate the pose of the grasped object. Our method is tested on diverse objects and configurations, achieving desired manipulation objectives and outperforming single-shot methods in estimation accuracy. The proposed approach holds potential for tasks requiring precise manipulation and limited intrinsic in-hand dexterity under visual occlusion, laying the foundation for closed-loop behavior in applications such as regrasping, insertion, and tool use. Please see https://sites.google.com/view/texterity for videos of real-world demonstrations."
Optimization of Flexible Bronchoscopy Shape Sensing Using Fiber Optic Sensors,"Xinran Liu, Hao Chen, Hongbin Liu","university of chinese academy of sciences,University of Chinese Academy of Sciences,Hong Kong Institute of Science & Innovation, Chinese Academy of ",Force and Tactile Sensing IV,"This work presents a novel shape evaluation and optimization approach for shape sensing, specifically targeting the constrained, irregular, and intricate spatial shapes of flexible bronchoscopes (FB) in human bronchial tree. The proposed evaluation criteria and optimization methods combine clinical significance related to bronchial anatomical structures and address issues related to singular points and discontinuities in traditional shape reconstruction models. Three-dimensional experiments were conducted within eight spatial complex configurations printed from a proportional bronchial model. The 3D experiment results demonstrate an average reduction of approximately 34.1% in shape reconstruction errors across all eight airway models compared to the traditional model, validating the effectiveness and feasibility."
Tactile-Informed Action Primitives Mitigate Jamming in Dense Clutter,"Dane Brouwer, Joshua Citron, Hojung Choi, Marion Lepert, Michael Lin, Jeannette Bohg, Mark Cutkosky",Stanford University,Force and Tactile Sensing IV,"It is difficult for robots to retrieve objects in densely cluttered lateral access scenes with movable objects as jamming against adjacent objects and walls can inhibit progress. We propose the use of two action primitives---burrowing and excavating---that can fluidize the scene to un-jam obstacles and enable continued progress. Even when these primitives are implemented in an open loop manner at clock-driven intervals, we observe a decrease in the final distance to the target location. Furthermore, we combine the primitives into a closed loop hybrid control strategy using tactile and proprioceptive information to leverage the advantages of both primitives without being overly disruptive. In doing so, we achieve a 10-fold increase in success rate above the baseline control strategy and significantly improve completion times as compared to the primitives alone or a naive combination of them."
Crosstalk-Free Impedance-Separating Array Measurement for Iontronic Tactile Sensors,"Funing Hou, Gang Li, Chenxing Mu, Mengqi Shi, Jixiao Liu, Shijie Guo","Fudan University,Hebei University of Technology",Force and Tactile Sensing IV,"Iontronic tactile sensors are promising to measure spatial-temporal contact information with high performance. However, no suitable measuring method has been presented, due to issues with crosstalk and non-negligible equivalent resistance. Hence, this study presents an impedance-separating method, which does not require complex analog components. A general Quadri-Terminal Impedance Network (QTIN) model is introduced to reduce crosstalk, which has specific compatibility with the impedance-separating method. The precise ranges are measured, showing non-rectangle shapes suitable for the response of iontronic tactile sensors. A simple denoising method is provided to reduce initial array noise obviously. This work could benefit various scenarios, such as human-robot interaction and physiological information monitoring."
Visual-Tactile Learning of Garment Unfolding for Robot-Assisted Dressing,"Fan Zhang, Yiannis Demiris","Honda Research Institute EU,Imperial College London",Force and Tactile Sensing IV,"Assistive robots have the potential to support disabled and elderly people in daily dressing activities. An intermediate stage of dressing is to manipulate the garment from a crumpled initial state to an unfolded configuration that facilitates robust dressing. Applying quasi-static grasping actions with vision feedback on garment unfolding usually suffers from occluded grasping points. In this work, we propose a dynamic manipulation strategy: tracing the garment edge until the hidden corner is revealed. We introduce a model-based approach, where a deep visual-tactile predictive model iteratively learns to perform servoing from raw sensor data. The predictive model is formalized as Conditional Variational Autoencoder with contrastive optimization, which jointly learns underlying visual-tactile latent representations, a latent garment dynamics model, and future predictions of garment states. Two cost functions are explored: the visual cost defined by garment corner positions guarantees the gripper to move towards the corner, while the tactile cost defined by garment edge poses prevents the garment from falling from the gripper. The experimental results demonstrate the improvement of our contrastive visual-tactile model predictive control over single sensing modality and baseline model learning techniques. The proposed method enables a robot to unfold back-opening hospital gowns and perform upper-body dressing."
Multimodal Visual-Tactile Representation Learning through Self-Supervised Contrastive Pre-Training,"Vedant Dave, Fotios Lygerakis, Elmar Rueckert","Montanuniversität Leoben,University of Leoben,Montanuniversitaet Leoben",Force and Tactile Sensing IV,"The rapidly evolving field of robotics necessitates methods that can facilitate the fusion of multiple modalities. Specifically, when it comes to interacting with tangible objects, effectively combining visual and tactile sensory data is key to understanding and navigating the complex dynamics of the physical world, enabling a more nuanced and adaptable response to changing environments. Nevertheless, much of the earlier work in merging these two sensory modalities has relied on supervised methods utilizing datasets labeled by humans. This paper introduces MViTac, a novel methodology that leverages contrastive learning to integrate vision and touch sensations in a self-supervised fashion. By availing both sensory inputs, MViTac leverages intra and inter-modality losses for learning representations, resulting in enhanced material property classification and more adept grasping prediction. Through a series of experiments, we showcase the effectiveness of our method and its superiority over existing state-of-the-art self-supervised and supervised techniques. In evaluating our methodology, we focus on two distinct tasks: material classification and grasping success prediction. Our results indicate that MViTac facilitates the development of improved modality encoders, yielding more robust representations as evidenced by linear probing assessments."
A Hierarchical Framework for Robot Safety Using Whole-Body Tactile Sensors,"Shuo Jiang, Lawson L.S. Wong",Northeastern University,Force and Tactile Sensing IV,"Using tactile signal is a natural way to perceive potential dangers and safeguard robots. One possible method is to use full-body tactile sensors on the robot and perform safety maneuvers when dangerous stimuli are detected. In this work, we proposed a method based on full-body tactile sensors that operates at three different levels of granularity to ensure that robot interacts with the environment safely. The results showed that our system dramatically reduced the overall collision chance compared with several baselines, and intelligently handled current collisions. Our proposed framework is generalizable to a wide variety of robots, enabling them to predict and avoid dangerous collisions and reactively handle accidental tactile stimuli."
Robust Jumping with an Articulated Soft Quadruped Via Trajectory Optimization and Iterative Learning,"Jiatao Ding, Mees Alexander Van Löben Sels, Franco Angelini, Jens Kober, Cosimo Della Santina","Delft University of Technology,TU Delft,University of Pisa",Legged Robots IV,"Quadrupeds deployed in real-world scenarios need to be robust to unmodelled dynamic effects. In this work, we aim to increase the robustness of quadrupedal periodic forward jumping (i.e., pronking) by unifying cutting-edge model-based trajectory optimization and iterative learning control. Using a reduced-order soft anchor model, the optimization-based motion planner generates the periodic reference trajectory. The controller then iteratively learns the feedforward control signal in a repetition process, without requiring an accurate full-body model. When enhanced by a continuous learning mechanism, the proposed controller can learn the control input without resetting the system at the end of each iteration. Simulations and experiments on a quadruped with parallel springs demonstrate that continuous jumping can be learned in a matter of minutes, with high robustness against various types of terrain."
Unlocking Versatile Locomotion: A Novel Quadrupedal Robot with 4-DoFs Legs for Roller Skating,"Jiawei Chen, Ripeng Qin, Longfei Huang, Zongbo He, Kun Xu, Xilun Ding","Beihang university,Inner Mongolia University of Science and Technology,Beihang Universityï¼ŒBeijing Institute of Spacecraft System,Beijing Institute of Sapcecraft System Engineering,Beijing University,Beijing Univerisity of Aeronautics and Astronautics",Legged Robots IV,
Efficient Terrain Map Using Planar Regions for Footstep Planning on Humanoid Robots,"Bhavyansh Mishra, Duncan Calvert, Sylvain Bertrand, Jerry Pratt, Hakki Erhan Sevil, Robert Griffin","Institute of Human and Machine Cognition, University of West Flo,IHMC, UWF,Institute for Human and Machine Cognition,Inst. for Human and Machine Cognition,University of West Florida,Institute for Human and Machine Cognition (IHMC)",Legged Robots IV,"Humanoid robots possess the ability to perform complex tasks in challenging environments. However, they require a model of the surroundings in a representation that is sufficient enough for downstream tasks such as footstep planning. The maps generated by existing mapping algorithms are either sparse, insufficient for footstep planning, memory intensive, or too slow for dynamic humanoid behaviors. In this work, we develop a mapping algorithm that combines planar region measurements along with kinematic-inertial state estimates to build a dense but efficient map of bounded planar surfaces. We present novel algorithms for plane feature matching, tracking and registration for mapping within a factor graph framework. The generated map is not only memory efficient, but also offers higher reliability and speed in bipedal footstep planning, than was possible earlier. The complete algorithm is also demonstrated using a full-scale humanoid robot, Nadia, walking over both flat ground and rough terrain utilizing the generated terrain map."
Convergent iLQR for Safe Trajectory Planning and Control of Legged Robots,"James Zhu, J. Joe Payne, Aaron Johnson",Carnegie Mellon University,Legged Robots IV,"In order to perform highly dynamic and agile maneuvers, legged robots typically spend time in underactuated domains (e.g. with feet off the ground) where the system has limited command of its acceleration and a constrained amount of time before transitioning to a new domain (e.g. foot touchdown). Meanwhile, these transitions can instantaneously change the systemâ€™s state, possibly causing perturbations to be mapped arbitrarily far away from the target trajectory. These properties make it difficult for local feedback controllers to effectively recover from disturbances as the system evolves through underactuated domains and hybrid impact events. To address this, we utilize the fundamental solution matrix that characterizes the evolution of perturbations through a hybrid trajectory and its 2-norm, which represents the worst- case growth of perturbations. In this paper, the worst-case perturbation analysis is used to explicitly reason about the tracking performance of a hybrid trajectory and is incorporated in an iLQR framework to optimize a trajectory while taking into account the closed-loop convergence of the trajectory under an LQR tracking controller. The generated convergent trajectories recover more effectively from perturbations, are more robust to large disturbances, and use less feedback control effort than trajectories generated with traditional methods."
Optimization Based Dynamic Skateboarding of Quadrupedal Robot,"Zhe Xu, Mohamed Al-khulaqui, Hanxin Ma, Jiajun Wang, Quanbin Xin, Yangwei You, Mingliang Zhou, Diyun Xiang, Shiwu Zhang","Beijing Institute of Technology,Xiaomi Inc.,BeihangUniversity,UBTECH Robotics,Beijing Xiaomi Robot Technology Co., Ltd,Institute for Infocomm Research,Beijing Xiaomi Mobile Software Co., Ltd.,XIAOMI,University of Science and Technology of China",Legged Robots IV,"Robot skateboarding is an unexplored and challenging task for legged robots. Accurately modeling the dynamics of dual floating bases and developing effective planning and control methods present significant complexities in accomplishing skateboarding behavior.This paper focuses on enabling the quadrupedal platform CyberDog2 to achieve dynamic balancing and acceleration on a skateboard. An optimization-based control pipeline is developed through careful derivation of the system's equations of motion, considering both the robot and skateboard dynamics. By accounting for system physical constraints, an advanced offline trajectory optimization method is employed to generate various acceleration trajectories, creating a motion library for the system. An online linear model predictive control with whole body control framework is used to track the generated trajectories and stablize the system in real-time. To validate its effectiveness , we conducted experiments in various scenarios. The quadrupedal robot successfully performed acceleration from a static state to various velocities and demonstrated the ability to balance and steer the skateboard."
Hierarchical Experience-Informed Navigation for Multi-Modal Quadrupedal Rebar Grid Traversal,"Maxwell Asselmeier, Evgeniia Ivanova, Ziyi Zhou, Patricio A. Vela, Ye Zhao","Georgia Institute of Technology,SkyMul",Legged Robots IV,"This study focuses on a layered, experience-based, multi-modal contact planning framework for agile quadrupedal locomotion over a constrained rebar environment. To this end, our hierarchical planner incorporates locomotion-specific modules into the high-level contact sequence planner and solves kinodynamically-aware trajectory optimization as the low-level motion planner. Through quantitative analysis of the experience accumulation process and experimental validation of the kinodynamic feasibility of the generated locomotion trajectories, we demonstrate that the experience planning heuristic offers an effective way of providing candidate footholds for a legged contact planner. Additionally, we introduce a guiding torso path heuristic at the global planning level to enhance the navigation success rate in the presence of environmental obstacles. Our results indicate that the torso-path guided experience accumulation requires significantly fewer offline trials to successfully reach the goal compared to regular experience accumulation. Finally, our planning framework is validated in both dynamics simulations and real hardware implementations on a quadrupedal robot provided by Skymul Inc."
Learning-Based Propulsion Control for Amphibious Quadruped Robots with Dynamic Adaptation to Changing Environment,"Qingfeng Yao, Linghan Meng, Qifeng Zhang, Jing Zhao, Joni Pajarinen, Sylvie Wang, Zhibin Li, Cong Wang","Shenyang Institute of Automation, Chinese Academy of Sciences,Shenyang Institute of Automation,Shenyang Institute of Automation, CAS,Shenyang Institute of Automation (SIA), Chinese Academy of Scien,Aalto University,Heriot-Watt University,University College London,Delft University of Technology (TU Delft)",Legged Robots IV,"This paper proposes a learning-based adaptive propulsion control (APC) method for a quadruped robot integrated with thrusters in amphibious environments, allowing it to move efficiently in water while maintaining its ground locomotion capabilities. We designed the specific reinforcement learning method to train the neural network to perform the vector propulsion control. Our approach coordinates the legs and propeller, enabling the robot to achieve speed and trajectory tracking tasks in the presence of actuator failures and unknown disturbances. Our simulated validations of the robot in water demonstrate the effectiveness of the trained neural network to predict the disturbances and actuator failures based on historical information, showing that the framework is adaptable to changing environments and is suitable for use in dynamically changing situations. Our proposed approach is suited to the hardware augmentation of quadruped robots to create avenues in the field of amphibious robotics and expand the use of quadruped robots in various applications."
Reinforcement Learning for Blind Stair Climbing with Legged and Wheeled-Legged Robots,"Simon Chamorro, Victor Klemm, Miguel De La Iglesia Valls, Chris Pal, Roland Siegwart","Université de Sherbrooke,ETH Zurich,ETH Zürich,Polytechnique Montreal",Legged Robots IV,"In recent years, legged and wheeled-legged robots have gained prominence for tasks in environments predominantly created for humans across various domains. One significant challenge faced by many of these robots is their limited capability to navigate stairs, which hampers their functionality in multi-story environments. This study proposes a method aimed at addressing this limitation, employing reinforcement learning to develop a versatile controller applicable to a wide range of robots. In contrast to the conventional velocity-based controllers, our approach builds upon a position-based formulation of the RL task, which we show to be vital for stair climbing. Furthermore, the methodology leverages an asymmetric actor-critic structure, enabling the utilization of privileged information from simulated environments during training while eliminating the reliance on exteroceptive sensors during real-world deployment. Another key feature of the proposed approach is the incorporation of a boolean observation within the controller, enabling the activation or deactivation of a stair-climbing mode. We present our results on different quadrupeds and bipedal robots in simulation and showcase how our method allows the balancing robot Ascento to climb 15cm stairs in the real world, a task that was previously impossible for this robot."
Modeling and Analysis of Combined Rimless Wheel with Tensegrity Spine,"Yuxuan Xiang, Yanqiu Zheng, Fumihiko Asano","Japan Advanced Institute of Science and Technology,Ritsumeikan University",Legged Robots IV,"In the natural world, benefited from the advantages of the spine, quadrupeds exhibiting extraordinary flexibility which allowing them to move efficiently on variable terrains. The previous researches have indicated the legged robots which efficiently utilizing their spine can achieve rapid and stable locomotion. However,within the field of legged robot dynamics, the design of the spine and understanding how it positively influences locomotion is unclear, which is significant for quadruped robot to achieve efficient and stable walking. In this study, we proposed a model formed by tensegrity spine and rimless wheel to represent quadrupeds, using passive dynamic walking as a method, which has been well-demonstrated for observing the inherent characteristics, exhibited the locomotion characteristic of the model proposed. By numerical simulation, we observed change trend of locomotion performance with the configurations of spine's shape, and found direction of spine design that have a positive impact on walking. These findings contribute to the design of spine structures in quadruped robots."
Robot-Camera Calibration in Tightly Constrained Environment Using Interactive Perception,"Fangxun Zhong, Bin Li, Wei Chen, Yunhui Liu","The Chinese University of Hong Kong,Chinese University of Hong Kong",Force Control and Sensing,"Manipulation in tight environment is challenging but increasingly common in vision-guided robotic applications. The significantly reduced amount of available feedback (limited visual cues, field of view, robot motion space, etc.) hinders solving the hand-eye relationship accurately. In this paper, we propose a new generic approach for online camera-robot calibration that could deal with the least feedback input available in tight environment: an arbitrarily restricted motion space and a single feature point with unknown position for the robot end-effector. We introduce the interactive perception to generate prescribed but tunable robot motions to reveal high-dimensional sensory feedback, which is not obtainable from static images. We then define the interactive feature plane (IFP), whose spatial property corresponds to the robot-actuating trajectories. A depth-free adaptive controller is proposed based on image feedback, where the converged orientation of IFP directly harvests the data for solving the hand-eye relationship. Our algorithm requires neither external calibration sensors/objects nor large-scale data acquisition process. Simulations demonstrate the va"
Degenerate Motions of Multisensor Fusion-Based Navigation,"Woosik Lee, Chuchu Chen, Guoquan Huang",University of Delaware,Force Control and Sensing,"The system observability analysis is of practical importance, for example, due to its ability to identify the unobservable directions of the estimated state which can influence estimation accuracy and help develop consistent and robust estimators. Recent studies focused on analyzing the observability of the state of various multisensor systems with a particular interest in unobservable directions induced by degenerate motions. However, those studies mostly stay in the specific sensor domain without aiding to extend the understanding to other heterogeneous systems. To this end, in this work, we provide degenerate motion analysis on general local and global sensor-paired systems, offering insights applicable to a wide range of existing navigation systems. Our analysis includes 9 degenerate motion identification including 5 already identified in literature and 4 new motions with both synchronous and asynchronous sensor-pair cases. Comprehensive numerical studies are conducted to verify those identified motions, show the effect of degenerate motion on state estimation, and demonstrate the generalizability of our analysis on various multisensor systems."
Interaction Control for Tool Manipulation on Deformable Objects Using Tactile Feedback,"Hanwen Zhang, Zeyu Lu, Wenyu Liang, Haoyong Yu, Yao Mao, Yan Wu","Institute of optics and electronics,CAS,National University of Singapore,Institute for Infocomm Research, A*STAR,A*STAR Institute for Infocomm Research",Force Control and Sensing,"The sense of touch enables humans to perform many delicate tasks on deformable objects and/or in a vision-denied environment. For a robot to achieve similar desirable interactions, such as administering a swab test, tactile information sensed beyond the tool-in-hand is correspondingly crucial for contact state estimation and contact tracking control. In this paper, we propose a tactile-guided planning and control framework using GTac, a hetero-Geneous Tactile sensor tailored for interaction with deformable objects beyond the immediate contact area. The biomimetic GTac in use is an improved version optimised for readout linearity which provides reliability in contact state estimation and force tracking. While a tactile-based classification and manipulation process is designed to estimate and align to the contact angle between the tool and the environment, a Koopman operator-based optimal control scheme is proposed to address challenges in the nonlinear control arising from the interaction with the deformable object. Several experiments are conducted to verify the effectiveness of the proposed framework. The experimental results demonstrate that the proposed framework can achieve accurate contact angle estimation as well as excellent tracking performance and strong robustness in force control."
Development of an Easy-To-Cut Six-Axis Force Sensor,"Takamasa Kawahara, Toshiaki Tsuji",Saitama University,Force Control and Sensing,"Although the potential demand for force sensors in both robotics and automation is high, the complexity of their structure increases the number of manufacturing processes. As a result, the rising cost of sensors has hindered the practical application of force measurement and force control. In this study, a flexure element comprising a structure that is easier to cut and process than conventional ones, as well as holes through the side of a cuboid, is proposed to simplify the manufacturing of force sensors. To ensure the safety of the proposed sensor design, an approximate equation is derived to predict the maximum von Mises stress on the flexure element using design parameters. Subsequently, we clarified a way to attach the strain gauge in a position that improves sensitivity. The results of the actual prototype sensor based on the proposed method show that the maximum nonlinearity error and decoupling error in the other axes are 0.442 %R.O. and 0.660 %R.O., respectively, and the performance is comparable to that of conventional force sensors. Because the prototype has a difference in resolution between the axes, a method for improving the resolution isotropy without changing the difficulty of machining is also proposed. In addition, the validity of the proposed method is demonstrated using experiments. Consequently, a force sensor with the same level of performance was developed using the proposed method, and the cutting process was made easier compared to that of convention"
An Ultra-Fast Intrinsic Contact Sensing Method for Medical Instruments with Arbitrary Shape,"Guanglin Cao, Mingcong Chen, Jian Hu, Hongbin Liu","Institute of Automation, Chinese Academy of Sciences,City University of Hong Kong,Hong Kong Institute of Science & Innovation, Chinese Academy of ",Force Control and Sensing,"Intraoperative contact sensing has the potential to reduce the risk of surgical errors and enhance manipulation capabilities for medical robots, particularly in contact force control. Current intrinsic force sensing (IFS) methods are limited in application to medical instruments with arbitrary shape, due to high computational time and reliance on precise surface equations. This study presents an ultra-fast IFS method that uses multiple planes to establish surface geometry descriptions. The method can reduce high-order contact mechanical models that need to be solved iteratively to a set of linear equations, and calculate contact location analytically. In addition, a robot motion control approach based on the contact sensing method is proposed to maintain stable contact force and regulate the probe's orientation for robotic ultrasound systems (RUSS). Experimental results show that the contact sensing method is robust to friction and can achieve a mean (Â±SD) displacement error of 1.04Â±0.43 mm in contact location with computational time less than 1 ms. The system has been evaluated on a phantom with sinusoidal motion. To the best of our knowledge, this is the first study to validate adaptiveness of RUSS under dynamic conditions. The results demonstrated that the system exhibits comparable manipulation capabilities to human operators with only force sensing, indicating a high level of adaptiveness."
Proprioceptive-Based Whole-Body Disturbance Rejection Control for Dynamic Motions in Legged Robots,"Zhengguo Zhu, Guoteng Zhang, Zhongkai Sun, Teng Chen, Xuewen Rong, Anhuan Xie, Yibin Li","Shandong University,Zhejiang University",Force Control and Sensing,"This paper presents a control framework for legged robots that enables self-perception and resistance to external disturbances. First, a novel proprioceptive-based disturbance estimator is proposed. Compared with other disturbance estimators, this estimator possesses notable advantages in terms of filtering foot-ground interaction noise and suppressing the accumulation of estimation errors. Additionally, our estimator is a fully proprioceptive-based estimator, eliminating the need for any exteroceptive devices or observers. Second, we present a hierarchical optimized whole-body controller (WBC), which takes into account the full body dynamics, the actuation limits, the external disturbances, and the interactive constraints. Finally, extensive experimental trials conducted on the point-foot biped robot BRAVER validate the capabilities of the proposed estimator and controller under various disturbance conditions."
Contact Force Estimation of Robot Manipulators with Imperfect Dynamic Model: On Gaussian Process Adaptive Disturbance Kalman Filter,"Yanran Wei, Shangke Lyu, Wenshuo Li, Xiang Yu, Lei Guo","Beihang University,Nanyang Technological University",Force Control and Sensing,"This paper is concerned with the contact force estimation problem of robot manipulators based on imperfect dynamic models of the manipulator and the contact force. To handle the imperfect dynamic information of the manipulator, a hybrid model, consisting of the nominal model and the residual dynamics, is established for the manipulator, and the Gaussian process regression (GPR) technique is employed to learn the mean and covariance of the residual dynamics. On this basis, a virtual measurement equation is established for contact force estimation and a Gaussian process adaptive disturbance Kalman filter (GPADKF) is developed where the variational Bayes technique is employed to achieve online identification of the noise statistics in the force dynamics. The GPADKF is capable of decoupling the contact force from residual dynamics and system noises, thereby reducing the dependence on accurate dynamic models of the manipulator and the contact force. Simulation and experimental results demonstrate that the proposed scheme outperforms the state-of-art methods."
On the Disentanglement of Tube Inequalities in Concentric Tube Continuum Robots,"Reinhard Grassmann, Anastasiia Senyk, Jessica Burgner-kahrs","University of Toronto,Ukrainian Catholic University",Medical Robots IV,"Concentric tube continuum robots utilize nested tubes, which are subject to a set of inequalities. Current approaches to account for inequalities rely on branching methods such as if-else statements. It can introduce discontinuities, may result in a complicated decision tree, has a high wall-clock time, and cannot be vectorized. This affects the behavior and result of downstream methods in control, learning, workspace estimation, and path planning, among others. In this paper, we investigate a mapping to mitigate branching methods. We derive a lower triangular transformation matrix to disentangle the inequalities and provide proof for the unique existence. It transforms the interdependent inequalities into independent box constraints. Further investigations are made for sampling, control, and workspace estimation. Approaches utilizing the proposed mapping are at least 14 times faster (up to 176 times faster), generate always valid joint configurations, are more interpretable, and are easier to extend."
Design and Experimental Investigation of a Vibro-Impact Capsule Robot for Colonoscopy,"Jiajia Zhang, Yang Liu, Jiyuan Tian, Dibin Zhu, Shyam Prasad","University of Exeter,Royal Devon and Exeter NHS Foundation Trust",Medical Robots IV,"Uptake of capsule endoscopy in the large intestine has been very limited due to both the risk of missed lesions and the often prolonged transit times, making it an unviable alternative to standard colonoscopy. In this letter, we presented a controllable and easy-to-use diagnostic tool, equipping a novel vibration module inside a capsule robot for colonoscopy. The capsuleâ€™s motion was controlled by applying an external alternating electromagnetic field to the capsuleâ€™s inner magnet to generate vibrations and impacts on the main body of the capsule. To optimise its motion, we provided a numerical solution for calculating its electromagnetic force and used it to guide a hand-held control panel for navigating the robot. The robot was firstly examined in a large intestine simulator modelled based on the colon-rectal morphometrics, and then tested in an ex vivo environment with porcine intestines. We verified the performance of the robot travelling through the entire large intestine with the maximum speeds of 54 mm/s and 40 mm/s in the simulator and ex vivo environments, respectively. It was found that changing the control frequency of the panel can help the robot to pass through different morphometrics, in particular the sharp turnings at the segment junctions."
3D Navigation of a Magnetic Swimmer Using a 2D Ultrasonography Probe Manipulated by a Robotic Arm for Position Feedback,"Premal Gorroochurn, Charles Hong, Carter Klebuc, Yitong Lu, Ngoc Tu Khue Phan, Javier Garcia Gonzalez, Aaron T. Becker, Leclerc Julien","Columbia University,Georgia Institute of Technology,University of Houston,Kerr High School",Medical Robots IV,"Millimeter-scale magnetic rotating swimmers have multiple potential medical applications. They could, for example, navigate inside the bloodstream of a patient toward an occlusion and remove it. Magnetic rotating swimmers have internal magnets and propeller fins with a helical shape. A rotating magnetic field applies torque on the swimmer and makes it rotate. The shape of the swimmer, combined with the rotational movement, generates a propulsive force. Visual feedback is suitable for in-vitro closed-loop control. However, in-vivo procedures will require different feedback modalities due to the opacity of the human body. In this paper, we provide new methods and tools that enable the 3D control of a magnetic swimmer using a 2D ultrasonography device attached to a robotic arm to sense the swimmer's position. We also provide an algorithm that computes the placement of the robotic arm and a controller that keeps the swimmer within the ultrasound imaging slice. The position measurement and closed-loop control were tested experimentally."
An Intelligent Robotic Endoscope Control System Based on Fusing Natural Language Processing and Vision Models,"Beili Dong, Junhong Chen, Zeyu Wang, Kaizhong Deng, Yiping Li, Benny Lo, George Mylonas",Imperial College London,Medical Robots IV,"In recent years, the area of Robot-Assisted Minimally Invasive Surgery (RAMIS) is standing on the the verge of a new wave of innovations. However, autonomy in RAMIS is still in a primitive stage. Therefore, most surgeries still require manual control of the endoscope and the robotic instruments, resulting in surgeons needing to switch attention between performing surgical procedures and moving endoscope camera. Automation may reduce the complexity of surgical operations and consequently reduce the cognitive load on the surgeon while speeding up the surgical process. In this paper, a hybrid robotic endoscope control system based on fusion model of natural language processing (NLP) and modified YOLO-V8 vision model is proposed. This proposed system can analyze the current surgical workflow and generate logs to summarize the procedure for teaching and providing feedback to junior surgeons. The user study of this system indicated a significant reduction of the number of clutching actions and mean task time, which effectively enhanced the surgical training."
AI-Assisted Dynamic Tissue Evaluation for Early Bowel Cancer Diagnosis Using a Vibrational Capsule,"Kenneth Afebu, Jiyuan Tian, Yang Liu, Evangelos Papatheou, Shyam Prasad","University of Exeter,Royal Devon and Exeter NHS Foundation Trust",Medical Robots IV,"With early sign of bowel cancer being changes in affected lesions biomechanical properties, an AI-assisted dynamic tissue evaluation is proposed for early bowel cancer diagnosis. Dynamic signals from a self-propelled vibrational capsule in contact with in-situ bowel lesions were processed and analysed for features that may be indicative of biomechanical changes in the lesions. Different combinations of the features were used to develop different lesion characterisation models. Supervised classification using Multi-Layer Perceptron (MLP) and Stacking Ensemble networks (SE) was carried out alongside unsupervised classification using K-means clustering. The SE base-learners comprised Support Vector Machine (SVM), Decision Tree, Naive Bayes and Random Forest. Cross-validation on simulated test data showed that the SEs outperformed their composite base-learners, however, SVM as a base-learner showed tendency to yield greater than 90% accuracy. The MLPs outperformed the SEs in accuracies and in numbers of high-performance models, hence, were the only supervised network used during experimental validation and they yielded an average accuracy of 96.5%. For unsupervised classification, both simulation and experimental data showed that the lesions are best clustered into two categories representing benign and malignant lesions."
AiAReSeg: Catheter Detection and Segmentation in Interventional Ultrasound Using Transformers,"Alex Ranne, Yordanka Velikova, Nassir Navab, Ferdinando Rodriguez Y Baena","Imperial College London,TU Munich,Imperial College, London, UK",Medical Robots IV,"To date, endovascular surgeries are performed using the golden standard of Fluoroscopy, which uses ionising radiation to visualise catheters and vasculature. Prolonged Fluoroscopic exposure is harmful for the patient and the clinician, and may lead to severe post-operative sequlae such as the development of cancer. Meanwhile, the use of interventional Ultrasound has gained popularity, due to its well-known benefits of small spatial footprint, fast data acquisition, and produce higher tissue contrast images. However, ultrasound images are hard to interpret, and it is difficult to localise vessels, catheters, and guidewires within them. This work proposed a solution using an adaptation of a state-of-the-art machine learning architecture (Transformers) to detect and segment catheters in axial interventional Ultrasound image sequences. The network architecture was inspired by the Attention in Attention mechanism, temporal tracking networks, and introduced a novel 3D segmentation head that performs 3D deconvolution across time. In order to facilitate training of such deep learning networks, we introduced a new data synthesis pipeline that used physics-based catheter insertion simulations, along with a convolutional ray-casting ultrasound simulator to produce synthetic ultrasound images of endovascular interventions. The proposed method was validated on a hold-out validation dataset, thus demonstrated robustness to ultrasound noise and a wide range of scanning angles. It was also tested on data collected from silicon-based aorta phantoms, thus demonstrated its potential for translation from sim-to-real. This work represents a significant step towards safer and more efficient endovascular surgery using interventional ultrasound."
Hybrid Robot for Percutaneous Needle Intervention Procedures: Mechanism Design and Experiment Verification,"Hanyi Zhang, Guocai Yao, Feifan Zhang, Fanchuan Lin, Fuchun Sun","Imperial College London,Tsinghua University,University College London,Beihang University",Medical Robots IV,"This paper presents a 6-DOF hybrid robot for percutaneous needle intervention procedures. The new robot combines the advantages of both serial robots and parallel robots, featuring compactness, high accuracy, and small footprint while overcoming the problems of the high cost of serial robots and the small workspace and singularity issue of parallel robots. Besides, by analyzing the workspace of the robot, the equation is derived between the structure parameter and workspace to adjust the parameters of the robot to satisfy different working scenes. According to the experiment, the accuracy of the robot is related to the position, distance, and insertion angle. The result shows that the performance is better when working near the center workspace and away from the servos and the average error of the robot is 1.39mm. The phantom experiment of lumbar puncture validates its feasibility."
Envibroscope: Real-Time Monitoring and Prediction of Environmental Motion for Enhancing Safety in Robot-Assisted Microsurgery,"Alireza Alikhani, Satoshi Inagaki, Shervin Dehghani, Mathias Maier, Nassir Navab, M. Ali Nasseri","Augen- klinik und Poliklinik, Klinikum rechts der Isar der Techn,NSK.Ltd,TUM,Klinikum rechts der Isar der TU München,TU Munich,Technische Universitaet Muenchen",Medical Robots IV,"Several robotic systems have been emerged in the recent past to enhance the precision of micro-surgeries such as retinal procedures. Significant advancements have recently been achieved to increase the precision of such systems beyond surgeon capabilities. However, little attention has been paid to the impact of non-predicted and sudden movements of the patient and the environment. Therefore, analyzing environmental motion and vibrations is crucial to ensuring the optimal performance and reliability of medical systems that require micron-level precision, especially in real-life scenarios. To address this challenge, this paper introduces a novel environmental motion analysis system that employs a grid layout with distributed sensing nodes throughout the environment. This system effectively tracks undesired movements (motions) at designated locations and predicts upcoming motions using neural network-based approaches. The outcomes of our experiments exhibit promising prospects for real-time motion monitoring and prediction, which has the potential to form a solid basis for enhancing the automation, safety, integration, and overall efficiency of robot-assisted micro-surgeries."
Cooperative vs. Teleoperation Control of the Steady Hand Eye Robot with Adaptive Sclera Force Control: A Comparative Study,"Mojtaba Esfandiari, Ji Woong Kim, Botao Zhao, Golchehr Amirkhani, Muhammad Hadi, Peter Gehlbach, Russell H. Taylor, Iulian Iordachita","Johns Hopkins University,Johns Hopkins Medical Institute,The Johns Hopkins University",Medical Robots IV,"A surgeon's physiological hand tremor can significantly impact the outcome of delicate and precise retinal surgery, such as retinal vein cannulation (RVC) and epiretinal membrane peeling. Robot-assisted eye surgery technology provides ophthalmologists with advanced capabilities such as hand tremor cancellation, hand motion scaling, and safety constraints that enable them to perform these otherwise challenging and high-risk surgeries with high precision and safety. Steady-Hand Eye Robot (SHER) with cooperative control mode can filter out surgeon's hand tremor, yet another important safety feature, that is, minimizing the contact force between the surgical instrument and sclera surface for avoiding tissue damage cannot be met in this control mode. Also, other capabilities, such as hand motion scaling and haptic feedback, require a teleoperation control framework. In this work, for the first time, we implemented a teleoperation control mode incorporated with an adaptive sclera force control algorithm using a PHANTOM Omni haptic device and a force-sensing surgical instrument equipped with Fiber Bragg Grating (FBG) sensors attached to the SHER 2.1 end-effector. This adaptive sclera force control algorithm allows the robot to dynamically minimize the tool-sclera contact force. Moreover, for the first time, we compared the performance of the proposed adaptive teleoperation mode with the cooperative mode by conducting a vessel-following experiment inside an eye phantom under a microscope."
Adaptive Motion Scaling for Robot-Assisted Microsurgery Based on Hybrid Offline Reinforcement Learning and Damping Control,"Peiyang Jiang, Wei Li, Yifan Li, Dandan Zhang","university of Bristol,Imperial College London,University of Bristol",Medical Robots IV,"Motion scaling is essential to empower users to conduct precise manipulation during teleoperation for robot-assisted microsurgery (RAMS). A constant, small motion scaling ratio can enhance the precision of teleoperation but hinder the operator from quickly reaching distant targets. The concept of self-adaptive motion scaling has been proposed in previous work. However, previous frameworks required extensive manual tuning of core parameters, which significantly depends on prior knowledge and may potentially lead to non-optimal solutions. This paper presents a hybrid offline reinforcement learning and damping control approach to regulate the motion scaling ratio for different operations during offline training. This method can take user-specific characteristics into consideration and help them achieve better teleoperation performance. Comparisons are made with and without using the adaptive motion-scaling algorithm. Detailed user studies indicate that a suitable motion-scaling ratio can be obtained and adjusted online. The overall performance of the operators in terms of time cost for task completion is significantly improved, while the variance of average speed and the total distance for robot operation is reduced."
Chained Flexible Capsule Endoscope: Unraveling the Conundrum of Size Limitations and Functional Integration for Gastrointestinal Transitivity,"Sishen Yuan, Guang Li, Baijia Liang, Lailu Li, Qingzhuo Zheng, Shuang Song, Zhen Li, Hongliang Ren","The Chinese University of Hong Kong,Chinese University of Hong Kong,Harbin Institute of Technology (Shenzhen),Qilu Hospital of Shandong University,Chinese Univ Hong Kong (CUHK) & National Univ Singapore(NUS)",Medical Robots IV,"Capsule endoscopes, predominantly serving diagnostic functions, provide lucid internal imagery but are devoid of surgical or therapeutic capabilities. Consequently, despite lesion detection, physicians frequently resort to traditional endoscopic or open surgical procedures for treatment, resulting in more complex, potentially risky interventions. To surmount these limitations, this study introduces a flexible capsule endoscope (FCE) design concept, specifically conceived to navigate the inherent volume constraints of capsule endoscopes whilst augmenting their therapeutic functionalities. The FCEâ€™s distinctive flexibility originates from a conventional rotating joint design and the incision pattern in the flexible material. In vitro experiments validated the passive navigation ability of the FCE in rugged intestinal tracts. Further, the FCE demonstrates consistent reptile-like peristalsis under the influence of an external magnetic field, and possesses the capability for film expansion and disintegration under high-frequency electromagnetic stimulation. These findings illuminate a promising path toward amplifying the therapeutic capacities of capsule endoscopes without necessitating a size compromise."
A Group Theoretic Metric for Robot State Estimation Leveraging Chebyshev Interpolation,"Varun Agrawal, Frank Dellaert","Georgia Institute of Technology,Verdant Robotics/Georgia Tech",Performance Evaluation and Benchmarking,"We propose a new metric for robot state estimation based on the recently introduced $text{SE}_2(3)$ Lie group definition. Our metric is related to prior metrics for SLAM but explicitly takes into account the linear velocity of the state estimate, improving over current pose-based trajectory analysis. This has the benefit of providing a single, quantitative metric to evaluate state estimation algorithms against, while being compatible with existing tools and libraries. Since ground truth data generally consists of pose data from motion capture systems, we also propose an approach to compute the ground truth linear velocity based on polynomial interpolation. Using Chebyshev interpolation and a pseudospectral parameterization, we can accurately estimate the ground truth linear velocity of the trajectory in an optimal fashion with best approximation error. We demonstrate how this approach performs on multiple robotic platforms where accurate state estimation is vital, and compare it to alternative approaches such as finite differences. The pseudospectral parameterization also provides a means of trajectory data compression as an additional benefit. Experimental results show our method provides a valid and accurate means of comparing state estimation systems, which is also easy to interpret and report."
AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-Based Dataset,"Dongsu Lee, Chanin Eom, Minhae Kwon",Soongsil University,Performance Evaluation and Benchmarking,"Offline reinforcement learning has emerged as a promising technology by enhancing its practicality through the use of pre-collected large datasets. Despite its practical benefits, most algorithm development research in offline reinforcement learning still relies on game tasks with synthetic datasets. To address such limitations, this paper provides autonomous driving datasets and benchmarks for offline reinforcement learning research. We provide 19 datasets, including real-world human driver's datasets, and seven popular offline reinforcement learning algorithms in three realistic driving scenarios. We also provide a unified decision-making process model that can operate effectively across different scenarios, serving as a reference framework in algorithm design. Our research lays the groundwork for further collaborations in the community to explore practical aspects of existing reinforcement learning methods. Dataset and codes can be found in https://sites.google.com/view/ad4rl."
The Cluttered Environment Picking Benchmark (CEPB) for Advanced Warehouse Automation,"Salvatore D'avella, Matteo Bianchi, Ashok Meenakshi Sundaram, Carlo Alberto Avizzano, Maximo A. Roa, Paolo Tripicchio","Sant'Anna School of Advanced Studies,University of Pisa,German Aerospace Center (DLR),Scuola Superiore Sant'Anna",Performance Evaluation and Benchmarking,"Autonomous and reliable robotic grasping is a desirable functionality in robotic manipulation and is still an open problem. Standardized benchmarks are important tools for evaluating and comparing robotic grasping and manipulation systems among different research groups, also sharing with the community the best practices to learn from errors. An ideal benchmarking protocol should encompass the different aspects underpinning grasp execution, including the mechatronic design of grippers, planning, perception, and control to give information on each aspect and the overall problem. The proposed work gives an overview of the benchmarks, datasets, and competitions that have been proposed and adopted in the last few years and presents a novel benchmark with protocols for different tasks that evaluate both the single components of the system and the system as a whole, introducing an evaluation metric that allows for a fair comparison in highly cluttered scenes taking into account the difficulty of the clutter. A website dedicated to the benchmark containing information on the different tasks, maintaining the leaderboards, and serving as a contact point for the community is also provided."
SceneReplica: Benchmarking Real-World Robot Manipulation by Creating Replicable Scenes,"Ninad Khargonkar, Sai Haneesh Allu, Yangxiao Lu, Jishnu Jaykumar P, B Prabhakaran, Yu Xiang","University of Texas at Dallas,The University of Texas at Dallas,the University of Texas at Dallas",Performance Evaluation and Benchmarking,"We present a new reproducible benchmark for evaluating robot manipulation in the real world, specifically focusing on a pick-and-place task. Our benchmark uses the YCB object set, a commonly used dataset in the robotics community, to ensure that our results are comparable to other studies. Additionally, the benchmark is designed to be easily reproducible in the real world, making it accessible to researchers and practitioners. We also provide our experimental results and analyzes for model-based and model-free 6D robotic grasping on the benchmark, where representative algorithms are evaluated for object perception, grasping planning, and motion planning. We believe that our benchmark will be a valuable tool for advancing the field of robot manipulation. By providing a standardized evaluation framework, researchers can more easily compare different techniques and algorithms, leading to faster progress in developing robot manipulation methods. Appendix, code and videos for the project are available at https://irvlutd.github.io/SceneReplica."
CRITERIA: A New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving,"Changhe Chen, Mozhgan Pourkeshavarz, Amir Rasouli","University of Toronto,Research scientist at Huawei,Huawei Technologies Canada",Performance Evaluation and Benchmarking,"Benchmarking is a common method for evaluating trajectory prediction models for autonomous driving. Existing benchmarks rely on datasets, which are biased towards more common scenarios, such as cruising, and distance-based metrics that are computed by averaging over all scenarios. Following such a regiment provides a little insight into the properties of the models both in terms of how well they can handle different scenarios and how admissible and diverse their outputs are. There exist a number of complementary metrics designed to measure the admissibility and diversity of trajectories, however, they suffer from biases, such as length of trajectories. In this paper, we propose a new benChmarking paRadIgm for evaluaTing trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a method for extracting driving scenarios at varying levels of specificity according to the structure of the roads, models' performance, and data properties for fine-grained ranking of prediction models; 2) A set of new bias-free metrics for measuring diversity, by incorporating the characteristics of a given scenario, and admissibility, by considering the structure of roads and kinematic compliancy, motivated by real-world driving constraints; 3) Using the proposed benchmark, we conduct extensive experimentation on a representative set of the prediction models using the large scale Argoverse dataset. We show that the proposed benchmark can produce a more accurate ranking of the models and serve as a means of characterizing their behavior. We further present ablation studies to highlight contributions of different elements that are used to compute the proposed metrics"
LET-3D-AP: Longitudinal Error Tolerant 3D Average Precision for Camera-Only 3D Detection,"Wei-chih Hung, Vincent Casser, Henrik Kretzschmar, Jyh-jing Hwang, Dragomir Anguelov",Waymo,Performance Evaluation and Benchmarking,"The 3D~Average Precision (3D AP) relies on the intersection over union between predictions and ground truth objects. However, camera-only detectors have limited depth accuracy, which may cause otherwise reasonable predictions that suffer from such longitudinal localization errors to be treated as false positives. We therefore propose variants of the 3D AP metric to be more permissive with respect to depth estimation errors. Specifically, our novel longitudinal error tolerant metrics, LET-3D-AP and LET-3D-APL, allow longitudinal localization errors of the prediction boxes up to a given tolerance. To evaluate the proposed metrics, we also construct a new test set for the Waymo Open Dataset, tailored to camera-only 3D detection methods. Surprisingly, we find that state-of-the-art camera-based detectors can outperform popular LiDAR-based detectors with our new metrics past at 10% depth error tolerance, suggesting that existing camera-based detectors already have the potential to surpass LiDAR-based detectors in downstream applications. We believe the proposed metrics and the new benchmark dataset will facilitate advances in the field of camera-only 3D detection by providing more informative signals that can better indicate the system-level performance."
HuNavSim: A ROS 2 Human Navigation Simulator for Benchmarking Human-Aware Robot Navigation,"Noe Perez-Higueras, Roberto Otero, Fernando Caballero, Luis Merino","University Pablo de Olavide,Universidad de Sevilla,Universidad Pablo de Olavide",Performance Evaluation and Benchmarking,"This work presents the Human Navigation Simulator (HuNavSim), a novel open-source tool for the simulation of different human-agent navigation behaviors in scenarios with mobile robots. The tool, the first programmed under the ROS 2 framework, can be used together with different well-known robotics simulators like Gazebo. The main goal is to facilitate the development and evaluation of human-aware robot navigation systems in simulation. In addition to a general human-navigation model, HuNavSim includes, as a novelty, a rich set of individual and varied human navigation behaviors and a comprehensive set of metrics for social navigation benchmarking."
"RobotPerf: An Open-Source, Vendor-Agnostic, Benchmarking Suite for Evaluating Robotics Computing System Performance","Victor Mayoral-Vilches, Jason Jabbour, Yu-Shun Hsiao, Zishen Wan, Martiño Crespo-Álvarez, Matthew Stewart, Juan Manuel Reina-muñoz, Prateek Nagras, Gaurav Vikhe, Mohammad Bakhshalipour, Martin Pinzger, Stefan Rass, Smruti Panigrahi, Giulio Corradi, Niladri Roy, Phillip Gibbons, Sabrina Neuman, Brian Plancher, Vijay Janapa Reddi","Klagenfurt University,Harvard University,Georgia Institute of Technology,Acceleration Robotics,Carnegie Mellon University,Universität Klagenfurt,Alpen-Adria Universität Klagenfurt,Ford Motor Company,AMD,,Intel,,Boston University,,Barnard College, Columbia University",Performance Evaluation and Benchmarking,"We introduce RobotPerf, a vendor-agnostic benchmarking suite designed to evaluate robotics computing performance across a diverse range of hardware platforms using ROS 2 as its common baseline. The suite encompasses ROS 2 packages covering the full robotics pipeline and integrates two distinct benchmarking approaches: black-box testing, which measures performance by eliminating upper layers and replacing them with a test application, and grey-box testing, an application-specific measure that observes internal system states with minimal interference. Our benchmarking framework provides ready-to-use tools and is easily adaptable for the assessment of custom ROS 2 computational graphs. Drawing from the knowledge of leading robot architects and system architecture experts, RobotPerf establishes a standardized approach to robotics benchmarking. As an open-source initiative, RobotPerf remains committed to evolving with community input to advance the future of hardware-accelerated robotics."
Standardization of Cloth Objects and Its Relevance in Robotic Manipulation,"Irene Garcia-Camacho, Alberta Longhini, Michael Welle, Guillem Alenyà, Danica Kragic, Julia Borras Sol","Institut de Robòtica i Informàtica Industrial CSIC-UPC,KTH Royal Institute of Technology,CSIC-UPC,KTH,Institut de Robòtica i Informàtica Industrial (CSIC-UPC)",Performance Evaluation and Benchmarking,"The field of robotics faces inherent challenges in manipulating deformable objects, particularly in understanding and standardising fabric properties like elasticity, stiffness, and friction. While the significance of these properties is evident in the realm of cloth manipulation, accurately categorising and comprehending them in real-world applications remains elusive. This study sets out to address two primary objectives: (1) to provide a framework suitable for robotics applications to characterise cloth objects, and (2) to study how these properties influence robotic manipulation tasks. Our preliminary results validate the framework's ability to characterise cloth properties and compare cloth sets, and reveal the influence that different properties have on the outcome of five manipulation primitives. We believe that, in general, results on the manipulation of clothes should be reported along with a better description of the garments used in the evaluation. This paper proposes a set of these measures."
A Model for Multi-Agent Autonomy That Uses Opinion Dynamics and Multi-Objective Behavior Optimization,"Tyler Paine, Michael Benjamin",Massachusetts Institute of Technology,Marine Robotics IV,"This paper reports a new hierarchical architecture for modeling autonomous multi-robot systems (MRSs): a non-linear dynamical opinion process is used to model high-level group choice, and multi-objective behavior optimization is used to model individual decisions. Using previously reported theoretical results, we show it is possible to design the behavior of the MRS by the selection of a relatively small set of parameters. The resulting behavior - both collective actions and individual actions - can be understood intuitively. The approach is entirely decentralized and the communication cost scales by the number of group options, not agents. We demonstrated the effectiveness of this approach using a hypothetical `explore-exploit-migrate' scenario in a two hour field demonstration with eight unmanned surface vessels (USVs). The results from our preliminary field experiment show the collective behavior is robust even with time-varying network topology and agent dropouts."
Convex Geometric Trajectory Tracking Using Lie Algebraic MPC for Autonomous Marine Vehicles,"Junwoo Jang, Sangli Teng, Maani Ghaffari","University of Michigan,University of Michigan, Ann Arbor",Marine Robotics IV,
Mission Planning for Multiple Autonomous Underwater Vehicles with Constrained in Situ Recharging,"Priti Singh, Geoffrey Hollinger",Oregon State University,Marine Robotics IV,"Persistent operation of Autonomous Underwater Vehicles (AUVs) without manual interruption for recharging saves time and total cost for offshore monitoring and data collection applications. In order to facilitate AUVs for long mission durations without ship support, they can be equipped with docking capabilities to recharge in situ at Wave Energy Converter (WEC) with dock recharging stations. However, the power generated at the recharging stations may be constrained depending on the sea conditions. Therefore, a robust mission planning framework is proposed using a centralized Evolutionary Algorithm (EA) and a decentralized Monte Carlo Tree Search (MCTS) method. Both methods incorporate the charge availability constraint at the recharging station in addition to the maximum charge capacity of each AUV. The planner utilizes a time-varying power profile of irregular waves incident at WECs for dock charging and generates efficient mission plans for AUVs by optimizing their time to visit the dock based on the imposed constraint. The effects of increasing the number of AUVs, increasing the number of points of interest in the mission area, and varying sea state on the mission duration are also analyzed."
Decentralized Multi-Robot Navigation for Autonomous Surface Vehicles with Distributional Reinforcement Learning,"Xi Lin, Yewei Huang, Fanfei Chen, Brendan Englot",Stevens Institute of Technology,Marine Robotics IV,"Collision avoidance algorithms for Autonomous Surface Vehicles (ASV) that follow the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) have been proposed in recent years. However, it may be difficult and unsafe to follow COLREGs in congested waters, where multiple ASVs are navigating in the presence of static obstacles and strong currents, due to the complex interactions. To address this problem, we propose a decentralized multi-ASV collision avoidance policy based on Distributional Reinforcement Learning, which considers the interactions among ASVs as well as with static obstacles and current flows. We evaluate the performance of the proposed Distributional RL based policy against a traditional RL-based policy and two classical methods, Artificial Potential Fields (APF) and Reciprocal Velocity Obstacles (RVO), in simulation experiments, which show that the proposed policy achieves superior performance in navigation safety, while requiring minimal travel time and energy. A variant of our framework that automatically adapts its risk sensitivity is also demonstrated to improve ASV safety in highly congested environments."
Real-Time Planning under Uncertainty for AUVs Using Virtual Maps,"Ivana Collado-Gonzalez, John Frederic Mcconnell, Jinkun Wang, Paul Szenher, Brendan Englot",Stevens Institute of Technology,Marine Robotics IV,"Reliable localization is an essential capability for marine robots navigating in GPS-denied environments. SLAM, commonly used to mitigate dead reckoning errors, still fails in feature-sparse environments or with limited-range sensors. Pose estimation can be improved by incorporating the uncertainty prediction of future poses into the planning process and choosing actions that reduce uncertainty. However, performing belief propagation is computationally costly, especially when operating in large-scale environments. This work proposes a computationally efficient planning under uncertainty framework suitable for large-scale, feature-sparse environments. Our strategy leverages SLAM graph and occupancy map data obtained from a prior exploration phase to create a virtual map, describing the uncertainty of each map cell using a multivariate Gaussian. The virtual map is then used as a cost map in the planning phase, and performing belief propagation at each step is avoided. A receding horizon planning strategy is implemented, managing a goal-reaching and uncertainty-reduction tradeoff. Simulation experiments in a realistic underwater environment validate this approach. Experimental comparisons against a full belief propagation approach and a standard shortest-distance approach are conducted."
Sea-U-Foil: A Hydrofoil Marine Vehicle with Multi-Modal Locomotion,"Zuoquan Zhao, Yu Zhai, Chuanxiang Gao, Wendi Ding, Ruixin Yan, Songqun Gao, Bingxin Han, Xuchen Liu, Zixuan Guo, Ben M. Chen","The Chinese University of Hong Kong,Chinese University of Hong Kong",Marine Robotics IV,"Autonomous Marine Vehicles (AMVs) have been widely used in many critical tasks such as surveillance, patrolling, marine environment monitoring, and hydrographic surveying. However, most typical AMVs cannot meet the diverse demands of different marine tasks. In this article, we design a new type of remote-controlled hydrofoil marine vehicle, named Sea-U-Foil, which is suitable for different marine scenarios. Sea-U-Foil features three distinct locomotion modes, displacement mode, foilborne mode, and submarine mode, which enable the platform flexible mobility, high-speed and high-load capacities, and superior concealment. Specifically, the submarine mode makes Sea-U-Foil unique among previous studies. In addition, the performance of Sea-U-Foil in foilborne mode outperforms those of most current unmanned surface vehicles (USVs) in terms of speed and payload. To the best of our knowledge, we are the first to introduce a new type of AMV that can work in displacement mode, foilborne mode, and submarine mode. We elaborate on the design principles and methodologies of Sea-U-Foil first, then validate the effectiveness of its tri-modal locomotion through extensive experiments."
Learning Which Side to Scan: Multi-View Informed Active Perception with Side Scan Sonar for Autonomous Underwater Vehicles,"Advaith Venkatramanan Sethuraman, Philip Baldoni, Katherine A. Skinner, James Mcmahon","University of Michigan,United States Naval Research Laboratory,The Naval Research Laboratory",Marine Robotics IV,"Autonomous underwater vehicles often perform surveys that capture multiple views of targets in order to provide more information for human operators or automatic target recognition algorithms. In this work, we address the problem of choosing the most informative views that minimize survey time while maximizing classifier accuracy. We introduce a novel active perception framework for multi-view adaptive surveying and reacquisition using side scan sonar imagery.Our framework addresses this challenge by using a graph formulation for the adaptive survey task. We then use Graph Neural Networks (GNNs) to both classify acquired sonar views and reinforcement learning to choose the next best view to capture based on the collected data. We evaluate our method using simulated surveys in a high-fidelity side scan sonar simulator. Our results demonstrate that our approach is able to surpass the state-of-the-art in classification accuracy and efficiency. This framework is a promising approach for more efficient autonomous missions involving side scan sonar, such as underwater exploration, marine archaeology, and environmental monitoring."
Development of a Lightweight Underwater Manipulator for Delicate Structural Repair Operations,"Juzheng Mao, Guangming Song, Shuang Hao, Mingquan Zhang, Song Aiguo",Southeast University,Marine Robotics IV,"In recent years, underwater robots have been increasingly used in the maintenance of hydraulic structures. Underwater manipulators are essential devices that are used to carry out such maintenance tasks. For delicate repair operations such as fixing tiny cracks, most existing underwater manipulators face limitations in terms of size, accuracy, and scalability. Therefore, in this letter, we present a novel electric underwater manipulator, named SEU-4. This four-degree-of-freedom manipulator weighs 8.91 kg and has a maximum payload of 9 kg. It has a rapid-switching interface that supports convenient mechanical and electrical connections for end-effectors. To compensate for the disturbances that are present in the complex underwater environment, a trajectory-tracking control strategy based on a disturbance observer and sliding-mode control (DOB-SMC) is proposed. A prototype of the proposed underwater manipulator was created, and a flowing-water experimental platform was constructed to test its trajectory-tracking performance in fast-flowing water. The experimental results show that the manipulator achieves a trajectory-tracking error of 1.03 mm in static water and 2.91 mm in flowing water at 1.2 m/s, which satisfies the requirements of delicate repair operations."
MPS: A New Method for Selecting the Stable Closed-Loop Equilibrium Attitude-Error Quaternion of a UAV During Flight,"Francisco Goncalves, Ryan Bena, Konstantin Matveev, Nestor O Perez-Arancibia","Washington State University,University of Southern California,Washington State University (WSU)",Mechanics and Control IV,"We present model predictive selection (MPS), a new method for selecting the stable closed-loop (CL) equilibrium attitude-error quaternion (AEQ) of an uncrewed aerial vehicle (UAV) during the execution of high-speed yaw maneuvers. In this approach, we minimize the cost of yawing measured with a performance figure of merit (PFM) that takes into account both the aerodynamic-torque control input and attitude-error state of the UAV. Specifically, this method uses a control law with a term whose sign is dynamically switched in real time to select, between two options, the torque associated with the lesser cost of rotation as predicted by a dynamical model of the UAV derived from first principles. This problem is relevant because the selection of the stable CL equilibrium AEQ significantly impacts the performance of a UAV during high-speed rotational flight, from both the power and control-error perspectives. To test and demonstrate the functionality and performance of the proposed method, we present data collected during one hundred real-time high-speed yaw-tracking flight experiments. These results highlight the superior capabilities of the proposed MPS-based scheme when compared to a benchmark controller commonly used in aerial robotics, as the PFM used to quantify the cost of flight is reduced by 60.30 %, on average. To our best knowledge, these are the first flight-test results that thoroughly demonstrate, evaluate, and compare the performance of a real-time controller capable of selecting the stable CL equilibrium AEQ during operation."
Realtime Brain-Inspired Adaptive Learning Control for Nonlinear Systems with Configuration Uncertainties,"Yanhui Zhang, Zheyu Tong, Yifan Zhang, Song Chen, Junyuan Yang, Weifang Chen","Zhejiang University,zhejiang university",Mechanics and Control IV,
Safety-Conscious Pushing on Diverse Oriented Surfaces with Underactuated Aerial Vehicles,"Tong Hui, Manuel Jesús Fernández González, Matteo Fumagalli","Technical University of Denmark,Automation and Control, Technical University of Denmark,Danish Technical University",Mechanics and Control IV,"Pushing tasks performed by aerial manipulators can be used for contact-based industrial inspections. Underactuated aerial vehicles are widely employed in aerial manipulation due to their widespread availability and relatively low cost. Industrial infrastructures often consist of diverse oriented work surfaces. When interacting with such surfaces, the coupled gravity compensation and interaction force generation of underactuated aerial vehicles can present the potential challenge of near-saturation operations. The blind utilization of these platforms for such tasks can lead to instability and accidents, creating unsafe operating conditions and potentially damaging the platform. In order to ensure safe pushing on these surfaces while managing platform saturation, this work establishes a safety assessment process. This process involves the prediction of the saturation level of each actuator during pushing across variable surface orientations. Furthermore, the assessment results are used to plan and execute physical experiments, ensuring safe operations and preventing platform damage."
Geranos: A Novel Tilted-Rotors Aerial Robot for the Transportation of Poles,"Nicolas Gorlo, Mario Sven Müller, Samuel Bamert, Tim Reinhart, Henriette Stadler, Rafael Cathomen, Gabriel Käppeli, Hua Shen, Eugenio Cuniato, Marco Tognon, Roland Siegwart","ETH Zurich,ETH Zürich,Inria Rennes",Mechanics and Control IV,"Building and maintaining structures like antennas and cable-car masts in challenging terrain often involves hazardous and expensive sling-loaded helicopter operations. In this work, we challenge this paradigm by proposing Geranos: a multicopter unmanned aerial vehicle (UAV) adept at precisely placing vertical poles, blending load transport with precision. Geranos minimizes the effects of the poles' large moment of inertia by adopting a ring design that accommodates the pole in its center. To grasp the load, we developed a two-part grasping mechanism, creating a near-rigid connection between the UAV and the load. This lightweight construction is reliable and robust while not relying on active forces to maintain the grasp. The UAV utilizes four main propellers to counteract gravity and four auxiliary ones for enhanced lateral positional accuracy, ensuring full actuation around hovering. In a demonstration mimicking the installation of antennas or cable-car masts, we show the capability of Geranos to assemble poles (mass of 3kg and length of 2m) on top of each other. In this scenario, Geranos demonstrates an impressive load-placement accuracy of less than 5cm."
Robust and Energy-Efficient Control for Multi-Task Aerial Manipulation with Automatic Arm-Switching,"Ying Wu, Zida Zhou, Mingxin Wei, Hui Cheng","Sun Yat-sen University,SUN YAT-SEN UNIVERSITY",Mechanics and Control IV,"Aerial manipulation has received increasing research interest with wide applications of drones. To perform specific tasks, robotic arms with various mechanical structures will be mounted on the drone. It results in sudden disturbances to the aerial manipulator when switching the robotic arm or interacting with the environment. Hence, it is challenging to design a generic and robust control strategy adapted to various robotic arms when achieving multi-task aerial manipulation. In this paper, we present a learning-based control algorithm that allows online trajectory optimization and tracking to accomplish various aerial interaction tasks without manual adjustment. The proposed energy-saved trajectory planning approach integrates coupled dynamics model with a single rigid body to generate the energy-efficient trajectory for the aerial manipulator. Addressing the challenges of precise control when performing aerial manipulation tasks, this paper presents a controller based on deep neural networks that classifies and learns accurate forces and moments caused by different robotic arms and interactions. Moreover, the forces arising from robotic arm motions are delicately used as part of the droneâ€™s power to save energy. Extensive real-world experiments demonstrate that the proposed method can adapt to various robotic arms and interactions when performing multi-task aerial manipulation."
Optimal Collaborative Transportation for Under-Capacitated Vehicle Routing Problems Using Aerial Drone Swarms,"Akash Kopparam Sreedhara, Deepesh Padala, Shashank Mahesh, Kai Cui, Mengguang Li, Heinz Koeppl","Vellore Institute of Technology, Vellore,Technische Universität Darmstadt",Mechanics and Control IV,"Swarms of aerial drones have recently been considered for last-mile deliveries in urban logistics or automated construction. At the same time, collaborative transportation of payloads by multiple drones is another important area of recent research. However, efficient coordination algorithms for collaborative transportation of many payloads by many drones remain to be considered. In this work, we formulate the collaborative transportation of payloads by a swarm of drones as a novel, under-capacitated generalization of vehicle routing problems (VRP), which may also be of separate interest. In contrast to standard VRP and capacitated VRP, we must additionally consider waiting times for payloads lifted cooperatively by multiple drones, and the corresponding coordination. Algorithmically, we provide a solution encoding that avoids deadlocks and formulate an appropriate alternating minimization scheme to solve the problem. On the hardware side, we integrate our algorithms with collision avoidance and drone controllers. The approach and the impact of the system integration are successfully verified empirically, both on a swarm of real nano-quadcopters and for large swarms in simulation. Overall, we provide a framework for collaborative transportation with aerial drone swarms, that uses only as many drones as necessary for the transportation of any single payload."
A Modular Aerial System Based on Homogeneous Quadrotors with Fault-Tolerant Control,"Mengguang Li, Kai Cui, Heinz Koeppl",Technische Universität Darmstadt,Mechanics and Control IV,"The standard quadrotor is one of the most popular and widely used aerial vehicle of recent decades, offering great maneuverability with mechanical simplicity. However, the under-actuation characteristic limits its applications, especially when it comes to generating desired wrench with six degrees of freedom (DOF). Therefore, existing work often compromises between mechanical complexity and the controllable DOF of the aerial system. To take advantage of the mechanical simplicity of a standard quadrotor, we propose a modular aerial system, IdentiQuad, that combines only homogeneous quadrotor-based modules. Each IdentiQuad can be operated alone like a standard quadrotor, but at the same time allows task-specific assembly, increasing the controllable DOF of the system. Each module is interchangeable within its assembly. We also propose a general controller for different configurations of assemblies, capable of tolerating rotor failures and balancing the energy consumption of each module. The functionality and robustness of the system and its controller are validated using physics-based simulations for different assembly configurations."
Observer-Based Controller Design for Oscillation Damping of a Novel Suspended Underactuated Aerial Platform,"Hemjyoti Das, Minh Nhat Vu, Tobias Egle, Christian Ott","Technical University of Vienna,TU Wien, Austria,TU Wien",Mechanics and Control IV,"In this work, we present a novel actuation strategy for a suspended aerial platform. By utilizing an underactuation approach, we demonstrate the successful oscillation damping of the proposed platform, modeled as a spherical double pendulum. A state estimator is designed in order to obtain the deflection angles of the platform, which uses only onboard IMU measurements. The state estimator is an extended Kalman filter (EKF) with intermittent measurements obtained at different frequencies. An optimal state feedback controller and a PD+ controller are designed in order to dampen the oscillations of the platform in the joint space and task space respectively. The proposed underactuated platform is found to be more energy-efficient than an omnidirectional platform and requires fewer actuators. The effectiveness of our proposed system is validated using both simulations and experimental studies."
MOAR Planner: Multi-Objective and Adaptive Risk-Aware Path Planning for Infrastructure Inspection with a UAV,"Louis Petit, Alexis Lussier Desbiens","McGill University,Université de Sherbrooke",Mechanics and Control IV,"The problem of autonomous navigation for UAV inspection remains challenging as it requires effectively navigating in close proximity to obstacles, while accounting for dynamic risk factors such as weather conditions, communication reliability, and battery autonomy. This paper introduces the MOAR path planner which addresses the complexities of evolving risks during missions. It offers real-time trajectory adaptation while concurrently optimizing safety, time, and energy. The planner employs a risk-aware cost function that integrates pre-computed cost maps, the new concepts of damage and insertion costs, and an adaptive speed planning framework. With that, the optimal path is searched in a graph using a discrete representation of the state and action spaces. The method is evaluated through simulations and real-world flight tests. The results show the capability to generate real-time trajectories spanning a broad range of evaluation metrics, around 90% of the range occupied by popular algorithms. The proposed framework contributes by enabling UAVs to navigate more autonomously and reliably in critical missions."
"SOL: A Compact, Portable, Telescopic, Soft-Robotic Sun-Tracking Mechanism for Improved Solar Power Production","Bryan Busby, Shifei Duan, Marcus Thompson, Minas Liarokapis","The University of Auckland,University of Auckland,Whanauka Limited",Field Robotics and Automation,"Solar power is becoming an increasingly popular option for energy production in commercial and private applications. While installing solar panels (photovoltaic cells) in a stationary configuration is simple and inexpensive, such a setup fails to maximise their potential solar energy production. Single- and dual-axis sun trackers automatically adjust the tilt angle of photovoltaic cells so as to directly face towards sun, but these also come with their own drawbacks such as increased cost and weight. This paper presents SOL, a soft-robotic, dual-axis, sun-tracking mechanism for improved solar panel efficiency. The proposed design was built to be compact, portable, and lightweight, and it utilises closed-loop control for the intelligent actuation of a set of soft telescopic structures that raise and tilt the solar panels in the direction of the sun. The performance of the proposed solar tracking platform was experimentally validated in terms of its maximum elevation at different azimuths and its ability to balance different loads. The result is a device that provides solar panel users with an accessible, affordable, and convenient means of increasing the efficiency of their solar energy system."
Measuring Ball Joint Faults in Parabolic-Trough Solar Plants with Data Augmentation and Deep Learning,"Miguel Angel Pérez Cutiño, Jesus Capitan, José-miguel Díaz-báñez, Juan Valverde","Universidad de Sevilla,University of Seville,Universidad Sevilla",Field Robotics and Automation,"Automatic inspection of parabolic-trough solar plants is key to preventing failures that can harm the environment and the production of green energy. In this work, we propose a novel methodology to inspect ball joints in parabolic trough collectors, which is a relevant problem that is not adequately covered in the literature. Images collected by an Unmanned Aerial Vehicle are segmented using deep learning to extract ball joint components. In order to generate rich training datasets, we develop a novel data augmentation technique by rotating joints and adding synthetic image background, and demonstrate its impact on the object detection accuracy. Then two types of faults are analyzed: fluid leaks, by means of image color filtering; and geometric shape anomalies, by measuring joint angles of the robotic arms. We propose metrics to quantify these faults and evaluate the damage of the inspected components. Our experimental results with images from operating commercial plants show that we can automatically detect leaks and anomalous angular geometry with a low failure rate compared to human labeling."
ECDP: Energy Consumption Disaggregation Pipeline for Energy Optimization in Lightweight Robots,"Juan Heredia, Robin Kirschner, Christian Schlette, Saeed Abdolshah, Sami Haddadin, Kjærgaard Mikkel","University of Southern Denmark,TU Munich, Institute for Robotics and Systems Intelligence,University of Southern Denmark (SDU),KUKA Deutschland GmbH,Technical University of Munich",Field Robotics and Automation,"Limited resources and resulting energy crises occurring all over the world highlight the importance of energy efficiency in technological developments such as robotic manipulators. Efficient energy consumption of manipulators is necessary to make them affordable and spread their application in the future industry. Previously, the power consumption of the robot motion was the main factor considered in the evaluation of energy efficiency. Lately, the paradigm in industrial robotics shifted towards lightweight robot manipulators which require a new investigation on the disaggregation of robot energy consumption. In this paper, we propose a novel pipeline to identify and disaggregate the energy use of mechatronic devices and apply it to lightweight industrial robots. The proposed method allows the identification of the electronic components consumption, mechanical losses, electrical losses, and required mechanical energy for robot motion. We evaluate the pipeline and understand the distribution of energy consumption using four different manipulators, namely, Universal Robot's UR5e, UR10e, Franka Emika's FR3, and Kinova Gen3. The experimental results show that most of the energy (60- 90%) is consumed by the electronic components of the robot control box. Using this knowledge, the approaches to further optimize their energy consumption need to shift towards efficient robot electronic design instead of efficient robot mass distribution or motion control."
Autonomous UAV Mission Cycling: A Mobile Hub Approach for Precise Landings and Continuous Operations in Challenging Environments,"Alexander Moortgat-pick, Marie Schwahn, Anna Adamczyk, Daniel Andre Duecker, Sami Haddadin","Technical University of Munich (TUM),Technical University of Munich",Field Robotics and Automation,"Environmental monitoring via UAVs offers unprecedented aerial observation capabilities. However, the limited flight durations of typical multirotors and the demands on human attention in outdoor missions call for more autonomous solutions. Addressing the specific challenges of precise UAV landingsâ€”especially amidst wind disturbances, obstacles, and unreliable global localizationâ€”we introduce a mobile hub concept. This hub facilitates continuous mission cycling for unmodified off-the-shelf UAVs. Our approach centers on a small landing platform affixed to a robotic arm, adeptly correcting UAV pose errors in windy conditions. Compact enough for installation in an economy car, the system emphasizes two novel strategies. Firstly, external visual tracking of the UAV informs the landing controls for both the drone and the robotic arm. The arm compensates for UAV positioning errors and aligns the platform's attitude with the UAV for stable landings, even on small platforms under windy conditions. Secondly, the robotic arm can transport the UAV inside the hub, perform maintenance tasks like battery replacements, and then facilitate direct relaunches. Importantly, our design places all operational responsibility on the hub, ensuring the UAV remains unaltered. This ensures broad compatibility with standard UAVs, only necessitating an API for attitude setpoints. Experimental results underscore the efficiency of our model, achieving safe landings with minimal errors (â‰¤ 7 cm) in winds up to 5 Beaufort (8.1 m/s). In essence, our mobile hub concept significantly boosts UAV mission availability, allowing for autonomous operations even under challenging conditions."
Low-To-High Resolution Path Planner for Robotic Gas Distribution Mapping,"Rohit Vishwajit Nanavati, Callum Rhodes, Matthew Coombes, Cunjia Liu","Loughborough University,Imperial College London",Field Robotics and Automation,"Robotic gas distribution mapping improves the understanding of a hazardous gas dispersion while putting the human operator out of danger. Generating an accurate gas distribution map quickly is of utmost importance in situations such as gas leaks and industrial incidents, so that the efficient use of resources in response to incidents can be facilitated. In this paper, to incorporate the operational requirement on map granularity, we propose a low-to-high resolution path planner that first guides a single robots to quickly and sparsely sample the region of interest to generate a low resolution gas distribution map, followed by high resolution sampling informed by the low resolution map as a prior. The low resolution prior acts as a coverage survey allowing the algorithm to perform a relatively exploitative search of high concentration regions, resulting in overall shorter mission times. The proposed framework is designed to iteratively identify the next best $T$ locations to sample, which prioritises the potentially high reward locations, while ensuring that the robot can travel to and sample the chosen locations within a user specified map update cycle. We present a simulation study to demonstrate the alternating exploration-exploitation like behaviour along with bench-marking its performance in contrast to the traditional sampling path planners and various reward functions."
Persistent Monitoring of Large Environments with Robot Deployment Scheduling in between Remote Sensing Cycles,"Kizito Masaba, Monika Roznere, Mingi Jeong, Alberto Quattrini Li",Dartmouth College,Field Robotics and Automation,"This paper proposes a novel decision-making framework for planning â€œwhenâ€ and â€œwhereâ€ to deploy robots based on prior data with the goal of persistently monitoring a spatio-temporal phenomenon in an environment. We specifically focus on large lake monitoring, where remote sensors, such as satellites, can provide a snapshot of the target phenomenon at regular cycles. Between these cycles, Autonomous Surface Vehicles (ASVs) can be deployed to maintain an up-to-date model of the phenomenon. However, deploying ASVs has a significant logistical overhead in terms of time and cost. It requires a team of people to go on site and spend typically a day to monitor the deployment. It is vital to not only be intentional about where to sample in the environment on a given day, but also determine the worth of deploying the ASVs that day at all. Therefore, we propose a persistent monitoring strategy that provides the days and locations of when and where to sample with the robots by leveraging Gaussian Process model estimates of future trends based on collected remote sensing and point measurement data. Our approach minimizes the number of days and locations for sampling, while preserving the quality of estimates. Through simulation experiments using realistic spatio-temporal datasets, we demonstrate the benefits of our approach over traditional deployment strategies, including significant savings on the effort and operational cost of deploying the ASVs."
System Calibration of a Field Phenotyping Robot with Multiple High-Precision Profile Laser Scanners,"Felix Esser, Gereon Tombrink, André Cornelißen, Lasse Klingbeil, Heiner Kuhlmann",University of Bonn,Field Robotics and Automation,"The creation of precise and high-resolution crop point clouds in agricultural fields has become a key challenge for high-throughput phenotyping applications. This work implements a novel calibration method to calibrate the laser scanning system of an agricultural field robot consisting of two industrial-grade laser scanners used for high-precise 3D crop point cloud creation. The calibration method optimizes the transformation between the scanner origins and the robot pose by minimizing 3D point omnivariances within the point cloud. Moreover, we present a novel factor graph-based pose estimation method that fuses total station prism measurements with IMU and GNSS heading information for high-precise pose determination during calibration. The root-mean-square error of the distances to a georeferenced ground truth point cloud results in 0.8 cm after parameter optimization. Furthermore, our results show the importance of a reference point cloud in the calibration method needed to estimate the vertical translation of the calibration. Challenges arise due to non-static parameters while the robot moves, indicated by systematic deviations to a ground truth terrestrial laser scan."
Atmospheric Aerosol Diagnostics with UAV-Based Holographic Imaging and Computer Vision,"Nathaniel Bristow, Nikolas Pardoe, Jiarong Hong","University of Minnesota,ME, UMN",Field Robotics and Automation,"Emissions of particulate matter into the atmosphere are essential to characterize, in terms of properties such as particle size, morphology, and composition, to better understand impacts on public health and the climate. However, there is no currently available technology capable of measuring individual particles with such high detail over the extensive domains associated with events such as wildfires or volcanic eruptions. To solve this problem, we present an autonomous measurement system involving an unmanned aerial vehicle (UAV) coupled with a digital inline holographic microscope for in situ particle diagnostics. The flight control uses computer vision to localize and then trace the movements of particle-laden flows while sampling particles to determine their properties as they are transported away from their source. We demonstrate this system applied to measuring particulate matter in smoke plumes and discuss broader implications for this type of system in similar applications."
WayFASTER: A Self-Supervised Traversability Prediction for Increased Navigation Awareness,"Mateus Valverde Gasparino, Arun Narenthiran Sivakumar, Girish Chowdhary","University of Illinois at Urbana-Champaign,University of Illinois at Urbana Champaign",Field Robotics and Automation,"Accurate and robust navigation in unstructured environments requires fusing data from multiple sensors. Such fusion ensures that the robot is better aware of its surroundings, including areas of the environment that are not immediately visible but were visible at a different time. To solve this problem, we propose a method for traversability prediction in challenging outdoor environments using a sequence of RGB and depth images fused with pose estimations. Our method, termed WayFASTER (Waypoints-Free Autonomous System for Traversability with Enhanced Robustness), uses experience data recorded from a receding horizon estimator to train a self-supervised neural network for traversability prediction, eliminating the need for heuristics. Our experiments demonstrate that our method excels at avoiding obstacles, and correctly detects that traversable terrains, such as tall grass, can be navigable. By using a sequence of images, WayFASTER significantly enhances the robotâ€™s awareness of its surroundings, enabling it to predict the traversability of terrains that are not immediately visible. This enhanced awareness contributes to better navigation performance in environments where such predictive capabilities are essential."
A Coarse-To-Fine Place Recognition Approach Using Attention-Guided Descriptors and Overlap Estimation,"Chencan Fu, Lin Li, Jianbiao Mei, Yukai Ma, Linpeng Peng, Xiangrui Zhao, Yong Liu","Zhejiang University,zhejiang unicersity",Localization IV,"Place recognition is a challenging but crucial task in robotics. Current description-based methods may be limited by representation capabilities, while pairwise similarity-based methods require exhaustive searches, which is time-consuming. In this paper, we present a novel coarse-to-fine approach to address these problems, which combines BEV (Bird's Eye View) feature extraction, coarse-grained matching and fine-grained verification. In the coarse stage, our approach utilizes an attention-guided network to generate attention-guided descriptors. We then employ a fast affinity-based candidate selection process to identify the Top-textit{K} most similar candidates. In the fine stage, we estimate pairwise overlap among the narrowed-down place candidates to determine the final match. Experimental results on the KITTI and KITTI-360 datasets demonstrate that our approach outperforms state-of-the-art methods. The code will be released publicly soon."
LHMap-Loc: Cross-Modal Monocular Localization Using LiDAR Point Cloud Heat Map,"Xinrui Wu, Jianbo Xu, Puyuan Hu, Guangming Wang, Hesheng Wang","Shanghai Jiao Tong University,SJTU,ShanghaiJiaoTongUniversity,University of Cambridge",Localization IV,"Localization using a monocular camera in the pre-built LiDAR point cloud map has drawn increasing attention in the field of autonomous driving and mobile robotics. However, there are still many challenges (e.g. difficulties of map storage, poor localization robustness in large scenes) in accurately and efficiently implementing cross-modal localization. To solve these problems, a novel pipeline termed LHMap-loc is proposed, which achieves accurate and efficient monocular localization in LiDAR maps. Firstly, feature encoding is carried out on the original LiDAR point cloud map by generating offline heat point clouds, by which the size of the original LiDAR map is compressed. Then, an end-to-end online pose regression network is designed based on optical flow estimation and spatial attention to achieve real-time monocular visual localization in a pre-built map. In addition, a series of experiments have been conducted to prove the effectiveness of the proposed method. Our code is available at: https://github.com/IRMVLab/LHMap-loc."
LocNDF: Neural Distance Field Mapping for Robot Localization,"Louis Wiesmann, Tiziano Guadagnino, Ignacio Vizzo, Nicky Zimmerman, Yue Pan, Haofei Kuang, Jens Behley, Cyrill Stachniss","University of Bonn,Dexory,University of Lugano",Localization IV,"Mapping an environment is essential for several robotic tasks, particularly for localization. In this paper, we address the problem of mapping the environment using LiDAR point clouds with the goal to obtain a map representation that is well suited for robot localization. To this end, we utilize a neural network to learn a discretization-free distance field of a given scene for localization. In contrast to prior approaches, we directly work on the sensor data and do not assume a perfect model of the environment or rely on normals. Inspired by the recently proposed NeRF representations, we supervise the network by points sampled along the measured beams, and our loss is designed to learn a valid distance field. Additionally, we show how to perform scan registration and global localization directly within the neural distance field. We illustrate the capabilities to globally localize within an indoor environment utilizing a particle filter as well as to perform scan registration by tracking the pose of a car based on matching LiDAR scans to the neural distance field."
Looking beneath More: A Sequence-Based Localizing Ground Penetrating Radar Framework,"Pengyu Zhang, Shuaifeng Zhi, Yuelin Yuan, Beizhen Bi, Qin Xin, Xiaotao Huang, Liang Shen","National University of Defense Technology,Hikauto",Localization IV,"Localizing ground penetrating radar (LGPR) has been proven to be a promising technology for robot localization in various dynamic environments. However, the extreme scarcity of underground features introduces false candidate matches and brings unique challenges to this task. In this paper, we propose a sequence-based framework for LGPR to address the aforementioned issues. Specifically, we first introduce a trainable strategy to extract robust underground features in multi-weather conditions. By further using sequential infor- mation, our LGPR system can observe richer underground scene contexts, and the associated multi-frame scans could also improve the performance of underground place recognition. We demonstrate the superiority of our proposed method by comparing it against several recent state-of-the-art baseline methods applied to GPR image tasks. Experimental results on large public and self-collected datasets show that our proposed framework significantly improves the performance of various baselines in different scenarios."
Increasing SLAM Pose Accuracy by Ground-To-Satellite Image Registration,"Yanhao Zhang, Yujiao Shi, Shan Wang, Ankit Vora, Akhil Perincherry, Yongbo Chen, Hongdong Li","University of Technology Sydney,The Australian National University,Ford Motor Company,Australian National University,Australian National university and NICTA",Localization IV,"Vision-based localization for autonomous driving has been of great interest among researchers. When a pre-built 3D map is not available, the techniques of visual simultaneous localization and mapping (SLAM) are typically adopted. Due to error accumulation, visual SLAM (vSLAM) usually suffers from long-term drift. This paper proposes a framework to increase the localization accuracy by fusing the vSLAM with a deep-learning based ground-to-satellite (G2S) image registration method. In this framework, a coarse (spatial correlation bound check) to fine (visual odometry consistency check) method is designed to select the valid G2S prediction. The selected prediction is then fused with the SLAM measurement by solving a scaled pose graph problem. To further increase the localization accuracy, we provide an iterative trajectory fusion pipeline. The proposed framework is evaluated on two well-known autonomous driving datasets, and the results demonstrate the accuracy and robustness in terms of vehicle localization."
EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization,"Zhendong Xiao, Changhao Chen, Yang Shan, Wu Wei","South China University of Technology,National University of Defense Technology,School of Automation Science and Engineering,South China Univers",Localization IV,"Camera relocalization is pivotal in computer vision, with applications in AR, drones, robotics, and autonomous driving. It estimates 3D camera position and orientation (6-DoF) from images. Unlike traditional methods like SLAM, recent strides use deep learning for direct end-to-end pose estimation. We propose EffLoc, a novel efficient Vision Transformer for single-image camera relocalization. EffLoc's hierarchical layout, memory-bound self-attention, and feed-forward layers boost memory efficiency and inter-channel communication. Our introduced sequential group attention (SGA) module enhances computational efficiency by diversifying input features, reducing redundancy, and expanding model capacity. EffLoc excels in efficiency and accuracy, outperforming prior methods, such as AtLoc and MapNet. It thrives on large-scale outdoor car-driving scenario, ensuring simplicity, end-to-end trainability, and eliminating handcrafted loss functions."
SAGE-ICP: Semantic Information-Assisted ICP,"Jiaming Cui, Jiming Chen, Liang Li","Zhejiang University,Zhejiang Univerisity",Localization IV,"Robust and accurate pose estimation in unknown environments is an essential part of robotic applications. We focus on LiDAR-based point-to-point ICP combined with effective semantic information. This paper proposes a novel semantic information-assisted ICP method named SAGE-ICP, which leverages semantics in odometry. The semantic information for the whole scan is timely and efficiently extracted by a 3D convolution network, and these point-wise labels are deeply involved in every part of the registration, including semantic voxel downsampling, data association, adaptive local map, and dynamic vehicle removal. Unlike previous semantic-aided approaches, the proposed method can improve localization accuracy in large-scale scenes even if the semantic information has certain errors. Experimental evaluations on KITTI and KITTI-360 show that our method outperforms the baseline methods, and improves accuracy while maintaining real-time performance, i.e., runs faster than the sensor frame rate."
HR-APR: APR-Agnostic Framework with Uncertainty Estimation and Hierarchical Refinement for Camera Relocalisation,"Changkun Liu, Shuai Chen, Yukun Zhao, Huajian Huang, Victor Prisacariu, Tristan Braud","The Hong Kong University of Science and Technology,University of Oxford,Hong Kong University of Science and Technology,HKUST",Localization IV,"Absolute Pose Regressors (APRs) directly estimate camera poses from monocular images, but their accuracy is unstable for different queries. Uncertainty-aware APRs provide uncertainty information on the estimated pose, alleviating the impact of these unreliable predictions. However, existing uncertainty modelling techniques are often coupled with a specific APR architecture, resulting in suboptimal performance compared to state-of-the-art (SOTA) APR methods. This work introduces a novel APR-agnostic framework, HR-APR, that formulates uncertainty estimation as cosine similarity estimation between the query and database features. It does not rely on or affect APR network architecture, which is flexible and computationally efficient. In addition, we take advantage of the uncertainty for pose refinement to enhance the performance of APR. The extensive experiments demonstrate the effectiveness of our framework, reducing 27.4% and 15.2% of computational overhead on the 7Scenes and Cambridge Landmarks datasets while maintaining the SOTA accuracy in single-image APRs."
Implicit Learning of Scene Geometry from Poses for Global Localization,"Mohammad Altillawi, Shile Li, Sai Manoj Prakhya, Ziyuan Liu, Joan Serrat","Huawei, Autonomous University of Barcelona,,Algolux Germany,Huawei Technologies Deutscheland GmbH,Huawei group,Computer Vision Center and Computer Science department, Universi",Localization IV,"Global visual localization estimates the absolute pose of a camera using a single image, in a previously mapped area. Obtaining the pose from a single image enables many robotics and augmented/virtual reality applications. Inspired by latest advances in deep learning, many existing approaches directly learn and regress 6 DoF pose from an input image. However, these methods do not fully utilize the underlying scene geometry for pose regression. The challenge in monocular relocalization is the minimal availability of supervised training data, which is just the corresponding 6 DoF poses of the images. In this paper, we propose to utilize these minimal available labels (.i.e, poses) to learn the underlying 3D geometry of the scene and use the geometry, in return, to estimate a 6 DoF pose in a geometric manner. We present a learning method that uses these pose labels and rigid alignment to learn two 3D geometric representations (X, Y, Z coordinates) of the scene, one in camera coordinate frame and the other in global coordinate frame. Given a single image, it estimates these two 3D scene representations, which are then aligned to estimate a pose that matches the pose label. This formulation allows for the active inclusion of additional learning constraints to minimize 3D alignment errors between the two 3D scene representations and 2D re-projection errors between the 3D global scene representation and 2D image pixels, which improves localization accuracy. At inference time, our mo"
KDD-LOAM: Jointly Learned Keypoint Detector and Descriptors Assisted LiDAR Odometry and Mapping,"Renlang Huang, Minglei Zhao, Jiming Chen, Liang Li","Zhejiang University,Zhejiang Univerisity",SLAM I,"Sparse keypoint matching based on distinct 3D feature representations can improve the efficiency and robustness of point cloud registration. Existing learning-based 3D descriptors and keypoint detectors are either independent or loosely coupled, so they cannot fully adapt to each other. In this work, we propose a tightly coupled keypoint detector and descriptor (TCKDD) based on a multi-task fully convolutional network with a probabilistic detection loss. In particular, this self-supervised detection loss fully adapts the keypoint detector to any jointly learned descriptors and benefits the self-supervised learning of descriptors. Extensive experiments on both indoor and outdoor datasets show that our TCKDD achieves textit{state-of-the-art} performance in point cloud registration. Furthermore, we design a keypoint detector and descriptors-assisted LiDAR odometry and mapping framework (KDD-LOAM), whose real-time odometry relies on keypoint descriptor matching-based RANSAC. The sparse keypoints are further used for efficient scan-to-map registration and mapping. Experiments on KITTI dataset demonstrate that KDD-LOAM significantly surpasses LOAM and shows competitive performance in odometry."
"Campus Map: A Large-Scale Dataset to Support Multi-View VO, SLAM and BEV Estimation","James Ross, Nimet Kaygusuz, Oscar Alejandro Mendez Maldonado, Richard Bowden",University of Surrey,SLAM I,"Significant advances in robotics and machine learning have resulted in many datasets designed to support research into autonomous vehicle technology. However, these datasets are rarely suitable for a wide variety of navigation tasks. For example, datasets that include multiple cameras often have short trajectories without loops that are unsuitable for the evaluation of longer-range SLAM or odometry systems, and datasets with a single camera often lack other sensors, making them unsuitable for sensor fusion approaches. Furthermore, alternative environmental representations such as semantic Bird's Eye View (BEV) maps are growing in popularity, but datasets often lack accurate ground truth and are not flexible enough to adapt to new research trends. To address this gap, we introduce Campus Map, a novel large-scale multi-camera dataset with 2M images from 6 mounted cameras that includes GPS data and 64-beam, 125k point LiDAR scans totalling 8M points (raw packets also provided). The dataset consists of 16 sequences in a large car park and 6 long-term trajectories around a university campus that provide data to support research into a variety of autonomous driving and parking tasks. Long trajectories (average 10~min) and many loops make the dataset ideal for the evaluation of SLAM, odometry and loop closure algorithms, and we provide several state-of-the-art baselines. We also include 40k semantic BEV maps rendered from a digital twin. This novel approach to ground truth generation allows us to produce more accurate and crisp semantic maps than are currently available. We make the simulation environment available to allow researchers to adapt the dataset to their specific needs."
DISO: Direct Imaging Sonar Odometry,"Shida Xu, Kaicheng Zhang, Ziyang Hong, Yuanchang Liu, Sen Wang","Imperial College London,Heriot-Watt University,University College London",SLAM I,"This paper introduces a novel sonar odometry system that estimates the relative spatial transformation between two sonar image frames. Considering the unique challenges, such as low resolution and high noise, of sonar imagery for odometry and Simultaneous Localization and Mapping (SLAM), the proposed Direct Imaging Sonar Odometry (DISO) system is designed to estimate the relative transformation between two sonar frames by minimizing the aggregated sonar intensity errors of points with high intensity gradients. Moreover, DISO is implemented to incorporate a multi-sensor window optimization technique, a data association strategy and an acoustic intensity outlier rejection algorithm for reliability and accuracy. The effectiveness of DISO is evaluated using both simulated and real-world sonar datasets, showing that it outperforms the existing geometric-only method on localization accuracy and achieves state-of-the-art sonar odometry performance. The source code is available at https://github.com/SenseRoboticsLab/DISO."
CURL-MAP: Continuous Mapping and Positioning with CURL Representation,"Kaicheng Zhang, Yining Ding, Shida Xu, Ziyang Hong, Xianwen Kong, Sen Wang","Heriot-Watt University,Imperial College London,Heriot-Watt Universiy",SLAM I,"Maps of LiDAR Simultaneous Localisation and Mapping (SLAM) are often represented as point clouds. They usually take up a huge amount of storage space for large-scale environments, otherwise much structural detail may not be kept. In this paper, a novel paradigm of LiDAR mapping and odometry is designed by leveraging the Continuous and Ultra-compact Representation of LiDAR (CURL). Termed CURL-MAP (Mapping and Positioning), the proposed approach can not only reconstruct 3D maps with a continuously varying density but also efficiently reduce map storage space by using CURL's spherical harmonics implicit encoding. Different from the popular Iterative Closest Point (ICP) based LiDAR odometry techniques, CURL-MAP formulates LiDAR pose estimation as a unique optimisation problem tailored for CURL. Experiment evaluation shows that CURL-MAP achieves state-of-the-art 3D mapping results and competitive LiDAR odometry accuracy. We will release the CURL-MAP codes for the community."
Degradation Resilient LiDAR-Radar-Inertial Odometry,"Morten Nissov, Nikhil Khedekar, Kostas Alexis","NTNU,NTNU - Norwegian University of Science and Technology",SLAM I,"Enabling autonomous robots to operate robustly in challenging environments is necessary in a future with increased autonomy. For many autonomous systems, estimation and odometry remains a single point of failure, from which it can often be difficult, if not impossible, to recover. As such robust odometry solutions are of key importance. In this work a method for tightly-coupled LiDAR-Radar-Inertial fusion for odometry is proposed, enabling the mitigation of the effects of LiDAR degeneracy by leveraging a complementary perception modality while preserving the accuracy of LiDAR in well-conditioned environments. The proposed approach combines modalities in a factor graph-based windowed smoother with sensor information-specific factor formulations which enable, in the case of degeneracy, partial information to be conveyed to the graph along the non-degenerate axes. The proposed method is evaluated in real-world tests on a flying robot experiencing degraded conditions including geometric self-similarity as well as obscurant occlusion. For the benefit of the community we release the datasets presented: https://github.com/ntnu-arl/lidar_degeneracy_datasets."
Design and Evaluation of a Generic Visual SLAM Framework for Multi Camera Systems,"Pushyami Kaveti, Shankara Narayanan Vaidyanathan, Arvind Thamil Chelvan, Hanumant Singh","Northeastern University,Northeatern University",SLAM I,"Multi-camera systems have been shown to improve the accuracy and robustness of SLAM estimates, yet state-of-the-art SLAM systems predominantly support monocular or stereo setups. This paper presents a generic sparse visual SLAM framework capable of running on any number of cameras and in any arrangement. Our SLAM system uses the generalized camera model, which allows us to represent an arbitrary multi-camera system as a single imaging device. Additionally, it takes advantage of the overlapping fields of view (FoV) by extracting cross-matched features across cameras in the rig. This limits the linear rise in the number of features with the number of cameras and keeps the computational load in check while enabling an accurate representation of the scene. We evaluate our method in terms of accuracy, robustness, and run time on indoor and outdoor datasets that include challenging real-world scenarios such as narrow corridors, featureless spaces, and dynamic objects. We show that our system can adapt to different camera configurations and allows real-time execution for typical robotic applications. Finally, we benchmark the impact of the critical design parameters - the number of cameras and the overlap between their FoV that define the camera configuration for SLAM. All our software and datasets are freely available for further research."
Ground-Fusion: A Low-Cost Ground SLAM System Robust to Corner Cases,"Jie Yin, Ang Li, Wei Xi, Wenxian Yu, Danping Zou","Shanghai Jiao Tong University,Nankai University,Shanghai Jiao Ton University",SLAM I,"We introduce Ground-Fusion, a low-cost sensor fusion simultaneous localization and mapping (SLAM) system for ground vehicles. Our system features efficient initialization, effective sensor anomaly detection and handling, real-time dense color mapping, and robust localization in diverse environments. We tightly integrate RGB-D images, inertial measurements, wheel odometer and GNSS signals within a factor graph to achieve accurate and reliable localization both indoors and outdoors. To ensure successful initialization, we propose an efficient strategy that comprises three different methods: stationary, visual, and dynamic, tailored to handle diverse cases. Furthermore, we develop mechanisms to detect sensor anomalies and degradation, handling them adeptly to maintain system accuracy. Our experimental results on both public and self-collected datasets demonstrate that Ground-Fusion outperforms existing low-cost SLAM systems in corner cases. We release the code and datasets at href{https://github.com/SJTU-ViSYS/Ground-Fusion}{https://github.com/SJTU-ViSYS/Ground-Fusion}."
HERO-SLAM: Hybrid Enhanced Robust Optimization of Neural SLAM,"Zhe Xin, Yufeng Yue, Liangjun Zhang, Chenming Wu","Meituan,Beijing Institute of Technology,Baidu,Baidu Research",SLAM I,"Simultaneous Localization and Mapping (SLAM) is a fundamental task in robotics, driving numerous applications such as autonomous driving and virtual reality. Recent progress on neural implicit SLAM has shown encouraging and impressive results. However, the robustness of neural SLAM, particularly in challenging or data-limited situations, remains an unresolved issue. This paper presents HERO-SLAM, a Hybrid Enhanced Robust Optimization method for neural SLAM, which combines the benefits of neural implicit field and feature-metric optimization. This hybrid method optimizes a multi-resolution implicit field and enhances robustness in challenging environments with sudden viewpoint changes or sparse data collection. Our comprehensive experimental results on benchmarking datasets validate the effectiveness of our hybrid approach, demonstrating its superior performance over existing implicit field-based methods in challenging scenarios. HERO-SLAM provides a new pathway to enhance the stability, performance, and applicability of neural SLAM in real-world scenarios. Project page: https://hero-slam.github.io."
RBI-RRT*: Efficient Sampling-Based Path Planning for High-Dimensional State Space,"Fang Chen, Yu Zheng, Zheng Wang, Wanchao Chi, Sicong Liu","Southern University of Science and Technology,Tencent",Motion and Path Planning II,"Sampling-based planning algorithms such as RRT have been proved to be efficient in solving path planning problems for robotic systems. Various improvements to the RRT algorithm have been presented to improve the performance of the extension and convergence of the random trees, such as Informed RRT*. However, with the growth of spatial dimensions, the time consumption of randomly sampling the entire state space and incrementally rewiring the random trees raises drastically before a feasible solution is found. In this paper, to enhance the convergence performance of optimal solutions, we present Reconstructed Bi-directional Informed RRT* (RBI-RRT*) path planning algorithm. The algorithm acts as RRT-Connect to rapidly find a feasible solution, which helps compress the sampling space as Informed RRT* does. After the random trees are transformed into RRT* structure by the reconstruction process in RBI-RRT*, the algorithm continues to find the near-optimal path. A series of simulations and real-world robot experiments were conducted to evaluate the algorithm against existing planning algorithms. Compared to Informed RRT* Connect, RBI-RRT* reduced the computation time of achieving a specific cost by 22.1% on average in simulations and 11.2% in the real-world robotic arm experiments. The results show that RBI-RRT* is more efficient in high-dimensional planning problems."
Quasi-Static Path Planning for Continuum Robots by Sampling on Implicit Manifold,"Yifan Wang, Yue Chen",Georgia Institute of Technology,Motion and Path Planning II,"Continuum robots (CR) offer excellent dexterity and compliance in contrast to rigid-link robots, making them suitable for navigating through, and interacting with, confined environments. However, the study of path planning for CRs while considering external elastic contact is limited. The challenge lies in the fact that CRs can have multiple possible configurations when in contact, rendering the forward kinematics not well-defined, and characterizing the set of feasible robot configurations is non-trivial. In this paper, we propose to perform quasi-static path planning on an implicit manifold. We model elastic obstacles as external potential fields and formulate the robot statics in the potential field as the extremal trajectory of an optimal control problem. We show that the set of stable robot configurations is a smooth manifold diffeomorphic to a submanifold embedded in the product space of the CR actuation and base internal wrench. We then propose to perform path planning on this manifold using AtlasRRT*, a sampling-based planner dedicated to planning on implicit manifolds. Simulations in different operation scenarios were conducted and the results show that the proposed planner outperforms Euclidean space planners in terms of success rate and computational efficiency."
Reconfiguration of a 2D Structure Using Spatio-Temporal Planning and Load Transferring,"Javier Garcia Gonzalez, Mike Yanuzzi, Peter Kramer, Christian Rieck, Sándor Fekete, Aaron T. Becker","University of Houston,TU Braunschweig,Technische Universität Braunschweig",Motion and Path Planning II,"We present progress on the problem of reconfiguring a 2D arrangement of building material by a cooperative group of robots. These robots must avoid collisions, deadlocks, and are subjected to the constraint of maintaining connectivity of the structure. We develop two reconfiguration methods, one based on spatio-temporal planning, and one based on target swapping, to increase building efficiency. The first method can significantly reduce planning times compared to other multi-robot planners. The second method helps to reduce the amount of time robots spend waiting for paths to be cleared, and the overall distance traveled by the robots."
Neural Informed RRT*: Learning-Based Path Planning with Point Cloud State Representations under Admissible Ellipsoidal Constraints,"Zhe Huang, Hongyu Chen, John Pohovey, Katherine Driggs-Campbell","University of Illinois at Urbana-Champaign,University of Illinois Urbana-Champaign",Motion and Path Planning II,"Sampling-based planning algorithms like Rapidly-exploring Random Tree (RRT) are versatile in solving path planning problems. RRT* offers asymptotic optimality but requires growing the tree uniformly over the free space, which leaves room for efficiency improvement. To accelerate convergence, rule-based informed approaches sample states in an admissible ellipsoidal subset of the space determined by the current path cost. Learning-based alternatives model the topology of the free space and infer the states close to the optimal path to guide planning. We propose Neural Informed RRT* to combine the strengths from both sides. We define point cloud representations of free states. We perform Neural Focus, which constrains the point cloud within the admissible ellipsoidal subset from Informed RRT*, and feeds into PointNet++ for refined guidance state inference. In addition, we introduce Neural Connect to build connectivity of the guidance state set and further boost performance in challenging planning problems. Our method surpasses previous works in path planning benchmarks while preserving probabilistic completeness and asymptotic optimality. We deploy our method on a mobile robot and demonstrate real world navigation around static obstacles and dynamic humans. Code is available at https://github.com/tedhuang96/nirrt_star."
Motions in Microseconds Via Vectorized Sampling-Based Planning,"Wil Thomason, Zachary Kingston, Lydia Kavraki",Rice University,Motion and Path Planning II,"Modern sampling-based motion planning algorithms typically take between hundreds of milliseconds to dozens of seconds to find collision-free motions for high degree-of-freedom problems. This paper presents performance improvements of more than 500x over the state-of-the-art, bringing planning times into the range of microseconds and solution rates into the range of kilohertz, without specialized hardware. Our key insight is how to exploit fine-grained parallelism within planning, providing generality-preserving algorithmic improvements to any such planner and significantly accelerating critical subroutines, such as forward kinematics and collision checking. We demonstrate our approach over a diverse set of challenging, realistic problems for complex robots ranging from 7 to 14 degrees-of-freedom. Moreover, we show that our approach does not require high-power hardware by also evaluating on a low-power single-board computer. The planning speeds demonstrated are fast enough to reside in the range of control frequencies and open up new avenues of motion planning research."
Gathering Data from Risky Situations with Pareto-Optimal Trajectories,"Brennan Brodt, Alyssa Pierson",Boston University,Motion and Path Planning II,"This paper proposes a formulation for the risk-aware path planning problem which utilizes multi-objective optimization to dynamically plan trajectories that satisfy multiple complex mission specifications. In the setting of persistent monitoring, we develop a method for representing environmental information and risk in a way that allows for local sampling to generate Pareto-dominant solutions over a receding horizon. We propose two algorithms capable of solving these problems: a dense sampling approach and an improved method utilizing noisy gradient descent. Simulation results demonstrate the efficacy of our methods at persistently gathering information while avoiding risk, robust to randomly-generated environments."
RETRO: Reactive Trajectory Optimization for Real-Time Robot Motion Planning in Dynamic Environments,"Apan Dastider, Hao Fang, Lin Mingjie",University of Central Florida,Motion and Path Planning II,"Reactive trajectory optimization for robotics presents formidable challenges, demanding the rapid generation of purposeful robot motion in complex and swiftly changing dynamic environments. While much existing research predominantly addresses robotic motion planning with predefined objectives, emerging problems in robotic trajectory optimization frequently involve dynamically evolving objectives and stochastic motion dynamics. However, effectively addressing such reactive trajectory optimization challenges for robot manipulators proves difficult due to inefficient, high-dimensional trajectory representations and a lack of consideration for time optimization. In response, we introduce a novel trajectory optimization framework called RETRO. RETRO employs adaptive optimization techniques that span both spatial and temporal dimensions. As a result, it achieves a remarkable computing complexity of O(T^ 2.4 ) +O(T n^2 ), a significant improvement over the naive application of DDP, which leads to a complexity of O(n^ 4 ) when reasonable time step sizes are used. To evaluate RETROâ€™s performance in terms of error, we conducted a comprehensive analysis of its regret bounds, comparing it to an Oracle value function obtained through an Oracle trajectory optimization algorithm. Our analytical findings demonstrate that RETROâ€™s total regret can be upper-bounded by a function of the chosen time step size. Moreover, our approach delivers smoothly optimized robot trajectories within the joint-space, offering flexibility and adaptability for various tasks. It seamlessly integrates task-specific requirements such as collision avoidance while maintaining real-time control rates. We validate the effectiveness of our framework through extensive simulations and real-world robot experiments in closed-loop manipulation scenarios. For further details and supplementary materials, please visit: https://sites.google.com/view/retro-optimal-control/home"
WiTHy A*: Winding-Constrained Motion Planning for Tethered Robot Using Hybrid A*,"Vishnu S. Chipade, Rahul Kumar, Sze Zheng Yong","University of Michigan,Northeastern University",Motion and Path Planning II,"In this paper, a variant of hybrid A* is developed to find the shortest path for a curvature-constrained robot, that is tethered at its start position, such that the tether satisfies user-defined winding angle constraints. A variant of tangent graphs is used as an underlying graph for searching a path using A* in order to reduce the overall computation and define appropriate cost metrics to ensure winding angle constraints are satisfied. Conditions are provided under which the proposed algorithm is guaranteed to find a winding angle-constrained path. The effectiveness and performance of the proposed algorithm are studied in simulation."
Differentiable Boustrophedon Paths That Enable Optimization Via Gradient Descent,"Thomas Manzini, Robin Murphy",Texas A&M,Motion and Path Planning II,"This paper introduces a differentiable representation for the optimization of boustrophedon path plans in convex polygons, explores an additional parameter of these path plans that can be optimized, discusses the properties of this representation that can be leveraged during the optimization process and shows that the previously published attempt at optimization of these path plans was too coarse to be practically useful. Experiments were conducted to show that this differentiable representation can reproduce scores from traditional discrete representations of boustrophedon path plans with high fidelity. Finally, optimization via gradient descent was attempted but found to fail because the search space is far more non-convex than was previously considered in the literature. The wide range of applications for boustrophedon path plans means that this work has the potential to improve path planning efficiency in numerous areas of robotics, including mapping and search tasks using uncrewed aerial systems, environmental sampling tasks using uncrewed marine vehicles, and agricultural tasks using ground vehicles, among numerous others applications."
Torsion-Induced Compliant Joints and Its Application to Flat-Foldable and Self-Assembling Robotic Arm,"Dong-wook Yang, Hyun-su Park, Keon-ik Jang, Jae-hung Han, Dae-Young Lee","Korea Advanced Institute of Science and Technology (KAIST),KAIST,Korea Advanced Institute of Science and Technology",Robot Design,"The joint design of origami-inspired robots is one of the most distinctive features that distinguishes them from conventional robots. A joint design using materialâ€™s compliance enables origami robots to implement complex transformational movements in a lightweight and simple manner. However, utilizing the continuum bending mode of materials brings critical problems, including undesired movements and joint radius. This study introduces a solution to these problems through a torsion-based compliant joint (T-C joint) design, which utilizes the torsion deformation of materials. The potential of the T-C joint is demonstrated in a flat-foldable and self-assembling robotic arm, providing its applicability in environments with form-factor limitations and minimal human intervention. The robotic armâ€”comprising links, joints, and a gripperâ€”can fold into a flat state, deploy with precision and minimal weight, and effectively manipulate target objects. This demonstration shows the real-world application of the proposed joint design."
"OriTrack: A Small, 3 Degree of Freedom, Origami Solar Tracker","Crystal Winston, Leo Casey","Stanford University,Google, Inc",Robot Design,"In response to the need for sustainable energy solutions, solar panels have gained significant traction. One way to increase the energy capture of solar systems is through solar tracking, a means of reorienting solar panels throughout the day in order to face the sun. The energy consumption increase that comes with solar tracking often far outweighs the amount of energy required to move the panel, which makes it a compelling strategy for improving solar systems. Unfortunately, while solar trackers are commonly used in large solar farms, they are rarely used on rooftops, an area where solar panels are commonly installed. This is for two primary reasons: (1) most commercially available solar trackers are too large to be installed on roofs and (2) even if traditional solar trackers were made in a more compact form-factor it would be difficult to densely lay them out on a roof without the trackers substantially shading each other. In order to address these issues, we introduce OriTrack, a small three-degree-of-freedom (3 DOF) solar tracker which reduces the area of its shadow by reducing its height as it tracks the sun. In this paper we discuss the design, manufacturing, and control of OriTrack. We then compare OriTrack to a flat reference panel, the solar energy solution commonly used on roofs today, and find that OriTrack demonstrates 23% increased energy production. This result suggests OriTrack could be used as a future solution for solar tracking on rooftops."
Reinforcement Learning for Freeform Robot Design,"Muhan Li, David Matthews, Sam Kriegman",Northwestern University,Robot Design,"Inspired by the necessity of morphological adaptation in animals, a growing body of work has attempted to expand robot training to encompass physical aspects of a robot's design. However, reinforcement learning methods capable of optimizing the 3D morphology of a robot have been restricted to reorienting or resizing the limbs of a predetermined and static topological genus. Here we show policy gradients for designing freeform robots with arbitrary external and internal structure. This is achieved through actions that deposit or remove bundles of atomic building blocks to form higher-level nonparametric macrostructures such as appendages, organs and cavities. Although results are provided for open loop control only, we discuss how this method could be adapted for closed loop control and sim2real transfer to physical machines in future."
A Helical Bistable Soft Gripper Enable by Pneumatic Actuation,"Xuanchun Yin, Junliang Xie, Pengyu Zhou, Sheng Wen, Jiantao Zhang","South China Agricultural University,South China Agricultural University",Robot Design,"There are many instances of helical mechanisms that are used to efficiently grasp different objects with various shapes and sizes in nature. Inspired by the helical grasping in the nature, we proposed a helical bistable soft gripper with high load capacity and energy saving. An off-the-shelf bistable steel shell (BSS) as the stiff element was inserted into a 3D printing soft helical exo-skeleton to achieve coiling around and holding the objects with-out energy consumption. Two air pouches were designed as the actuator to control the transition between the two stable states. To facilitate gripper design, a simplified model of the gripper was conducted, and the geometric parameters of the gripper are listed in a table for reference. The transition pressures between the two stable states were experimentally characterized. Moreover, we conduct experiments to demonstrate the capability of the gripper in two working modes. The gripper exhibits coiling diameters ranging between 40 mm and 60 mm and is successfully attached to various slender objects of different geometries with a maximum holding force of 92.67 N (up to 135.1 times of its mass) in hanging mode. Finally, the gripper was integrated into a robot arm and successfully grasped different objects, and the maximum grasping weight is 221.6 g in the grasping mode."
Singularity Analysis of Kinova's Link 6 Robot~Arm Via Grassmann Line Geometry,"Milad Asgari, Ilian Bonev, Clement Gosselin","École de technologie supérieure,Université Laval",Robot Design,"Unlike parallel robots, for which hundreds of different architectures have been proposed, the vast majority of six-degree-of-freedom (DOF) serial robots have one of two simple architectures. In both architectures, the inverse kinematics can be solved in closed form and the singularities described by trivial geometric and algebraic conditions. These conditions can be readily obtained by analyzing the determinant of the robot's Jacobian matrix, and provide an in-depth understanding of the robot's singularities, which is essential for its optimal use. However, for various reasons, robot arms with unorthodox architectures are occasionally designed. Such arms do not have closed-form inverse kinematics and little insight into their singularities can be gained by analyzing the determinant of their Jacobian. One such robot arm for which the conventional singularity analysis approach fails is the new Link~6 collaborative robot by Kinova. In this paper, we study the complex singularities of Link~6 by investigating all possibilities for screw dependencies, deriving a simple equation for each case, and then describing each singularity type using Grassmann line geometry. Twelve different singularity configurations are identified and described with seven relatively simple geometric conditions. Our approach is general and can be applied to other robot arms."
"Design and Testing of a Multi-Module, Tetherless, Soft Robotic Eel","Robin Hall, Gabriel Espinosa, Shou-shan Chiang, Cagdas Onal","Worcester Polytechnic Institute,WPI",Robot Design,"This paper presents a free-swimming, tetherless, cable-driven modular soft robotic fish. The body comprises a series of 3D-printed wave spring structures that create a flexible biologically inspired shape that is capable of an anguilliform swimming gait. A three-module soft robotic fish was designed, fabricated, and evaluated. The motion of the robot was characterized and different combinations of actuation amplitude, frequency, and phase shift were determined experimentally to determine the optimal parameters that maximized speed and minimized the cost of transport (COT). The maximum speed recorded was 0.20 BL/s (body lengths per second) with a COT of 15.82. These results were compared against other robotic and biological fish. We operated the robot, untethered, in a variety of environments to test how it was able to function outside of laboratory settings."
Untethered Underwater Soft Robot with Thrust Vectoring,"Robin Hall, Cagdas Onal","Worcester Polytechnic Institute,WPI",Robot Design,"This paper introduces DRAGON: Deformable Robot for Agile Guided Observation and Navigation, a free-swimming deformable impeller-powered vectored underwater vehicle (VUV). A 3D-printed wave spring structure directs the water drawn through the center of the robot by an impeller, enabling it to move smoothly in different directions. The robot is designed to have a narrow cylindrical profile to lower drag and improve agility. It has a maximum recorded speed of 2.1 BL/s (body lengths per second) and a minimum cost of transport (COT) of 2.9. The robot has two degrees of freedom (DoFs) and is capable of performing a variety of maneuvers including a full circle with a radius of 0.23 m (1.4 BL) and a figure eight, which it completed in 4.98 s (72.3 degree/s) and 10.74 s respectively. We operated the robot, untethered, in various environments to test the robustness of the design and analyze its motion and performance."
A Backdrivable Axisymmetric Kinematically Redundant (6+3)-Degree-Of-Freedom Hybrid Parallel Manipulator,"Jehyeok Kim, Clement Gosselin",Université Laval,Robot Design,"A kinematically redundant (6+3)-degree-of-freedom (DOF) hybrid parallel robot with an axisymmetric workspace is proposed. By arranging the first revolute joint of each leg such that they have the same rotation axis, this robot can achieve an axisymmetric workspace, resulting in a large reachable workspace. In addition, type II singularities, which critically limit the orientational workspace, can be fully avoided by utilizing kinematic redundancy. A gripper mechanism is developed to increase the orientational workspace by exploiting the redundant DOFs. Moreover, the orientational workspace can be further increased by introducing a redundant DOF with a constant angle. As a result, the proposed hybrid parallel robot achieves a high workspace-to-footprint ratio comparable to that of serial robots. A CAD model of the robot and computer animations are provided to demonstrate the large workspaces and the gripper mechanism. A significant advantage of the proposed robot over serial architectures is that the robot is backdrivable since it uses direct-drive or quasi-direct-drive actuators."
Design of a Fully Pulley-Guided Wire-Driven Prismatic Tensegrity Robot: Friction Impact to Robot Payload Capacity,"Azamat Yeshmukhametov, Koichi Koganezawa","Nazarbayev University,Tokai University",Robot Design,"The tensegrity structure was initially created as a static structure, but it has gained significant attention among robotics researchers due to its benefits, including high payload capability, shock resistance, and resiliency. However, implementing tensegrity structures in robotics presents new technical challenges, primarily related to their wire-driven structure, such as wire-routing and wire-friction problems. Therefore, this research letter proposes a technical solution for the aforementioned problems. The main contribution of this research is the design of frictionless pulley-guided nodes. To validate the proposed concept, we conducted comparative experiments between a common tensegrity prototype and a pulley-guided prototype, evaluating wire tension distribution and payload capacity."
Motion Planning and Inertia Based Control for Impact Aware Manipulation,"Harshit Khurana, Aude G. Billard",EPFL,Kinematics and Dynamics,"In this paper, we propose a metric called hitting flux which is used in the motion generation and controls for a robot manipulator to interact with the environment through a hitting or a striking motion. Given the task of placing a known object outside of the workspace of the robot, the robot needs to come in contact with it at a non zero relative speed. The configuration of the robot and the speed at contact matter because they affect the motion of the object. The physical quantity called hitting flux depends on the robot's configuration, the robot speed and the properties of the environment. An approach to achieve the desired directional pre-impact flux for the robot through a combination of a dynamical system (DS) for motion generation and a control system that regulates the directional inertia of the robot is presented. Furthermore, a Quadratic Program (QP) formulation for achieving a desired inertia matrix at a desired position while following a motion plan constrained to the robot limits is presented. The system is tested for different scenarios in simulation showing the repeatability of the procedure and in real scenarios with KUKA LBR iiwa 7 robot."
"RASCAL: A Scalable, High-Redundancy Robot for Automated Storage and Retrieval Systems","Richard Black, Marco Caballero, Andromachi Chatzieleftheriou, Tim Deegan, Philip Heard, Freddie Hong, Russell Joyce, Sergey Legtchenko, Ant Rowstron, Adam Smith, David Sweeney, Hugh Williams","Microsoft,Microsoft Research,Microsoft Research, Cambridge, UK",Kinematics and Dynamics,"Automated storage and retrieval systems (ASRS) are a key component of the modern storage industry, and are used in a wide range of applications, carrying anything from lightweight tape cartridges to entire pallets of goods. Many of these systems are under pressure to maximise the use of space by growing in height and density, but this can create challenges for the the robots that service them. In this context, we present RASCAL, a novel ASRS robot for small payload items in structured environments, with a focus on system-level scalability and redundancy. We describe the design objectives of RASCAL and how they address some of the limitations of existing robotic systems in this area, such as scalability and redundancy. We then demonstrate the viability of our design with a proof-of-concept implementation of a data centre storage media robot, and show through a series of experiments that its design, speed, accuracy, and energy efficiency are appropriate for this application."
Virtual Passive-Joint Space Based Time-Optimal Trajectory Planning for a 4-DOF Parallel Manipulator,"Jie Zhao, Guilin Yang, Haoyu Shi, Silu Chen, Chin-Yin Chen, Chi Zhang","Chinese Academy of Sciences,Ningbo Institute of Material Technology and Engineering, Chines,University of Nottingham, Ningbo China,Ningbo Institute of Materials Technology and Engineering, CAS,Ningbo Institute of Material Technology and Engineering, CAS,Ningbo Institute of Material Technology and Engineering,CAS",Kinematics and Dynamics,"The 4-DOF (3T1R) 4PPa-2PaR parallel manipulator is developed for high-speed pick-and-place operations. However, conventional trajectory planning methods in either active joint space or Cartesian space have some shortcomings due to its high nonlinear kinematics. Owing to its unique four-to-two leg structure, the middle link that connects to the two proximal parallelogram four-bar linkages in each side only generates 2-DOF translational motions in a vertical plane. By treating each of the middle link as a 2-DOF virtual passive joint, a new trajectory planning method in the 4-DOF virtual passive-joint space is proposed, which not only simplifies the kinematic analysis, but also decreases the kinematics nonlinearity. By introducing the virtual passive joints, both displacement and velocity analyses are readily investigated. The Lagrangian method is employed to formulate the closed-form dynamic model. The quintic B-spline is utilized to generate trajectories in the virtual passive-joint space, while the Genetic Algorithm is implemented to search for the time optimal trajectory. The simulation results indicate that the optimal time planned in the virtual passive-joint space is decreased by 2.8% and 8.1% compared with the active-joint space method and Cartesian space method respectively. The average and peak jerks of the moving platform are decreased by 14.6% and 37.6% compared with the active-joint space method."
Direct Kinematic Singularities and Stability Analysis of Sagging Cable-Driven Parallel Robots,"Sébastien Briot, Jean-Pierre Merlet","LS,N,INRIA",Kinematics and Dynamics,"Sagging cable-driven parallel robots (CDPRs) are often modelled by using the Irvine's model. We will show that their configurations may be unstable, and moreover, that assessing the stability of the robot with the Irvine's model cannot be done by checking the spectrum of a stiffness matrix associated with the platform motions. In the present paper, we show that the static configurations of the sagging CDPRs are local extrema of the functional describing the robot potential energy. For assessing the stability, it is then necessary to check two conditions: The Legendre-Clebsch and the Jacobi conditions, both well known in optimal control theory. We will also (i) prove that there is a link between some singularities of the CDPRs and the limits of stability and (ii) show that singularities of the platform wrench system are not singularities of the geometric model of the sagging CDPRs, contrary to what happens in rigid-link parallel robotics. The stability prediction results are validated in simulation by cross-validating them by using a lumped model, for which the stability can be assessed by analyzing the spectrum of a reduced Hessian matrix of the potential energy."
Towards Solving Cable-Driven Parallel Robot Inaccuracy Due to Cable Elasticity,"Adolfo Suarez Roos, Zane Zake, Tahir Rasheed, Nicolo Pedemonte, Stephane Caro","IRT Jules Verne,IRCCyN - ECN - IRT JV,CNRS/LS,N",Kinematics and Dynamics,"Cable elasticity can significantly impact the accuracy of Cable-Driven Parallel Robots (CDPRs). However, itâ€™s frequently disregarded as negligible in CDPR simulations and designs. In this paper, we propose a numerical approach, referred to as SEECR, which is designed to estimate the behavior of a CDPR featuring elastic cables while ensuring the Static Equilibrium (SE) of the Moving-Platform (MP). By modeling the cables as elastic springs, the proposed approach correctly predicts which cables become slack, estimates the tension distribution among cables and computes unwanted MP motions, allowing to predict the impact of design choices. The results have been validated experimentally on two cable types and configurations."
Wrench and Twist Capability Analysis for Cable-Driven Parallel Robots with Consideration of the Actuator Torque-Speed Relationship,"Ngo Foon Chan, Wai Yi Lam, Darwin Lau",The Chinese University of Hong Kong,Kinematics and Dynamics,"The wrench and twist feasibility are the workspace conditions that indicate whether the mobile-platform (MP) of the cable-driven parallel robots (CDPRs) can provide a sufficient amount of wrench and twist. Traditionally, these two quantities are evaluated independently from the actuator's torque and speed limits, which are assumed to be fixed in the literature, but they are indeed coupled. This results in a conservative usage of the actuator capability and hence hinders the robot's actual feasibility. In this study, new approaches to analyzing and commanding CDPRs by considering the coupling effect are proposed. First, the required wrench of the MP is mapped into the twist space by the motors' torque-speed relationship and becomes the wrench-dependent available twist set. Then a new workspace condition and a new metric are introduced based on the available twist set. The metric shows the maximum allowable MP speed map of the workspace. Finally, a varying speed trajectory is designed based on the metric to optimize the total MP traveling time. This study shows the potential of robot wrench-twist capability and enhances the robot hardware effectiveness without any ha"
RicMonk: A Three-Link Brachiation Robot with Passive Grippers for Energy-Efficient Brachiation,"Grama Srinivas Shourie Grama Srinivas Shouri, Mahdi Javadi, Shivesh Kumar, Hossein Zamani Boroujeni, Frank Kirchner","Deutsches Forschungszentrum für Künstliche Intelligenz, Bremen,German Research Center for Artificial Intelligence- Robotics Inn,DFKI GmbH,DFKI-Robotics Innovation Center,University of Bremen",Kinematics and Dynamics,"This paper presents the design, analysis, and performance evaluation of RicMonk, a novel three-link brachiation robot equipped with passive hook-shaped grippers. Brachiation, an agile and energy-efficient mode of locomotion observed in primates, has inspired the development of RicMonk to explore versatile locomotion and maneuvers on ladder-like structures. The robotâ€™s anatomical resemblance to gibbons and the integration of a tail mechanism for energy injection contribute to its unique capabilities. The paper discusses the use of the Direct Collocation methodology for optimizing trajectories for the robotâ€™s dynamic behaviors and stabilization of these trajectories using a Time-varying Linear Quadratic Regulator. With RicMonk we demonstrate bidirectional brachiation, and provide comparative analysis with its predecessor, AcroMonk - a two-link brachiation robot, to demonstrate that the presence of a passive tail helps improve energy efficiency. The system design, controllers, and software implementation are publicly available on GitHub at https://github.com/dfki-ric-underactuated-lab/ricmonk and the video demonstration of the experiments can be viewed at https://youtu.be/hOuDQI7CD8w."
"Gaussian Process-Enhanced, External and Internal Convertible Form-Based Control of Underactuated Balance Robots","Feng Han, Jingang Yi",Rutgers University,Kinematics and Dynamics,"External and internal convertible (EIC) form-based motion control (i.e., EIC-based control) is one of the effective approaches for underactuated balance robots. By sequentially controller design, trajectory tracking of the actuated subsystem and balance of the unactuated subsystem can be achieved simultaneously. However, with certain conditions, there exists uncontrolled robot motion under the EIC-based control. We first identify these conditions and then propose an enhanced EIC-based control with a Gaussian process data-driven robot dynamic model. Under the new enhanced EIC-based control, the stability and performance of the closed-loop system are guaranteed. We demonstrate the GP-enhanced control experimentally using two examples of underactuated balance robots."
"Automation and Artificial Intelligence Technology in Surface Mining: State of the Art, Challenges and Opportunities","Raymond Leung, Andrew John Hill, Arman Melkumyan","The University of Sydney,University of Sydney",Multi-Robot Systems V,"This survey article provides a synopsis on some of the engineering problems, technological innovations, robotic development and automation efforts encountered in the mining industry---particularly in the Pilbara iron-ore region of Western Australia. The goal is to paint the technology landscape and highlight issues relevant to an engineering audience to raise awareness of AI and automation trends in mining. It assumes the reader has no prior knowledge of mining and builds context gradually through focused discussion and short summaries of common open-pit mining operations. The principal activities that take place may be categorized in terms of resource development, mine-, rail- and port operations. From mineral exploration to ore shipment, there are roughly nine steps in between. These include: geological assessment, mine planning and development, production drilling and assaying, blasting and excavation, transportation of ore and waste, crush and screen, stockpile and load-out, rail network distribution, and ore-car dumping. The objective is to describe these processes and provide insights on some of the challenges/opportunities from the perspective of a decade-long industry-university R&D partnership."
Hierarchical Traffic Management of Multi-AGV Systems with Deadlock Prevention Applied to Industrial Environments,"Federico Pratissoli, Riccardo Brugioni, Nicola Battilani, Lorenzo Sabattini","Università degli studi di Modena e Reggio Emilia,RSEngineering srl,University of Modena and Reggio Emilia",Multi-Robot Systems V,"This paper concerns the coordination and the traffic management of a group of Automated Guided Vehicles (AGVs) moving in a real industrial scenario, such as an automated factory or warehouse. The proposed methodology is based on a three-layer control architecture, which is described as follows: 1) the Top Layer (or Topological Layer) allows to model the traffic of vehicles among the different areas of the environment; 2) the Middle Layer allows the path planner to compute a traffic sensitive path for each vehicle; 3) the Bottom Layer (or Roadmap Layer) defines the final routes to be followed by each vehicle and coordinates the AGVs over time. In the paper we describe the coordination strategy we propose, which is executed once the routes are computed and has the aim to prevent congestions, collisions and deadlocks. The coordination algorithm exploits a novel deadlock prevention approach based on time-expanded graphs. Moreover, the presented control architecture aims at grounding theoretical methods to an industrial application by facing the typical practical issues such as graphs difficulties (load/unload locations, weak connections,), a predefined roadmap (constrained by the plant layout), vehicles errors, dynamical obstacles, etc. In this paper we propose a flexible and robust methodology for multi-AGVs traffic-aware management. Moreover, we propose a coordination algorithm, which does not rely on ad hoc assumptions or rules, to prevent collisions and deadlocks and to deal"
Task Allocation in Heterogeneous Multi-Robot Systems Based on Preference-Driven Hedonic Game,"Liwang Zhang, Minglong Li, Wenjing Yang, Shaowu Yang","National University of Defense Technology,State Key Laboratory of High Performance Computing (HPCL), Schoo",Multi-Robot Systems V,"Multiple preferences between robots and tasks have been largely overlooked in previous research on Multi-Robot Task Allocation (MRTA) problems. In this paper, we propose a preference-driven approach based on hedonic game to address the task allocation problem of muti-robot systems in emergency rescue scenarios. We present a distributed framework considering various preferences between robots and tasks to determine the division of coalitions in such problems and evaluate the scalability and adaptability of our algorithm through relevant experiments. Furthermore, considering the strict communication limitations in emergency rescue scenarios, we have verified that our algorithm can efficiently converge to a Nash-stable coalition partition even in conditions of insufficient communication distance."
Persistent Monitoring of Multiple Moving Targets Using High Order Control Barrier Functions,"Lorenzo Balandi, Nicola De Carli, Paolo Robuffo Giordano","Centre Inria de l'Université de Rennes,CNRS,IRISA CNRS UMR,,,,",Multi-Robot Systems V,"This paper considers the problem of persistently monitoring a set of moving targets using a team of aerial vehicles. Each agent in the network is assumed equipped with a camera with limited range and Field of View (FoV) providing bearing measurements and it implements an Information Consensus Filter (ICF) to estimate the state of the target(s). The ICF can be proven to be uniformly globally exponentially stable under a Persistency of Excitation (PE) condition. We then propose a distributed control scheme that allows maintaining a prescribed minimum PE level so as to ensure filter convergence. At the same time, the agents in the group are also allowed to perform additional tasks of interest while maintaining a collective observability of the target(s). In order to enforce satisfaction of the observability constraint, we leverage two main tools: (i) the weighted Observability Gramian with a forgetting factor as a measure of the cumulative acquired information, and (ii) the use of High Order Control Barrier Functions (HOCBF) as a mean to maintain a minimum level of observability for the targets. Simulation results are reported to prove the effectiveness of this approach."
Singularity Analysis of Rigid Directed Bearing Graphs for Quadrotor Formations,"Julian Erskine, Sébastien Briot, Isabelle Fantoni, Abdelhamid Chriette","Ecole Centrale de Nantes,LS,N,CNRS,Ecole centrale de Nantes",Multi-Robot Systems V,"The decentralization of formations using onboard sensing is important for multi-robot systems, improving the robustness and independence of fleet operations. Bearing measurements (obtainable from embedded cameras) are an attractive choice for use in decentralized formation control, however this requires that the formation framework be bearing rigid. Rigidity may be checked numerically for a given formation framework, however it remains difficult to determine the geometric conditions under which otherwise rigid formations become flexible. This paper models the sensor and robot constraints in bearing formations of quadrotors as a kinematic mechanism with analogous properties to find geometric conditions for the degeneration of bearing rigidity (singularities) and the resulting uncontrollable motions. A classification of singularities based on graph substructures is developed, and it is shown that arbitrarily large formations may be designed for which all singularities lie within a known set of geometric conditions. An application on how to use the knowledge of all singularity cases in a formation for singularity-free control maintenance is provided."
EM-Patroller: Entropy Maximized Multi-Robot Patrolling with Steady State Distribution Approximation,"Hongliang Guo, Qi Kang, Wei Yun Yau, Marcelo Ang, Daniela Rus","Agency for Science Technology and Research,National University of Singapore,I,R,MIT",Multi-Robot Systems V,"This paper investigates the multi-robot patrolling (MuRP) problem in a discrete environment with the objective of approaching the uniform node coverage probability distribution by the robot team. Prevailing MuRP solutions for uniform node coverage either incur high (non-polynomial) computational complexity operations for the global optimal solution, or recourse to simple yet effective heuristics for approximate solutions without any performance guarantee. In this paper, we bridge the gap by proposing an efficient iterative algorithm, namely Entropy Maximized Patroller (EM-Patroller), with the per-iteration performance improvement guarantee and polynomial computational complexity. We reformulate the multi-robot patrolling problem in discrete environments as an 'unnormalized' joint steady state distribution entropy maximization problem, and employ multi-layer perceptron (MLP) to model the relationship between each robot's patrolling strategy and the individual steady state distribution. Then, we derive a multi-agent model-based policy gradient method to gradually update the robots' patrolling strategies towards the optimum. Complexity analysis indicates the polynomial computational complexity of EM-Patroller, and we also show that EM-Patroller has additional benefits of catering to miscellaneous user-defined joint steady state distributions and incorporating other objectives, e.g., entropy maximization of individual steady state distribution, into the objective. We compare E"
Behavioral-Based Circular Formation Control for Robot Swarms,"Jesús Bautista, Hector Garcia De Marina","University of Granada,Universidad de Granada",Multi-Robot Systems V,"This paper focuses on coordinating a robot swarm orbiting a convex path without collisions among the individuals. The individual robots lack braking capabilities and can only adjust their courses while maintaining their constant but different speeds. Instead of controlling the spatial relations between the robots, our formation control algorithm aims to deploy a dense robot swarm that mimics the behavior of tornado schooling fish. To achieve this objective safely, we employ a combination of a scalable overtaking rule, a guiding vector field, and a control barrier function with an adaptive radius to facilitate smooth overtakes. The decision-making process of the robots is distributed, relying only on local information. Practical applications include defensive structures or escorting missions with the added resiliency of a swarm without a centralized command. We provide a rigorous analysis of the proposed strategy and validate its effectiveness through numerical simulations involving a high density of unicycles."
Optimization and Evaluation of a Multi Robot Surface Inspection Task through Particle Swarm Optimization,"Darren Chiu, Radhika Nagpal, Bahar Haghighat","University of Southern California,Harvard University,University of Groningen",Multi-Robot Systems V,"Robot swarms can be tasked with a variety of automated sensing and inspection applications in aerial, aquatic, and surface environments. In this paper, we study a simplified two-outcome surface inspection task. We task a group of robots to inspect and collectively classify a 2D surface section based on a binary pattern projected on the surface. We use a decentralized Bayesian decision-making algorithm and deploy a swarm of 3-cm sized wheeled robots to inspect a randomized black and white tiled surface section of size 1m x 1m in simulation. We first describe the model parameters that characterize our simulated environment, the robot swarm, and the inspection algorithm. We then employ a noise-resistant heuristic optimization scheme based on the Particle Swarm Optimization (PSO) using a fitness evaluation that combines the swarm's classification decision accuracy and decision time. We use our fitness measure definition to asses the optimized parameters through 100 randomized simulations that vary surface pattern and initial robot poses. The optimized algorithm parameters show up to 55% improvement in median of fitness evaluations against an empirically chosen parameter set."
Hierarchical Planning for Long-Horizon Multi-Agent Collective Construction,"Shambhavi Singh, Zejian Huang, Akshaya Ks, Geordan Gutow, Bhaskar Vundurthy, Howie Choset",Carnegie Mellon University,Multi-Robot Systems V,"We develop a planner that directs robots to construct a 3D target structure composed of blocks. The robots themselves are cubes of the same size as the blocks, and they may place, carry, or remove one block at a time. When moving, robots are also allowed to climb or descend a block. A construction plan may thus build a staircase-like scaffolding of blocks to reach other blocks at higher levels. The order of block placement is important; for example, a block that sits atop other blocks must be placed after the blocks below it, and a block that needs scaffolding cannot be placed until after the scaffolding is. Prior works focus on end-to-end approaches that simultaneously plan for block placement order and inter-robot collisions. Larger structures are either intractable or yield high-cost solutions. A prior approach mitigates this by decomposing the structure into smaller components that can be planned for independently, but the computational challenge remains. We present a hierarchical approach that first 1) uses A* to determine a sequence of block placements and removals while ignoring inter-robot collision, then 2) identifies ordering constraints between block placement and removal actions, and finally (3) computes collision-free paths for multiple robots to perform said actions. Compared to an optimization approach that minimizes the number of timesteps to complete the structure, we observe a 100x reduction in computation time for comparable solutions."
Enhancing mmWave Radar Point Cloud Via Visual-Inertial Supervision,"Cong Fan, Shengkai Zhang, Kezhong Liu, Shuai Wang, Zheng Yang, Wei Wang","Wuhan University of Technology,Southeast University,Tsinghua University,Huazhong University of Science and Technology",Sensor Fusion I,"Complementary to prevalent LiDAR and camera systems, millimeter-wave (mmWave) radar is robust to adverse weather conditions like fog, rainstorms, and blizzards but offers sparse point clouds. Current techniques enhance the point cloud by the supervision of LiDAR's data. However, high-performance LiDAR is notably expensive and is not commonly available on vehicles. This paper presents mmEMP, a supervised learning approach that enhances radar point clouds using a low-cost camera and an inertial measurement unit (IMU), enabling crowdsourcing training data from commercial vehicles. Bringing the visual-inertial (VI) supervision is challenging due to the spatial agnostic of dynamic objects. Moreover, spurious radar points from the curse of RF multipath make robots misunderstand the scene. mmEMP first devises a dynamic 3D reconstruction algorithm that restores the 3D positions of dynamic features. Then, we design a neural network that densifies radar data and eliminates spurious radar points. We build a new dataset in the real world. Extensive experiments show that mmEMP achieves competitive performance compared with the SOTA approach training by LiDAR's data. In addition, we use the enhanced point cloud to perform object detection, localization, and mapping to demonstrate mmEMP's effectiveness."
Influence of Camera-LiDAR Configuration on 3D Object Detection for Autonomous Driving,"Ye Li, Hanjiang Hu, Zuxin Liu, Xiaohao Xu, Xiaonan Huang, Ding Zhao","University of Michigan,Carnegie Mellon University,Carnegie mellon university",Sensor Fusion I,"Cameras and LiDARs are both important sensors for autonomous driving, playing critical roles in 3D object detection. Camera-LiDAR Fusion has been a prevalent solution for robust and accurate driving perception. In contrast to the vast majority of existing arts that focus on how to improve the performance of 3D target detection through cross-modal schemes, deep learning algorithms, and training tricks, we devote attention to the impact of sensor configurations on the performance of learning-based methods. To achieve this, we propose a unified information-theoretic surrogate metric for camera and LiDAR evaluation based on the proposed sensor perception model. We also design an accelerated high-quality framework for data acquisition, model training, and performance evaluation that functions with the CARLA simulator. To show the correlation between detection performance and our surrogate metrics, We conduct experiments using several camera-LiDAR placements and parameters inspired by self-driving companies and research institutions. Extensive experimental results of representative algorithms on nuScenes dataset validate the effectiveness of our surrogate metric, demonstrating that sensor configurations significantly impact point-cloud-image fusion based detection models, which contribute up to 30% discrepancy in terms of the average precision."
Chasing Day and Night: Towards Robust and Efficient All-Day Object Detection Guided by an Event Camera,"Jiahang Cao, Xu Zheng, Yuanhuiyi Lyu, Jiaxu Wang, Renjing Xu, Lin Wang","The Hong Kong University of Science and Technology (Guangzhou),The Hong Kong University of Science and Technology,Hong Kong University of Science and Technology (Guangzhou),HKUST",Sensor Fusion I,"The ability to detect objects in all lighting (i.e., normal-, over-, and under-exposed) conditions is crucial for real-world applications, such as self-driving. Traditional RGB-based detectors often fail under such varying lighting conditions. Therefore, recent works utilize novel event cameras to supplement or guide the RGB modality; however, these methods typically adopt asymmetric network structures that rely predominantly on the RGB modality, resulting in limited robustness for all-day detection. In this paper, we propose EOLO, a novel object detection framework that achieves robust and efficient all-day detection by fusing both RGB and event modalities. Our EOLO framework is built based on a lightweight spiking neural network (SNN) to efficiently leverage the asynchronous property of events. Buttressed by it, we first introduce an Event Temporal Attention (ETA) module to learn the high temporal information from events while preserving crucial edge information. % for fusion with RGB modality. Secondly, as different modalities exhibit varying levels of importance under diverse lighting conditions, we propose a novel Symmetric RGB-Event Fusion (SREF) module to effectively fuse RGB-Event features without relying on a specific modality, thus ensuring a balanced and adaptive fusion for all-day detection.In addition, to compensate for the lack of paired RGB-Event datasets for all-day training and evaluation, we propose an event synthesis approach based on the randomized optical flow that allows for directly generating the event frame from a single exposure image. We further build two new datasets, E-MSCOCO and E-VOC based on the popular benchmarks MSCOCO and PASCAL VOC. Extensive experiments demonstrate that our EOLO outperforms the state-of-the-art detectors, e.g., RENet, by a substantial margin (+3.74 mAP50) in all lighting conditions.Our code and datasets will be available at https://vlislab22.github.io/EOLO/."
CMDFusion: Bidirectional Fusion Network with Cross-Modality Knowledge Distillation for LIDAR Semantic Segmentation,"Jun Cen, Shiwei Zhang, Yixuan Pei, Kun Li, Hang Zheng, Maochun Luo, Yingya Zhang, Qifeng Chen","The Hong Kong University of Science and Technology,Alibaba Group,Xi'an Jiaotong University,alibaba group,HKUST",Sensor Fusion I,"2D RGB images and 3D LIDAR point clouds provide complementary knowledge for the perception system of autonomous vehicles. Several 2D and 3D fusion methods have been explored for the LIDAR semantic segmentation task, but they suffer from different problems. 2D-to-3D fusion methods require strictly paired data during inference, which may not be available in real-world scenarios, while 3D-to-2D fusion methods cannot explicitly make full use of the 2D information. Therefore, we propose a Bidirectional Fusion Network with Cross-Modality Knowledge Distillation (CMDFusion) in this work. Our method has two contributions. First, our bidirectional fusion scheme explicitly and implicitly enhances the 3D feature via 2D-to-3D fusion and 3D-to-2D fusion, respectively, which surpasses either one of the single fusion schemes. Second, we distillate the 2D knowledge from a 2D network (Camera branch) to a 3D network (2D knowledge branch) so that the 3D network can generate 2D information even for those points not in the FOV (field of view) of the camera. In this way, RGB images are not required during inference anymore since the 2D knowledge branch provides 2D information according to the 3D LIDAR input. We show that our CMDFusion achieves the best performance among all fusion-based methods on SemanticKITTI and nuScenes datasets. The code will be released at https://github.com/Jun-CEN/CMDFusion."
SK-Net: Spectral-Based Knowledge Distillation in Low-Light Thermal Imagery for Robotic Perception,"Aniruddh Sikdar, Jayant Teotia, Suresh Sundaram","Indian Institute of Science, Bangalore,Indian Institute of Science",Sensor Fusion I,"Enhancing the generalization capacity of robotic perception systems for safety-critical applications is vital, especially for environments with low-light and adverse conditions. Multi-spectral fusion techniques aim to maintain the merits of optical (EO) and infrared (IR) modalities, e.g., retaining functional highlights and capturing detailed textures from both modalities. However, these techniques encounter limitations when faced with scenarios involving missing modalities, especially during inference when only IR images are available. To address this issue, this paper proposes a novel contrastive learning-based spectral knowledge distillation technique known as SK-Net to improve the performance of deep learning models for missing modality scenarios for semantic segmentation tasks. Gated Spectral Unit (GSU) is also proposed to combine information from both modalities. SK-Net aims to extract valuable semantic information from optical images while preserving spectral knowledge from the IR modality within the feature space. The model retains the style information in the shallow layers while simultaneously fusing the high-level semantic context obtained from optical (EO) and IR modalities to improve the feature generation capacity when dealing with only IR images during inference. SK-Net outperforms state-of-the-art multi-modal fusion and distillation models in scenarios with missing modalities when using only IR data during inference in two public benchmarking datasets. This performance increase is achieved without additional computational costs compared to the baseline segmentation models."
Use Your Imagination: A Detector-Independent Approach for LiDAR Quality Booster,"Zeping Zhang, Tianran Liu, Robert Laganière",University of Ottawa,Sensor Fusion I,"Features from LiDAR and cameras are considered to be complementary. However, due to the sparsity of the LiDAR point clouds, a dense and accurate RGB/3D projective relationship is difficult to establish especially for distant scene points. Recent works try to solve this problem by designing a network that learns missing points or dense point density distributions to compensate for the sparsity of the LiDAR point cloud. In this work, we propose to use an imagine-and-locate process, called UYI. The objective of this module is to improve the point cloud quality and is independent of the detection network used for inference. We accomplish this task through a GAN based cross-modality module which uses an image as input to infer a dense LiDAR shape. Boosted by our UYI block, our experiments show a significant performance improvement in all tested baseline models. In fact, benefiting from the plug-and-play characteristics of our module, we were able to push the performance of existing state-of-the-art model to a new height. Code will be made available."
SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map Generation,"Hao Dong, Weihao Gu, Xianjing Zhang, Jintao Xu, Rui Ai, Huimin Lu, Juho Kannala, Xieyuanli Chen","ETH Zürich,HAOMO.AI Technology Co., Ltd.,National University of Defense Technology,Aalto University",Sensor Fusion I,"High-definition (HD) semantic map generation of the environment is an essential component of autonomous driving. Existing methods have achieved good performance in this task by fusing different sensor modalities, such as LiDAR and camera. However, current works are based on raw data or network feature-level fusion and only consider short-range HD map generation, limiting their deployment to realistic autonomous driving applications. In this paper, we focus on the task of building the HD maps in both short ranges, i.e., within 30 m, and also predicting long-range HD maps up to 90 m, which is required by downstream path planning and control tasks to improve the smoothness and safety of autonomous driving. To this end, we propose a novel network named SuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels. We use LiDAR depth to improve image depth estimation and use image features to guide long-range LiDAR feature prediction. We benchmark our SuperFusion on the nuScenes dataset and a self-recorded dataset and show that it outperforms the state-of-the-art baseline methods with large margins on all intervals. Additionally, we apply the generated HD map to a downstream path planning task, demonstrating that the long-range HD maps predicted by our method can lead to better path planning for autonomous vehicles. Our code and self-recorded dataset have been released at https://github.com/haomo-ai/SuperFusion."
Continuous-Time Ultra-Wideband-Inertial Fusion,"Kailai Li, Ziyu Cao, Uwe D. Hanebeck","Linköping University,Karlsruhe Institute of Technology (KIT)",Sensor Fusion I,"We introduce a novel framework of continuous-time ultra-wideband-inertial sensor fusion for online motion estimation. Quaternion-based cubic cumulative B-splines are exploited for parameterizing motion states continuously over time. Systematic derivations of analytic kinematic interpolations and spatial differentiations are further provided. Based thereon, a new sliding-window spline fitting scheme is established for asynchronous multi-sensor fusion and online calibration. We conduct a dedicated validation of the quaternion spline fitting method, and evaluate the proposed system, SFUISE (spline fusion-based ultra-wideband-inertial state estimation), in real-world scenarios using public data set and experiments. The proposed sensor fusion system is real-time capable and delivers superior performance over state-of-the-art discrete-time schemes. We release the source code and own experimental data at https://github.com/KIT-ISAS/SFUISE."
GICI-LIB: A GNSS/INS/Camera Integrated Navigation Library,"Cheng Chi, Xin Zhang, Jiahui Liu, Yulong Sun, Zihao Zhang, Xingqun Zhan","Shanghai Jiao Tong University,Shanghai Jiao Tong Univeristy",Sensor Fusion I,"Accurate navigation is essential for autonomous robots and vehicles. In recent years, the integration of the Global Navigation Satellite System (GNSS), Inertial Navigation System (INS), and camera has garnered considerable attention due to its robustness and high accuracy in diverse environments. However, leveraging the full capacity of GNSS is cumbersome because of the diverse choices of formulations, error models, satellite constellations, signal frequencies, and service types, which lead to different precision, robustness, and usage dependencies. To clarify the capacity of GNSS algorithms and accelerate the development efficiency of employing GNSS in multi-sensor fusion algorithms, we open source the GNSS/INS/Camera Integration Library (GICI-LIB), together with detailed documentation and a comprehensive land vehicle dataset. A factor graph optimization-based multi-sensor fusion framework is established, which combines almost all GNSS measurement error sources by fully considering temporal and spatial correlations between measurements. The graph structure is designed for flexibility, making it easy to form any kind of integration algorithm. For illustration, RTK, PPP, and four Real-Time Kinematic (RTK)-based algorithms from GICI-LIB are evaluated using our dataset and public datasets. Results confirm the potential of the GICI system to provide continuous precise navigation solutions in a wide spectrum of urban environments."
Semi-Supervised Learning for Visual Bird's Eye View Semantic Segmentation,"Junyu Zhu, Lina Liu, Yu Tang, Feng Wen, Wanlong Li, Yong Liu","zhejiang University,Zhejiang University,HuaWei,Huawei Technologies Co., Ltd,Beijing Huawei Digital Technologies Co., Ltd.",Visual Perception and Learning II,"Visual bird's eye view (BEV) semantic segmentation helps autonomous vehicles understand the surrounding environment only from front-view (FV) images, including static elements (e.g., roads) and dynamic elements (e.g., vehicles, pedestrians). However, the high cost of annotation procedures of full-supervised methods limits the capability of the visual BEV semantic segmentation, which usually needs HD maps, 3D object bounding boxes, and camera extrinsic matrixes. In this paper, we present a novel semi-supervised framework for visual BEV semantic segmentation to boost performance by exploiting unlabeled images during the training. A consistency loss that makes full use of unlabeled data is then proposed to constrain the model on not only semantic prediction but also the BEV feature. Furthermore, we propose a novel and effective data augmentation method named conjoint rotation which reasonably augments the dataset while maintaining the geometric relationship between the FV images and the BEV semantic segmentation. Extensive experiments on the nuScenes dataset show that our semi-supervised framework can effectively improve prediction accuracy. To the best of our knowledge, this is the first work that explores improving visual BEV semantic segmentation performance using unlabeled data. The code is available at https://github.com/Junyu-Z/Semi-BEVseg."
OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-Modal 3D Data,"Yijie Zhou, Likun Cai, Xianhui Cheng, Zhongxue Gan, Xiangyang Xue, Wenchao Ding","Fudan University,University of Toronto",Visual Perception and Learning II,"In the era of big data and large models, automatic annotating functions for multi-modal data are of great significance for real-world AI-driven applications, such as autonomous driving and embodied AI.Unlike traditional closed-set annotation, open-vocabulary annotation is essential to achieve human-level cognition capability. However, there are few open-vocabulary auto-labeling systems for multi-modal 3D data. In this paper, we introduce OpenAnnotate3D, an open-source open-vocabulary auto-labeling system that can automatically generate 2D masks, 3D masks, and 3D bounding box annotations for vision and point cloud data. Our system integrates the chain-of-thought capabilities of Large Language Models (LLMs) and the cross-modality capabilities of vision-language models (VLMs). To the best of our knowledge, OpenAnnotate3D is one of the pioneering works for open-vocabulary multi-modal 3D auto-labeling. We conduct comprehensive evaluations on both public and in-house real-world datasets, which demonstrate that the system significantly improves annotation efficiency compared to manual annotation while providing accurate open-vocabulary auto-annotating results."
SAM-Event-Adapter: Adapting Segment Anything Model for Event-RGB Semantic Segmentation,"Bowen Yao, Yongjian Deng, Yuhan Liu, Hao Chen, You-Fu Li, Zhen Yang","Beijing University of Technology,beijing university of technology,City University of Hong Kong,Beijing university of technology",Visual Perception and Learning II,"Semantic segmentation, a fundamental visual task ubiquitously employed in sectors ranging from transportation and robotics to healthcare, has always captivated the research community. In the wake of rapid advancements in large model research, the foundation model for semantic segmentation tasks, termed the Segment Anything Model (SAM), has been introduced. This model substantially addresses the dilemma of poor generalizability of previous segmentation models and the disadvantage in requiring to retrain the whole model on variant datasets. Nonetheless, segmentation models developed on SAM remain constrained by the inherent limitations of RGB sensors, particularly in scenarios characterized by complex lighting conditions and high-speed motion. Motivated by these observations, a natural recourse is to adapt SAM to additional visual modalities without compromising its robust generalizability. To achieve this, we introduce a lightweight SAM-Event-Adapter (SE-Adapter) module, which incorporates event camera data into a cross-modal learning architecture based on SAM, with only limited tunable parameters incremental. Capitalizing on the high dynamic range and temporal resolution afforded by event cameras, our proposed multi-modal Event-RGB learning architecture effectively augments the performance of semantic segmentation tasks. In addition, we propose a novel paradigm for representing event data in a patch format compatible with transformer-based models, employing multi-spatiotemporal scale encoding to efficiently extract motion and semantic correlations from event representations. Exhaustive empirical evaluations conducted on the DSEC-Semantic and DDD17 datasets provide validation of the effectiveness and rationality of our proposed approach."
BuFF: Burst Feature Finder for Light-Constrained 3D Reconstruction,"Ahalya Ravendran, Mitch Bryson, Donald G Dansereau","The Commonwealth Scientific and Industrial Research Organisation,University of Sydney",Visual Perception and Learning II,"Robots operating in low-light conditions with conventional cameras face significant challenges due to the low signal-to-noise ratio in the images. Previous work has demonstrated the use of burst-imaging techniques to partially overcome this issue. This study proposes a novel feature finder that enhances vision-based reconstruction under extremely low light conditions. The approach locates features with well-defined scale and apparent motion within each burst by jointly searching in a scale-slope space. We demonstrate improved performance in feature detection, camera pose estimation and reconstruction compared to state-of-the-art feature extractors on conventional and burst-merged images. This work opens avenues for robotic applications where low-light conditions often pose difficulties such as disaster recovery and drone delivery at night."
Unsupervised Spike Depth Estimation Via Cross-Modality Cross-Domain Knowledge Transfer,"Jiaming Liu, Qizhe Zhang, Xiaoqi Li, Jianing Li, Guanqun Wang, Ming Lu, Tiejun Huang, Shanghang Zhang","Peking University,Nanjing University,Intel Labs",Visual Perception and Learning II,"Neuromorphic spike data, an upcoming modality with high temporal resolution, has shown promising potential in autonomous driving by mitigating the challenges posed by high-velocity motion blur. However, training the spike depth estimation network holds significant challenges in two aspects: sparse spatial information for pixel-wise tasks and difficulties in achieving paired depth labels for temporally intensive spike streams. Therefore, we introduce open-source RGB data to support spike depth estimation, leveraging its annotations and spatial information. The inherent differences in modalities and data distribution make it challenging to directly apply transfer learning from open-source RGB to target spike data. To this end, we propose a cross-modality cross-domain (BiCross) framework to realize unsupervised spike depth estimation by introducing simulated mediate source spike data. Specifically, we design a Coarse-to-Fine Knowledge Distillation (CFKD) approach to facilitate comprehensive cross-modality knowledge transfer while preserving the unique strengths of both modalities, utilizing a spike-oriented uncertainty scheme. Then, we propose a Self-Correcting Teacher-Student (SCTS) mechanism to screen out reliable pixel-wise pseudo labels and ease the domain shift of the student model, which avoids error accumulation in target spike data. To verify the effectiveness of BiCross, we conduct extensive experiments on four scenarios, including Synthetic to Real, Extreme Weather, Scene Changing, and Real Spike. Our method achieves state-of-the-art (SOTA) performances, compared with RGB-oriented unsupervised depth estimation methods. The code and dataset are available at https://github.com/anonymous-4869/BiCross."
PillarGen: Enhancing Radar Point Cloud Density and Quality Via Pillar-Based Point Generation Network,"Jisong Kim, Geonho Bang, Kwangjin Choi, Minjae Seong, Jae Chang Yoo, Eunjong Pyo, Jun Won Choi","Hanyang university,Hanyang University,HL Klemove,Seoul National University",Visual Perception and Learning II,"In this paper, we present a novel point generation model, referred to as Pillar-based Point Generation Network (PillarGen), which facilitates the transformation of point clouds from one domain into another. PillarGen can produce synthetic point clouds with enhanced density and quality based on the provided input point clouds. The PillarGen model performs the following three steps: 1) pillar encoding, 2) Occupied Pillar Prediction (OPP), and 3) Pillar to Point Generation (PPG). The input point clouds are encoded using a pillar grid structure to generate pillar features. Then, OPP determines the active pillars used for point generation and predicts the center of points and the number of points to be generated for each active pillar. PPG generates the synthetic points for each active pillar based on the information provided by OPP. We evaluate the performance of PillarGen using our proprietary radar dataset, focusing on enhancing the density and quality of short-range radar data using the long-range radar data as supervision. Our experiments demonstrate that PillarGen outperforms traditional point upsampling methods in quantitative and qualitative measures. We also confirm that when PillarGen is incorporated into birdâ€™s eye view object detection, a significant improvement in detection accuracy is achieved."
Deep Dynamic Layout Optimisation of Photogrammetry Camera Position Based on Digital Twin,"Likun Wang, Zi Wang, Peter Kendall, Kevin Gumma, Alison Turner, Svetan Ratchev","Hunan University, University of Nottingham,University of Nottingham,KGTec Ltd,The University of Nottingham",Visual Perception and Learning II,"The photogrammetry system has been widely used in industrial manufacturing applications, such as high-precision assembly, reverse engineering and additive manufacturing. In order to meet the demand of the product variety and short product lifecycle, the factory facilities including photogrammetry devices, should be relocated in response to rapid change in mechanical structure and hardware integration. Nevertheless, the camera position of the photogrammetry system is difficult to select to guarantee an optimal field of view (FoV) coverage of retro-reflective targets during the whole production horizon. Especially in a reconfigurable manufacturing work cell, scaling and calibration of a photogrammetry system requires professional skills and these would cost tremendous labour for rapid configuration each time. In this paper, we propose a novel deep optimisation framework for the photogrammetry camera position for the dynamic layout design based on digital twin. The optimisation framework follows an effective coarse-to-fine procedure to evaluate the FoV visibility over the target frame. In addition, the deep Q-learning algorithm is utilised to find the maximum FoV coverage and avoid collision. Three experiments are implemented to verify the application feasibility of the proposed deep camera position optimisation framework."
MMA-Net: Multiple Morphology-Aware Network for Automated Cobb Angle Measurement,"Zhengxuan Qiu, Jie Yang, Jiankun Wang",Southern University of Science and Technology,Visual Perception and Learning II,"Scoliosis diagnosis and assessment depend largely on the measurement of the Cobb angle in spine X-ray images. With the emergence of deep learning techniques that employ landmark detection, tilt prediction, and spine segmentation, automated Cobb angle measurement has become increasingly popular. However, these methods encounter difficulties such as high noise sensitivity, intricate computational procedures, and exclusive reliance on a single type of morphological information. In this paper, we introduce the Multiple Morphology-Aware Network (MMA-Net), a novel framework that improves Cobb angle measurement accuracy by integrating multiple spine morphology as attention information. In the MMA-Net, we first feed spine X-ray images into the segmentation network to produce multiple morphological information (spine region, centerline, and boundary) and then concatenate the original X-ray image with the resulting segmentation maps as input for the regression module to perform precise Cobb angle measurement. Furthermore, we devise joint loss functions for our segmentation and regression network training, respectively. We evaluate our method on the AASCE challenge dataset and achieve superior performance with the SMAPE of 7.28% and the MAE of 3.18Â°, indicating a strong competitiveness compared to other outstanding methods. Consequently, we can offer clinicians automated, efficient, and reliable Cobb angle measurement."
Synset Boulevard: A Synthetic Image Dataset for VMMR,"Anne Sielemann, Stefan Wolf, Masoud Roschani, Jens Ziehn, Jürgen Beyerer","Fraunhofer IOSB,Fraunhofer Gesellschaft",Visual Perception and Learning II,"We present and discuss the Synset Boulevard dataset, designed for the task of surveillance-nature vehicle make and model recognition (VMMR)â€”to the best of our knowledge the first entirely synthetically generated large-scale VMMR image dataset. Through the simulation of image data rather than the manual annotation of real data, we intend to mitigate common challenges in state-of-the-art VMMR datasets, namely bias, human error, privacy, and the challenge of providing systematic updates. On the other hand, the provision and use of synthetic data introduce individual challenges, such as potential domain gaps and a less pronounced intra-class variance. Our approach to address these challenges, using path tracing and physically-based, data-driven models, is evaluated on an existing large real-world dataset. Overall, our synthetic dataset contains 32,400 independent images (each with different imaging simulations and with/without masked license plates, leading to a total of 259,200 images) from 162 different vehicle models of 43 makes depicted in front view. It is split into 8 subdatasets to investigate the influence of optical/imaging effects on the classification ability."
IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control,"Rohan Chitnis, Yingchen Xu, Bobak Hashemi, Lucas Lehnert, Ürün Dogan, Zheqing Zhu, Olivier Delalleau","Massachusetts Institute of Technology,Meta AI,Ruhr-University Bochum, Institut für Neuroinformatik,Stanford University,NVIDIA",Learning in Control I,"Model-based reinforcement learning (RL) has shown great promise due to its sample efficiency, but still struggles with long-horizon sparse-reward tasks, especially in offline settings where the agent learns from a fixed dataset. We hypothesize that model-based RL agents struggle in these environments due to a lack of long-term planning capabilities, and that planning in a temporally abstract model of the environment can alleviate this issue. In this paper, we make two key contributions: 1) we introduce an offline model-based RL algorithm, IQL-TD-MPC, that extends the state-of-the-art Temporal Difference Learning for Model Predictive Control (TD-MPC) with Implicit Q-Learning (IQL); and 2) we propose to use IQL-TD-MPC as a Manager in a hierarchical setting with any off-the-shelf offline RL algorithm as a Worker. More specifically, we pre-train a temporally abstract IQL-TD-MPC Manager to predict ""intent embeddings"", which roughly correspond to subgoals, via planning. We show that augmenting state representations with intent embeddings generated by an IQL-TD-MPC manager significantly improves off-the-shelf offline RL agents' performance on some of the most challenging D4RL benchmark tasks. For instance, the offline RL algorithms AWAC, TD3-BC, DT, and CQL all get zero or near-zero normalized evaluation scores on the medium and large antmaze tasks, while our modification gives an average score over 40."
SLIM: Skill Learning with Multiple Critics,"David Emukpere, Bingbing Wu, Julien Perez, Jean-michel Renders","Naver Labs Europe,NAVER LABS EUROPE",Learning in Control I,"Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful and safe manipulation behaviors. Furthermore, tackling this by augmenting skill discovery rewards with additional rewards through a naive combination might fail to produce desired behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop manipulation, we demonstrate the applicability of our novel skill discovery approach to acquire safe and efficient motor primitives in a hierarchical reinforcement learning fashion and leverage them through planning, significantly surpassing baseline approaches for skill discovery."
Learning Robotic Milling Strategies Based on Passive Variable Operational Space Interaction Control,"Jamie Hathaway, Alireza Rastegarpanah, Rustam Stolkin","Department of Metallurgy and Materials Science, University of Bi,University of Birmingham",Learning in Control I,"This paper addresses the problem of robotic cutting during disassembly of products for materials separation and recycling. Waste handling applications differ from milling in manufacturing processes, as they engender considerable variety and uncertainty in the parameters (e.g. hardness) of materials which the robot must cut. To address this challenge, we propose a learning-based approach incorporating elements of interaction control, in which the robot can adapt key parameters, such as feed rate, depth of cut, and mechanical compliance during task execution. We show how a mathematical model of cutting mechanics, embedded in a simulation environment, can be used to rapidly train the system without needing large amounts of data from physical cutting trials. The simulation approach was validated on a real robot setup based on four case study materials with varying structural and mechanical properties. We demonstrate the proposed method minimises process force and path deviations to a level similar to offline optimal planning methods, while the average time to complete a cutting task is within 25% of the optimum, at the expense of reduced volume of material removed per pass. A key advantage of our approach over similar works is that no prior knowledge about the material is required."
SPRINT: Scalable Policy Pre-Training Via Language Instruction Relabeling,"Jesse Zhang, Karl Pertsch, Jiahui Zhang, Joseph Lim","University of Southern California,Korea Advanced Institute of Science and Technology",Learning in Control I,"Pre-training robots with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining with offline reinforcement learning. As a result, SPRINT pre-training equips robots with a richer repertoire of skills that can help an agent generalize to new tasks. Experiments in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/sprint."
Effective Representation Learning Is More Effective in Reinforcement Learning Than You Think,"Jiawei Zheng, Yonghong Song",Xi'an Jiaotong University,Learning in Control I,"In reinforcement learning (RL), learning directly from pixels, is commonly known as vision-based RL. Effective state representations are crucial for high performance in vision-based RL. However, in order to learn effective state representations, most current vision-based RL methods based on contrastive unsupervised learning use auxiliary tasks similar to those in computer vision, which does not guarantee the effective information interaction between representation learning and RL. To learn more efficient states, we propose a simple and effective vision-based RL method. It leverages the representations acquired through contrastive learning by the Teacher Encoder and the Student Encoder to collaboratively estimate the Q-function. This cooperative process utilizes the TD error to steer updates to the Teacher Encoder, thereby ensuring effective information exchange between representation learning and RL. We refer to this approach as Reinforcement Learning with Teacher-Student Collaboration (RLTSC). RLTSC incorporates recent advancements in contrastive unsupervised learning, endowing it with potent representation learning capabilities. It provides a robust estimate of the Q-function with minimal variance and effectively guides the Teacher Ecoder to update and acquire a more efficient representation. RLTSC substantially enhances data efficiency in vision-based RL, surpassing state-of-the-art methods on various continuous and discrete control benchmarks. Remarkably, RLTSC even outperforms RL methods based on physical state features in terms of data efficiency for continuous control benchmarks. This may enlighten us: effective representation learning is more effective in reinforcement learning than you think!"
Learning Highly Dynamic Behaviors for Quadrupedal Robots,"Chong Zhang, Jiapeng Sheng, Tingguang Li, He Zhang, Cheng Zhou, Qingxu Zhu, Rui Zhao, Yizheng Zhang, Lei Han","Tencent Robotics X,Shandong University,The Chinese University of Hong Kong,Tencent",Learning in Control I,"Learning highly dynamic behaviors for robots has been a longstanding challenge. Traditional approaches have demonstrated robust locomotion, but the exhibited behaviors lack diversity and agility. They employ approximate models, which lead to compromises in performance. Data-driven approaches have been shown to reproduce agile behaviors of animals, but typically have not been able to learn highly dynamic behaviors. In this paper, we propose a learning-based approach to enable robots to learn highly dynamic behaviors from animal motion data. The learned controller is deployed on a quadrupedal robot and the results show that the controller is able to reproduce highly dynamic behaviors including sprinting, jumping and sharp turning. Various behaviors can be activated through human interaction using a stick with makers attached to it. Based on the motion pattern of the stick, the robot exhibits walking, running, sitting and jumping, much like the way humans interact with a pet."
TWIST: Teacher-Student World Model Distillation for Efficient Sim-To-Real Transfer,"Jun Yamada, Marc Rigter, Jack Collins, Ingmar Posner","University of Oxford,Oxford University",Learning in Control I,"Model-based RL is a promising approach for real-world robotics due to its improved sample efficiency and generalization capabilities compared to model-free RL. However, effective model-based RL solutions for vision-based real-world applications require bridging the sim-to-real gap for any world model learnt. Due to its significant computational cost, standard domain randomisation does not provide an effective solution to this problem. This paper proposes TWIST (Teacher-Student World Model Distillation for Sim-to-Real Transfer) to achieve efficient sim-to-real transfer of vision-based model-based RL using distillation. Specifically, TWIST leverages state observations as readily accessible, privileged information commonly garnered from a simulator to significantly accelerate sim-to-real transfer. Specifically, a teacher world model is trained efficiently on state information. At the same time, a matching dataset is collected of domain-randomised image observations. The teacher world model then supervises a student world model that takes the domain-randomised image observations as input. By distilling the learned latent dynamics model from the teacher to the student model, TWIST achieves efficient and effective sim-to-real transfer for vision-based model-based RL tasks. Experiments in simulated and real robotics tasks demonstrate that our approach outperforms naive domain randomisation and model-free methods in terms of sample efficiency and task performance of sim-to-real transfer."
Learning Vision-Based Pursuit-Evasion Robot Policies,"Andrea Bajcsy, Antonio Loquercio, Ashish Kumar, Jitendra Malik","Carnegie Mellon University,UC Berkeley",Learning in Control I,"Learning strategic robot behavior---like that required in pursuit-evasion interactions---under real-world constraints is extremely challenging. It requires exploiting the dynamics of the interaction, and planning through both physical state and latent intent uncertainty. In this paper, we transform this intractable problem into a supervised learning problem, where a fully-observable robot policy generates supervision for a partially-observable one. We find that the quality of the supervision signal for the partially-observable pursuer policy depends on two key factors: the balance of diversity and optimality of the evaderâ€™s behavior, and the strength of the modeling assumptions in the fully-observable policy. We deploy our policy on a physical quadruped robot with an RGB-D camera on pursuit-evasion interactions in the wild. Despite all the challenges, the sensing constraints bring about creativity: the robot is pushed to gather information when uncertain, predict intent from noisy measurements, and anticipate in order to intercept."
Learning to Catch Reactive Objects with a Behavior Predictor,"Kai Lu, Jia-xing Zhong, Bo Yang, Bing Wang, Andrew Markham","University of Oxford,The Hong Kong Polytechnic University,Oxford University",Learning in Grasping and Manipulation II,"Tracking and catching moving objects is an important ability for robots in a dynamic world. Whilst some objects have highly predictable state evolution e.g., the ballistic trajectory of a tennis ball, reactive targets alter their behavior in response to the motion of the manipulator. Reactive applications range from gently capturing living animals such as snakes or fish for biological investigations, to smoothly interacting with and assisting a person. Existing works for dynamic catching usually perform target prediction followed by planning, but seldom account for highly non-linear reactive behaviors. Alternatively, Reinforcement Learning (RL) based methods simply treat the target and its motion as part of the observation of the world-state, but perform poorly due to the weak reward signal. In this work, we blend the approach of an explicit, yet learned, target state predictor with RL. We further show how a tightly coupled predictor which `observes' the state of the robot leads to significantly improved anticipatory action, especially with targets that seek to evade the robot following a simple policy. Experiments show that our method achieves an 86.4% (open plane area) and a 73.8% (room) success rate on evasive objects, outperforming monolithic reinforcement learning and other techniques. We also demonstrate the efficacy of our approach across varied targets and trajectories. All code, data, and additional videos: https://kl-research.github.io/dyncatch."
Enhancing Task Performance of Learned Simplified Models Via Reinforcement Learning,"Hien Bui, Michael Posa",University of Pennsylvania,Learning in Grasping and Manipulation II,"In contact-rich tasks, the hybrid, multi-modal nature of contact dynamics poses great challenges in model representation, planning, and control. Recent efforts have attempted to address these challenges via data-driven meth- ods, learning dynamical models in combination with model predictive control. Those methods, while effective, rely solely on minimizing forward prediction errors to hope for better task performance with MPC controllers. This weak correlation can result in data inefficiency as well as limitations to overall performance. In response, we propose a novel strategy: using a policy gradient algorithm to find a simplified dynamics model that explicitly maximizes task performance. Specifically, we parameterize the stochastic policy as the perturbed output of the MPC controller, thus, the learned model representation can directly associate with the policy or task performance. We apply the proposed method to contact-rich tasks where a three-fingered robotic hand manipulates previously unknown objects. Our method significantly enhances task success rate by up to 15% in manipulating diverse objects compared to the existing method while sustaining data efficiency. Our method can solve some tasks with success rates of 70% or higher using under 30 minutes of data. All videos and codes are available at https://sites.google.com/view/lcs-rl."
Leveraging the Efficiency of Multi-Task Robot Manipulation Via Task-Evoked Planner and Reinforcement Learning,"Haofu Qian, Haoyang Zhang, Jun Shao, Jiatao Zhang, Jason Gu, Wei Song, Shiqiang Zhu","Zhejiang University,Dalhousie University,Zhejiang Lab",Learning in Grasping and Manipulation II,"Multi-task learning has expanded the boundaries of robotic manipulation, enabling the execution of increasingly complex tasks. However, policies learned through reinforcement learning frequently exhibit limited generalization and narrow distributions, which restrict their effectiveness in multi-task training. Addressing the challenge of obtaining policies with generalization and stability represents a non-trivial problem. To tackle this issue, we propose a planning-guided reinforcement learning method. It leverages a task-evoked planner(TEP) and a reinforcement learning approach with planner's guidance. TEP utilizes reusable samples as the source, with the aim of learning reachability information across different task scenarios. Then in reinforcement learning, TEP assesses and guides the Actor towards better outputs and smoothly enhances the performance in multi-task benchmarks. We evaluate this approach within the Meta-World framework and compare it with other typical multi-task algorithms in terms of learning efficiency and effectiveness. Depending on experimental results, our method has more efficiency, higher success rates, and demonstrates more realistic behavior."
Generalize by Touching: Tactile Ensemble Skill Transfer for Robotic Furniture Assembly,"Haohong Lin, Radu Ioan Corcodel, Ding Zhao","Carnegie Mellon University,Mitsubishi Electric Research Laboratories,Carnegie mellon university",Learning in Grasping and Manipulation II,"Furniture assembly remains an unsolved problem in robotic manipulation due to its long task horizon and nongeneralizable operations plan. This paper presents the Tactile Ensemble Skill Transfer (TEST) framework, a pioneering offline reinforcement learning (RL) approach that incorporates tactile feedback in the control loop. TEST's core design is to learn a skill transition model for high-level planning, along with a set of adaptive intra-skill goal-reaching policies. Such design aims to solve the robotic furniture assembly problem in a more generalizable way, facilitating seamless chaining of skills for this long-horizon task. We first sample demonstration from a set of heuristic policies and trajectories consisting of a set of randomized sub-skill segments, enabling the acquisition of rich robot trajectories that capture skill stages, robot states, visual indicators, and crucially, tactile signals. Leveraging these trajectories, our offline RL method discerns skill termination conditions and coordinates skill transitions. Our evaluations highlight the proficiency of TEST on the in-distribution furniture assemblies, its adaptability to unseen furniture configurations, and its robustness against visual disturbances. Ablation studies further accentuate the pivotal role of two algorithmic components: the skill transition model and tactile ensemble policies. Results indicate that TEST can achieve a success rate of 90% and is over 4 times more efficient than the heuristic policy in both in-distribution and generalization settings, suggesting a scalable skill transfer approach for contact-rich manipulation."
Sim2Real Manipulation on Unknown Objects with Tactile-Based Reinforcement Learning,"Entong Su, Chengzhe Jia, Yuzhe Qin, Wenxuan Zhou, Annabella Macaluso, Binghao Huang, Xiaolong Wang","University of California San Diego,University of California SanDiego,UC San Diego,Carnegie Mellon University,University of California, San Diego",Learning in Grasping and Manipulation II,"Using tactile sensors for manipulation remains one of the most challenging problems in robotics. At the heart of these challenges is generalization: How can we train a tactile-based policy that can manipulate unseen and diverse objects? In this paper, we propose to perform Reinforcement Learning with only visual tactile sensing inputs on diverse objects in a physical simulator. By training with diverse objects in simulation, it enables the policy to generalize to unseen objects. However, leveraging simulation introduces the Sim2Real transfer problem. To mitigate this problem, we study different tactile representations and evaluate how each affects real-robot manipulation results after transfer. We conduct our experiments on diverse real-world objects and show significant improvements over baselines. Our project page is available at: https://tactilerl.github.io"
Synchronized Dual-Arm Rearrangement Via Cooperative MTSP,"Wenhao Li, Shishun Zhang, Sisi Dai, Hui Huang, Ruizhen Hu, Xiaohong Chen, Kai Xu","National University of Defense Technology,Shenzhen University,Hunan University of Technology and Business",Learning in Grasping and Manipulation II,"Synchronized dual-arm rearrangement is widely studied as a common scenario in industrial applications. It often faces scalability challenges due to the computational complexity of robotic arm rearrangement and the high-dimensional nature of dual-arm planning. To address these challenges, we formulated the problem as cooperative mTSP, a variant of mTSP where agents share cooperative costs, and utilized reinforcement learning for its solution. Our approach involved representing rearrangement tasks using a task state graph that captured spatial relationships and a cooperative cost matrix that provided details about action costs. Taking these representations as observations, we designed an attention-based network to effectively combine them and provide rational task scheduling. Furthermore, a cost predictor is also introduced to directly evaluate actions during both training and planning, significantly expediting the planning process. Our experimental results demonstrate that our approach outperforms existing methods in terms of both performance and planning efficiency."
EquivAct: SIM(3)-Equivariant Visuomotor Policies Beyond Rigid Object Manipulation,"Jingyun Yang, Congyue Deng, Jimmy Wu, Rika Antonova, Leonidas Guibas, Jeannette Bohg","Stanford University,Stanford,Princeton University",Learning in Grasping and Manipulation II,"If a robot masters folding a kitchen towel, we would also expect it to master folding a beach towel. However, existing works for policy learning that rely on data augmentation are still limited in achieving this level of generalization. Our insight is to add equivariance to both the visual object representation and policy architecture. We propose EquivAct which utilizes SIM(3)-equivariant network structures that guarantee generalization across all possible object translations, 3D rotations, and scales by construction. EquivAct is trained in two phases. We first pre-train a SIM(3)-equivariant visual representation on simulated scene point clouds. Then, we learn a SIM(3)-equivariant visuomotor policy on top of the pre-trained visual representation using a small amount of source task demonstrations. We show that the learned policy directly transfers to objects that substantially differ in scale, position, and orientation from the source demonstrations. In simulation, we evaluate our method in three manipulation tasks involving deformable and articulated objects that go beyond typical rigid object manipulation tasks that prior work considered. We show that our method outperforms prior works that do not use equivariant architectures or do not use our contrastive pre-training procedure. We also show real robot experiments where the robot watches 20 demonstrations of a tabletop task and transfers zero-shot to a mobile manipulation task in a much larger setup. Project website: https://equivact.github.io"
Trade-Off between Robustness and Rewards Adversarial Training for Deep Reinforcement Learning under Large Perturbations,"Jeffrey Huang, Ho Jin Choi, Nadia Figueroa",University of Pennsylvania,Learning in Grasping and Manipulation II,"Deep Reinforcement Learning (DRL) has become a popular approach for training robots due to its generalization promise, complex task capacity and minimal human intervention. Nevertheless, DRL-trained controllers are vulnerable to even the smallest of perturbations on its inputs which can lead to catastrophic failures in real-world human-centric environments with large and unexpected perturbations. In this work, we study the vulnerability of state-of-the-art DRL subject to large perturbations and propose a novel adversarial training framework for robust control. Our approach generates aggressive attacks on the state space and the expected state-action values to emulate real-world perturbations such as sensor noise, perception failures, physical perturbations, observations mismatch, etc. To achieve this, we reformulate the adversarial risk to yield a trade-off between rewards and robustness (TBRR). We show that TBRR-aided DRL training is robust to aggressive attacks and outperforms baselines on standard DRL benchmarks (Cartpole, Pendulum), Meta-World tasks (door manipulation) and a vision-based grasping task with a 7DoF manipulator. Finally, we show that the vision-based grasping task trained in simulation via TBRR transfers sim2real with 70% success rate subject to sensor impairment and physical perturbations without any retraining."
Multi Actor-Critic DDPG for Robot Action Space Decomposition: A Framework to Control Large 3D Deformation of Soft Linear Objects,"Mélodie Daniel, Anh Trinh Quoc, Miguel Aranda, Laurent Lequievre, Juan Antonio Corrales Ramon, Roberto Iglesias, Youcef Mezouar","LaBRI - Université de Bordeaux,INP Clermont,Universidad de Zaragoza,Université Clermont Auvergne - CNRS,Universidade de Santiago de Compostela,Univ of Santiago de Compostela,Clermont Auvergne INP - SIGMA Clermont",Learning in Grasping and Manipulation II,"Robotic manipulation of deformable linear objects (DLOs) has great potential for applications in diverse fields such as agriculture or industry. However, a major challenge lies in acquiring accurate deformation models that describe the relationship between robot motion and DLO deformations. Such models are difficult to calculate analytically and vary among DLOs. Consequently, manipulating DLOs poses significant challenges, particularly in achieving large deformations that require highly accurate global models. To address these challenges, this paper presents MultiAC6: a new multi Actor-Critic framework for robot action space decomposition to control large 3D deformations of DLOs. In our approach, two deep reinforcement learning (DRL) agents orient and position a robot gripper to deform a DLO into the desired shape. Unlike previous DRL-based studies, MultiAC6 is able to solve the sim-to-real gap, achieving large 3D deformations up to 40 cm in real-world settings. Experimental results also show that MultiAC6 has a 66% higher success rate than a single-agent approach. Further experimental studies demonstrate that MultiAC6 generalizes well, without retraining, to DLOs with different lengths or materials."
DiPPeR: Diffusion-Based 2D Path Planner Applied on Legged Robots,"Jianwei Liu, Maria Stamatopoulou, Dimitrios Kanoulas",University College London,Collision Avoidance II,"In this work, we present DiPPeR, a novel and fast 2D path planning framework for quadrupedal locomotion, leveraging diffusion-driven techniques. Our contributions include a scalable dataset generator for map images and corresponding trajectories, an image-conditioned diffusion planner for mobile robots, and a training/inference pipeline employing CNNs. We validate our approach in several mazes, as well as in real-world deployment scenarios on Boston Dynamic's Spot and Unitree's Go1 robots. DiPPeR performs on average 23 times faster for trajectory generation against both search based and data driven path planning algorithms with an average of 87% consistency in producing feasible paths of various length in maps of variable size, and obstacle structure."
Efficient Polynomial Sum-Of-Squares Programming for Planar Robotic Arms,"Daniel Keren, Amit Shahar, Roi Poranne",University of Haifa,Collision Avoidance II,"Collision-avoiding motion planning for articulated robotic arms is one of the major challenges in robotics. The difficulty of the problem arises from its high dimensionality and the intricate geometry of the feasible space.Our goal is to seek large convex domains in configuration space, which contain no obstacles.In these domains, simple linear trajectories are guaranteed to be collision free, and can be leveraged for further optimization. To find such domains, practitioners have harnessed a methodology known as Sum-Of-Squares (SOS) Programming. SOS programs, however, are notorious for their poor scaling properties, which makes it challenging to employ them for complex problems. In this paper, we explore a simple formulation for a two-dimensional arm, which results in smaller SOS programs than previous suggested ones. We show that this formulation can express a variety of scenarios in a unified manner."
PathRL: An End-To-End Path Generation Method for Collision Avoidance Via Deep Reinforcement Learning,"Wenhao Yu, Jie Peng, Quecheng Qiu, Hanyu Wang, Lu Zhang, Jianmin Ji","University of Science and Technology of China,School of Data Science, USTC, Hefei ,,,,,,, China,Institute of Artificial Intelligence, Hefei Comprehensive Nation",Collision Avoidance II,"Robot navigation using deep reinforcement learning (DRL) has shown great potential in improving the performance of mobile robots. Nevertheless, most existing DRL-based navigation methods primarily focus on training a policy that directly commands the robot with low-level controls, like linear and angular velocities, which leads to unstable speeds and unsmooth trajectories of the robot during the long-term execution. An alternative method is to train a DRL policy that outputs the navigation path directly. Then the robot can follow the generated path smoothly using sophisticated velocity-planning and path-following controllers, whose parameters are specified according to the hardware platform. However, two roadblocks arise for training a DRL policy that outputs paths:(1) The action space for potential paths often involves higher dimensions comparing to low-level commands, which increases the difficulties of training; (2) It takes multiple time steps to track a path instead of a single time step, which requires the path to predicate the interactions of the robot w.r.t. the dynamic environment in multiple time steps. This, in turn, amplifies the challenges associated with training. In response to these challenges, we propose PathRL, a novel DRL method that trains the policy to generate the navigation path for the robot. Specifically, we employ specific action space discretization techniques and tailored state space representation methods to address the associated challenges. Curriculum learning is employed to expedite the training process, while the reward function also takes into account the smooth transition between adjacent paths. In our experiments, PathRL achieves better success rates and reduces angular rotation variability compared to other DRL navigation methods, facilitating stable and smooth robot movement. We demonstrate the competitive edge of PathRL in both real-world scenarios and multiple challenging simulation environments."
ZAPP! Zonotope Agreement of Prediction and Planning for Continuous-Time Collision Avoidance with Discrete-Time Dynamics,"Luca Paparusso, Shreyas Kousik, Edward Schmerling, Francesco Braghin, Marco Pavone","Politecnico di Milano,Georgia Institute of Technology,Stanford University",Collision Avoidance II,"The past few years have seen immense progress on two fronts that are critical to safe, widespread mobile robot deployment: predicting uncertain motion of multiple agents, and planning robot motion under uncertainty. However, the numerical methods required on each front have resulted in a mismatch of representation for prediction and planning. In prediction, numerical tractability is usually achieved by coarsely discretizing time, and by representing multimodal multi-agent interactions as distributions with infinite support. On the other hand, safe planning typically requires very fine time discretization, paired with distributions with compact support, to reduce conservativeness and ensure numerical tractability. The result is, when existing predictors are coupled with planning and control, one may often find unsafe motion plans. This paper proposes ZAPP (Zonotope Agreement of Prediction and Planning) to resolve the representation mismatch. ZAPP unites a prediction-friendly coarse time discretization and a planning-friendly zonotope uncertainty representation; the method also enables differentiating through a zonotope collision check, allowing one to integrate prediction and planning within a gradient-based optimization framework. Numerical examples show how ZAPP can produce safer trajectories compared to baselines in interactive scenes."
Certifying Bimanual RRT Motion Plans in a Second,"Alexandre Amice, Peter Werner, Russ Tedrake","MIT,Massachusetts Institute of Technology",Collision Avoidance II,"We present an efficient method for certifying non-collision for piecewise-polynomial motion plans in algebraic reparametrizations of configuration space. Such motion plans include those generated by popular randomized methods including RRTs and PRMs, as well as those generated by many methods in trajectory optimization. Based on Sums-of-Squares optimization, our method provides exact, rigorous certificates of non-collision; it can never falsely claim that a motion plan containing collisions is collision-free. We demonstrate that our formulation is practical for real world deployment, certifying the safety of a twelve degree of freedom motion plan in just over a second. Moreover, the method is capable of discriminating the safety or lack thereof of two motion plans which differ by only millimeters."
Cross View Capture for Distributed Image Compression with Decoder Side Information,"Yankai Yin, Zhe Sun, Peiying Ruan, Feng Duan, Ruidong Li, Chi Zhu","Nankai University,RIKEN,NVIDIA,Kanazawa University,Maebashi Institute of Technology",Collision Avoidance II,"Image compression is increasingly important in applications like intelligent driving and smart surveillance systems. This study presents a novel cross view capture distributed image compression network (CVCDIC) to improve the compression quality by using decoder side information. The CVCDICâ€™s decoder utilizes feature extraction networks to extract features from both the primary image and the side information. Furthermore, a multi-level cross view attention module is designed to capture interrelated details between images at multiple hierarchical levels. Finally, a spatial refinement module, constructed on the foundation of information distillation networks, is designed to further refine the quality of reconstructed images. The results show that CVCDIC can achieve an MS-SSIM of 0.978 at 0.15 bpp, surpassing DSIN (0.925), NDIC (0.956), and ATN (0.955) on the KITTI Stereo dataset."
Planning with Learned Subgoals Selected by Temporal Information,"Xi Huang, Gergely Sóti, Christoph Ledermann, Björn Hein, Torsten Kroeger","Karlsruhe Institute of Technology,Karlsruhe University of Applied Sciences,Karlsruher Institut für Technologie (KIT)",Collision Avoidance II,"Path planning in a changing environment is a challenging task in robotics, as moving objects impose time-dependent constraints. Recent planning methods focus primarily on the spatial aspects, lacking the capability to directly incorporate time constraints. In this paper, we propose a method that leverages a generative model to decompose a complex planning problem into small manageable ones by incrementally outputting subgoals given the current planning context. Then, we take into account the temporal information and use learned time estimators based on different statistic distributions to examine and select the generated subgoal candidates. Experiments show that planning from the current robot state to the selected subgoal can satisfy the given time-dependent constraints while being goal-oriented."
Neural Potential Field for Obstacle-Aware Local Motion Planning,"Muhammad Alhaddad, Konstantin Mironov, Aleksei Staroverov, Aleksandr Panov","Moscow Institute of Physics and Technology,MIPT,AIRI",Collision Avoidance II,"Model predictive control (MPC) may provide local motion planning for mobile robotic platforms. The challenging aspect is the analytic representation of collision cost for the case when both the obstacle map and robot footprint are arbitrary. We propose a Neural Potential Field: a neural network model that returns a differentiable collision cost based on robot pose, obstacle map, and robot footprint. The differentiability of our model allows its usage within the MPC solver. It is computationally hard to solve problems with a very high number of parameters. Therefore, our architecture includes neural image encoders, which transform obstacle maps and robot footprints into embeddings, which reduce problem dimensionality by two orders of magnitude. The reference data for network training are generated based on algorithmic calculation of a signed distance function. Comparative experiments showed that the proposed approach is comparable with existing local planners: it provides trajectories with outperforming smoothness, comparable path length, and safe distance from obstacles."
Unconstrained Model Predictive Control for Robot Navigation under Uncertainty,"Senthil Hariharan Arul, Jong Jin Park, Vishnu Prem, Yang Zhang, Dinesh Manocha","University of Maryland, College Park,Amazon Lab,,,,Amazon,Latitude Ai,University of Maryland",Collision Avoidance II,"In this paper, we present a probabilistic and unconstrained model predictive control formulation for robot navigation under uncertainty. We present (1) a closed-form approximation of the probability of collision that naturally models the propagation of uncertainty over the planning horizon and is computationally cheap to evaluate, and (2) a collision-cost formulation which provably preserves forward invariance (i.e., keeps the robot away from obstacles) when combined with the probability formulation. Notably, our formulation avoids hard constraints by construction, which in turn avoids abrupt transitions in robot behavior around the constraint boundaries leading to graceful navigation. Further, we present proof for the forward invariance and the stability of the approach. We compare the efficacy of our method with the baseline [1], which the proposed approach builds on. We demonstrate that the approach results in confident and safe robot navigation in tight spaces by smoothly slowing down the robot in low survivability environments (e.g., tight corridors), but also allows it to move away from obstacles safely when needed."
Rotenna: Harnessing Seamless Integration of Continuum Robot for Dynamic Electromagnetic Reconfiguration,"Feiyang Deng, Shilong Yao, Kwai Man Luk","City University of HongKong,City University of Hong Kong/Southern University of Science and ,City University of Hong Kong",Soft Robot Applications I,"This paper presents a novel concept, Rotenna (Robot-Antenna), which aims to address the limitations of existing reconfigurable electromagnetic devices by seamlessly integrating antennas with robotic structures. Based on this concept, we develop a prototype demonstrating enhanced flexibility and reconfigurability in electromagnetic devices. The prototype employs a continuum robot as its supporting structure, featuring a flexible leaky coaxial cable as the backbone, which allows for continuous tunability and precise control. By manipulating the continuum robot structure, we achieve real-time reconfigurable responses and adaptive adjustments in various radiation directions, gains, and modes. The interdisciplinary approach, combining electromagnetic theory and robotics engineering, shows promising potential for advancing various fields, including wireless communications, medicine, the Internet of Things, and intelligent manufacturing. Future research will focus on expanding the Rotenna concept to different types of robotic systems and electromagnetic devices, further enhancing the capabilities of reconfigurable electromagnetic systems."
Efficient RRT*-Based Safety-Constrained Motion Planning for Continuum Robots in Dynamic Environments,"Peiyu Luo, Shilong Yao, Yiyao Yue, Hong Yan, Jiankun Wang, Max Qing Hu Meng","Southern University of Science and Technology,City University of Hong Kong/Southern University of Science and ,City University of Hong Kong,The Chinese University of Hong Kong",Soft Robot Applications I,"Continuum robots, characterized by their high flexibility and infinite degrees of freedom (DoFs), have gained prominence in applications such as minimally invasive surgery and hazardous environment exploration. However, the intrinsic complexity of continuum robots requires a significant amount of time for their motion planning, posing a hurdle to their practical implementation. To tackle these challenges, efficient motion planning methods such as Rapidly Exploring Random Trees (RRT) and its variant, RRT*, have been employed. This paper introduces a unique RRT*-based motion control method tailored for continuum robots. Our approach embeds safety constraints derived from the robots' posture states, facilitating autonomous navigation and obstacle avoidance in rapidly changing environments. Simulation results show efficient trajectory planning amidst multiple dynamic obstacles and provide a robust performance evaluation based on the generated postures. Finally, preliminary tests were conducted on a two-segment cable-driven continuum robot prototype, confirming the effectiveness of the proposed planning approach. This method is versatile and can be adapted and deployed for various types of continuum robots through parameter adjustments."
Ultrafast Capturing In-Flight Objects with Reprogrammable Working Speed Ranges,"Yongkang Jiang, Xin Tong, Zhongqing Sun, Yanmin Zhou, Zhipeng Wang, Shuo Jiang, Zhen Yin, Yulong Ding, Bin He, Yingtian Li","Tongji University,Shenzhen institute of advanced technology, CAS,Shenzhen Institute of Advanced Technologyï¼ŒChinese Academy,Shenzhen Institutes of Advanced Technology, Chinese Academy of S",Soft Robot Applications I,"In-flight high-speed object capturing is crucial in nature to improve survival and adaptation to the environment, such as the predation of frogs, leopards, and eagles. Despite its ubiquitousness in nature, capturing fast-moving objects is extremely challenging in engineering implementations. In this paper, we report an ultrafast gripper based on tunable bistable structures. Different from current designs which are only suitable for objects with certain speed ranges once the grippers are fabricated, the working range of object speed of the proposed gripper could be reprogrammed by controlling the sensitivity of the structures. We present the design and fabrication of the proposed gripper in detail. A theoretical model is introduced to construct the energy landscape of the structures and the force response of the gripper when programmed to different states. The results show that in the original state, the gripper is capable of capturing a flying table tennis ball with a high speed of 15 m/s in only 6 ms. When the proposed gripper is controlled to the ultra-sensitive state, a flying ball with only 1 m/s could also be captured. This work broadens the frontiers of in-flight capturing design, and we envision broader promising applications."
"A Restorable, Variable Stiffness Pneumatic Soft Gripper Based on Jamming of Strings of Beads","Fenglin Han, Lei Fei, Run Zou, Weijian Li, Jinghao Zhou, Haiming Zhao",Central South University,Soft Robot Applications I,"Soft robots based on particle jamming cannot return to the initial position and initial mechanical state due to the accumulation of particles after removing the particle jamming,which means poor restorability, and the compliance of the robots during deformation will be reduced because of the jamming effect.Here,we present the design, fabrication, and tests of a novel soft actuator with good restorability and compliance. To improve the restorability of the actuator, we used cotton threads to connect the spherical acrylic beads into form strings instead of discrete beads.The beads could be pulled to the initial position by the threads, the actuator also returns to the initial state. To avoid the jamming effect during the deformation of the actuator, we used compressed air to drive the actuator and injected the beads into the actuator after the active deformation. To reduce the driving pressure and facilitate the flow of the beads, an initial noncontact, frame-type strain constraint structure was designed for the soft actuator. Experimental data show that the actuator was flexible during bending and the stiffness can increase more than 12-fold to resist the external load. By pulling the threads, the actuator could be restored to the initial state with an error of less than 3% of the actuator length after an operation cycle. The soft gripper based on the actuator can grasp repeatedly or laterally. The gripper can grasp soft objects such as a piece of tofu and a balloon of water."
"Hard Shell, Soft Core: Binary Actuators for Deep-Sea Applications","Cora Maria Sourkounis, Ditzia Susana Garcia Morales, Tom Kwasnitschka, Annika Raatz","Leibniz University Hannover,Leibniz Universität Hannover,GEOMAR Helmholtz Centre for Ocean Research Kiel",Soft Robot Applications I,"Deep-sea research represents invaluable opportunities to unravel hidden ecosystems, uncover unknown biodiversity, and provide critical insights into the Earthâ€™s history and the impacts of climate change. Due to the extreme conditions, exploring the deep-sea traditionally requires costly equipment, such as specific diving robots, engineered to withstand the high pressure. Our research aims to reduce the costs of deep-sea sediment sampling by introducing a novel actuation system for suction samplers, that capitalises the advantages of soft material actuators. At first glance, soft material actuators may not appear suitable for the harsh conditions that prevail in the deepsea, but when combined with a rigid, bistable mechanism there is great potential for improving the accessibility of sampling and research in this challenging environment. The binary actuation system that results from this combination, is modular, scalable,lightweight, and low cost in comparison to existing solutions."
Tip-Clutching Winch for High Tensile Force Application with Soft Growing Robots,"Obumneme Godson Osele, Kentaro Barhydt, Nicholas Cerone, Allison Okamura, Harry Asada","Stanford University,Massachusetts Institute of Technology,MIT",Soft Robot Applications I,"The navigational abilities of tip-everting soft growing robots, known as vine robots, are compromised when tip-mount devices are added to enable carrying of payloads. We present a new method for securing a vine robot to objects or its environment that exploits the unique eversion-based growth mechanism and flexibility of vine robots, while keeping the tip of the vine robot free of encumbrance. Our implementation is a tip-clutching winch, into which vine robots can insert themselves and anchor to via powerful overlapping belt friction. The device enables passive, high-strength, and reversible fastening, and can easily release the vine robot. This approach enables carrying of loads of at least 28 kg (limited by the tensile strength of the vine robot body material and winch actuator torque capacity), as well as novel material transport and locomotion capabilities."
Symmetry-Aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist,"Nguyen Hai, Tadashi Kozuno, Cristian Beltran-Hernandez, Masashi Hamaya","Northeastern University,Omron Sinic X,OMRON SINIC X,OMRON SINIC X Corporation",Soft Robot Applications I,"This study tackles the representative yet challenging contact-rich peg-in-hole task of robotic assembly, using a soft wrist that can operate more safely and tolerate lower-frequency control signals than a rigid one. Previous studies often use a fully observable formulation, requiring external setups or estimators for the peg-to-hole pose. In contrast, we use a partially observable formulation and deep reinforcement learning from demonstrations to learn a memory-based agent that acts purely on haptic and proprioceptive signals. Moreover, previous works do not incorporate potential domain symmetry and thus must search for solutions in a bigger space. Instead, we propose to leverage the symmetry for sample efficiency by augmenting the training data and constructing auxiliary losses to force the agent to adhere to the symmetry. Results in simulation with five different symmetric peg shapes show that our proposed agent can be comparable to or even outperform a state-based agent. In particular, the sample efficiency also allows us to learn directly on the hardware within 3 hours."
Force Estimation at the Bionic Soft Armâ€™s Tool-Center-Point During the Interaction with the Environment,"Samuel Pilch, Daniel Müller, Oliver Sawodny",University of Stuttgart,Soft Robot Applications I,"Soft continuum robots enable new application areas in contrast to standard rigid robots, such as interaction with a varying environment. Due to their compliant continuous structure, they are inherently safe and adaptive to environmental conditions. In this paper, the interaction with the environment is performed at the tool-center-point of a soft continuum manipulator and is realized by a hybrid force-position control. For this, a force estimation model is derived to substitute the force sensor at the tool-center-point. The force estimation is probabilistic and relies on normal distributions considering model parameters and deviations from model identification of the soft continuum robot. It also provides a qualitative measure for the contact estimation. This paper first presents the probabilistic force estimation model and then shows the hybrid force-position control using the presented model. From the results, it is concluded that force sensing is replaceable for the environment interaction."
Field-Evaluated Closed Structure Soft Gripper Enhances the Shelf Life of Harvested Blackberries,"Philip. H Johnson, Kai Junge, Charles Whitfield, Josie Hughes, Marcello Calisti","University of Lincoln,École polytechnique fédérale de Lausanne,National Institute of Agricultural Botany,EPFL,The University of Lincoln",Soft Robot Applications I,"Soft robotic grippers are intrinsically delicate while grasping objects, and can rely on mechanical deformation to adapt to different shapes without explicit control. These characteristics are particularly appealing for agriculture, where items of produce from the same crop can vary significantly in shape and size, and delicate harvesting is among the first concerns for fruit quality. Various soft robotic grippers have been proposed for harvesting different produce types, however their employment in field testing has been extremely limited. In this paper we developed the first closed structure soft gripper for the harvest of blackberries. We adapted an existing gripper concept, initially testing it on a sensorised raspberry physical twin. Then, followed grower-guided protocols to pick blackberries in farm polytunnels, and to evaluate the shelf life in comparison with berries picked by professional human pickers. Our results with ten experimental varieties showed a picking success rate of 95.4% demonstrating the capability of a closed structure gripper to adapt mechanically to fruit-shape variability. Moreover, a shelf life assessment on seven measured traits reported greatly improved shelf life of between 30 and 150%, across all traits for gripper harvested blackberries. Our study demonstrates the potential of soft grippers for delicate fruit harvesting, and indicates how to increase the impact of robotics in agriculture."
Translating Universal Scene Descriptions into Knowledge Graphs for Robotic Environment,"Giang Nguyen, Daniel Beßler, Simon Stelter, Mihai Pomarlan, Michael Beetz","University of Bremen,Universität Bremen,Universitatea Politehnica Timisoara",Semantic Scene Understanding II,"Robots performing human-scale manipulation tasks require an extensive amount of knowledge about their surroundings in order to perform their actions competently and human-like. In this work, we investigate the use of virtual reality technology as an implementation for robot environment modeling, and present a technique for translating scene graphs into knowledge bases. To this end, we take advantage of the Universal Scene Description (USD) format which is an emerging standard for the authoring, visualization and simulation of complex environments. We investigate the conversion of USD-based environment models into Knowledge Graph (KG) representations that facilitate semantic querying and integration with additional knowledge sources."
SeMLaPS: Real-Time Semantic Mapping with Latent Prior Networks and Quasi-Planar Segmentation,"Jingwen Wang, Juan Jose Tarrio, Lourdes Agapito, Pablo Fernández Alcantarilla, Alexander Vakhitov","University College London,SLAMCore,SLAMcore Ltd.,Slamcore",Semantic Scene Understanding II,"The availability of real-time semantics greatly improves the core geometric functionality of SLAM systems, enabling numerous robotic and AR/VR applications. We present a new methodology for real-time semantic mapping from RGB-D sequences that combines a 2D neural network and a 3D network based on a SLAM system with 3D occupancy mapping. When segmenting a new frame we perform latent feature re-projection from previous frames based on differentiable rendering. Fusing re-projected feature maps from previous frames with current-frame features greatly improves image segmentation quality, compared to a baseline that processes images independently. For 3D map processing, we propose a novel geometric quasi-planar over-segmentation method that groups 3D map elements likely to belong to the same semantic classes, relying on surface normals. We also describe a novel neural network design for lightweight semantic map post-processing. Our system achieves state-of-the-art semantic mapping quality within 2D-3D networks-based systems and matches the performance of 3D convolutional networks on three real indoor datasets, while working in real-time. Moreover, it shows better cross-sensor generalization abilities compared to 3D CNNs, enabling training and inference with different depth sensors. Code and data will be available upon paper acceptance."
Robotic Exploration through Semantic Topometric Mapping,"Scott Fredriksson, Akshit Saradagi, George Nikolakopoulos","Luleå University of Technology,Luleå University of Technology, Luleå, Sweden",Semantic Scene Understanding II,"In this article, we introduce a novel strategy for robotic exploration in unknown environments using a semantic topometric map. As it will be presented, the semantic topometric map is generated by segmenting the grid map of the currently explored parts of the environment into regions, such as intersections, pathways, dead-ends, and unexplored frontiers, which constitute the structural semantics of an environment. The proposed exploration strategy leverages metric information of the frontier, such as distance and angle to the frontier, similar to existing frameworks, with the key difference being the additional utilization of structural semantic information, such as properties of the intersections leading to frontiers. The algorithm for generating semantic topometric mapping utilized by the proposed method is lightweight, resulting in the method's online execution being both rapid and computationally efficient. Moreover, the proposed framework can be applied to both structured and unstructured indoor and outdoor environments, which enhances the versatility of the proposed exploration algorithm. We validate our exploration strategy and demonstrate the utility of structural semantics in exploration in two complex indoor environments by utilizing a Turtlebot3 as the robotic agent. Compared to traditional frontier-based methods, our findings indicate that the proposed approach leads to faster exploration and requires less computation time."
Open-Fusion: Real-Time Open-Vocabulary 3D Mapping and Queryable Scene Representation,"Kashu Yamazaki, Taisei Hanyu, Khoa Vo, Trong Thang Pham, Tran Minh, Gianfranco Doretto, Anh Nguyen, Ngan Le","University of Arkansas,West Virginia University,University of Liverpool",Semantic Scene Understanding II,"Precise 3D environmental mapping with semantics is essential in robotics. Existing methods often rely on predefined concepts during training or are time-intensive when generating semantic maps. This paper presents Open-Fusion, an approach for real-time open-vocabulary 3D mapping and queryable scene representation using RGB-D data. Open-Fusion harnesses the power of a pretrained vision-language foundation model (VLFM) for open-set semantic comprehension and employs the Truncated Signed Distance Function (TSDF) for swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based embeddings and their associated confidence maps. These are then integrated with the 3D knowledge from TSDF using an enhanced Hungarian-based feature matching mechanism. In particular, Open-Fusion delivers outstanding annotation-free 3D segmentation for open-vocabulary query without the need for additional 3D training. Benchmark tests on the ScanNet dataset against leading zero-shot methods highlight Open-Fusion's superiority. Furthermore, it seamlessly combines the strengths of region-based VLFM and TSDF, facilitating real-time 3D scene comprehension that includes object concepts and open-world semantics. We encourage the readers to view the demos on our project page: url{https://uark-aicv.github.io/OpenFusion}"
Mask4Former: Mask Transformer for 4D Panoptic Segmentation,"Kadir Yilmaz, Jonas Schult, Alexey Nekrasov, Bastian Leibe",RWTH Aachen University,Semantic Scene Understanding II,"Accurately perceiving and tracking instances over time is vital for the decision-making processes of autonomous agents safely interacting in dynamic environments. With this intention, we propose Mask4Former for the challenging task of 4D panoptic segmentation of LiDAR point clouds. Mask4Former is the first transformer-based approach unifying semantic instance segmentation and tracking of sparse and irregular sequences of 3D point clouds into a single joint model. Our model directly predicts semantic instances and their temporal associations without relying on any hand-engineered non-learned association strategies such as probabilistic clustering or voting-based center predictions. Instead, Mask4Former introduces spatio-temporal instance queries which encode the semantic and geometric properties of each semantic tracklet in the sequence. In an in-depth study, we discover that it is critical to promote spatially compact instance predictions as spatio-temporal instance queries tend to merge multiple semantically similar instances, even if they are spatially distant. To this end, we regress 6-DOF bounding box parameters from spatio-temporal instance queries, which is used as an auxiliary task to foster spatially compact predictions. Mask4Former achieves a new state-of-the-art on SemanticKITTI test with a score of 68.4 LSTQ."
Mask4D: End-To-End Mask-Based 4D Panoptic Segmentation for LiDAR Sequences,"Rodrigo Marcuzzi, Lucas Nunes, Louis Wiesmann, Elias Ariel Marks, Jens Behley, Cyrill Stachniss",University of Bonn,Semantic Scene Understanding II,"Scene understanding is crucial for autonomous systems to reliably navigate in the real world. Panoptic segmentation of 3D LiDAR scans allows us to semantically describe a vehicleâ€™s environment by predicting semantic classes for each 3D point and to identify individual instances through different instance IDs. To describe the dynamics of the surroundings, 4D panoptic segmentation further extends this information with temporarily consistent instance IDs to identify the different instances in the scans consistently over whole sequences. Previous approaches for 4D panoptic segmentation rely on post-processing steps and are often not end-to-end trainable. In this paper, we propose a novel approach that can be trained end-to-end and directly predicts a set of non-overlapping masks along with their semantic classes and instance IDs that are consistent over time without any post-processing like clustering or associations between predictions. We extend a mask-based 3D panoptic segmentation model to 4D by reusing queries that decoded instances in previous scans. This way, each query decodes the same instance over time, carries its ID and the tracking is performed implicitly. This enables us to jointly optimize segmentation and tracking and directly supervise for 4D panoptic segmentation. We plan to provide the code and pre-trained models in case of paper acceptance."
HSPNav: Hierarchical Scene Prior Learning for Visual Semantic Navigation towards Real Settings,"Jiaxu Kang, Bolei Chen, Ping Zhong, Haonan Yang, Sheng Yu, Jianxin Wang",Central South University,Semantic Scene Understanding II,"Visual Semantic Navigation (VSN) aims at navigating a robot to a given target object in a previously unseen scene. To tackle this task, the robot must learn a nimble navigation policy by utilizing spatial patterns and semantic co-occurrence relations among objects in the scene. Prevailing approaches extract scene priors from the instant visual observations and solidify them in neural episodic memory to achieve flexible navigation. However, due to the oblivion and underuse of the scene priors, these methods are plagued by repeated exploration, effective-knowledge sparsity, and wrong decisions. To alleviate these issues, we propose a novel VSN policy, HSPNav, based on Hierarchical Scene Priors (HSP) and Deep Reinforcement Learning (DRL). The HSP contains two components, i.e., the egocentric semantic map-based Local Scene Priors (LSP) and the commonsense relational graph-based Global Scene Priors (GSP). Then, efficient semantic navigation is achieved by employing an immediate LSP to retrieve conducive contextual memories from the GSP. By utilizing the MP3D dataset, the experimental results in the Habitat simulator demonstrate that our HSP brings a significant boost over the baselines. Furthermore, we take an essential step from simulation to reality by bridging the gap from Habitat to ROS. The migration evaluations show that HSPNav can generalize to realistic settings well and achieve promising performance."
Belief Scene Graphs: Expanding Partial Scenes with Objects through Computation of Expectation,"Mario Alberto Valdes Saucedo, Akash Patel, Akshit Saradagi, Christoforos Kanellakis, George Nikolakopoulos","Lulea University of Technology,Luleå University of Technology,Luleå University of Technology, Luleå, Sweden,LTU",Semantic Scene Understanding II,"In this article, we propose the novel concept of Belief Scene Graphs, which are utility-driven extensions of partial 3D scene graphs, that enable efficient high-level task planning with partial information. We propose a graph-based learning methodology for the computation of belief (also referred to as expectation) on any given 3D scene graph, which is then used to strategically add new nodes (referred to as blind nodes) that are relevant to a robotic mission. We propose the method of Computation of Expectation based on Correlation Information (CECI), to reasonably approximate real Belief/Expectation, by learning histograms from available training data. A novel Graph Convolutional Neural Network (GCN) model is developed, to learn CECI from a repository of 3D scene graphs. As no database of 3D scene graphs exists for the training of the novel CECI model, we present a novel methodology for generating a 3D scene graph dataset based on semantically annotated real-life 3D spaces. The generated dataset is then utilized to train the proposed CECI model and for extensive validation of the proposed method. We establish the novel concept of textit{Belief Scene Graphs} (BSG), as a core component to integrate expectations into abstract representations. This new concept is an evolution of the classical 3D scene graph concept and aims to enable high-level reasoning for task planning and optimization of a variety of robotics missions. The efficacy of the overall framework has been evaluated in an object search scenario, and has also been tested in a real-life experiment to emulate human common sense of unseen-objects. For a video of the article, showcasing the experimental demonstration, please refer to the following link: https://youtu.be/hsGlSCa12iY"
A Guided Gaussian-Dirichlet Random Field for Scientist-In-The-Loop Inference in Underwater Robotics,"Chad Samuelson, Joshua Mangelson","University,Brigham Young University",Semantic Scene Understanding II,"Visual topic modeling (VTM) provides key insight into data sets based on learned semantic topic models. The Gaussian-Dirichlet Random Field (GDRF), a state-of-the-art VTM technique, models these semantic topics in continuous space as densities. However, ambiguity in learned topics is a disadvantage of such Dirichlet-based VTM algorithms. We propose the Guided Gaussian-Dirichlet Random Field (GGDRF). Our method applies Dirichlet Forest priors from natural language processing (NLP) to the vision domain as a way to embed visual scientific knowledge into the estimation process. This modification and addition to the GDRF provides a key shift from unsupervised machine learning to semi-supervised machine learning in the robotic VTM domain. We show through simulation and real-world underwater data that the proposed GGDRF outperforms the previous GDRF method both quantitatively and qualitatively by improving alignment between estimated topics and scientific interests."
Fine-Tuning Point Cloud Transformers with Dynamic Aggregation,"Jiajun Fei, Zhidong Deng",Tsinghua University,Transfer Learning,"Point clouds play an important role in 3D analysis, which has broad applications in robotics and autonomous driving. The pre-training fine-tuning paradigm has shown great potential in the point cloud domain. Full fine-tuning is generally effective but leads to a heavy storage and computational burden, which becomes inefficient and unacceptable as the size of pretrained models scales. Although efficient fine-tuning approaches have significant progress in other domains, they generally perform worse for point clouds. To overcome this dilemma, we revisit the official Point-MAE implementation and find the critical role of aggregation in fine-tuning performances. Inspired by such discoveries, we propose a novel dynamic aggregation (DA) method to replace previous static aggregation like mean or max pooling for pre-trained point cloud Transformers. Besides standard metrics such as accuracy or mIoU, we evaluate the number of tunable parameters and additional FLOPs for a fair comparison of our method to different fine-tuning approaches. We construct several DA variants and validate them through extensive experiments. Experimental results demonstrate that DA has competitive performances against full fine-tuning and other efficient fine-tuning approaches. The code is publicly available at https://github.com/JaronTHU/DynamicAggregation."
MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation,"Cao Haozhi, Yuecong Xu, Jianfei Yang, Pengyu Yin, Shenghai Yuan, Lihua Xie","Nanyang Technological University,National University of Singapore,NANYANG TECHNOLOGICAL UNIVERSITY,NanyangTechnological University",Transfer Learning,"Multi-modal unsupervised domain adaptation (MM-UDA) for 3D semantic segmentation is a practical solution to embed semantic understanding in autonomous systems without expensive point-wise annotations. While previous MM-UDA methods can achieve overall improvement, they suffer from significant class-imbalanced performance, restricting their adoption in real applications. This imbalanced performance is mainly caused by: 1) self-training with imbalanced data and 2) the lack of pixel-wise 2D supervision signals. In this work, we propose Multi-modal Prior Aided (MoPA) domain adaptation to improve the performance of rare objects. Specifically, we develop Valid Ground-based Insertion (VGI) to rectify the imbalance supervision signals by inserting prior rare objects collected from the wild while avoiding introducing artificial artifacts that lead to trivial solutions. Meanwhile, our SAM consistency loss leverages the 2D prior semantic masks from SAM as pixel-wise supervision signals to encourage consistent predictions for each object in the semantic mask. The knowledge learned from modal-specific prior is then shared across modalities to achieve better rare object segmentation. Extensive experiments show that our method achieves state-of-the-art performance on the challenging MM-UDA benchmark. Code will be available at https://github.com/AronCao49/MoPA."
Cross Domain Policy Transfer with Effect Cycle-Consistency,"Ruiqi Zhu, Tianhong Dai, Oya Celiktutan","King's College London,Imperial College London",Transfer Learning,"Training a robotic policy from scratch using deep reinforcement learning methods can be prohibitively expensive due to sample inefficiency. To address this challenge, transferring a pre-trained policy in the source domain to the target domain becomes an attractive solution. Previous research has typically focused on domains with similar state and action spaces but differing in other aspects. In this paper, our primary focus lies in domains with different state and action spaces, which has broader practical implications, i.e. transfer the policy from robot A to robot B. Unlike prior methods that rely on paired data, we propose a novel approach for learning the mapping functions between state and action spaces across domains with unpaired data. We propose effect cycle-consistency, which aligns the effects of transitions across two domains through a symmetrical optimization structure for learning these mapping functions. Once the mapping functions are learned, we can seamlessly transfer the policy from the source domain to the target domain without the need for additional fine-tuning. Our approach has been tested through experiments conducted on three locomotion tasks and two robotic manipulation tasks. The empirical results demonstrate that our method not only achieves better performance of the transferred policies but also reduces alignment errors significantly compared to the baselines."
Parameter-Efficient Prompt Learning for 3D Point Cloud Understanding,"Hongyu Sun, Yongcai Wang, Wang Chen, Haoran Deng, Deying Li","Renmin University of China,RENMIN UNIVERSITY of CHINA",Transfer Learning,"This paper presents a parameter-efficient prompt tuning method, named PPT, to adapt a large multi-modal model for 3D point cloud understanding. Existing strategies are quite expensive in computation and storage, and depend on time-consuming prompt engineering. We address the problems from three aspects. Firstly, a PromptLearner module is devised to replace hand-crafted prompts with learnable contexts to automate the prompt tuning process. Then, we lock the pre-trained backbone instead of adopting the full fine-tuning paradigm to substantially improve the parameter efficiency. Finally, a lightweight PointAdapter module is arranged near target tasks to enhance prompt tuning for 3D point cloud understanding. Comprehensive experiments are conducted to demonstrate the superior parameter and data efficiency of the proposed method. Meanwhile, we obtain new records on 4 public datasets and multiple 3D tasks, i.e., point cloud recognition, few-shot learning, and part segmentation. The implementation is available at https://github.com/auniquesun/PPT."
BEVUDA: Multi-Geometric Space Alignments for Domain Adaptive BEV 3D Object Detection,"Jiaming Liu, Rongyu Zhang, Xiaoqi Li, Xiaowei Chi, Zehui Chen, Ming Lu, Yandong Guo, Shanghang Zhang","Peking University,Nanjing University,Hong Kong University of Science and Technology,University of Science and Technology of China,Intel Labs,OPPO Research Institute",Transfer Learning,"Vision-centric bird-eye-view (BEV) perception has shown promising potential in autonomous driving. Recent works mainly focus on improving efficiency or accuracy but neglect the challenges when facing environment changing, resulting in severe degradation of transfer performance. For BEV perception, we figure out the significant domain gaps existing in typical real-world cross-domain scenarios and make the first attempt to solve the Domain Adaption (DA) problem for multi-view 3D object detection. Since BEV perception approaches are complicated and contain several components, the domain shift accumulation on multiple geometric spaces (i.e., 2D, 3D Voxel, BEV) makes BEV DA even challenging. In this paper, we propose a Multi-space Alignment Teacher-Student (MATS) framework to ease the domain shift accumulation, which consists of a Depth-Aware Teacher (DAT) and a Geometric-space Aligned Student (GAS) model. DAT tactfully combines target lidar and reliable depth prediction to construct depth-aware information, extracting target domain-specific knowledge in Voxel and BEV feature spaces. It then transfers the sufficient domain knowledge of multiple spaces to the student model. In order to jointly alleviate the domain shift, GAS projects multi-geometric space features to a shared geometric embedding space and decreases data distribution distance between two domains. To verify the effectiveness of our method, we conduct BEV 3D object detection experiments on three cross-domain scenarios and achieve state-of-the-art performance. The code will be released at https://github.com/liujiaming1996/BEV_UDA."
6-DOF Grasp Pose Evaluation and Optimization Via Transfer Learning from NeRFs,"Gergely Sóti, Xi Huang, Christian Wurll, Björn Hein","Karlsruhe University of Applied Sciences,Karlsruhe Institute of Technology",Transfer Learning,"We address the problem of robotic grasping of known and unknown objects using implicit behavior cloning. We train a grasp evaluation model from a small number of demonstrations that outputs higher values for grasp candidates that are more likely to succeed in grasping. This evaluation model serves as an objective function, that we maximize to identify successful grasps. Key to our approach is the utilization of learned implicit representations of visual and geometric features derived from a pre-trained NeRF. Though trained exclusively in a simulated environment with simplified objects and 4-DoF top-down grasps, our evaluation model and optimization procedure demonstrate generalization to 6-DoF grasps and novel objects both in simulation and in real-world settings, without the need for additional data. Supplementary material is available at: https://gergely-soti.github.io/grasp"
Multi-Level Progressive Reinforcement Learning for Control Policy in Physical Simulations,"Kefei Wu, Xuming He, Yang Wang, Xiaopei Liu","ShanghaiTech University,Shanghaitech University,SHANGHAITECH UNIVERSITY",Transfer Learning,"Training model-free intelligent agents in complex real-world scenarios using reinforcement learning (RL) often necessitates simulation-based environments due to high physical expenses. However, when simulation takes a long time, e.g., in an unsteady 3D fluid simulation with interactions to the controllable solids, existing RL algorithms meet difficulty to accomplish training within a reasonable timeframes. In this paper, we propose a novel multi-level framework for RL to accelerate convergence as the first attempt to address this difficulty. Motivated by the idea of multi-grid solver, the control policy on a virtual agent over time can be decomposed into different frequency bands, which can be progressively learned via a set of simulations in a coarse-to-fine manner. It is expected that most RL trials are performed in coarse simulations to learn lower frequency bands with efficient convergence, while higher frequency levels require much less RL trials, thus significantly accelerating the learning process. To implement our idea, we designed a novel multi-level residual network with a filter module attached, where each level of the network is learned by performing RL for a given simulation resolution. The proposed framework is evaluated by conducting policy learning experiments on virtual aerial (2D) and underwater (3D) robots, both requiring time-consuming physical simulations. Our results demonstrate a 50% decrease in learning time compared to a direct RL approach, while achieving similar control performance."
Kalman Filter-Based One-Shot Sim-To-Real Transfer Learning,"Dongqingwei Dongqingwei, Peng Zeng, Guangxi Wan, Yunpeng He, Xiaoting Dong","Shenyang Institute of Automation, Chinese Academy of Sciences,Shenyang Institute of Automation Chinese Academy of Sciences,Shenyang institute of automation, Chinese academy of Sciences,Shenyang Institute of Automation, Chinese Academic of Science",Transfer Learning,"Deep reinforcement learning algorithms offer a promising method for industrial robots to tackle unstructured and complex scenarios that are difficult to model. However, due to constraints related to equipment lifespan and safety requirements, acquiring a number of samples directly from the physical environment is often infeasible. With the development of increasingly realistic simulators, it has become feasible for industrial robots to acquire complex motion skills within simulated environments. Nonetheless, the â€reality gapâ€ frequently results in performance degradation when transferring policies trained in simulators to physical systems. In this paper, we treat the reality gap between a physical environment (target domain) and a simulated environment (source domain) as a Gaussian perturbation and utilize Kalman filtering to reduce the discrepancy between source and target domain data. We refine the source domain controller using target domain data to enhance the controllerâ€™s adaptability to the target domain. The efficacy of the proposed method is demonstrated in reaching tasks and peg-in-hole tasks conducted on PR2 and UR5 robotic platforms."
Shaping Social Robot to Play Games with Human Demonstrations and Evaluative Feedback,"Chuanxiong Zheng, Lei Zhang, Hui Wang, Randy Gomez, Eric Nichols, Guangliang Li","ocean university of china,Ocean University of China,Honda Research Institute Japan Co., Ltd.,Honda Research Institute Japan",Human-Robot Collaboration II,"In this paper, building on recent advances in the fields of gaming AI and social robotics, we present a new approach to facilitate the social robot Haru to imitate game strategies from human players' demonstrated trajectories and evaluative feedback in a real-time two-player game. Our research shows that Haru is able to learn and imitate human different game strategies from human players in a human time scale. In addition, our results show that human evaluative feedback plays an important role in allowing Haru to obtain a better performance via our method than human player's demonstrations. Finally, results of our user study indicate that Haru imitating human playerâ€™s game strategies via our method is perceived to be more human-like and have better game performance and experience than self-learning from pre-defined reward functions via traditional deep reinforcement learning method."
Running Guidance for Visually Impaired People Using Sensory Augmentation Technology Based Robotic System,"Zhenyu Liao, Jose Victorio Salazar Luces, Ankit Ravankar, Yasuhisa Hirata",Tohoku University,Human-Robot Collaboration II,"Participating in sports is of great significance to peopleâ€™s physical and mental well-being. While physical activity is commonplace for healthy individuals, it presents challenges for those with visual impairments, as they are difficult to rely on visual cues to perceive essential information related to sports participation, such as their surroundings. Many related studies including our previous work for assisting users in doing sports using sensory augmentation technology, which couples haptic feedback with peopleâ€™s desired movements, are proposed for this challenge. On the basis of these studies, we propose a system for guiding visually impaired users running outdoors using a drone-based robotic system to locate a user and a track, calculate desired moving directions, and provide haptic feedback to the user. We conduct an experiment to explore how accurately people can recognize the directions conveyed by the proposed guidance method. Subjects were asked to select their felt directions on a tablet while running on a treadmill at 6.5 km/h and 7.5 km/h. The results show subjects could recognize the cued directions with an average resolution of 19.8â—¦ and 19.6â—¦ at different speeds, respectively, and there is no significant difference exist between the two speeds. In addition, we guide users in realistic running scenarios on sports tracks. Subjects in this experiment wore an eye mask to simulate the visually impaired. They were instructed to run by following the perc"
Rider Cooperative Control of Rear-Wheel-Swing Motorcycle Based on Divergent Component of Motion,"Tadashi Sumioka, Kazushi Akimoto, Takuya Tsujimura, Sho Takayanagi, Katsuhiko Fukushima, Tsubasa Nose","Honda R&D Co., Ltd.,Honda R & D co., Ltd.,Honda Motor Co., Ltd.",Human-Robot Collaboration II,"We previously proposed a motorcycle with a balance assist function that can generate a self-balancing moment by changing the front wheel fork slant angle. Although this method reduces the risk of falling over, it makes it difficult for the rider to go in the desired direction due to interference by the front wheel assist. In order to solve this problem, this paper proposes a rear-wheel-swing assist mechanism that minimizes the influence on the front wheel steering operation. A method for realizing cooperative control with the rider using the divergent component of motion is also proposed. The results of an extremely low-speed U-turn test are used to show that the proposed methods provide stability while maintaining drivability."
MORPHeus: A Multimodal One-Armed Robot-Assisted Peeling System with Human Users In-The-Loop,"Ruolin Ye, Yifei Hu, Yuhan (anjelica) Bian, Luke Kulm, Tapomayukh Bhattacharjee",Cornell University,Human-Robot Collaboration II,"Meal preparation is an important instrumental activity of daily living (IADL). While existing research has explored robotic assistance in meal preparation tasks like cutting and cooking, the crucial task of peeling has received less attention. Peeling, conventionally a bimanual task, is challenging for care recipients using one robot arm mounted on their wheelchair due to ergonomic and transferring challenges. This paper introduces a real-world robot-assisted peeling system utilizing a single robotic arm and an assistive cutting board, inspired by how individuals with one functional hand do meal preparation. Our system incorporates a multimodal active perception module, a human-in-the-loop long-horizon planning through a natural language interface, and a compliant controller for adaptive motion. Our robot-assisted peeling system uses visual, haptic, and vibration sensing modalities to peel a diverse range of food items with varying physical properties and can successfully adapt to different environments featuring multiple specialized cutting boards. Videos and supplementary materials are available at https://emprise.cs.cornell.edu/morpheus/."
MIntNet: Rapid Motion Intention Forecasting of Coupled Human-Robot Systems with Simulation-To-Real Autoregressive Neural Networks,"John Atkins, Hyunglae Lee",Arizona State University,Human-Robot Collaboration II,"This paper describes the use of a simulation-to-real training pipeline using autoregressive neural networks (MIntNet) for coupled-human robot motion intention prediction. Using only general prior knowledge about the interaction task, a large simulation dataset was generated and used to train a multi-output variation of the classic autoregressive model. The network used an encoding-decoding method to construct condensed representations of the coupled system kinematics over a sequence of time windows and generated their condensed latent representations to predict multiple sequences of the future system states. This method was then tested on 10 real human subjectsâ€™ data for the interaction task and the simulation-to-real generalization performance was evaluated for the proposed network along with alternative implementations of standard multilayered perceptron, convolutional, and long-short term memory based networks. Results show the proposed network has better generalization performance compared to the alternatives, capable of closely predicting positions during fast motion along non-constant curvatures subject to low-frequency disturbances. The MIntNet was able to accurately predict future positions in a 200 ms window with errors of 3.1 Â± 4.8 mm averaged over the prediction window with inference times of 0.26 Â± 0.44 ms. Performance was higher for short range predictions with errors over the time window growing as 2.3 Â± 3.4 mm at 50 ms, 2.4 Â± 4.4 mm at 100 ms, and 5.5 Â± 6.7 mm."
Language to Map: Topological Map Generation from Natural Language Path Instructions,"Hideki Deguchi, Kazuki Shibata, Shun Taguchi","Toyota Central R&D labs., Inc,Toyota Central R&D Labs., INC.,Toyota Central R&D Labs., Inc.",Human-Robot Collaboration II,"In this paper, a method for generating a map from path information described using natural language (textual path) is proposed. In recent years, robotics research mainly focus on vision-and-language navigation (VLN), a navigation task based on images and textual paths. Although VLN is expected to facilitate user instructions to robots, its current implementation requires users to explain the details of the path for each navigation session, which results in high explanation costs for users. To solve this problem, we proposed a method that creates a map as a topological map from a textual path and automatically creates a new path using this map. We believe that large language models (LLMs) can be used to understand textual path. Therefore, we propose and evaluate two methods, one for storing implicit maps in LLMs, and the other for generating explicit maps using LLMs. The implicit map is in the LLMâ€™s memory. It is created using prompts. In the explicit map, a topological map composed of nodes and edges is constructed and the actions at each node are stored. This makes it possible to estimate the path and actions at waypoints on an undescribed path, if enough information is available. Experimental results on path instructions generated in a real environment demonstrate that generating explicit maps achieves significantly higher accuracy than storing implicit maps in the LLMs."
Human-Centered Autonomy for UAS Target Search,"Hunter Ray, Zakariya Laouar, Zachary Sunberg, Nisar Ahmed","University of Colorado Boulder,University of Colorado",Human-Robot Collaboration II,"Current methods of deploying robots that operate in dynamic, uncertain environments, such as Uncrewed Aerial Systems in search & rescue missions, require nearly continuous human supervision for vehicle guidance and operation. These methods do not consider high-level mission context resulting in cumbersome manual operation or inefficient exhaustive search patterns. We present a human-centered autonomous framework that infers geospatial mission context through dynamic feature sets, which then guides a probabilistic target search planner. Operators provide a set of diverse inputs, including priority definition, spatial semantic information about ad-hoc geographical areas, and reference waypoints, which are probabilistically fused with geographical database information and condensed into a geospatial distribution representing an operator's preferences over an area. An online, POMDP-based planner, optimized for target searching, is augmented with this reward map to generate an operator-constrained policy. Our results, simulated based on input from five professional rescuers, display effective task mental model alignment, 18% more victim finds, and 15 times more efficient guidance plans then current operational methods."
Gaze-Based Human-Robot Interaction System for Infrastructure Inspections,"Sunwoong Choi, Zaid Al-Sabbag, Sriram Narasimhan, Chul Min Yeum","University of California, Los Angeles,University of Waterloo",Human-Robot Collaboration II,"Routine inspections for critical infrastructures such as bridges are required in most jurisdictions worldwide. Such routine inspections are largely visual in nature, which are qualitative, subjective, and not repeatable. Although robotic infrastructure inspections address such limitations, they cannot replace the superior ability of experts to make decisions in complex situations, thus making human-robot interaction systems a promising technology. This study presents a novel gaze-based human-robot interaction system, designed to augment the visual inspection performance through mixed reality. Through holograms from a mixed reality device, gaze can be utilized effectively to estimate the properties of the defect in real-time. Additionally, inspectors can monitor the inspection progress online, which enhances the speed of the entire inspection process. Limited controlled experiments demonstrate its effectiveness across various users and defect types. To our knowledge, this is the first demonstration of the real-time application of eye gaze in civil infrastructure inspections."
Facile Integration of Robots into Experimental Orchestration at Scientific User Facilities,"Warnakulasuriya Chandima Fernando, Stuart Campbell, Daniel Olds, Phillip Maffettone","Brookhaven National Lab,Brookhaven National Laboratory",Human-Robot Collaboration II,"Integration of robots into scientific user facilities, such as the National Synchrotron Light Source II, improves their efficiency and capacity. Many such facilities use the open-source Bluesky project for experimental control and orchestration. However, there remains an open challenge in deploying robotic solutions at these facilities that are reconfigurable, extensible, and compatible with pre-existing software infrastructure. Herein, we introduce a framework that uses the Robotic Operating System 2 (ROS2) and Bluesky to provide extensible robotic applications, while working under the operational constraints of a large-scale user facility. We demonstrated this framework by integrating a robotic arm to pick and place a sample holder at a beamline, recording a 90% repeatability rate. This provides the groundwork for further new robotics applications at large-scale scientific user facilities that depend on Bluesky."
SEQUEL: Semi-Supervised Preference-Based RL with Query Synthesis Via Latent Interpolation,"Daniel Marta, Simon Holk, Christian Pek, Iolanda Leite","KTH Royal Institute of Technology,Delft University of Technology",Human Factors and Human-In-The-Loop II,"Preference-based reinforcement learning (RL) poses as a recent research direction in robot learning, by allowing humans to teach robots through preferences on pairs of desired behaviours. Nonetheless, to obtain realistic robot policies, an arbitrarily large number of queries is required to be answered by humans. In this work, we approach the sample-efficiency challenge by presenting a technique which synthesizes queries, in a semi-supervised learning perspective. To achieve this, we leverage latent variational autoencoder (VAE) representations of trajectory segments (sequences of state-action pairs). Our approach manages to produce queries which are closely aligned with those labeled by humans, while avoiding excessive uncertainty according to the human preference predictions as determined by reward estimations. Additionally, by introducing variation without deviating from the original human's intents, more robust reward function representations are achieved. We compare our approach to recent state-of-the-art preference-based RL semi-supervised learning techniques. Our experimental findings reveal that we can enhance the generalization of the estimated reward function without requiring additional human intervention. Lastly, to confirm the practical applicability of our approach, we conduct experiments involving actual human users in a simulated social navigation setting. Videos of the experiments can be found at https://sites.google.com/view/rl-sequel"
Learning When to Ask for Help: Efficient Interactive Navigation Via Implicit Uncertainty Estimation,"Ifueko Igbinedion, Sertac Karaman",Massachusetts Institute of Technology,Human Factors and Human-In-The-Loop II,"Robots operating alongside humans often encounter unfamiliar environments that make autonomous task completion challenging. Though improving models and increasing dataset size can enhance a robot's performance in unseen environments, data collection and model refinement may be impractical in every environment. Approaches that utilize human demonstrations through manual operation can aid in refinement and generalization, but often require significant data collection efforts to generate enough demonstration data to achieve satisfactory task performance. Interactive approaches allow for humans to provide correction to robot action in real time, but intervention policies are often based on explicit factors related to state and task understanding that may be difficult to generalize. Addressing these challenges, we train a lightweight interaction policy that allows robots to decide when to proceed autonomously or request expert assistance at estimated times of uncertainty. An implicit estimate of uncertainty is learned via evaluating the feature extraction capabilities of the robot's visual navigation policy. By incorporating part-time human interaction, robots recover quickly from their mistakes, significantly improving the odds of task completion. Incorporating part-time interaction yields an increase in success of 0.38 with only a 0.3 expert interaction rate within the Habitat simulation environment using a simulated human expert. We further show success transferring this approach to a new domain with a real human expert, improving success from less than 0.1 with an autonomous agent to 0.92 with a 0.23 human interaction rate. This approach provides a practical means for robots to interact and learn from humans in real-world settings."
JaywalkerVR: A VR System for Collecting Safety-Critical Pedestrian-Vehicle Interactions,"Kenta Mukoya, Erica Weng, Rohan Choudhury, Kris Kitani",Carnegie Mellon University,Human Factors and Human-In-The-Loop II,"Developing autonomous vehicles that can safely interact with pedestrians requires large amounts of pedestrian and vehicle data in order to learn accurate pedestrian-vehicle interaction models. However, gathering data that include crucial but rare scenarios - such as pedestrians jaywalking into heavy traffic - can be costly and unsafe to collect. We propose a virtual reality human-in-the-loop simulator, JaywalkerVR, to obtain vehicle-pedestrian interaction data to address these challenges. Our system enables efficient, affordable, and safe collection of long-tail pedestrian-vehicle interaction data. Using our proposed simulator, we create a high-quality dataset with vehicle-pedestrian interaction data from safety critical scenarios called CARLA-VR. The CARLA-VR dataset addresses the lack of long-tail data samples in commonly used real world autonomous driving datasets. We demonstrate that models trained with CARLA-VR improve displacement error and collision rate by 10.7% and 4.9%, respectively, and are more robust in rare vehicle-pedestrian scenarios."
Human Preference-Aware Rebalancing and Charging for Shared Electric Micromobility Vehicles,"Heng Tan, Yukun Yuan, Hua Yan, Shuxin Zhong, Yu Yang","Lehigh University,University of Tennessee at Chattanooga,LEHIGH UNIVERSITY,Rutgers University",Human Factors and Human-In-The-Loop II,"Shared electric micromobility has surged to a popular model of urban transportation due to its efficiency in short-distance trips and environmentally friendly characteristics compared to traditional automobiles. However, managing thousands of shared electric micromobility vehicles, including rebalancing and charging to meet users' travel demands still has been a challenge. Existing methods generally ignore human preferences in vehicle selection and assume all nearby vehicles have an equal chance of being selected, which is unrealistic based on our findings. To address this problem, we design PERCEIVE, a human preference-aware rebalancing and charging framework for shared electric micromobility vehicles. Specifically, we model human preferences in vehicle selection based on vehicle usage history and current status (e.g., energy level) and incorporate the vehicle selection model into a robust adversarial reinforcement learning framework. We further utilize conformal prediction to quantify human preference uncertainty and fuse it with the reinforcement learning framework. We evaluate our framework using two months of real-world electric micromobility operation data in a city. Experimental results show that our method achieves a performance gain of at least 4.02% in the net revenue and offers more robust performance in worst-case scenarios compared to state-of-the-art baselines."
A Trajectory-Based Flight Assistive System for Novice Pilots in Drone Racing Scenario,"Yuhang Zhong, Guangyu Zhao, Qianhao Wang, Guangtong Xu, Chao Xu, Fei Gao","Zhejiang Unviersity,Zhejiang University",Human Factors and Human-In-The-Loop II,"Drone racing has become a popular international competition and has attained wide attention in recent years. However, the requirements of high-level operation keep the novice pilots away from participating in it. This paper presents a trajectory-based flight assistive system that enables various operators to fly the drone in a racing scene at a high speed. The whole system is structured hierarchically, consisting of both offline and online components. In the offline part, a global time-optimal trajectory is generated as the expert reference, and a dense flight corridor is constructed to provide sufficiently large safe region. In the online part, a remote control-mapped primitive is designed to fast encapsulate pilots' inputs, and the time mapping based trajectory progress is customized to further capture intention. Then, a trajectory planner is proposed to efficiently generate intention-aligned, smooth, feasible, and safe trajectories periodically. Additionally, a yaw planning that provides the pilot with the best suitable view angle is employed to further alleviate the operation difficulty. Simulations and real world experiments are implemented to verify the performance of our system. The maximum velocity can reach 6.0 m/s for a novice drone pilot in a real racing scene. We will open source our code later."
A Power-Aware Control Strategy for an Elbow Effort-Compensation Device,"Emir Mobedi, Sebastian Hjorth, Wansoo Kim, Elena De Momi, Nikos Tsagarakis, Arash Ajoudani","Istituto Italiano di Tecnologia,Istituto italiano di Technologia,Hanyang University ERICA,Politecnico di Milano",Human Factors and Human-In-The-Loop II,"This work presents a reactive control strategy for loading and sudden unloading of an elbow effort-compensation device controlled in force. Through this control strategy, in addition to an individual's forearm weight, an external load can be detected and adaptively compensated via a feed-forward force reference, facilitating the execution of arbitrary movements by the wearer. In case of a sudden contact/load loss, a power-aware strategy is implemented to immediately eliminate the portion of external loading in the force reference. The adaptive compensation of the external loads is achieved through an electromyography interface. Instead, to react to sudden load releases, we set a power limit on the tendon, and continuously measure it through an encoder and a load cell connected with the cable. Two sets of experiments are designed to test the proposed load-releasing method on a bench-top setup with 2 kg, and 3.9 kg, and a human subject with 0.5 and 1 kg. Next, the overall scenario including load-compensation and load-releasing are carried out on eight human subjects with 0.5 and 1 kg loads to evaluate the release and compensation time, and the effort reduction with respect to non-powered exoskeleton case. Results show that the average compensation/release time (payload) among subjects is measured as 0.98/0.91 seconds (0.5 kg), and 1/0.86 seconds (1 kg). The average effort reduction among the subjects are also reported as 66.4%, and 67.11% for 0.5 kg, and 1 kg, respectively."
A Planar Compliant Contact Control Applied to Multi-Dimensional Elastic Gripper for Unexpected Contact,"Junnan Huang, Xuefeng Wang, Chongkun Xia, Houde Liu, Mingqi Shao, Bin Liang","Tsinghua University,Peking University,Shenzhen Graduate School, Tsinghua University,Tsinghua Shenzhen International Graduate School",Human Factors and Human-In-The-Loop II,"It is difficult to guarantee an empty living environment to prevent unexpected contact between the object being manipulated by the robot and unplanned obstacles. In this paper, we propose a planar compliant contact control method for planar manipulation to cope with unexpected contact. We first use sheet gel as a multi-dimensional passive elastic element and combine it with a two-finger gripper to design an elastic gripper. Subsequently, we explore the lumped parameter model for the force-displacement relationship of gel deformation and combine the model with the highly impedance motion of robots to design an elastic interaction controller. The controller not only actively adjusts the deformation of the gel to provide the desired contact force and torque depending on contact, but also performs avoidance by following the surface of obstacles. Finally, we design and deploy several planar compliant contact experiments to validate the proposed method and demonstrate the unexpected contact response in humanrobot co-packing. The results show that our method enables the robot to remain compliance in the face of unexpected contact caused by unplanned obstacles, which provides a guarantee for safe manipulation. Physics experiments can be viewed in the attached video."
Enabling Passivity for Cartesian Workspace Restrictions,"Sebastian Hjorth, Johannes Lachner, Arash Ajoudani, Dimitrios Chrysostomou","Istituto italiano di Technologia,Massachusetts Institute of Technology,Istituto Italiano di Tecnologia,Aalborg University",Human Factors and Human-In-The-Loop II,"An emerging trend in the field of human-robot collaboration is the disassembly of end-of-life products. Safety is a crucial requirement of the disassembly process since wornout or damaged products could break, possibly resulting in dangerous behavior of the robot. To protect the user from such behavior, this work addresses this challenge through the implementation of an energy-aware Cartesian impedance controller, combined with virtual workspace restrictions. Hereby, the passivity of the robotic system is ensured. The paper proposed two approaches to ensure the passivity of the system when subjected to workspace restrictions due to unplanned interactions and contact loss. The first approach employs an augmented energy tank with restricted energy flow. The second approach monitors the overall energy flow, regulating and separating non-passive behavior, caused by workspace restrictions. The approaches are evaluated and compared with each other, by using a KUKA LBR iiwa robot. The results highlight the potential of virtual workspace restrictions in human-robot collaborative disassembly tasks."
X-Tacformer : Spatio-Tempral Attention Model for Tactile Recognition,"Jiarui Hu, Yanmin Zhou, Zhipeng Wang, Xin Li, Yongkang Jiang, Bin He",Tongji University,Haptics and Haptic Interfaces,"Recently, tactile sensing has attracted great interests in robotics, especially for exploring unstructured objects. Sensor arrays play an important role in the exploration, which generates rich spatio-temporal information. In this work, we propose an efficient tactile recognition model, X-Tacformer. This model pays attention to both spatial and temporal features of tactile sequences from sensor arrays, which is verified by four public datasets, Ev-Objects, Ev-Containers, Augment8000 and BioTac-Dos. Comparative studies show that our model has resulted in a significant improvement of the recognition accuracy by 0.0223, 0.1416, 0.2735 and 0.1592 in these datasets.In order to verify its performances on dataset with rich spatio-temporal features, a self-designed dataset, ALU-Textures, was constructed with 10 fabrics from everyday textiles, aiming to extend the data collection action modes of current datasets by simulating human rubbing movements with the thumb and index fingers of an Allegro hand. Our model also demonstrates efficient salient feature learning capabilities on ALU-Textures, which is further augmented by tactile data augmentation methods."
The Joint-Space Reconstruction of Human Fingers by Using a Highly Under-Actuated Exoskeleton,"Yuan Su, Gaofeng Li, Yongsheng Deng, Ioannis Sarakoglou, Nikos Tsagarakis, Jiming Chen","Northeastern University,Zhejiang University,Northeastern University, China,Fondazione Istituto Italiano di Tecnologia,Istituto Italiano di Tecnologia",Haptics and Haptic Interfaces,"Hand motion tracking is essential in many fields, e.g., immersive virtual reality, teleoperation of robotic hand, and hand rehabilitation of stroke patient, as human hand plays a crucial role in our daily life. The highly under-actuated hand exoskeleton, which can track the 6-DoF motions of each fingertip via a highly under-actuated kinematic chain, exhibits many benefits in wearability and portability over other solutions. However, due to the non-anthropomorphic linkage, this hand exoskeleton also encounters difficulties in measuring human-finger's joint angles. While the joint-space is important in many scenarios, such as teleoperating a robotic hand with anthropomorphic kinematics but with different size to human. Here we proposed a new method to reconstruct the human finger joints by using a highly under-actuated hand exoskeleton. Our key contribution is the arc-fitting algorithm, which is able to calibrate the misalignment between the exoskeleton's and the human-finger's base frames and estimate the length of human's phalanxes, by using the fingertip's circular motions. With knowing the aforementioned informations, the joint angles can be reconstructed in high precision based on the inverse kinematics models of human fingers. Furthermore, our proposed method is compared with a baseline method, in which the joint angles obtained by a motion capture system are served as ground-truth. The results demonstrate that our proposed method exhibits excellent performance in reconstructing finger's joint configurations."
Prosthetic Upper-Limb Sensory Enhancement (PULSE): A Dual Haptic Feedback Device in a Prosthetic Socket,"Alessia Silvia Ivani, Federica Barontini, Manuel Giuseppe Catalano, Giorgio Grioli, Matteo Bianchi, Antonio Bicchi","Fondazione Istituto Italiano di Tecnologia,University of Pisa,Istituto Italiano di Tecnologia",Haptics and Haptic Interfaces,"This study presents the Prosthetic Upper-Limb Sensory Enhancement (PULSE), a novel dual feedback device completely integrated into a prosthetic socket. The core of the system includes two compact vibrotactile actuators and two silicone chambers in contact with the user's skin. These components provide high-frequency tactile cues for initial contact and surface information (e.g. texture) as well as pressure stimuli related to grasping force. Ten able-bodied participants and one subject with limb loss validated the system, accomplishing an object discrimination task in two different modalities (with and without the feedback). Standardized questionnaires evaluate usersâ€™ satisfaction and workload, enabling a systematic and robust device assessment. The results show that the PULSE device enhanced performance compared to its absence without causing discomfort for a prosthetic user and able-bodied participants. The findings highlight the potential of dual haptic feedback to enhance sensory perception in prosthetic applications and offer valuable insights for future prosthetic design."
Point-Wise Vibration Pattern Production Via a Sparse Actuator Array for Surface Tactile Feedback,"Xiaosa Li, Runze Zhao, Chengyue Lu, Xiao Xiao, Wenbo Ding","Tsinghua University,Tsinghua university Shenzhen graduate school",Haptics and Haptic Interfaces,"Surface vibration tactile feedback is capable of conveying various semantic information to humans via handheld electronic devices, such as smartphones, touch panels, and game controllers. However, covering the entire contacting surface of the device with a dense arrangement of actuators can affect its normal use. Determining how to produce desired vibration patterns at any contact point with only a few sparse actuators deployed on the surface of the handheld device remains a significant challenge. In this work, we develop a tactile feedback board in the size of a smartphone with only five actuators, and achieve the precise production of vibration patterns that can focus at any desired position on the board. Specifically, we investigate the vibration characteristics of a single passive coil actuator and construct its vibration pattern model for any position on the feedback board surface. Optimal phase and amplitude modulation, determined using the simulated annealing algorithm, is employed with five actuators in a sparse array. The vibration patterns from all actuators are superimposed linearly to synthetically generate different onboard vibration energy distributions for tactile sensing. Experiments demonstrated that point-wise vibration pattern production on our tactile board achieved an average level of about 0.9 in the Structural Similarity Index Measure (SSIM) evaluation, when compared to the ideal single-point-focused target vibration pattern. Four point-wise patterns focused on the top, bottom, left, and right parts of the tactile board were applied, to guide continuous directional movements without visual assistance, which shows significant implications for machine-assisted cognition based on vibration tactile feedback."
Implementation and Assessment of an Augmented Training Curriculum for Surgical Robotics,"Alberto Rota, Ke Fan, Elena De Momi",Politecnico di Milano,Haptics and Haptic Interfaces,"The integration of high-level assistance algorithms in surgical robotics training curricula may be beneficial in establishing a more comprehensive and robust skillset for aspiring surgeons, improving their clinical performance as a consequence. This work presents the development and validation of a haptic- enhanced Virtual Reality simulator for surgical robotics training, featuring 8 surgical tasks that the trainee can interact with thanks to the embedded physics engine. This virtual simulated environment is augmented by the introduction of high-level haptic interfaces for robotic assistance that aim at re-directing the motion of the traineeâ€™s hands and wrists toward targets or away from obstacles, and providing a quantitative performance score after the execution of each training exercise. An experimental study shows that the introduction of enhanced robotic assistance into a surgical robotics training curriculum improves performance during the training process and, crucially, promotes the transfer of the acquired skills to an unassisted surgical scenario, like the clinical one."
Hapstick: A Soft Flexible Joystick for Stiffness Rendering Via Fiber Jamming,"Ayush Giri, Robert Bloom, Tania K. Morimoto",University of California San Diego,Haptics and Haptic Interfaces,"Continuum robots are well-suited for applications in delicate and constrained environments, such as minimally invasive surgery, due to their inherent compliance and ability to conform to highly curved paths. Yet the kinematic dissimilarity between continuum robots and conventional, off-the-shelf input devices, along with the general lack of haptic feedback available with such devices, can lead to non-intuitive control. In this work, we present Hapstick --- a soft, flexible haptic joystick that uses fiber jamming to modulate its stiffness and provide feedback to users during teleoperation tasks. We characterize the performance of Hapstick, showing that the bending stiffness increases linearly with the increase in applied vacuum load. A psychophysical study is also conducted to obtain the just noticeable difference in stiffness that users can perceive using Hapstick. Lastly, we perform a study in which participants use Hapstick to teleoperate a physical tendon-driven continuum robot in a simulated colorectal cancer screening task. Users correctly identify the position and development stages of cancerous tissues in 25 out of 27 trials, illustrating the potential of jamming-based mechanisms as bidirectional interfaces capable of providing effective haptic feedback."
Fingertip Ultrasonic Array for Tactile Rendering,"Jace Rozsa, Sarah Costrell, Melisa Orta Martinez, Gary K. Fedder",Carnegie Mellon University,Haptics and Haptic Interfaces,"A miniature haptic stimulation device utilizes focused ultrasound to deliver a tactile haptic sensation to the finger. The 1-3 piezocomposite device has a 1 cm^2 footprint, which is an order of magnitude smaller than other ultrasonic haptic devices and is a good candidate for wearable tactile rendering systems. The device focuses energy to a 1 mm^3 voxel. The current prototype was validated with a small, preliminary human subject study and requires an average input voltage of 68.8 V to elicit tactile sensation. The sensory drive voltage threshold will decrease with future refinement of mechanical impedance matching and focusing."
Active Exploration for Real-Time Haptic Training,"Jake Ketchum, Ahalya Prabhakar, Todd Murphey","Northwestern University,Yale University",Haptics and Haptic Interfaces,"Tactile perception is important for robotic systems that interact with the world through touch. Touch is an active sense in which tactile measurements depends on the contact properties of an interaction---e.g., velocity, force, acceleration---as well as properties of the sensor and object under test. These dependencies make training tactile perceptual models challenging. Additionally, the effects of limited sensor life and the near-field nature of tactile sensors preclude the practical collection of exhaustive data sets even for fairly simple objects. Active learning provides a mechanism for focusing on only the most informative aspects of an object during data collection. Here we use an active learning approach that uses a data-driven model's entropy as an uncertainty measure and explore relative to that entropy conditioned on the sensor state variables. Using a coverage-based controller ergodic controller, we train perceptual models in near-real time. We demonstrate our approach using a biomimentic sensor, exploring ""tactile scenes"" composed of shapes, textures, and objects. Each learned representation provides a perceptual sensor model for a particular tactile scene. Models trained on actively collected data outperform their randomly collected counterparts in real-time training tests. Additionally, we find that the resulting network entropy maps can be used to identify high salience portions of a tactile scene."
A Multi-Stable Curved Line Shape Display,"Wing-sum Adrienne Law, Sofia Wyetzner, Raymond Zhen, Sean Follmer",Stanford University,Haptics and Haptic Interfaces,"Shape-changing displays enable real-time visual- ization and haptic exploration of 3D surfaces. However, many shape-changing displays are composed of individually actuated rigid bodies, which makes them both mechanically complex and unable to form smooth surfaces. In this work, we build a multi- stable curved line display inspired by physical splines. By using circular splines to initialize a discrete elastic rods simulator, we can model multiple stable shapes that fit specific boundary conditions. We then generate actuation instructions based on the circular spline initialization to drive the physical display. We demonstrate our displayâ€™s ability to create 16 shapes with 8 different boundary conditions. Our display is consistent in shape output, with an average standard deviation in height of 0.75 mm or 0.47% of the displayâ€™s maximum vertical range. We also show that our model is consistent with our display, with a mean RMSE of 6.68 mm or 3.85% of the displayâ€™s maximum vertical range for shapes we could stably simulate. We then demonstrate potential scalability by simulating a multi- segment version of the system and show the displayâ€™s ability to withstand loads during contour following in haptic exploration."
Seeing through the Grass: Semantic Pointcloud Filter for Support Surface Learning,"Anqiao Li, Chenyu Yang, Jonas Frey, Joonho Lee, Cesar D. Cadena Lerma, Marco Hutter",ETH Zurich,Legged Robots and Learning I,"Mobile ground robots require perceiving and understanding their surrounding support surface to move around autonomously and safely. The support surface is commonly estimated based on exteroceptive depth measurements, e.g., from LiDARs. However, the measured depth fails to align with the true support surface in the presence of high grass or other penetrable vegetation. In this work, we present the semantic pointcloud filter (SPF), a convolutional neural network (CNN) that learns to adjust LiDAR measurements to align with the underlying support surface. The SPF is trained in a semi-self-supervised manner and takes as an input a LiDAR pointcloud and RGB image. The network predicts a binary segmentation mask that identifies the specific points requiring adjustment, along with estimating their corresponding depth values. To train the segmentation task, 464 distinct images are manually labeled into rigid and non-rigid terrain. The depth estimation task is trained in a self-supervised manner by utilizing the future footholds of the robot to estimate the support surface based on a Gaussian process. Our method can correctly adjust the support surface prior to interacting with the terrain and is extensively tested on the quadruped robot ANYmal. We show the qualitative benefits of SPF in natural environments for elevation mapping and traversability estimation compared to using raw sensor measurements and existing smoothing methods. Quantitative analysis is performed in various natural e"
Manipulator As a Tail: Promoting Dynamic Stability for Legged Locomotion,"Huang Huang, Antonio Loquercio, Ashish Kumar, Neerja Thakkar, Ken Goldberg, Jitendra Malik","University of California at Berkeley,UC Berkeley",Legged Robots and Learning I,"For locomotion, is an arm on a legged robot a liability or an asset for locomotion? Biological systems evolved additional limbs beyond legs that facilitates postural control. This work shows how a manipulator can be an asset for legged locomotion at high speeds or under external perturbations, where the arm serves beyond manipulation. Since the system has 15 degrees of freedom (twelve for the legged robot and three for the arm), off-the-shelf reinforcement learning (RL) algorithms struggle to learn effective locomotion policies. Inspired by Bernsteinâ€™s neurophysiological theory of animal motor learning, we develop an incremental training procedure that initially freezes some degrees of freedom and gradually releases them, using behaviour cloning (BC) from an early learning procedure to guide optimization in later learning. Simulation experiments show that our policy increases the success rate by up to 61 percentage points over the baselines. Simulation and real robot experiments suggest that our policy learns to use the arm as a â€œtailâ€ to initiate robot turning at high speeds and to stabilize the quadruped under external perturbations. Quantitatively, in simulation experiments, we cut the failure rate up to 43.6% during high-speed turning and up to 31.8% for quadruped under external forces compared to using a locked arm."
Two-Stage Learning of Highly Dynamic Motions with Rigid and Articulated Soft Quadrupeds,"Francesco Vezzi, Jiatao Ding, Antonin Raffin, Jens Kober, Cosimo Della Santina","Technical University of Delft,Delft University of Technology,DLR,TU Delft",Legged Robots and Learning I,"Controlled execution of dynamic motions in quadrupedal robots, especially those with articulated soft bodies, presents a unique set of challenges that traditional methods struggle to address efficiently. In this study, we tackle these issues by relying on a simple yet effective two-stage learning framework to generate dynamic motions for quadrupedal robots. First, a gradient-free evolution strategy is employed to discover simply represented control policies, eliminating the need for a predefined reference motion. Then, we refine these policies using deep reinforcement learning. Our approach enables the acquisition of complex motions like pronking and back-flipping, effectively from scratch. Additionally, our method simplifies the traditionally labour-intensive task of reward shaping, boosting the efficiency of the learning process. Importantly, our framework proves particularly effective for articulated soft quadrupeds, whose inherent compliance and adaptability make them ideal for dynamic tasks but also introduce unique control challenges."
High-Dimensional Controller Tuning through Latent Representations,"Alireza Sarmadi, Farshad Khorrami, Prashanth Krishnamurthy","New York University,New York University Tandon School of Engineering",Legged Robots and Learning I,"In this paper, we propose a method to automatically and efficiently tune high-dimensional vectors of controller parameters. The proposed method first learns a mapping from the high-dimensional controller parameter space to a lower dimensional space using a machine learning-based algorithm. This mapping is then utilized in an actor-critic framework using Bayesian optimization (BO). The proposed approach is applicable to complex systems (such as quadruped robots). In addition, the proposed approach also enables efficient generalization to different control tasks while also reducing the number of evaluations required while tuning the controller parameters. We evaluate our method on a legged locomotion application. We show the efficacy of the algorithm in tuning the high-dimensional controller parameters and also reducing the number of evaluations required for the tuning. Moreover, it is shown that the method is successful in generalizing to new tasks and is also transferable to other robot dynamics."
Expert Composer Policy: Scalable Skill Repertoire for Quadruped Robots,"Guilherme Christmann, Ying-sheng Luo, Wei-chao Chen","Inventec Corporation,Inventec Inc.",Legged Robots and Learning I,"We propose the expert composer policy, a framework to reliably expand the skill repertoire of quadruped agents. The composer policy links pair of experts via transitions to a sampled target state, allowing experts to be composed sequentially. Each expert specializes in a single skill, such as a locomotion gait or a jumping motion. Instead of a hierarchical or mixture-of-experts architecture, we train a single composer policy in an independent process that is not conditioned on the other expert policies. By reusing the same composer policy, our approach enables adding new experts without affecting existing ones, enabling incremental repertoire expansion and preserving original motion quality. We measured the transition success rate of 72 transition pairs and achieved an average success rate of 99.99%, which is over 10% higher than the baseline random approach, and outperforms other state-of-the-art methods. Using domain randomization during training we ensure a successful transfer to the real world, where we achieve an average transition success rate of 97.22% (N=360) in our experiments."
Learning Agile Bipedal Motions on a Quadrupedal Robot,"Yunfei Li, Jinhan Li, Wei Fu, Yi Wu",Tsinghua University,Legged Robots and Learning I,"Can a quadrupedal robot perform bipedal motions like humans? Although developing human-like behaviors is more often studied on costly bipedal robot platforms, we present a solution over a lightweight quadrupedal robot that unlocks the agility of the quadruped in an upright standing pose and is capable of a variety of human-like motions. Our framework is with a hierarchical structure. At the low level is a motion-conditioned control policy that allows the quadrupedal robot to track desired base and front limb movements while balancing on two hind feet. The policy is commanded by a high-level motion generator that gives trajectories of parameterized human-like motions to the robot from multiple modalities of human input. We for the first time demonstrate various bipedal motions on a quadrupedal robot, and showcase interesting human-robot interaction modes including mimicking human videos, following natural language instructions, and physical interaction. The video is available at https://sites.google.com/view/bipedal-motions-quadruped."
LAGOON: Language-Guided Motion Control,"Shusheng Xu, Huaijie Wang, Yutao Ouyang, Jiaxuan Gao, Zhiyu Mei, Chao Yu, Yi Wu","Tsinghua University,Xiamen University",Legged Robots and Learning I,"Abstractâ€” We aim to control a robot to physically behave in the real world following any high-level language command like â€œcartwheelâ€ or â€œkickâ€. Although human motion datasets exist, this task remains particularly challenging since generative models can produce physically unrealistic motions, which will be more severe for robots due to different body structures and physical properties. Deploying such a motion to a physical robot can cause even greater difficulties due to the sim2real gap. We develop LAnguage-Guided mOtion cONtrol (LAGOON), a multi-phase reinforcement learning (RL) method to generate physically realistic robot motions under language commands. LAGOON first leverages a pre-trained model to generate a human motion from a language command. Then an RL phase trains a control policy in simulation to mimic the generated human motion. Finally, with domain randomization, our learned policy can be deployed to a quadrupedal robot, leading to a quadrupedal robot that can take diverse behaviors in the real world under natural language commands."
Learning Quadrupedal Locomotion with Impaired Joints Using Random Joint Masking,"Mincheol Kim, Ukcheol Shin, Jung-Yup Kim","Seoul National University of Sciences and Technology,CMU(Carnegie Mellon University),Seoul National University of Science & Technology",Legged Robots and Learning I,"Quadrupedal robots have played a crucial role in various environments, from structured environments to complex harsh terrains, thanks to their agile locomotion ability. However, these robots can easily lose their locomotion functionality if damaged by external accidents or internal malfunctions. In this paper, we propose a novel deep reinforcement learning frame-work to enable a quadrupedal robot to walk with impaired joints. The proposed framework consists of three components: 1) a random joint masking strategy for simulating impaired joint scenarios, 2) a joint state estimator to predict an implicit status of current joint condition based on past observation history, and 3) progressive curriculum learning to allow a single network to conduct both normal gait and various joint-impaired gaits. We verify that our framework enables the Unitreeâ€™s Go1 robot to walk under various impaired joint conditions in real-world indoor and outdoor environments."
Multi-Task Learning of Active Fault-Tolerant Controller for Leg Failures in Quadruped Robots,"Taixian Hou, Jiaxin Tu, Xiaofei Gao, Zhiyan Dong, Peng Zhai, Lihua Zhang","FuDan University,Beijing Zhitong Robot Technology Co., Ltd.,Fudan University",Legged Robots and Learning I,"Electric quadruped robots used in outdoor exploration are susceptible to leg-related electrical or mechanical failures. Unexpected joint power loss and joint locking can immediately pose a falling threat. Typically, controllers lack the capability to actively sense the condition of their own joints and take proactive actions. Maintaining the original motion patterns could lead to disastrous consequences, as the controller may produce irrational output within a short period of time, further creating the risk of serious physical injuries. This paper presents a hierarchical fault-tolerant control scheme employing a multi-task training architecture capable of actively perceiving and overcoming two types of leg joint faults. The architecture simultaneously trains three joint task policies for health, power loss, and locking scenarios in parallel, introducing a symmetric reflection initialization technique to ensure rapid and stable gait skill transformations. Experiments demonstrate that the control scheme is robust in unexpected scenarios where a single leg experiences concurrent joint faults in two joints. Furthermore, the policy retains the robot's planar mobility, enabling rough velocity tracking. Finally, zero-shot Sim2Real transfer is achieved on the real-world SOLO8 robot, countering both electrical and mechanical failures."
Motion Planning for 4WS Vehicle with Autonomous Selection of Steering Modes Via an MIQP-MPC Controller,"Ngoc Thinh Nguyen, Pranav Tej Gangavarapu, Nicolas Mandel, Ralf Bruder, Floris Ernst","University of Luebeck,University zu Luebeck,University of Lübeck",Optimization and Optimal Control I,"Navigation in agricultural fields imposes various constraints on manoeuvrability, which can be tackled by using four-wheel steering (4WS) vehicles which are capable of switching between multiple steering mechanisms with distinct kinematic properties. For example, parallel positive steering (PPS) with four wheels in parallel to each other can maintain the vehicle's heading when moving along a curve. Symmetric negative steering (SNS) with two wheels on each side sharing the same steering angle can turn with a small radius. This paper presents a controller capable of selecting and switching between the two aforementioned modes autonomously for better trajectory tracking performance with special heading requirements for agricultural applications. The controller is implemented as a Model Predictive Control (MPC) controller formulated as a mixed-integer quadratic programming (MIQP) problem for the 4WS vehicle. Practical constraints, such as limits on wheel velocities, steering angles and their rate-of-changes are taken into account. A Python implemention confirms the real-time execution capability of the controller and simulation results highlight its effectiveness."
On the Performance of Jerk-Constrained Time-Optimal Trajectory Planning for Industrial Manipulators,"Jee-Eun Lee, Andrew Bylard, Zhouwen Sun, Luis Sentis","The University of Texas at Austin,Stanford University,Dexterity Inc",Optimization and Optimal Control I,"Jerk-constrained trajectories offer a wide range of advantages that collectively improve the performance of robotic systems, including increased energy efficiency, durability, and safety. In this paper, we present a novel approach to jerk-constrained time-optimal trajectory planning (TOTP), which follows a specified path while satisfying up to third-order constraints to ensure safety and smooth motion. One significant challenge in jerk-constrained TOTP is a non-convex formulation arising from the inclusion of third-order constraints. Approximating inequality constraints can be particularly challenging because the resulting solutions may violate the real constraints. We address this by leveraging convexity within the proposed formulation to form conservative inequality constraints. We then obtain the desired trajectory by solving an $n$-dimensional Sequential Linear Program (SLP) iteratively until convergence. Lastly, we evaluate the performance of trajectories generated with and without jerk limits in terms of peak power, torque efficiency, and tracking capability."
Symmetric Stair Preconditioning of Linear Systems for Parallel Trajectory Optimization,"Xueyi Bu, Brian Plancher","Fu Foundation School of Engineering and Applied Science, Columbi,Barnard College, Columbia University",Optimization and Optimal Control I,"There has been a growing interest in parallel strategies for solving trajectory optimization problems. One key step in many algorithmic approaches to trajectory optimization is the solution of moderately-large and sparse linear systems. Iterative methods are particularly well-suited for parallel solves of such systems. However, fast and stable convergence of iterative methods is reliant on the application of a high-quality preconditioner that reduces the spread and increase the clustering of the eigenvalues of the target matrix. To improve the performance of these approaches, we present a new parallel-friendly symmetric stair preconditioner. We prove that our preconditioner has advantageous theoretical properties when used in conjunction with iterative methods for trajectory optimization such as a more clustered eigenvalue spectrum. Numerical experiments with typical trajectory optimization problems reveal that as compared to the best alternative parallel preconditioner from the literature, our symmetric stair preconditioner provides up to a 34% reduction in condition number and up to a 25% reduction in the number of resulting linear system solver iterations."
MPCGPU: Real-Time Nonlinear Model Predictive Control through Preconditioned Conjugate Gradient on the GPU,"Emre Adabag, Miloni Atal, William Gerard, Brian Plancher","Columbia University,Barnard College, Columbia University",Optimization and Optimal Control I,"Nonlinear Model Predictive Control (NMPC) is a state-of-the-art approach for locomotion and manipulation which leverages trajectory optimization at each control step. While the performance of this approach is computationally bounded, implementations of direct trajectory optimization that use iterative methods to solve the underlying moderately-large and sparse linear systems, are a natural fit for parallel hardware acceleration. In this work, we introduce MPCGPU, a GPU-accelerated, real-time NMPC solver that leverages an accelerated preconditioned conjugate gradient (PCG) linear system solver at its core. We show that MPCGPU increases the scalability and real-time performance of NMPC, solving larger problems, at faster rates. In particular, for tracking tasks using the Kuka IIWA manipulator, MPCGPU is able to scale to kilohertz control rates with trajectories as long as 512 knot points. This is driven by a custom PCG solver which outperforms state-of-the-art, CPU-based, linear system solvers by at least 10x for a majority of solves and 3.6x on average."
Efficient and Robust Time-Optimal Trajectory Planning and Control for Agile Quadrotor Flight,"Ziyu Zhou, Gang Wang, Sun Jian, Jikai Wang, Jie Chen","Beijing Institute of Technology,Tongji University",Optimization and Optimal Control I,"Agile quadrotor flight relies on rapidly planning and accurately tracking time-optimal trajectories, a technology critical to their application in the wild. However, the computational burden of computing time-optimal trajectories based on the full quadrotor dynamics (typically on the order of minutes or even hours) can hinder its ability to respond quickly to changing scenarios. Additionally, modeling errors and external disturbances can lead to deviations from the desired trajectory during tracking in real time. This paper proposes a novel approach to computing time-optimal trajectories, by fixing the nodes with waypoint constraints and adopting separate sampling intervals for trajectories between waypoints, which significantly accelerates trajectory planning. Furthermore, the planned paths are tracked via a time-adaptive model predictive control scheme whose allocated tracking time can be adaptively adjusted on-the-fly, therefore enhancing the tracking accuracy and robustness. We evaluate our approach through simulations and experimentally validate its performance in dynamic waypoint scenarios for time-optimal trajectory replanning and trajectory tracking."
Invariant Descriptors of Motion and Force Trajectories for Interpreting Object Manipulation Tasks in Contact,"Maxim Vochten, Ali Mousavi, Arno Verduyn, Tinne De Laet, Erwin Aertbelien, Joris De Schutter","KU Leuven,Department of Mechanical Engineering, KU Leuven,University of Leuven",Optimization and Optimal Control I,"Invariant descriptors of point and rigid-body motion trajectories have been proposed in the past as representative task models for motion recognition and generalization. Currently, no invariant descriptor exists for representing force trajectories, which appear in contact tasks. This paper introduces invariant descriptors for force trajectories by exploiting the duality between motion and force. Two types of invariant descriptors are presented depending on whether the trajectories consist of screw or vector coordinates. Methods and software are provided for robustly calculating the invariant descriptors from noisy measurements using optimal control. Using experimental human demonstrations of 3D contour following and peg-on-hole alignment tasks, invariant descriptors are shown to result in task representations that do not depend on the calibration of reference frames or sensor locations. The tuning process for the optimal control problems is shown to be fast and intuitive. Similar to motions in free space, the proposed invariant descriptors for motion and force trajectories may prove useful for the recognition and generalization of constrained motions, such as during object manipulation in contact."
Generalizing Trajectory Retiming to Quadratic Objective Functions,"Gerry Chen, Frank Dellaert, Seth Hutchinson","Georgia Institute of Technology,Verdant Robotics/Georgia Tech",Optimization and Optimal Control I,"Trajectory retiming is the task of computing a feasible time parameterization to traverse a path. It is commonly used in the decoupled approach to trajectory optimization whereby a path is first found, then a retiming algorithm computes a speed profile that satisfies kino-dynamic and other constraints. While trajectory retiming is most often formulated with the minimum-time objective (i.e. traverse the path as fast as possible), it is not always the most desirable objective, particularly when we seek to balance multiple objectives or when bang-bang control is unsuitable. In this paper, we present a novel algorithm based on factor graph variable elimination that can solve for the global optimum of the retiming problem with quadratic objectives as well (e.g. minimize control effort or match a nominal speed by minimizing squared error), which may extend to arbitrary objectives with iteration. Our work extends prior works, which find only solutions on the boundary of the feasible region, while maintaining the same linear time complexity from a single forward-backward pass. We experimentally demonstrate that (1) we achieve better real- world robot performance by using quadratic objectives in place of the minimum-time objective, and (2) our implementation is comparable or faster than state-of-the-art retiming algorithms."
A Distributed Processing Approach for Smooth Task Transitioning in Strict Hierarchical Control,"Francesco Tassi, Arash Ajoudani",Istituto Italiano di Tecnologia,Optimization and Optimal Control I,"To enhance robots' applicability in real-world scenarios, it is essential to establish a complex and multi-tasking behaviour, inspired by human nature. To this purpose, from a hardware perspective, a high number of degrees of freedom is necessary, as is the case for humanoids and collaborative mobile manipulators. From a software standpoint instead, complex hierarchical strategies are often used to define a set of behaviours that the robot should reflect in strict hierarchical order. Their main issue however, is related to the lack of continuity when their stack of tasks is changed. Existing works that address this issue clearly present a trade-off between optimality assurance during transition and computational costs. Here, we employ a distributed processing approach that enables not only the minimization of computational costs, but also continuous optimality and constraints feasibility even under sharp transitions. The approach is tested during three task transitions, for different tasks such as constrained trajectory tracking, obstacle avoidance, and postural optimization. Two mobile manipulators are used, each having 10 DoF, and the results confirm the smoothness of the generated solutions."
Risk-Averse Trajectory Optimization Via Sample Average Approximation,"Thomas Lew, Riccardo Bonalli, Marco Pavone","Stanford University,Laboratoire des Signaux et Systèmes",Optimization and Optimal Control I,"Trajectory optimization under uncertainty underpins a wide range of applications in robotics. However, existing methods are limited in terms of reasoning about sources of epistemic and aleatoric uncertainty, space and time correlations, nonlinear dynamics, and non-convex constraints. In this work, we first introduce a continuous-time planning formulation with an average-value-at-risk constraint over the entire planning horizon. Then, we propose a sample-based approximation that unlocks an efficient and general-purpose algorithm for risk-averse trajectory optimization. We prove that the method is asymptotically optimal and derive finite-sample error bounds. Simulations demonstrate the high speed and reliability of the approach on problems with stochasticity in nonlinear dynamics, obstacle fields, interactions, and terrain parameters."
Towards a Novel Soft Magnetic Laparoscope for Single Incision Laparoscopic Surgery,"Hui Liu, Ning Li, Shuai Li, Gregory Mancini, Jindong Tan","University of Tennessee Knoxville,The University of Tennessee,The University of Tennessee Graduate School of Medicine,University of Tennessee, Knoxville",Medical Robots V,"In single-incision laparoscopic surgery (SILS), magnetic anchoring and guidance system (MAGS) is a promising technique to prevent clutter in surgical workspace and provide a larger vision field. Existing camera designs mainly rely on rigid structure design, resulting in risks of losing magnetic coupling and impacting tissue during the insertion and coupling procedure. In this paper, we proposed a wireless MAGS consisting of soft material and structure design. The camera can bend at the exit of the trocar and maintain strong coupling with the external actuator. The operation principle and modeling were established to investigate the parameter design. An easier insertion procedure was introduced and demonstrated in experiment. The bendability was tested showing the camera could reach 20 degrees in bending angle and 16.4mm in displacement. The insertion and deployment took less than 2 minutes on average."
Magnetic-Guided Flexible Origami Robot Toward Long-Term Phototherapy of H. Pylori in the Stomach,"Sishen Yuan, Baijia Liang, Po Wa Wong, Mingjing Xu, Chi Hsuan Li, Zhen Li, Hongliang Ren","The Chinese University of Hong Kong,Chinese University of Hong Kong,Qilu Hospital of Shandong University,Chinese Univ Hong Kong (CUHK) & National Univ Singapore(NUS)",Medical Robots V,"Helicobacter pylori, a pervasive bacterial infection associated with gastrointestinal disorders such as gastritis, peptic ulcer disease, and gastric cancer, impacts approximately 50% of the global population. The efficacy of standard clinical eradication therapies is diminishing due to the rise of antibiotic-resistant strains, necessitating alternative treatment strategies. Photodynamic therapy (PDT) emerges as a promising prospect in this context. This study presents the development and implementation of a magnetically-guided origami robot, incorporating flexible printed circuit units for sustained and stable phototherapy of Helicobacter pylori. Each integrated unit is equipped with wireless charging capabilities, producing an optimal power output that can concurrently illuminate up to 15 LEDs at their maximum intensity. Crucially, these units can be remotely manipulated via a magnetic field, facilitating both translational and rotational movements. We propose an openloop manual control sequence that allows the formation of a stable, compliant triangular structure through the interaction of internal magnets. This adaptable configuration is uniquely designed to withstand the dynamic squeezing environment prevalent in real-world gastric applications. The research herein represents a significant stride in leveraging technology for innovative medical solutions, particularly in the management of antibiotic-resistant Helicobacter pylori infections."
Flexible Tactile-Sensing Gripper Design and Excessive Force Protection Function for Endovascular Surgery Robots,"Chuqiao Lyu, Shuxiang Guo, Yonggan Yan, Yongxin Zhang, Yongwei Zhang, Pengfei Yang, Jianmin Liu","Beijing Institute of Technology,Kagawa University,Changhai hospital",Medical Robots V,"Research on endovascular surgery robots (ESR) is continuously developing, because ESR can protect surgeons from radiation exposure. For designing an ESR manipulator, the main challenge is controlling the soft surgical tools and measuring the endovascular stress simultaneously. To solve these problems, a flexible tactile-sensing gripper (FTG) is designed in this study. Firstly, a catheter grasping model is constructed, and the factors affecting the force measurement are quantitatively analyzed. Secondly, the simulation experiments based on FTG models with three different sizes are implemented. When the catheter force is too large, shrinking the grasping distance of FTG can avoid the surgical risk. This method protects the surgeon's behavior and controls the catheter force at the same time, which is named excessive force protection function (EFPF). Thirdly, the FTG prototype which meet the surgical requirements is made and integrated into the ESR manipulator. This manipulator can measure the catheter forces by detecting the coordinate of marks on FTG surface. The calibrated FTG gets the average and maximum errors of force sensing approximately 37 mN and 223 mN, respectively. Finally, in the experiment of carotid artery catheterization, EFPF can control the catheter force within 393 mN, which is far less than the control group's 1351 mN."
Simultaneous Estimation of Shape and Force Along Highly Deformable Surgical Manipulators Using Sparse FBG Measurement,"Yiang Lu, Bin Li, Wei Chen, Junyan Yan, Shing Shin Cheng, Jiangliu Wang, Jianshu Zhou, Qi Dou, Yunhui Liu","The Chinese University of Hong Kong,Chinese University of Hong Kong",Medical Robots V,"Recently, fiber optic sensors such as fiber Bragg gratings (FBGs) have been widely investigated for shape reconstruction and force estimation of flexible surgical robots. However, most existing approaches need precise model parameters of FBGs inside the fiber and their alignments with the flexible robots for accurate sensing results. Another challenge lies in online acquiring external forces at arbitrary locations along the flexible robots, which is highly required when with large deflections in robotic surgery. In this paper, we propose a novel data-driven paradigm for simultaneous estimation of shape and force along highly deformable flexible robots by using sparse strain measurement from a single-core FBG fiber. A thin-walled soft sensing tube helically embedded with FBG sensors is designed for a robotic-assisted flexible ureteroscope with large deflection up to 270 degrees and a bend radius under 10 mm. We introduce and study three learning models by incorporating spatial strain encoders, and compare their performances in both free space without interactions as well as constrained environments with contact forces at different locations. The experimental results in terms of dynamic shape-force sensing accuracy demonstrate the effectiveness and superiority of the proposed methods."
Autonomous System for Tumor Resection (ASTR)-Dual-Arm Robotic Midline Partial Glossectomy,"Jiawei Ge, Michael Kam, Justin Opfermann, Hamed Saeidi, Simon Leonard, Leila Mady, Martin Schnermann, Axel Krieger","Johns Hopkins University,University of North Carolina Wilmington,The Johns Hopkins University,National Cancer Institute",Medical Robots V,"Head and neck cancer is the seventh most common cancer worldwide, with squamous cell carcinoma being the most prevalent histologic type. Surgical resection is a primary treatment modality, and precisely identifying tumor edges and ensuring adequate resection margins are critical for optimizing oncologic outcomes. This letter presents an innovative autonomous system for tumor resection (ASTR) and conducts a feasibility study by performing autonomous midline partial glossectomy for pseudotumor with millimeter accuracy. ASTR consists of a dual-camera vision system, an electrosurgical tool, a vacuum grasping tool, two 6-DOF manipulators, and an autonomous control system. The letter introduces an ontology-based research framework for creating and implementing a complex autonomous surgical workflow, using the glossectomy as a case study. Porcine tongues are used in this study, and marked using color inks and near-infrared fluorescent (NIRF) markers to indicate the pseudotumor. ASTR monitors the NIRF markers and gathers spatial and color data from the samples, enabling planning and execution of robot trajectories in accordance with the proposed glossectomy workflow. The system successfully performs six consecutive supervised autonomous pseudotumor resections on porcine samples. The average surface and depth resection errors measure 0.73Â±0.60mm and 1.89Â±0.54mm, respectively. The resection accuracy is demonstrated to be on par with manual glossectomy performed by an otolaryngologist."
A Semi-Autonomous Data Driven Shared Control Framework for Robotic Manipulation and Cutting of an Unknown Deformable Tissue,"Nicholas Strohmeyer, Ji Hwan Park, Braden Murphy, Farshid Alambeigi","University of Texas at Austin,The University of Texas at Austin",Medical Robots V,"In this work, we propose a semi-autonomous scheme to synergistically share the complicated task of manipulation and cutting of an unknown deformable tissue (U-DT) between a remote surgeon and a surgical robot. Particularly, utilizing the da Vinci Research Kit (dVRK) platform, we have designed and successfully demonstrated a fully functional shared control scheme for an autonomous tensioning and tele-cutting of a U-DT. We have shown the system's ability to cooperate with a remote surgeon by leveraging an online data-driven learning and adaptive control method coupled with a reduced-order trajectory planning module that depends on just two parameters. By performing 25 experiments on custom-designed silicon phantoms and defining a set of success/failure metrics, we have put forward findings that establish a causal relationship between these two important parameters and the success or failure of the performed experiments."
Design and Evaluation of a Modular Robotic System for Microsurgery,"Jenireth Torrealba Molina, Toqa Abubaker, Yanpei Huang, Xiaoxiao Cheng, Alexis Devillard, Etienne Burdet","Imperial College London,Imperial College of Science, Technology and Medicine, London UK,imperial college london",Medical Robots V,"The manipulation of instruments under a microscope suffers from physiological tremor and human errors, which are inevitable in long microsurgery interventions. Robotic systems developed in recent years for microsurgery are expensive and not flexible, as they cannot use standard instruments, and need the surgeon to modify their operative skills and strategies. In this paper, we introduce a modular robotic system for microsurgery enabling the surgeon to operate using conventional instruments. Our system was implemented using a commercial Kinova robot and a dedicated modular end-effector that uses standard microsurgery instruments. An initial teleoperation validation was carried out by eleven participants, who could successfully control the microsurgery tools to perform basic surgical movements. Furthermore, participants performed a simple anastomosis task with the robot and compared it to manual control. The results showed that robotic control is superior to manual control in simple surgical tasks and the converse in complex tasks. Participants preferred the proposed robotic system due to its user-friendliness and effort reduction."
Analyzing Accessibility in Robot-Assisted Vitreoretinal Surgery: Integrating Eye Posture and Robot Position,"Satoshi Inagaki, Alireza Alikhani, Nassir Navab, Mathias Maier, M. Ali Nasseri","NSK.Ltd,Augen- klinik und Poliklinik, Klinikum rechts der Isar der Techn,TU Munich,Klinikum rechts der Isar der TU München,Technische Universitaet Muenchen",Medical Robots V,"Several robotic frameworks have been recently developed to assist ophthalmic surgeons in performing complex vitreoretinal procedures such as subretinal injection. However, in order to intuitively integrate robots into the surgical workflow, it is crucial to emphasize that an accessibility analysis framework for vitreoretinal surgery must be considered as an essential component. Such a framework, ideally, considers the comprehensive factors of the eye anatomy and its positioning, the insertion point, and the initial pose and position of the robot. By combining the mobilization of the eyeball and adjusting the pose and position of the robot, the accessibility of such systems is significantly optimized. At the same time, the accessible-visible area is better and faster matched to the working volume of the robot. This paper presents an analysis of an expansion strategy for the robot's accessibility and visibility area. The outcomes of this method demonstrate the promising potential to enhance the robot's accessibility, as evidenced in our analytical and experimental findings from 22.4% to 99.0% of the required working area on an adjustable phantom model."
The Flux One Magnetic Navigation System: A Preliminary Assessment for Stent Implantation,"Christoff Heunis, Bruno Silva, Giulia Sereni, Martijn Cw Lam, Besma Belakhal, Arold Gaborit, Bryan Wermelink, Bob Rh Geelkerken, Sarthak Misra",University of Twente,Medical Robots V,"Minimally-invasive surgery for stent implantation is a complex procedure requiring specialized instruments. It often leads to prolonged patient recovery and interrupted operations due to incorrect instrument selection and complex anatomy. This paper presents a novel magnetic navigation system that addresses these challenges by using magnetic fields and computed tomography imaging to precisely navigate minimally-invasive surgical guidewires. The system is designed based on consultations with surgeons, and analysis of technical and clinical requirements of stent implantation procedures. Results from a human-in-the-loop case study with eleven operators indicate a 47% reduction in guidewire navigation times, while successfully reaching all predetermined luminal targets. The clinical relevance, usability, and operator satisfaction are also measured using a user experience questionnaire, and interviews and showed positive results. Compared to conventional, manual guidewire navigation, the magnetic navigation system has the potential to significantly impact the efficiency of the clinical workflow and improve complex endovascular aortic repair procedures."
Fault Tolerant Neural Control Barrier Functions for Robotic Systems under Sensor Faults and Attacks,"Hongchao Zhang, Luyao Niu, Andrew Clark, Radha Poovendran","Washington University in St. Louis,University of Washington",Robot Safety I,"Safety is a fundamental requirement of many robotic systems. Control barrier function (CBF)-based approaches have been proposed to guarantee the safety of robotic systems. However, the effectiveness of these approaches highly relies on the choice of CBFs. Inspired by the universal approximation power of neural networks, there is a growing trend toward representing CBFs using neural networks, leading to the notion of neural CBFs (NCBFs). Current NCBFs, however, are trained and deployed in benign environments, making them ineffective for scenarios where robotic systems experience sensor faults and attacks. In this paper, we study safety-critical control synthesis for robotic systems under sensor faults and attacks. Our main contribution is the development and synthesis of a new class of CBFs that we term fault tolerant neural control barrier function (FT-NCBF). We derive the necessary and sufficient conditions for FT-NCBFs to guarantee safety, and develop a data-driven method to learn FT-NCBFs by minimizing a loss function constructed using the derived conditions. Using the learned FT-NCBF, we synthesize a control input and formally prove the safety guarantee provided by our approach. We demonstrate our proposed approach using two case studies: obstacle avoidance problem for an autonomous mobile robot and spacecraft rendezvous problem."
Belief Control Barrier Functions for Risk-Aware Control,"Matti Vahs, Christian Pek, Jana Tumova","KTH Royal Institute of Technology, Stockholm,Delft University of Technology,KTH Royal Institute of Technology",Robot Safety I,"Ensuring safety in real-world robotic systems is often challenging due to unmodeled disturbances and noisy sensor measurements. To account for such stochastic uncertainties, many robotic systems leverage probabilistic state estimators such as Kalman filters to obtain a robot's belief, i.e. a probability distribution over possible states. We propose belief control barrier functions (BCBFs) to enable risk-aware control synthesis, leveraging all information provided by state estimators. This allows robots to stay in predefined safety regions with desired confidence under these stochastic uncertainties. BCBFs are general and can be applied to a variety of robotic systems that use extended Kalman filters as state estimator. We demonstrate BCBFs on a quadrotor that is exposed to external disturbances and varying sensing conditions. Our results show improved safety compared to traditional state-based approaches while allowing control frequencies of up to 1kHz."
A Novel Algorithmic Approach to Obtaining Maneuverable Control-Invariant Sets,"Prashant Solanki, Jasper Van Beers, A. Jamshidnejad, Coen De Visser","Delft University of Technology,TU Delft",Robot Safety I,"Much effort has been devoted in the field of reachability analysis to obtaining control-invariant sets which ensure that a system inside of these sets does not have to leave these sets, and are thus essential for guaranteeing a system's safety. However, control invariance does not imply that a system can move from any state in the control-invariant set to any other state within the given time horizon. In this paper we develop an algorithm to obtain a control-invariant set that allows a given system to move from any state in the set to any other state in the set within a given time horizon without having to leave the set. We call this set 'maneuver set' or set $mathcal{M}$. We substantiate the algorithm's efficacy through mathematical proof, affirming that the maneuver set obtained through the application of the algorithm is indeed control-invariant. Furthermore, we prove that the system is indeed able to move from any state within this set to any other state. To illustrate the use of our algorithm, we provide the numerical example of a Dubins car, utilising HJB reachability analysis along with the algorithm to obtain the maneuver set."
Safe Navigation and Obstacle Avoidance Using Differentiable Optimization Based Control Barrier Functions,"Bolun Dai, Rooholla Khorrambakht, Prashanth Krishnamurthy, Vinicius Mariano Gonçalves, Antonios Tzes, Farshad Khorrami","New York University,New York University Tandon School of Engineering,New York University Abu Dhabi, United Arab Emirates,New York University Abu Dhabi",Robot Safety I,"Control barrier functions (CBFs) have been widely applied to safety-critical robotic applications. However, the construction of control barrier functions for robotic systems remains a challenging task. Recently, collision detection using differentiable optimization has provided a way to compute the minimum uniform scaling factor that results in an intersection between two convex shapes and to also compute the Jacobian of the scaling factor. In this paper, we propose a framework that uses this scaling factor, with an offset, to systematically define a CBF for obstacle avoidance tasks. We provide theoretical analyses of the continuity and continuous differentiability of the proposed CBF. We empirically evaluate the proposed CBF's behavior and show that the resulting optimal control problem is computationally efficient, which makes it applicable for real-time robotic control. We validate our approach, first using a 2D mobile robot example, then on the Franka-Emika Research~3 (FR3) robot manipulator both in simulation and experiment."
Achieving Autonomous Cloth Manipulation with Optimal Control Via Differentiable Physics-Aware Regularization and Safety Constraints,"Yutong Zhang, Fei Liu, Xiao Liang, Michael Yip","University of California San Diego,UCSD,University of California, San Diego",Robot Safety I,"Cloth manipulation is a category of deformable object manipulation of great interest to the robotics community, from applications of automated laundry-folding and home organizing to textiles and flexible manufacturing. Despite the desire for automated cloth manipulation, the thin-shell dynamics and under-actuation nature of cloth present significant challenges for robots to effectively interact with them. Many recent works omit explicit modeling in favor of learning-based methods that may yield control policies directly. However, these methods require large training sets that must be collected and curated. In this regard, we create a framework for differentiable modeling of cloth dynamics leveraging an Extended Position-based Dynamics (XPBD) algorithm. Together with the desired control objective, physics-aware regularization terms are designed for better results, including trajectory smoothness and elastic potential energy. In addition, safety constraints, such as avoiding obstacles, can be specified using signed distance functions (SDFs). We formulate the cloth manipulation task with safety constraints as a constrained optimization problem, which can be effectively solved by mainstream gradient-based optimizers thanks to the end-to-end differentiability of our framework. Finally, we assess the framework with various safety thresholds and demonstrate the feasibility of result trajectories on a surgical robot. The effects of the regularization terms are analyzed in an additional ablation study."
Online Data-Driven Safety Certification for Systems Subject to Unknown Disturbances,"Nick Rober, Karan Mahesh, Tyler Paine, Max L. Greene, Steven Lee, Sildomar Monteiro, Michael Benjamin, Jonathan Patrick How","Massachusetts Institute of Technology,Aurora Flight Sciences,University of Florida,Carnegie Mellon University,Boeing Research and Technology",Robot Safety I,"Deploying autonomous systems in safety critical settings necessitates methods to verify their safety properties. This is challenging because real-world systems may be subject to disturbances that affect their performance, but are unknown a priori. This work develops a safety-verification strategy wherein data is collected online and incorporated into a reachability analysis approach to check in real-time that the system avoids dangerous regions of the state space. Specifically, we employ an optimization-based moving horizon estimator (MHE) to characterize the disturbance affecting the system, which is incorporated into an online reachability calculation. Reachable sets are calculated using a computational graph analysis tool to predict the possible future states of the system and verify that they satisfy safety constraints. We include theoretical arguments proving our approach generates reachable sets that bound the future states of the system, as well as numerical results demonstrating how it can be used for safety verification. Finally, we present results from hardware experiments demonstrating our approach's ability to perform online reachability calculations for an unmanned surface vehicle subject to currents and actuator failures."
"Towards Standardized Disturbance Rejection Testing of Legged Robot Locomotion with Linear Impactor: A Preliminary Study, Observations, and Implications","Bowen Weng, Guillermo Castillo, Yun-seok Kang, Ayonga Hereid","Iowa State University,The Ohio State University,Ohio State University",Robot Safety I,"Dynamic locomotion in legged robots is close to industrial collaboration, but a lack of standardized testing obstructs commercialization. The issues are not merely political, theoretical, or algorithmic but also physical, indicating limited studies and comprehension regarding standard testing infrastructure and equipment. For decades, the approaches we have been testing legged robots were rarely standardizable with hand-pushing, foot-kicking, rope-dragging, stick-poking, and ball-swinging. This paper aims to bridge the gap by proposing the use of the linear impactor, a well-established tool in other standardized testing disciplines, to serve as an adaptive, repeatable, and fair disturbance rejection testing equipment for legged robots. A pneumatic linear impactor is also adopted for the case study involving the humanoid robot Digit. Three locomotion controllers are examined, including a commercial one, using a walking-in-place task against frontal impacts. The statistically best controller was able to withstand the impact momentum (26.376 kgâ‹…m/s) on par with a reported average effective momentum from straight punches by Olympic boxers (26.506 kgâ‹…m/s). Moreover, the case study highlights other anti-intuitive observations, demonstrations, and implications that, to the best of the authors' knowledge, are first-of-its-kind revealed in real-world testing of legged robots."
Detecting and Mitigating System-Level Anomalies of Vision-Based Controllers,"Aryaman Gupta, Kaustav Chakraborty, Somil Bansal","Indian Institute of Technology (BHU), Varanasi,University of Southern California",Robot Safety I,"Autonomous systems, such as self-driving cars and drones, have made significant strides in recent years by leveraging visual inputs and machine learning for decision-making and control. Despite their impressive performance, these vision-based controllers can make erroneous predictions when faced with novel or out-of-distribution inputs. Such errors can cascade to catastrophic system failures and compromise system safety. In this work, we introduce a run-time anomaly monitor to detect and mitigate such closed-loop, system-level failures. Specifically, we leverage a reachability-based framework to stress-test the vision-based controller offline and mine its system-level failures. This data is then used to train a classifier that is leveraged online to flag inputs that might cause system breakdowns. The anomaly detector highlights issues that transcend individual modules and pertain to the safety of the overall system. We also design a fallback controller that robustly handles these detected anomalies to preserve system safety. We validate the proposed approach on an autonomous aircraft taxiing system that uses a vision-based controller for taxiing. Our results show the efficacy of the proposed approach in identifying and handling system-level anomalies, outperforming methods such as prediction error-based detection and ensembling, thereby enhancing the overall safety and robustness of autonomous systems."
Generative Modeling of Residuals for Real-Time Risk-Sensitive Safety with Discrete-Time Control Barrier Functions,"Ryan Cosner, Igor Sadalski, Jana Woo, Preston Culbertson, Aaron Ames","California Institute of Technology,Stanford University",Robot Safety I,"A key source of brittleness for robotic systems is the presence of model uncertainty and external disturbances. Existing approaches to robust control either seek to bound the worst-case disturbance (which results in conservative behavior), or to learn a deterministic dynamics model (which is unable to capture uncertain dynamics or disturbances). This work proposes a different approach: training a state-conditioned generative model to represent the distribution of errors between the nominal dynamics and the actual system. This learned disturbance model can be used in conjunction with probabilistic safety methods such as Discrete-Time Control Barrier Functions (DTCBFs) to ensure the safety of the system up to some level of risk. For this work, we focus on learning the dynamics uncertainties of a quadrotor drone, which is subject to complex aerodynamic interactions between the aircraft and the environment or which is carrying an unmodeled payload. We use a conditional variational autoencoder (CVAE) to learn a state-conditioned disturbance distribution, and find the resulting probabilistic safety controller exhibits less conservative behavior while retaining theoretical safety properties."
An Open-Source Solution for Fast and Accurate Underwater Mapping with a Low-Cost Mechanical Scanning Sonar,"Tim Hansen, Andreas Birk",Constructor University,Marine Robotics V,"An open-source software framework is presented that allows real-time underwater mapping with popular marine robotics components, namely a BlueRobotics BlueROV2 with its standard Ping360 Mechanical Scanning Sonar (MSS) and a A50 Doppler Velocity Log (DVL), which are low-cost devices for their respective types - if not even the most affordable ones on the market. The software runs with low computational power on a Raspberry Pi4. The framework builds upon Synthetic Scan Formation (SSF) where single MSS beams or scan-lines are embedded into a pose-graph. The rendering of scans is not only based on navigation, but based on the graph itself. Scans formed from scan-lines can be optimized by online Simultaneous Localization and Mapping (SLAM) and result in improved scans, based on the current state of the graph. In subsequent steps this leads to improved registration results. To this end, a combination of two different types of loop-closures is presented. Namely a consecutive loop closure, and a proximity based loop closure, which reduces the overall drift. The framework is validated in three different test-environments, namely a pool, a test-tank with a gantry for ground truth motion, and the flooded basement of a WW-II submarine bunker. Among others, it is shown that there is an increased accuracy compared to conventional SLAM and that the software is usable in real-time during a mission with the low-cost hardware."
Boundary Factors for Seamless State Estimation between Autonomous Underwater Docking Phases,"Aldo Terán Espinoza, antonio teran, John Folkesson, Peter Sigray, Jakob Kuttenkeuler","KTH Royal Institute of Technology,Massachusetts Institute of Technology,KTH",Marine Robotics V,"Autonomous underwater docking is of the utmost importance for expanding the capabilities of Autonomous Underwater Vehicles (AUVs). Due to a historical focus on underwater docking to only static targets, the research gap in underwater docking to dynamically active targets has been left relatively untouched. We address the state estimation problem that arises when trying to rendezvous a chaser AUV with a dynamic target by modeling the scenario as a factor graph optimization-based Simultaneous Localization and Mapping problem. We present a set of boundary factors that aid the inference process by seamlessly transitioning the target's state between the different observability stages, intrinsic to any dynamic docking scenario. We benchmark the performance of our approach using the Stonefish simulated environment."
Vision-Based Water Clearance Determination in Maritime Environment,"Carl Schiller, Deran Maas, Bruno Arsenali, Jukka Peltola, Kalevi Tervo, Stefano Maranò","ABB Corporate Research, Baden-Dättwil, Switzerland,ABB,ABB Marine and Ports,Aalto University School of Science and Technology,ABB Corporate Research",Marine Robotics V,"Determining the distances from the hull of the own ship to obstacles or land, i.e. water clearance, is a fundamental task in navigation. This is particularly relevant during maneuvering in the harbor or navigating in confined waters. We introduce the concepts of area water clearance and line water clearance. Area water clearance is important especially for path planning and obstacle avoidance. Line water clearance is critical for maneuvering when approaching the quay. In this work, we present a vision-based approach to determine the water clearance. A single calibrated camera together with a semantic segmentation network is used to detect the water region in an image, and back-projection to determine the water clearance on the sea surface in world units. We validate the proposed approach on real data collected from two distinct vessels, where the proposed method is able to produce reliable water clearance for distances beyond one kilometer. During harbor maneuvering 90% of the relative water clearance errors were found to be between âˆ’2.3% and 3%."
Adaptive Landmark Color for AUV Docking in Visually Dynamic Environments,"Corey Knutson, Zhipeng Cao, Junaed Sattar","University of Minnesota - Twin Cities,University of Minnesoata -- Twin Cities,University of Minnesota",Marine Robotics V,"Autonomous Underwater Vehicles (AUVs) conduct missions underwater without the need for human intervention. A docking station (DS) can extend mission times of an AUV by providing a location for the AUV to recharge its batteries and receive updated mission information. Various methods for locating and tracking a DS exist, but most rely on expensive acoustic sensors, or are vision-based, which is significantly affected by water quality. In this paper, we present a vision-based method that utilizes adaptive color LED markers and dynamic color filtering to maximize landmark visibility in varying water conditions. Both AUV and DS utilize cameras to determine the water background color in order to calculate the desired marker color. No communication between AUV and DS is needed to determine marker color. Experiments conducted in a pool and lake show our method performs 10 times better than static color thresholding methods as background color varies. DS detection is possible at a range of 5 meters in clear water with minimal false positives."
Navigable Area Detection and Perception-Guided Model Predictive Control for Autonomous Navigation in Narrow Waterways,"Jonghwi Kim, Changyu Lee, Dongha Chung, Jinwhan Kim",KAIST,Marine Robotics V,"This paper presents an integrated navigation and control strategy for an autonomous surface vehicle (ASV) to operate in narrow waterways without relying on GPS. The proposed method uses a camera and a light detection and ranging (LiDAR) sensor to detect navigable regions in the waterway. A deep learning-based semantic segmentation algorithm is applied to detect the navigable region in camera images, and the segmented region is projected onto the water surface using planar homography. A line-detection algorithm is also introduced to improve the reliability of detecting navigable regions from LiDAR measurements. A safe collision-free path for the ASV is generated within the navigable regions using model predictive control-based local path planning and control algorithms. The performance and practical utility of the proposed method were demonstrated through field experiments using a small cruise boat, modified as an autonomous surface vehicle."
An Online Self-Calibrating Refractive Camera Model with Application to Underwater Odometry,"Mohit Singh, Mihir Rahul Dharmadhikari, Kostas Alexis","NTNU: Norwegian University of Science and Technology,NTNU - Norwegian University of Science and Technology",Marine Robotics V,"This work presents a camera model for refractive media such as water and its application in underwater visual-inertial odometry. The model is self-calibrating in real-time and is free of known correspondences or calibration targets. It is separable as a distortion model (dependent on refractive index n and radial pixel coordinate) and a virtual pinhole model (as a function of n). We derive the self-calibration formulation leveraging epipolar constraints to estimate the refractive index and subsequently correct for distortion. Through experimental studies using an underwater robot integrating cameras and inertial sensing, the model is validated regarding the accurate estimation of the refractive index and its benefits for robust odometry estimation in an extended envelope of conditions. Lastly, we show the transition between media and the estimation of the varying refractive index online, thus allowing computer vision tasks across refractive media."
Enhancing Visual Inertial SLAM with Magnetic Measurements,"Bharat Joshi, Ioannis Rekleitis",University of South Carolina,Marine Robotics V,"This paper presents an extension to visual inertial odometry (VIO) by introducing tightly-coupled fusion of magnetometer measurements. A sliding window of keyframes is optimized by minimizing re-projection errors, relative inertial errors, and relative magnetometer orientation errors. The results of IMU orientation propagation are used to efficiently transform magnetometer measurements between frames producing relative orientation constraints between consecutive frames. The soft and hard iron effects are calibrated using an ellipsoid fitting algorithm. The introduction of magnetometer data results in significant reductions in the orientation error and also in recovery of the true yaw orientation with respect to the magnetic north. The proposed framework operates in all environments with slow-varying magnetic fields, mainly outdoors and underwater. We have focused our work on the underwater domain, especially in underwater caves, as the narrow passage and turbulent flow make it difficult to perform loop closures and reset the localization drift. The underwater caves present challenges to VIO due to the absence of ambient light and the confined nature of the environment, while also being a crucial source of fresh water and providing valuable historical records. Experimental results from underwater caves demonstrate the improvements in accuracy and robustness introduced by the proposed VIO extension."
Acoustic-VINS: Tightly Coupled Acoustic-Visual-Inertial Navigation System for Autonomous Underwater Vehicles,"Jiangbo Song, Wanqing Li, Xiangwei Zhu",Sun Yat-sen University,Marine Robotics V,"In this work, we present an acoustic-visual-inertial navigation system (Acoustic-VINS) for underwater robot localization. Specifically, we address the problem of the global position of the underwater visual-inertial navigation system being inappreciable by tightly coupling the long baseline (LBL) system into an optimization-based visual-inertial SLAM. In our proposed Acoustic-VINS, the reprojection error, IMU preintegration error, and raw LBL measurement error are jointly minimized within a sliding window factor graph framework. Furthermore, we propose an acoustic-aided initialization method to exhibit an accurate initial state for successful state estimation. Additionally, for wider application, we extend the sensor data of the real-world AQUALOC dataset to obtain the LBL-AQUALOC dataset. Experimental results on the ten sequences of the LBL-AQUALOC dataset in challenging underwater scenes show that our proposed approach outperforms state-of-the-art visual-inertial SLAM."
Underwater Volumetric Mapping Using Imaging Sonar and Free-Space Modeling Approach,"António José Oliveira, Bruno Ferreira, Nuno Cruz","INESC TEC,University of Porto",Marine Robotics V,"Lack of information and perceptual ambiguity are key problems in sonar-based mapping applications. We propose a technique for mapping of underwater environments, building on the finite, positive, sonar beamwidth. Our approach models the free-space covered by each emitted acoustic pulse, employing volumetric techniques to create grid-based submaps of the unoccupied water volumes through images collected from imaging sonars. A representation of the occupied space is obtained by exploration of the free-space frontier. Special attention is given to acoustic image preparation and segmentation. Experimental results are provided based on real data collected from a dam shaft scenario."
Light-Weight Approach for Safe Landing in Populated Areas,"Tilemahos Mitroudas, Vasiliki Balaska, Athanasios Psomoulis, Antonios Gasteratos",Democritus University of Thrace,Motion Control and Planning,"Landing safety is a challenge heavily engaging the research community recently, due to the increasing interest in applications availed by aerial vehicles. In this paper, we propose a landing safety pipeline based on state of the art object detectors and OctoMap. First, a point cloud of surface obstacles is generated, which is then inserted in an OctoMap. The unoccupied areas are identified, thus resulting to a sum of safe landing points. Due to the low processing time achieved by state of the art object detectors and the efficient point cloud manipulation using OctoMap, it is feasible for our approach to deploy on low-weight embedded systems. The proposed pipeline has been evaluated in many simulation scenarios, varying in people density, number, and movement.Simulations were executed with an Nvidia Jetson Nano in the loop to confirm the pipeline's performance and robustness in a low computing power hardware. The experiments yielded promising results with a 87% success rate."
Time-Optimal Gate-Traversing Planner for Autonomous Drone Racing,"Chao Qin, Maxime Simon Joseph Michet, Jingxiang Chen, Hugh Hong-Tao Liu",University of Toronto,Motion Control and Planning,"In drone racing, the time-minimum trajectory is affected by the drone's capabilities, the layout of the race track, and the configurations of the gates (e.g., their shapes and sizes). However, previous studies neglect the configuration of the gates, simply rendering drone racing a waypoint-passing task. This formulation often leads to a conservative choice of paths through the gates, as the spatial potential of the gates is not fully utilized. To address this issue, we present a time-optimal planner that can faithfully model gate constraints with various configurations and thereby generate the most time-efficient trajectory while considering the single-rotor-thrust limits. Our approach excels in computational efficiency which only takes a few seconds to compute the full state and control trajectories of the drone through tracks with dozens of different gates. Extensive simulations and experiments confirm the effectiveness of the proposed methodology, showing that the lap time can be further reduced by taking into account the gate's configuration. We validate our planner in real-world flights and demonstrate super-extreme flight trajectory through race tracks."
Design and Evaluation of Motion Planners for Quadrotors in Environments with Varying Complexities,"Yifei Shao, Yuwei Wu, Laura Jarin-Lipschitz, Pratik Chaudhari, Vijay Kumar",University of Pennsylvania,Motion Control and Planning,"Motion planning techniques for quadrotors have advanced significantly over the past decade. Most successful planners have two stages: a front-end that determines a path that incorporates geometric (or kinematic or input) constraints and specifies the homotopy class of the trajectory, and a back-end that optimizes this path to respect dynamics and input constraints. While there are many different choices for each stage, the eventual performance depends critically not only on these choices, but also on the environment. Given a new environment, it is difficult to decide a priori how one should design a motion planner. In this work, we develop (i) a procedure to construct parametrized environments, (ii) metrics that characterize the difficulty of motion planning in these environments, and (iii) an open-source software stack that can be used to combine a wide variety of two-stage planners seamlessly. We perform experiments in simulations and a real platform. We find, somewhat conveniently, that geometric front-ends are sufficient for environments with varying complexities if combined with dynamics-aware backends. The metrics we designed faithfully capture the planning difficulty in a given environment. All code is available at https://github.com/KumarRobotics/kr_mp_design."
AutoTrans: A Complete Planning and Control Framework for Autonomous UAV Payload Transportation,"Haojia Li, Haokun Wang, Chen Feng, Fei Gao, Boyu Zhou, Shaojie Shen","The Hong Kong University of Science and Technology,Hong Kong University of Science and Technology,Zhejiang University,Sun Yat-sen University",Motion Control and Planning,"The robotics community is increasingly interested in autonomous aerial transportation. Unmanned aerial vehicles with suspended payloads have advantages over other systems, including mechanical simplicity and agility, but pose great challenges in planning and control. To realize fully autonomous aerial transportation, this paper presents a systematic solution to address these difficulties. First, we present a real-time planning method that generates smooth trajectories considering the time-varying shape and non-linear dynamics of the system, ensuring whole-body safety and dynamic feasibility. Additionally, an adaptive NMPC with a hierarchical disturbance compensation strategy is designed to overcome unknown external perturbations and inaccurate model parameters. Extensive experiments show that our method is capable of generating high-quality trajectories online, even in highly constrained environments, and tracking aggressive flight trajectories accurately, even under significant uncertainty. We plan to release our code to benefit the community."
Bat Planner: Aggressive Flying Ball Player,"Huan Yu, Jie Tu, Pengqin Wang, Zhi Zheng, Kewen Zhang, Guodong Lu, Fei Gao, Jin Wang","Zhejiang University,The Hong Kong University of Science and Technology,Zhejiang University of Technology",Motion Control and Planning,"In this paper, an aggressive quadrotor Ball plAying sysTem called BAT is proposed, whose goal is to intercept a flying ball and volley it towards a designated target. Aggressive means BAT operates the quadrotor aggressively to intercept balls that are far away and hit them to distant positions in ways that are beyond the reach of existing methods. The trajectory prediction of the ball is achieved by integrating forward the current position and velocity estimates using an extended kalman filter, and implementing cubic interpolation at the time resolution to calculate the continuous gradient for optimization. Facing the challenge of finding feasible hitting actions under extreme circumstances, we propose a two-stage planning approach, including transition point design and hitting primitive generation, with a simplified expression of uncoupled hitting actions. To obtain the best hitting motion, a trajectory optimization method is proposed, which can jointly optimize the hitting terminal states and time cost, considering dynamic feasibility and anticollision constraints. To avoid pathological hitting, a defensive rule constraint and its constraint transcription method are proposed. A large number of simulation and real-world experiments are conducted, which prove the flying ball player can hit arriving balls from different directions and distances to arbitrary targets."
An NMPC Framework for Tracking and Releasing a Cable-Suspended Load to a Ground Target Using a Multirotor UAV,"Fotis Panetsos, George Karras, Kostas Kyriakopoulos","National Technical University of Athens,University of Thessaly,New York University - Abu Dhabi",Motion Control and Planning,"In this work, we present a nonlinear Model Predictive Control (NMPC) scheme for tracking a ground target using a multirotor with a cable-suspended load. The NMPC framework relies on the dynamic model of the UAV with the suspended load and, hence, an estimate of the load state is obtained by fusing the measurements of a downward-facing camera and a load cell with an Unscented Kalman Filter (UKF). Additionally, since the NMPC relies on the future behavior of the system, the trajectory of the ground target throughout the predicted time horizon of the NMPC, is required. Towards this direction, BÃ©zier curves are employed in order to predict the future trajectory of the target, which moves in an arbitrary way. The ultimate goal of the proposed framework is to release the suspended load to the ground target and, consequently, a condition is checked at each time instant that triggers the opening of a gripper, located at the lower edge of the cable. The performance of the proposed control scheme is experimentally validated using an octorotor."
Multi-Vehicle Dynamic Water Surface Monitoring,"Frantisek Nekovar, Jan Faigl, Martin Saska",Czech Technical University in Prague,Motion Control and Planning,"Repeated exploration of a water surface to detect objects of interest and their subsequent monitoring is important in search-and-rescue or ocean clean-up operations. Since the location of any detected object is dynamic, we propose to address the combined surface exploration and monitoring of the detected objects by modeling spatio-temporal reward states and coordinating a team of vehicles to collect the rewards. The model characterizes the dynamics of the water surface and enables the planner to predict future system states. The state reward value relevant to the particular water surface cell increases over time and is nullified by being in a sensor range of a vehicle. Thus, the proposed multi-vehicle planning approach is to minimize the collective value of the dynamic model reward states. The purpose is to address vehicles' motion constraints by using model predictive control on receding horizon and fully exploiting the utilized vehicles' motion capabilities. Based on the evaluation results, the approach indicates improvement in a solution to the kinematic orienteering problem and the team orienteering problem in the monitoring task compared to the existing solutions. The proposed approach has been experimentally verified, supporting its feasibility in real-world monitoring tasks."
Aerial Physical Human Robot Interaction for Payload Transportation,"Pratik Prajapati, Vineet Vashista",Indian Institute of Technology Gandhinagar,Motion Control and Planning,"Recent human-robot interaction paradigms on aerial robots unfold many potential applications, and efforts are further being made to explore this field. Physical interaction with aerial robots can provide an intuitive way of delivering high-level commands and allowing humans to perform collaborative tasks. The presented work demonstrates the feasibility of deploying the aerial robot to physically work with the human operator to transport the payload collaboratively in outdoor settings. A system comprised of a rigid object lifted by a human and a quadcopter from its end is considered. Custom build sensor systems, namely Human Handle Device and Cable Attitude Device, have been designed to estimate human commands and state feedback reliably. A control strategy for the quadcopter is designed to interact naturallyÂwith the operator for safer and smooth collaborative payload transportation. Successful outdoor experiments with five novice subjects are presented that demonstrate the feasibility and potential application of the proposed modality."
On Experimental Emulation of Printability and Fleet Aware Generic Mesh Decomposition for Enabling Aerial 3D Printing,"Marios-nektarios Stamatopoulos, Avijit Banerjee, George Nikolakopoulos",Luleå University of Technology,Motion Control and Planning,"This article introduces an experimental emulation of a novel chunk-based flexible multi-DoF aerial 3D printing framework. The experimental demonstration of the overall autonomy focuses on precise motion planning and task allocation for a UAV, traversing through a series of planned space-filling paths involved in the aerial 3D printing process without physically depositing the overlaying material. The flexible multi-DoF aerial 3D printing is a newly developed framework and has the potential to strategically distribute the envisioned 3D model to be printed into small, manageable chunks suitable for distributed 3D printing. Moreover, by harnessing the dexterous flexibility due to the 6 DoF motion of UAV, the framework enables the provision of integrating the overall autonomy stack, potentially opening up an entirely new frontier in additive manufacturing. However, it's essential to note that the feasibility of this pioneering concept is still in its very early stage of development, which yet needs to be experimentally verified. Towards this direction, experimental emulation serves as the crucial stepping stone, providing a pseudo mockup scenario by virtual material deposition, helping to identify technological gaps from simulation to reality. Experimental emulation results, supported by critical analysis and discussion, lay the foundation for addressing the technological and research challenges to significantly push the boundaries of the state-of-the-art 3D printing mechanism."
Path and Trajectory Planning of a Tethered UAV-UGV Marsupial Robotic System,"Simon Martinez-rozas, David Alejo, Fernando Caballero, Luis Merino","Universidad de Antofagasta,University Pablo de Olavide,Universidad de Sevilla,Universidad Pablo de Olavide",Aerial Systems,"This letter addresses the problem of trajectory planning in a marsupial robotic system consisting of an unmanned aerial vehicle (UAV) linked to an unmanned ground vehicle (UGV) through a non-taut tether with controllable length. revF{To the best of our knowledge, this is the first method that addresses the trajectory planning of a marsupial UGV-UAV with a non-taut tether.} The objective is to determine a synchronized collision-free trajectory for the three marsupial system agents: UAV, UGV, and tether. First, we present a path planning solution based on optimal Rapidly-exploring Random Trees (RRT*) with novel sampling and steering techniques to speed-up the computation. This algorithm is able to obtain collision-free paths for the UAV and the UGV, taking into account the 3D environment and the tether. Then, the letter presents a trajectory planner based on non-linear least squares. The optimizer takes into account aspects not considered in the path planning, like temporal constraints of the motion imposed by limits on the velocities and accelerations of the robots, or raising the tether's clearance. Simulated and field test results demonstrate that the approach generates obstacle-free, smooth, and feasible trajectories for the marsupial system."
Cooperative Exploration of Heterogeneous UAVs in Mountainous Environments by Constructing Steady Communication,"Han Jiang, Yanchun Chang, Liying Yang, Xu Liu, Yuqing He","the State Key Laboratory of Robotics, Shenyang Institute of Auto,Shenyang Institute of Automation,Shenyang Institute of Automation, Chinese Academy of Sciences",Aerial Systems,"Unmanned aerial vehicles (UAVs) must fly at low altitudes to execute certain missions when operating in complex mountainous areas. However, in these environments, UAVs lose their line-of-sight (LOS) communication with the ground station (GS) due to the obstruction of the mountains and are unable to retransmit information such as video, which will lead to mission failure or affect the flight safety of UAVs. To address this difficulty, this study proposes a cooperative planning method for heterogeneous UAVs by ensuring steady communication based on the shortest total mission time. To accomplish this goal, a relay UAV is positioned to enable indirect but constant LOS connectivity between the mission UAV and the GS. Specifically, to alleviate data storage pressure, a terrain lightweight modeling method is employed. In addition, a new LOS judgment model that constructs communication relay LOS links between communication nodes for complex mountain environments is presented. This study considers different types of UAVs; for the fixed-wing UAV, the minimum turning radius constraints are taken into account to plan a flyable trajectory. The problem is formulated as the multi-step optimization model that accounts for communication constraints, obstacle and collision avoidance, and the performance constraints of heterogeneous UAVs to plan the trajectories of the mission UAV and relay UAV in order to maintain LOS links between the mission UAV and the ground station"
Perception-And-Energy-Aware Motion Planning for UAV Using Learning-Based Model under Heteroscedastic Uncertainty,"Reiya Takemura, Genya Ishigami",Keio University,Aerial Systems,"Global navigation satellite systems (GNSS) denied environments/conditions require unmanned aerial vehicles (UAVs) to energy-efficiently and reliably fly. To this end, this study presents perception-and-energy-aware motion planning for UAVs in GNSS-denied environments. The proposed planner solves the trajectory planning problem by optimizing a cost function consisting of two indices: the total energy consumption of a UAV and the perception quality of light detection and ranging (LiDAR) sensor mounted on the UAV. Before online navigation, a high-fidelity simulator acquires a flight dataset to learn energy consumption for the UAV and heteroscedastic uncertainty associated with LiDAR measurements, both as functions of the horizontal velocity of the UAV. The learned models enable the online planner to estimate energy consumption and perception quality, reducing UAV battery usage and localization errors. Simulation experiments in a photorealistic environment confirm that the proposed planner can address the trade-off between energy efficiency and perception quality under heteroscedastic uncertainty. The open-source code is released at https://gitlab.com/ReI08/perception-energy-planner."
Representing On-Orbit Rendezvous and Proximity Operations with Fully-Actuated Multirotor Aerial Platforms,"Alessandro Garzelli, Kumud Darshan Yadav, Alessandro Scalvini, Antonio Gonzalez-morgado, Alejandro Suarez, Aníbal Ollero","GRVC Robotics Lab, University Seville,GRVC Robotics Lab, University of Seville,GRVC University of Seville,Universidad de Sevilla,University of Seville,AICIA. G,,,,,,,,",Aerial Systems,"Ground testing is of paramount importance to verify and validate space operations and the associated control algorithms before on-orbit deployment. Although state-of-the-art facilities are capable of reproducing zero-G environment with high degree of fidelity, these infrastructures can be complemented with multi-rotors emulating free flying or free floating conditions, exploiting the similarities and analogies between both domains in terms of floating nature, attitude dynamics, and thrust-wrench relation through the mixer matrix. Furthermore, the effective workspace of the testbed can be extended to the dimensions of the flight area and the coverage of the positioning system. Therefore, this papers introduces a new way to recreate orbital motion within an indoor facility, considering the case study of trajectories derived from the Clohessyâ€“Wiltshire equations. This advancement opens up avenues for replicating close-proximity operations between chaser and target satellites employing fully-actuated multi-rotors that allow decoupling translational and attitude dynamics."
On-Device Self-Supervised Learning of Visual Perception Tasks Aboard Hardware-Limited Nano-Quadrotors,"Elia Cereda, Manuele Rusci, Alessandro Giusti, Daniele Palossi","USI and SUPSI,KU Leuven,IDSIA USI-SUPSI,ETH Zurich",Aerial Systems,"Sub-SI{50}{gram} nano-drones are gaining momentum in both academia and industry. Their most compelling applications rely on onboard deep learning models for perception despite severe hardware constraints (ie sub-SI{100}{milliwatt} processor). When deployed in unknown environments not represented in the training data, these models often underperform due to domain shift. To cope with this fundamental problem, we propose, for the first time, on-device learning aboard nano-drones, where the first part of the in-field mission is dedicated to self-supervised fine-tuning of a pre-trained convolutional neural network (CNN). Leveraging a real-world vision-based regression task, we thoroughly explore performance-cost trade-offs of the fine-tuning phase along three axes: textit{i}) dataset size (more data increases the regression performance but requires more memory and longer computation); textit{ii}) methodologies (eg fine-tuning all model parameters vs. only a subset); and textit{iii}) self-supervision strategy. Our approach demonstrates an improvement in mean absolute error up to 30% compared to the pre-trained baseline, requiring only SI{22}{second} fine-tuning on an ultra-low-power GWT GAP9 System-on-Chip. Addressing the domain shift problem via on-device learning aboard nano-drones not only marks a novel result for hardware-limited robots but lays the ground for more general advancements for the entire robotics community."
Aerobatic Trajectory Generation for a VTOL Fixed-Wing Aircraft Using Differential Flatness,"Ezra Tal, Gilhyun Ryou, Sertac Karaman","MIT,Massachusetts Institute of Technology",Aerial Systems,"This article proposes a novel algorithm for aerobatic trajectory generation for a vertical take-off and landing (VTOL) tailsitter flying wing aircraft. The algorithm differs from existing approaches for fixed-wing trajectory generation, as it considers a realistic six-degree-of-freedom (6-DOF) flight dynamics model, including aerodynamic equations. Using a global dynamics model enables the generation of aerobatics trajectories that exploit the entire flight envelope, allowing agile maneuvering through the stall regime, sideways uncoordinated flight, inverted flight, etc. The method uses the differential flatness property of the global tailsitter flying wing dynamics, which is derived in this work. By performing snap minimization in the differentially flat output space, a computationally efficient algorithm, suitable for online motion planning, is obtained. The algorithm is demonstrated in extensive flight experiments encompassing six aerobatic maneuvers, a time-optimal drone racing trajectory, and an airshowlike aerobatic sequence for three tailsitter aircraft."
EVOLVER: Online Learning and Prediction of Disturbances for Robot Control,"Jindou Jia, Wenyu Zhang, Kexin Guo, Jianliang Wang, Xiang Yu, Yang Shi, Lei Guo","Beihang University,Hangzhou Innovation Institute of Beihang University,University of Victoria",Aerial Systems,"In nature, when encountering unexpected uncertainty, animals tend to react quickly to ensure safety as the top priority, and gradually adapt to it based on fresh experience. We present a framework, namely EVOLVER, to mimic the bio-behavior for robotics to achieve rapid transient reaction ability and high-precision steady state performance simultaneously. In particular, the Koopman operator is leveraged to explore the unknown model of uncertainties, which is subsequently utilized in an evolutionary model-based disturbance observer. The resulting observer can guarantee a provable convergence in optimal conditions. Several practical considerations, including construction of a training dataset, data noise handling, and lifting functions selection, are elaborated in pursuit of the theoretical optimality in real applications. The lightweight nature of EVOLVER enables online computation. The framework is thoroughly evaluated by 1) trajectory prediction of an irregular free-flying object subject to aerodynamic drag, 2) agile flight of a quadrotor subject to wind gust, and 3) high-precision end-effector control of a manipulator subject to base moving disturbance."
Time-Optimal Path Planning in a Constant Wind for Uncrewed Aerial Vehicles Using Dubins Set Classification,"Brady Moon, Sagar Sachdev, Junbin Yuan, Sebastian Scherer",Carnegie Mellon University,Aerial Systems,
Robust and Efficient Depth-Based Obstacle Avoidance for Autonomous Miniaturized UAVs,"Hanna Müller, Vlad Niculescu, Tommaso Polonelli, Michele Magno, Luca Benini","ETH Zürich,ETH Zurich,University of Bologna",Aerial Systems,"Nano-size drones hold enormous potential to explore unknown and complex environments. Their small size makes them agile and safe for operation close to humans and allows them to navigate through narrow spaces. However, their tiny size and payload restrict the possibilities for on-board computation and sensing, making fully autonomous flight extremely challenging. The first step towards full autonomy is reliable obstacle avoidance, which has proven to be challenging by itself in a generic indoor environment. Current approaches utilize vision-based or 1-dimensional sensors to support nano-drone perception algorithms. This work presents a lightweight obstacle avoidance system based on a novel millimeter form factor 64 pixels multizone Time-of-Flight (ToF) sensor and a generalized model-free control policy. In-field tests are based on the Crazyflie 2.1, extended by a custom multi-zone ToF deck, featuring a total flight mass of 35 g. The algorithm only uses 0.3% of the on-board processing power (210 Î¼s execution time) with a frame rate of 15 fps. The presented autonomous nano-size drone reaches 100% reliability at 0.5 m/s in a generic and previously unexplored indoor environment."
Visual Localization in Repetitive and Symmetric Indoor Parking Lots Using 3D Key Text Graph,"Joo Hyung Kim, Gunhee Koo, Heewon Park, Nakju Doh","Korea University,Samsung Electronics",Localization V,"Indoor parking lots are the GPS-denied spaces to which vision-based localization approaches have usually been applied to solve localization problems. However, due to the repetitiveness and symmetry of the spaces, visual localization methods commonly confront difficulties in estimating precise 3D poses. In this study, we propose four novel modules that improve localization precision by imposing the existing methods with the spatial discerning ability. The first module constructs a key text graph that represents the topology of key texts in the space and becomes the basis for discerning repetitiveness and symmetry. Next, the orientation filtering module estimates the unknown 3D orientation of the query image and resolves spatial symmetric ambiguity. The similarity scoring module sorts out the top-scored database images, discerning the spatial repetitiveness based on detected key text bounding boxes. Our pose verification module evaluates the pose confidence of top-scored candidates and determines the most reliable pose. Our method has been validated in two real indoor parking lots, achieving new state-of-the-art performance levels."
VOLoc: Visual Place Recognition by Querying Compressed Lidar Map,"Xudong Cai, Yongcai Wang, Zhe Huang, Yu Shao, Deying Li",Renmin University of China,Localization V,"The availability of city-scale Lidar maps enables the potential of city-scale place recognition using mobile cameras. However, the city-scale Lidar maps generally need to be compressed for storage efficiency, which increases the difficulty of direct visual place recognition in compressed Lidar maps. This paper proposes VOLoc, an accurate and efficient visual place recognition method that exploits geometric similarity to directly query the compressed Lidar map via the real-time captured image sequence. In the offline phase, VOLoc compresses the Lidar maps using a emph{Geometry-Preserving Compressor} (GPC), in which the compression is reversible, a crucial requirement for the downstream 6DoF pose estimation. In the online phase, VOLoc proposes an online Geometric Recovery Module (GRM), which is composed of online Visual Odometry (VO) and a point cloud optimization module, such that the local scene structure around the camera is online recovered to build the emph{Querying Point Cloud} (QPC). Then the QPC is compressed by the same GPC, and is aggregated into a global descriptor by an attention-based aggregation module, to query the compressed Lidar map in the vector space. A transfer learning mechanism is also proposed to improve the accuracy and the generality of the aggregation network. Extensive evaluations show that VOLoc provides localization accuracy even better than the Lidar-to-Lidar place recognition, setting up a new record for utilizing the compressed Lidar map by low-end mobile cameras. The code are publicly available at href{https://github.com/Master-cai/VOLoc}{https://github.com/Master-cai/VOLoc}."
VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition,"Adam D. Hines, Peter Stratton, Michael Milford, Tobias Fischer","Queensland University of Technology,University of Queensland",Localization V,"Spiking Neural Networks (SNNs) are at the forefront of neuromorphic computing thanks to their potential energy-efficiency, low latencies, and capacity for continual learning. While these capabilities are well suited for robotics tasks, SNNs have seen limited adaptation in this field thus far. This work introduces a SNN for Visual Place Recognition (VPR) that is both trainable within minutes and queryable in milliseconds, making it well suited for deployment on compute- constrained robotic systems. Our proposed system, VPRTempo, overcomes slow training and inference times using an abstracted SNN that trades biological realism for efficiency. VPRTempo employs a temporal code that determines the timing of a single spike based on a pixelâ€™s intensity, as opposed to prior SNNs relying on rate coding that determined the number of spikes; improving spike efficiency by over 100%. VPRTempo is trained using Spike-Timing Dependent Plasticity and a supervised delta learning rule enforcing that each output spiking neuron responds to just a single place. We evaluate our system on the Nordland and Oxford RobotCar benchmark localization datasets, which include up to 27k places. We found that VPRTempoâ€™s accuracy is comparable to prior SNNs and the popular NetVLAD place recognition algorithm, while being several orders of magnitude faster and suitable for real-time deployment â€“ with inference speeds over 50 Hz on CPU. VPRTempo could be integrated as a loop closure component for online SLAM on resource-constrained systems such as space and underwater robots."
17-Point Algorithm Revisited: Toward a More Accurate Way,"Chen Xie, Rui Xing, Ning Hao, Fenghua He",Harbin Institute of Technology,Localization V,"17-point algorithm is a popular method in relative pose estimation of multi-cameras. However, the role of overlap in 17-point algorithm remains unexplored. And the relaxed way in solving constrained normal equation leads to sub-optimal results. Both of them influence accuracy of the estimated pose. In this paper, we theoretically analyze the influence of overlap and the solvability of 17-point algorithm. In addition, we show that the abuse of overlap can harm accuracy in practice. In light of these findings, we propose an improved 17-point algorithm, which avoids using overlaps and derives a simple way to solve normal equation on manifold. Both simulations and real world data experiments demonstrate the proposed one outperforms the traditional 17-point algorithm in term of accuracy."
AnyLoc: Towards Universal Visual Place Recognition,"Nikhil Varma Keetha, Avneesh Mishra, Jay Karhade, Krishna Murthy, Sebastian Scherer, Madhava Krishna, Sourav Garg","Carnegie Mellon University,International Institute of Information Technology, Hyderabad,MIT,IIIT Hyderabad,University of Adelaide",Localization V,"Visual Place Recognition (VPR) is vital for robot localization. To date, the most performant VPR approaches are environment- and task-specific: while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust real-world deployment. In this work, we develop a universal solution to VPR -- a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or fine-tuning. We demonstrate that general-purpose feature representations derived from off-the-shelf self-supervised models with no VPR-specific training are the right substrate upon which to build such a universal VPR solution. Combining these derived features with unsupervised feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X significantly higher performance than existing approaches. We further obtain a 6% improvement in performance by characterizing the semantic properties of these features, uncovering unique domains which encapsulate datasets from similar environments. Our detailed experiments and analysis lay a foundation for building VPR solutions that may be deployed anywhere, anytime, and across anyview. We encourage the readers to explore our project page and interactive demos: https://anyloc.github.io/"
Lightweight Ground Texture Localization,"Aaron Wilhelm, Nils Napp",Cornell University,Localization V,"We present a lightweight ground texture based localization algorithm (L-GROUT) that improves the state of the art in performance and can be run in real-time on single board computers without GPU acceleration. Such computers are ubiquitous on small indoor robots and thus this work enables high-precision, millimeter-level localization without instrumenting, marking, or modifying the environment. The key innovations are an improved database feature extraction algorithm, a dimensionality reduction method based on locality preserving projections (LPP) that can accommodate faster-to-compute binary features, and an improved spatial filtering step that better preserves performance when the databases are tuned for lightweight applications. We demonstrate the approach by running the whole system on a low-cost single board computer (Raspberry Pi 4) to produce global localization estimates at greater than 4Hz on an outdoor asphalt dataset."
NeRF-VINS: A Real-Time Neural Radiance Field Map-Based Visual-Inertial Navigation System,"Saimouli Katragadda, Woosik Lee, Yuxiang Peng, Patrick Geneva, Chuchu Chen, Chao Guo, Mingyang Li, Guoquan Huang","University of Delaware,Google,Alphabet Inc.",Localization V,"Achieving efficient and consistent localization with a prior map remains challenging in robotics. Conventional keyframe-based approaches often suffer from sub-optimal view- points due to limited field of view (FOV) and/or constrained motion, thus degrading the localization performance. To ad- dress this issue, we design a real-time tightly-coupled Neural Radiance Fields (NeRF)-aided visual-inertial navigation system (VINS). In particular, by effectively leveraging the NeRFâ€™s potential to synthesize novel views, the proposed NeRF-VINS overcomes the limitations of traditional keyframe-based maps (with limited views) and optimally fuses IMU, monocular images, and synthetically rendered images within an efficient filter-based framework. This tightly-coupled fusion enables efficient 3D motion tracking with bounded errors. We extensively validate the proposed NeRF-VINS against the state-of-the-art methods that use prior map information, and demonstrate its ability to perform real-time localization, at 15 Hz, on a resource- constrained Jetson AGX Orin embedded platform."
Night-Rider: Nocturnal Vision-Aided Localization in Streetlight Maps Using Invariant Extended Kalman Filtering,"Tianxiao Gao, Mingle Zhao, Cheng-zhong Xu, Hui Kong","University of Macao,University of Macau",Localization V,"Vision-aided localization for low-cost mobile robots in diverse environments has attracted widespread attention recently. Although many current systems are applicable in daytime environments, nocturnal visual localization is still an open problem owing to the lack of stable visual information. An insight from most nocturnal scenes is that the static and bright streetlights are reliable visual information for localization. Hence we propose a nocturnal vision-aided localization system in streetlight maps with a novel data association and matching scheme using object detection methods. We leverage the Invariant Extended Kalman Filter (InEKF) to fuse IMU, odometer, and camera measurements for consistent state estimation at night. Furthermore, a tracking recovery module is also designed for tracking failures. Experimental results indicate that our proposed system achieves accurate and robust localization with less than 0.2% relative error of trajectory length in four nocturnal environments."
ONeK-SLAM: A Robust Object-Level Dense SLAM Based on Joint Neural Radiance Fields and Keypoints,"Yue Zhuge, Haiyong Luo, Runze Chen, Yushi Chen, Jiaquan Yan, Jiang Zhuqing","Institute of Computing Technology, Chinese Academy of Sciences; ,Institute of Computing Technology, Chinese Academy of Sciences,Beijing University of Posts and Telecommunications",SLAM II,"Neural implicit representation has recently achieved significant advancements, especially in the field of SLAM(Simultaneous Localization and Mapping). Previous NeRF-based SLAM methods have difficulties with object-level localization and reconstruction and struggle in dynamic and illumination-varied environments. We propose ONeK-SLAM, a robust object-level SLAM system that effectively combines feature points and neural radiance fields. ONeK-SLAM uses the joint information at the object level to improve localization accuracy and enhance reconstruction details. Moreover, our approach detects and eliminates dynamic objects based on the joint errors, while also harnessing the illumination invariance offered by feature points. Consequently, ONeK-SLAM achieves high-precision localization and detailed object-level mapping, even in dynamic and illumination-varying environments. Our evaluations, conducted on three public datasets that include both dynamic and variable lighting sequences, demonstrate that our method outperforms recent NeRF-based SLAM method in both localization and reconstruction."
A Two-Step Nonlinear Factor Sparsification for Scalable Long-Term SLAM Backend,"Binqian Jiang, Shaojie Shen",Hong Kong University of Science and Technology,SLAM II,"This paper proposes a new nonlinear factor sparsification paradigm for general feature-based long-term SLAM backend. Given a pose sparsification policy, we aim to scale the SLAM problem with space explored instead of time in a principled way, so that the number of time-indexed poses can be limited while their influence and the long-lived landmarks are appropriately maintained. To do this, we propose a new two-step sparsification pipeline. Given a pose node to remove, the first step is performed in the Markov blankets of affected landmarks. It transforms pose-landmark constraints into pose-pose constraints while preserving observability and minimizing information loss in the blanket. Moreover, since landmarks are conditionally independent, we can do this in parallel, disconnecting a pose from all the landmarks. The second step marginalizes the pose of interest with pure pose-wise constraints without affecting any landmarks. Our method decouples the management of landmarks from pose-only measurements, making it general for any feature-based SLAM. We also give a practical example of how our backend works by concatenating it to a monocular VIO frontend. In simulation and real-world dataset, our sparsified backend is shown to be accurate and efficient. We open-source our backend, along with the VIO+Backend example, with the aim of contributing to the betterment of the community."
Effectively Detecting Loop Closures Using Point Cloud Density Maps,"Saurabh Gupta, Tiziano Guadagnino, Benedikt Mersch, Ignacio Vizzo, Cyrill Stachniss","University of Bonn,Dexory",SLAM II,"The ability to detect loop closures plays an essential role in any SLAM system. Loop closures allow correcting the drifting pose estimates from a sensor odometry pipeline. In this paper, we address the problem of effectively detecting loop closures in LiDAR SLAM systems in various environments with longer lengths of sequences and agnostic of the scanning pattern of the sensor. While many approaches for loop closures using 3D LiDAR sensors rely on individual scans, we propose the usage of local maps generated from locally consistent odometry estimates. Several recent approaches compute the maximum elevation map on a bird's eye view projection of point clouds to compute feature descriptors. In contrast, we use a density image bird's eye view representation, which is robust to viewpoint changes. The utilization of dense local maps allows us to reduce the complexity of features describing these maps, as well as the size of the database required to store these features over a long sequence. This yields a real-time application of our approach for a typical robotic 3D LiDAR sensor. We perform extensive experiments to evaluate our approach against other state-of-the-art approaches and show the benefits of our proposed approach."
LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric Information Estimation,"Kai Huang, Junjiao Zhao, Zhongyang Zhu, Chen Ye, Tiantian Feng","Tongji University,Tongji university",SLAM II,"Local geometric information, i.e., normal and distribution of points, is crucial for LiDAR-based simultaneous localization and mapping (SLAM) because it provides constraints for data association, which further determines the direction of optimization and ultimately affects the accuracy of localization. However, estimating normal and distribution of points are time-consuming tasks even with the assistance of kdtree or volumetric maps. To achieve fast normal estimation, we look into the structure of LiDAR scan and propose a ring-based fast approximate least squares (Ring FALS) method. With the Ring structural information, estimating the normal requires only the range information of the points when a new scan arrives. To efficiently estimate the distribution of points, we extend the ikd-tree to manage the map in voxels and update the distribution of points in each voxel incrementally while maintaining its consistency with the normal estimation. We further fix the distribution after its convergence to balance the time consumption and the correctness of representation. Based on the extracted and maintained local geometric information, we devise a robust and accurate hierarchical data association scheme where point-to-surfel association is prioritized over point-toplane. Extensive experiments on diverse public datasets demonstrate the advantages of our system compared to other state-ofthe-art methods. Our code is available at https://github.com/tiev-tongji/LOG-LIO."
Radar-Only Odometry and Mapping for Autonomous Vehicles,"Daniel Casado Herraez, Matthias Zeller, Le Chang, Ignacio Vizzo, Michael Heidingsfeld, Cyrill Stachniss","University of Bonn & CARIAD SE,CARIAD SE,University of Stuttgart,Dexory,University of Bonn",SLAM II,"Odometry and mapping play a pivotal role in the navigation of autonomous vehicles. In this paper, we address the problem of pose estimation and map creation using only radar sensors. We focus on two odometry estimation approaches followed by a mapping step. The first one is a new point-to-point ICP approach that leverages the velocity information provided by 3D radar sensors. The second one is advantageous for 2D radars with a low number of samples, and particularly useful for scenarios where the sensor is being blocked by large dynamic obstacles. It exploits a constant velocity filter and the measured Doppler velocities to estimate the vehicleâ€™s ego-motion. We enrich this with a filtering step to improve the accuracy of the points in the resulting map. We put our work to the test using the View of Delft and NuScenes datasets, which involve 3D and 2D radar sensors. Our findings illustrate state-of-the-art performance of our odometry techniques in terms of accuracy when compared to existing alternatives. Moreover, we demonstrate that our map filtering methodology achieves higher similarity rates than the raw unfiltered map when benchmarked against a corresponding LiDAR map."
IPC: Incremental Probabilistic Consensus-Based Consistent Set Maximization for SLAM Backends,"Emilio Olivastri, Alberto Pretto","University of Padua,University of Padova",SLAM II,"In SLAM (Simultaneous localization and mapping) problems, Pose Graph Optimization (PGO) is a technique to refine an initial estimate of a set of poses (positions and orientations) from a set of pairwise relative measurements. The optimization procedure can be negatively affected even by a single outlier measurement, with possible catastrophic and meaningless results. Although recent works on robust optimization aim to mitigate the presence of outlier measurements, robust solutions capable of handling large numbers of outliers are yet to come. This paper presents IPC, acronym for Incremental Probabilistic Consensus, a method that approximates the solution to the combinatorial problem of finding the maximally consistent set of measurements in an incremental fashion. It evaluates the consistency of each loop closure measurement through a consensus-based procedure, possibly applied to a subset of the global problem, where all previously integrated inlier measurements have veto power. We evaluated IPC on standards benchmarks against several state-of-the-art methods. Although it is simple and relatively easy to implement, IPC competes with or outperforms the other tested methods in handling outliers while providing online performances. We release with this paper an open-source implementation of the proposed method."
Generalized Correspondence Matching Via Flexible Hierarchical Refinement and Patch Descriptor Distillation,"Yu Han, Ziwei Long, Yanting Zhang, Wu Jin, Zhijun Fang, Rui Fan","Donghua University,Tongji University,UESTC,School of computer science and technology, Donghua University",SLAM II,"Correspondence matching plays a crucial role in numerous robotics applications. In comparison to conventional hand-crafted methods and recent data-driven approaches, there is significant interest in plug-and-play algorithms that make full use of pre-trained backbone networks for multi-scale feature extraction and leverage hierarchical refinement strategies to generate matched correspondences. The primary focus of this paper is to address the limitations of deep feature matching (DFM), a state-of-the-art (SoTA) plug-and-play correspondence matching approach. First, we eliminate the pre-defined threshold employed in the hierarchical refinement process of DFM by leveraging a more flexible nearest neighbor search strategy, thereby preventing the exclusion of repetitive yet valid matches during the early stages. Our second technical contribution is the integration of a patch descriptor, which extends the applicability of DFM to accommodate a wide range of backbone networks pre-trained across diverse computer vision tasks, including image classification, semantic segmentation, and stereo matching. Taking into account the practical applicability of our method in real-world robotics applications, we also propose a novel patch descriptor distillation strategy to further reduce the computational complexity of correspondence matching. Extensive experiments conducted on three public datasets demonstrate the superior performance of our proposed method. Specifically, it achieves an overall performance in terms of mean matching accuracy of 0.68, 0.92, and 0.95 with respect to the tolerances of 1, 3, and 5 pixels, respectively, on the HPatches dataset, outperforming all other SoTA algorithms."
VOOM: Robust Visual Object Odometry and Mapping Using Hierarchical Landmarks,"Yutong Wang, Chaoyang Jiang, Xieyuanli Chen","beijing institute of technology,Beijing Institute of Technology,National University of Defense Technology",SLAM II,"In recent years, object-oriented simultaneous localization and mapping (SLAM) has attracted increasing attention due to its ability to provide high-level semantic information while maintaining computational efficiency. Some researchers have attempted to enhance localization accuracy by integrating the modeled object residuals into bundle adjustment. However, few have demonstrated better results than feature-based visual SLAM systems, as the generic coarse object models, such as cuboids or ellipsoids, are less accurate than feature points. In this paper, we propose a Visual Object Odometry and Mapping framework VOOM using high-level objects and low-level points as the hierarchical landmarks in a coarse-to-fine manner instead of directly using object residuals in bundle adjustment. Firstly, we introduce an improved observation model and a novel data association method for dual quadrics, employed to represent physical objects. It facilitates the creation of a 3D map that closely reflects reality. Next, we use object information to enhance the data association of feature points and consequently update the map. In the visual object odometry backend, the updated map is employed to further optimize the camera pose and the objects. Meanwhile, local bundle adjustment is performed utilizing the objects and points-based covisibility graphs in our visual object mapping process. Experiments show that VOOM outperforms both object-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms of localization. The implementation of our method is available at https://github.com/yutongwangBIT/VOOM.git."
Lite-SVO: Towards a Lightweight Self-Supervised Semantic Visual Odometry Exploiting Multi-Feature Sharing Architecture,"Wenhui Wei, Jiantao Li, Kaizhu Huang, Jiadong Li, Xin Liu, Yangfan Zhou","University of Science and Technology of China,Duke Kunshan University,Suzhou Institute of Nano-Tech and Nano-Bionics, Chinese Academy ,Suzhou Institute of Nano-Tech and Nano-Bionics (SINANO), Chinese,Chinese Academy of Sciences",SLAM II,"Not relying on ground-truth data for training, self-supervised semantic visual odometry (SVO) has recently gained considerable attention. Within self-supervised SVO, feature representation inconsistency between semantic/depth and pose tasks presents a significant challenge, as it may disrupt cross-task feature representations and lead to notable performance degradation. Regrettably, existing self-supervised SVO lacks an effective solution to address this obstacle, for either overlooking this issue or exploiting a too heavy architecture. In response to this challenge, we propose a groundbreaking solution within the textit{Single-Stream} architecture, known as Lite-SVO, which is a lightweight yet efficient multi-feature sharing architecture. Lite-SVO is designed to bolster self-supervised SVO, facilitating its adoption on edge devices without compromising accuracy and performance. The crucial innovation lies in the multi-feature sharing architecture, which fuses the semantic and depth maps as pose features, thus significantly reducing the model complexity and boosting the speed in edge devices. Built upon the novel feature sharing framework, Lite-SVO is able to incorporate the fine-grained feature sharing representation to further optimize the performance. Specifically, the proposed cross-feature sharing module alleviates the impact of object boundary in depth estimation, and the designed multi-sharing module focuses on fine-grained features, thereby boosting the performance of Lite-SVO. Experimental results demonstrate that our method is at least $84.46%$ faster than the state-of-the-art textit{Single-Stream} approaches, and excitingly, our pose accuracy is about $79.83%$ higher than theirs."
Constant-Time Motion Planning with Anytime Refinement for Manipulation,"Itamar Mishani, Hayden Feddock, Maxim Likhachev","Carnegie Mellon University, Robotics Institute,University of Pittsburgh,Carnegie Mellon University",Motion and Path Planning III,"Robotic manipulators are essential for future autonomous systems, yet limited trust in their autonomy has confined them to rigid, task-specific systems. The intricate configuration space of manipulators, coupled with the challenges of obstacle avoidance and constraint satisfaction, often makes motion planning the bottleneck for achieving reliable and adaptable autonomy. Recently, a class of constant-time motion planners (CTMP) was introduced. These planners employ a preprocessing phase to compute data structures that enable online planning provably guarantee the ability to generate motion plans, potentially sub-optimal, within a user defined time bound. This framework has been demonstrated to be effective in a number of time-critical tasks. However, robotic systems often have more time allotted for planning than the online portion of CTMP requires, time that can be used to improve the solution. To this end, we propose an anytime refinement approach that works in combination with CTMP algorithms. Our proposed framework, as it operates as a constant time algorithm, rapidly generates an initial solution within a user-defined time threshold. Furthermore, functioning as an anytime algorithm, it iteratively refines the solution's quality within the allocated time budget. This enables our approach to strike a balance between guaranteed fast plan generation and the pursuit of optimization over time. We support our approach by elucidating its analytical properties, showing the convergence of the anytime component towards optimal solutions. Additionally, we provide empirical validation through simulation and real-world demonstrations on a 6 degree-of-freedom robot manipulator, applied to an assembly domain."
VAPOR: Legged Robot Navigation in Unstructured Outdoor Environments Using Offline Reinforcement Learning,"Kasun Weerakoon, Adarsh Jagan Sathyamoorthy, Mohamed Elnoor, Dinesh Manocha","University of Maryland, College Park,University of Maryland",Motion and Path Planning III,"We present VAPOR, a novel method for autonomous legged robot navigation in unstructured, densely vegetated outdoor environments using offline Reinforcement Learning (RL). Our method trains a novel RL policy using an actor-critic network and arbitrary data collected in real outdoor vegetation. Our policy uses height and intensity-based cost maps derived from 3D LiDAR point clouds, a goal cost map, and processed proprioception data as state inputs, and learns the physical and geometric properties of the surrounding obstacles such as height, density, and solidity/stiffness. The fully-trained policy's critic network is then used to evaluate the quality of dynamically feasible velocities generated from a novel context-aware planner. Our planner adapts the robot's velocity space based on the presence of entrapment including vegetation, and narrow passages in dense environments. We demonstrate our method's capabilities on a Spot robot in complex real-world outdoor scenes, including dense vegetation. We observe that VAPOR's actions improve success rates by up to 40%, decrease the average current consumption by up to 2.9%, and decrease the normalized trajectory length by up to 11.2% compared to existing end-to-end offline RL and other outdoor navigation methods."
EDMP: Ensemble-Of-Costs-Guided Diffusion for Motion Planning,"Kallol Saha, Vishal Reddy Mandadi, Jayaram Gurram, Ajit Srikanth, Aditya Agarwal, Bipasha Sen, Arun Singh, Madhava Krishna","International Instititute of Information Technology, Hyderabad,International Institute of Information Technology, Hyderabad,IIIT Hyderabad,International Institute of Information Technology,University of Tartu",Motion and Path Planning III,"Classical motion planning for robotic manipulation includes a set of general algorithms that aim to minimize a scene-specific cost of executing a given plan. This approach offers remarkable adaptability, as they can be directly used off-the-shelf for any new scene without needing specific training datasets. However, without a prior understanding of what diverse valid trajectories are and without specially designed cost functions for a given scene, the overall solutions tend to have low success rates within a certain time limit. While deep-learning-based algorithms tremendously improve success rates, they are much harder to adopt without specialized training datasets. We propose EDMP, an Ensemble-of-costs-guided Diffusion for Motion Planning that aims to combine the strengths of classical and deep-learning-based motion planning. Our diffusion-based network is trained on a set of diverse kinematically valid trajectories. Like classical planning, for any new scene at the time of inference, we compute scene-specific costs such as ""collision cost"" and guide the diffusion to generate valid trajectories that satisfy the scene-specific constraints. Further, instead of a single cost function that may be insufficient in capturing diversity across scenes, we use an ensemble of costs to guide the diffusion process, significantly improving the success rate compared to classical planners. EDMP performs comparably with SOTA deep-learning-based methods while retaining the generalization capabilities primarily associated with classical planners."
Approximating Robot Configuration Spaces with Few Convex Sets Using Clique Covers of Visibility Graphs,"Peter Werner, Alexandre Amice, Tobia Marcucci, Daniela Rus, Russ Tedrake","Massachusetts Institute of Technology,MIT",Motion and Path Planning III,"Many computations in robotics can be dramatically accelerated if the robot configuration space is described as a collection of simple sets. For example, recently developed motion planners rely on a convex decomposition of free space to design collision-free trajectories using fast convex optimization. In this work, we present an efficient method for approximately covering complex configuration spaces with a small number of polytopes. The approach constructs a visibility graph using sampling, and generates a clique cover of this graph to find clusters of samples that have mutual line of sight. These clusters are then inflated into large, full-dimensional, polytopes. We evaluate our method on a variety of robotic systems, and show that it consistently covers larger portions of free configuration space, with fewer polytopes, and in a fraction of the time compared to previous methods."
Asymptotically-Optimal Multi-Robot Visibility-Based Pursuit-Evasion,"Nicholas Stiffler, Jason O'kane","University of Dayton,Texas A&M University",Motion and Path Planning III,"The multi-robot visibility-based pursuit-evasion problem tasks a team of robots with systematically searching an environment to detect (capture) an evader. Previous techniques to generate search strategies for the pursuit team have shown to be either computationally intractable or permit poor solution quality. This paper presents a novel asymptotically optimal algorithm for generating a joint motion strategy for the pursuers. To explore the space of possible pursuer motion strategies, the algorithm utilizes a trio of hierarchical graph data structures that each capture certain elements of the problem such as connectivity (valid single pursuer motion), coordination (multiple pursuer motion), and tracking information (evaluating where an evader may be). The algorithm is inspired by well-known methods in the motion planning literature and inherits its asymptotic optimality from those techniques. In addition, we describe a method that can improve upon solutions found during the formative stages of the main algorithm, using a ""fast-forward"" approach that foregoes guarantees of asymptotic optimality, implementing heuristics that concentrate future samples into improving the path quality of the nominal solution. The algorithms were validated in simulation and results are provided."
APP: A* Post-Processing Algorithm for Robots with Bidirectional Shortcut and Path Perturbation,"Yong Li, Hui Cheng","Guangzhou Shiyuan Electronic Technology Co., Ltd,Sun Yat-sen University",Motion and Path Planning III,"Paths generated by A* and other graph-search-based planners are widely used in the robotic field. Due to the restricted node-expansion directions, the resulting paths are usually not the shortest. Besides,unnecessary heading changes, or zig-zag patterns, exist even when no obstacle is nearby, which is inconsistent with the human intuition that the path segments should be straight in wide-open space due to the absence of obstacles. This article puts forward a general and systematic post-processing algorithm for A* and other graph-search-based planners.The A* post-processing algorithm, called APP, is developed based on the costmap, which is widely used in commercial service robots. First, a bidirectional vertices reduction algorithm is proposed to tackle the asymmetry of the path and the environments. During the forward and backward vertices reduction, a thorough shortcut strategy is put forward to improve the path-shortening performance and avoid unnecessary heading changes.Second, an iterative path perturbation algorithm is adopted to locally reduce the number of unnecessary heading changes and improve the path smoothness. Comparative experiments are then carried out to validate the superiority of the proposed method. Quantitative performance indexes show that APP outperforms the existing methods in planning time, path length as well as the number of unnecessary heading changes. Finally, field navigation experiments are carried out toverify the icability of APP."
"Scaling Infeasibility Proofs Via Concurrent, Codimension-One, Locally-Updated Coxeter Triangulation","Sihui Li, Neil Dantam",Colorado School of Mines,Motion and Path Planning III,"A complete motion planner has long been desired but is hard to achieve in high dimensions. Previous work proposed an asymptotically complete motion planner that reports a plan or an infeasibility proof given long enough time. The algorithm trains a manifold using configuration space samples as data and triangulates the manifold to ensure its existence in the obstacle region of the configuration space. In this paper, we extend the construction of infeasibility proofs to higher dimensions by adapting Coxeter triangulation's manifold tracing and cell construction procedures to concurrently triangulate the configuration space codimension-one manifold, and we apply a local elastic update to fix the triangulation when part is in the free space. We perform experiments on 4-DOF, 5-DOF, and 6-DOF serial manipulators. Infeasibility proofs in 4D are two orders of magnitude faster than previous results. Infeasibility proofs in 5D complete within minutes."
Skeleton Disk-Graph Roadmap: A Sparse Deterministic Roadmap for Safe 2D Navigation and Exploration,"Thibault Noël, Antoine Lehuger Lehuger, Eric Marchand, Francois Chaumette","INRIA Rennes,Groupe Créative,Univ Rennes, Inria, CNRS, IRISA,Inria center at University of Rennes",Motion and Path Planning III,"In this paper, we describe a novel roadmap construction method in unknown environments, which relies on the extraction of the Hamilton-Jacobi skeleton of the free space. This skeleton is used to construct a graph of free-space bubbles, effectively compressing the skeleton information in a sparse data structure but retaining its topology. The bubbles also enforce safety directly in the roadmap structure. We first demonstrate the relevance of this approach for standard path-planning tasks. We also propose a frontiers-based exploration strategy able to autonomously and safely build a complete 2D map of the environment."
SMUG Planner: A Safe Multi-Goal Planner for Mobile Robots in Challenging Environments,"Changan Chen, Jonas Frey, Philip Arm, Marco Hutter",ETH Zurich,Motion and Path Planning III,"Robotic exploration or monitoring missions require mobile robots to autonomously and safely navigate between multiple target locations in potentially challenging environments. Currently, this type of multi-goal mission often relies on humans designing a set of actions for the robot to follow in the form of a path or waypoints. In this work, we consider the multi-goal problem of visiting a set of pre-defined targets, each of which could be visited from multiple potential locations. To increase autonomy in these missions, we propose a safe multi-goal (SMUG) planner that generates an optimal motion path to visit those targets. To increase safety and efficiency, we propose a hierarchical state validity checking scheme, which leverages robot-specific traversability learned in simulation. We use LazyPRM* with an informed sampler to accelerate collision-free path generation. Our iterative dynamic programming algorithm enables the planner to generate a path visiting more than ten targets within seconds. Moreover, the proposed hierarchical state validity checking scheme reduces the planning time by 30% compared to pure volumetric collision checking and increases safety by avoiding high-risk regions. We deploy the SMUG planner on the quadruped robot ANYmal and show its capability to guide the robot in multi-goal missions fully autonomously on rough terrain."
Non-Singular Fast Terminal Adaptive Visual Tracking Control with Reduced Tuning Parameters for an Aerial Vehicle under Perturbations,"Gustavo Olivas-martínez, Armando Miranda-moya, Carlos Katt, Herman Castaneda","Instituto Tecnológico de Estudios Superiores de Monterrey,Tecnologico de Monterrey,Tecnológico de Monterrey",Robust/Adaptive Control,"This paper presents a robust image-based visual servoing design for a quad-rotor unmanned aerial vehicle performing a visual target-tracking operation in the presence of turbulent wind. Image information is extracted and processed to control the positioning and heading of the aerial vehicle. A novel adaptive non-singular fast terminal sliding mode strategy is introduced to manage the visual servoing error. Unlike other sliding mode methods, the proposed approach diminishes the complexity of the system due to the reduction of its control parameters while providing practical finite-time convergence, robustness against bounded external disturbances and model uncertainties, non-overestimation of the control gains, and chattering attenuation. Furthermore, the stability of the system in closed loop is guaranteed through Lyapunov theory. Finally, simulation results demonstrate the capabilities and performance of such a controller in a high-fidelity scenario using the Robot Operating System and Gazebo frameworks."
Quadrotor Neural Network Adaptive Control: Design and Experimental Validation,"Gan Yu, Joel Reis, Carlos Silvestre","Shanghai Jiao Tong University,University of Macau",Robust/Adaptive Control,"This letter presents the design and experimental study of an adaptive nonlinear controller for Unmanned Aerial Vehicles (UAVs) in the presence of unknown time-varying disturbances, and model parametric uncertainty. We employ an adaptive Neural Network (NN), used to approximate the partially unknown system, in tandem with a simple controller designed for trajectory tracking of a point located along the UAVâ€™s vertical body axis instead of the center of mass. This strategy allows: (i) to avoid the two-subsystems control paradigm generally adopted by conventional UAV controllers; (ii) all control inputs to be defined at once; and (iii) to lump all unknown dynamics from both translational and rotational levels into a single vector term. The weights of the NN are determined online by an adaptive law based on the Lyapunov synthesis method. The tracking and adaption errors are shown to be uniformly ultimately bounded. Simulation and experimental results, including comparison data, are provided to validate and assess the proposed control solution."
Parameter Identifying Disturbance Rejection Control with Asymptotic Error Convergence,"RadosÅ‚aw Patelski, Dariusz Pazderski",Poznan University of Technology,Robust/Adaptive Control,"In this paper, a new kind of adaptive controller for the problem of output feedback tracking is proposed on the basis of the Active Disturbance Rejection Control (ADRC) paradigm. The controller is synthesized for the systems linear in parameters by combining the classic ADRC algorithm with a recent Parameter Identifying Extended State Observer (PIESO) which employs a gradient adaptation law to actively identify the parameters of the plant. By means of the Lyapunov analysis, the asymptotic convergence of tracking, estimation, and identification errors is proved in the nominal case and the stability conditions of the closed-loop system are formulated."
Smooth Computation without Input Delay: Robust Tube-Based Model Predictive Control for Robot Manipulator Planning,"Qie Sima, Yu Luo, Tianying Ji, Fuchun Sun, Huaping Liu, Jianwei Zhang","Tsinghua University,University of Hamburg",Robust/Adaptive Control,"Model Predictive Control (MPC) has exhibited remarkable capabilities in optimizing objectives and meeting constraints. However, the substantial computational burden associated with solving the Optimal Control Problem (OCP) at each triggering instant introduces significant delays between state sampling and control application. These delays limit the practicality of MPC in resource-constrained systems when engaging in complex tasks. The intuition to address this issue in this paper is that by predicting the successor state, the controller can solve the OCP one time step ahead of time thus avoiding the delay of the next action. To this end, we compute deviations between real and nominal system states, predicting forthcoming real states as initial conditions for the imminent OCP solution. Anticipatory computation stores optimal control based on current nominal states, thus mitigating the delay effects. Additionally, we establish an upper bound for linearization error, effectively linearizing the nonlinear system, reducing OCP complexity, and enhancing response speed. We provide empirical validation through two numerical simulations and corresponding real-world robot tasks, demonstrating significant performance improvements and augmented response speed (up to 90%) resulting from the seamless integration of our proposed approach compared to conventional time-triggered MPC strategies."
Nullspace Adaptive Model-Based Trajectory-Tracking Control for a 6-DOF Underwater Vehicle with Unknown Plant and Actuator Parameters: Theory and Preliminary Simulation Evaluation,"Annie Mao, Joseph Moore, Louis Whitcomb","Johns Hopkins University,Johns Hopkins University Applied Physics Lab,The Johns Hopkins University",Robust/Adaptive Control,"We report a novel model-based nullspace adaptive trajectory-tracking control (NS-ATTC) algorithm for fully-actuated 6-degree-of-freedom (DOF) underwater vehicles which estimates unknown plant and actuator model parameters simultaneously. We provide a stability and convergence analysis with proof of asymptotically stable tracking error convergence, as well as a preliminary simulation study demonstrating 6-DOF trajectory tracking. The NS-ATTC algorithm does not require acceleration instrumentation and provides a stable online parameter estimate, enabling robust model-based autonomy."
Adaptive Planning and Control with Time-Varying Tire Models for Autonomous Racing Using Extreme Learning Machine,"Dvij Kalaria, Qin Lin, John Dolan","Carnegie Mellon University,Cleveland State University",Robust/Adaptive Control,"Autonomous racing is a challenging problem, as the vehicle needs to operate at the friction or handling limits in order to achieve minimum lap times. Autonomous race cars require highly accurate perception, state estimation, planning, and control. Adding to this complexity is the need to accurately identify vehicle model parameters governing lateral tire slip effects, which can evolve over time due to factors such as tire wear and tear. Current approaches to this problem typically either propose offline model identification methods or rely on initial parameters within a narrow range (typically within 15-20% of the actual values). However, these approaches fall short in accounting for significant changes in tire models that can occur during actual races, particularly when pushing the vehicle to its handling limits. we present a unified framework that not only learns the tire model in real time from collected data but also adapts the model to environmental changes, even when the model parameters exhibit substantial deviations. We validate our approach through testing in simulators, encompassing a 1:43 scale race car and a full-size car, and also through experiments with a physical F1/10 autonomous race car."
Risk-Sensitive Extended Kalman Filter,"Armand Jordana, Avadesh Meduri, Etienne Arlaud, Justin Carpentier, Ludovic Righetti","New York University,INRIA",Robust/Adaptive Control,"Designing robust algorithms in the face of estimation uncertainty is a challenging task. Indeed, controllers seldom consider estimation uncertainty and only rely on the most likely estimated state. Consequently, sudden changes in the environment or the robot's dynamics can lead to catastrophic behaviors. Leveraging recent results in risk-sensitive optimal control, this paper presents a risk-sensitive Extended Kalman Filter that can adapt its estimation to the control objective, hence allowing safe output-feedback Model Predictive Control (MPC). By taking a pessimistic estimate of the value function resulting from the MPC controller, the filter provides increased robustness to the controller in phases of uncertainty as compared to a standard Extended Kalman Filter (EKF). The filter has the same computational complexity as an EKF and can be used for real-time control. The paper evaluates the risk-sensitive behavior of the proposed filter when used in a nonlinear MPC loop on a planar drone and industrial manipulator in simulation, as well as on an external force estimation task on a real quadruped robot. These experiments demonstrate the ability of the approach to significantly improve performance in face of uncertainties."
Global Terminal Sliding Mode Control of Tethered Satellites Formation with Chattering Reduction Via PID Laws,"Bowen Su, Fan Zhang, Panfeng Huang","Northwestern Polytechnical University,Northwestern Polytechnical Univeristy",Robust/Adaptive Control,"This paper researches a novel global terminal sliding mode control(GTSMC) on a tethered satellites system(TSS) under outer disturbances, and the effect of PI/PD compensation in restraining chattering on sliding surface is appended. By taking advantage of the finite-time convergence of traditional terminal sliding surface, the sliding surface with global and terminal sliding motion is proposed, and the convergent time by GTSMC is qualitatively evaluated by the sliding surface. Then the integral/derivative function of the low-pass filtered switching control is appended in GTSMC, by virtue of the accuracy of integral and the damping of derivative, respectively, the persisting on sliding surface is eliminated, such that the chattering effect of the controlled system on the surface is restrained consequently. Finally, simulations of the proposed control on TSS is shown to validate the theoretical analyses."
Robust Feedback Quadratic Programming for Kinematic-Controlled Robots,"Mohamed Djeha, Pierre Gergondet, Abderrahmane Kheddar","Université de Montpellier,CNRS,CNRS-AIST",Robust/Adaptive Control,"Task-space quadratic programming (QP) is an elegant approach for controlling robots subject to constraints. Yet, in the case of kinematic-controlled (i.e., high-gains position or velocity) robots, closed-loop QP control scheme can be prone to instability depending on how the gains related to the tasks or to the constraints are chosen. In this paper, we address such instability shortcomings. First, we highlight the non-robustness of the closed-loop system against non-modeled dynamics, such as those relative to joint-dynamics, flexibilities, external perturbations, etc. Then, we propose a robust QP control formulation based on high-level integral feedback terms in the task-space including the constraints. The proposed method is formally proved to ensure closed-loop robust stability, and is intended to be applied to any kinematic-controlled robots under practical assumptions. We assess our approach through experiments on a fixed-base robot performing stable fast motions, and a floating-base humanoid robot robustly reacting to perturbations to keep its balance."
Model Predictive Control for an Autonomous Underwater Robot with Fully Vectored Propulsion,"Tianzhu Gao, Yudong Luo, Chao Lv, Weirong Luo, Xianping Fu, Na Zhao, Xi Luo, Yudong Luo","Dalian Maritime University,Yichang Testing Tech. Research Institution,University of Nevada, Reno",Dynamics,"Due to the low motion efficiency and maneuverability of underwater robots with six degrees of freedom, it is challenging for them to quickly respond to the attitude requirements during underwater autonomous maneuvering. This paper presents a novel autonomous underwater robot with fully vectored propulsion, combined with a model predictive control method, to autonomously achieve more agile and efficient movements. In detail, we first design an eight vector-distributed thruster layout for fully vectored propulsion of the robot and construct software architecture based on the robot operating system (ROS). Then, we establish the hydrodynamic model for the robot by adopting the Fossen approach, thus constructing a 13-dimensional system state-space equation, which is discretized by using the explicit fourth-order Runge-Kutta method. To achieve the autonomous maneuver, model predictive control is employed along with physical constraints of the custom-built robot to enable real-time prediction and optimization of the robot's state for control purposes. Finally, numerical simulations and experiments of the Point-to-Point Motion are conducted to test the robot's performance. Experimental results reveal that the average error of each direction is 0.0027 m, 0.0031 m, and 0.0368 m in the x-axis, y-axis, and z-axis, respectively, and 0.8502 degrees, 2.1941 degrees, 0.2408 degrees corresponding to three attitude angles, which verify the performance of employing MPC to control an autonomous underwater robot with fully vectored propulsion."
Attitude Control for Morphing Quadrotor through Model Predictive Control with Constraints,"Na Zhao, Yudong Luo, Chaojun Qin, Xi Luo, Rong Chen, Yudong Luo","Dalian Maritime University,Yichang Testing Tech. Research Institution,University of Nevada, Reno",Dynamics,"Morphing quadrotors that can be potentially applied to confined spaces such as warehouses, tanks, and pipelines have flourished in recent years. Most work has focused on the mechanical feasibility of the morphing systems and high-level flight controller design, with limited discussions on low-level control. In this paper, a constrained model predictive control (MPC) is proposed and applied to solve the attitude control problem of a morphing quadrotor. Prior to controller design, a custom-built morphing quadrotor is introduced with the kinematic and dynamic models established and corresponding issues and challenges presented. In the controller, to eliminate the steady-state error, an embedded integrator is adopted by exploiting the differential variables; then, the constraints of the morphing quadrotor are incorporated into the MPC formulation to simulate actual flight conditions, and an orthonormal function is employed to approximate the control input sequences in the controller to alleviate the computational burden. In the comparative studies, several scenarios are considered to demonstrate the effectiveness of the proposed control strategy in attitude control."
NNgTL: Neural Network Guided Optimal Temporal Logic Task Planning for Mobile Robots,"Ruijia Liu, Shaoyuan Li, Xiang Yin","Shanghai Jiao Tong University,Shanghai Jiao Tong Univ",Dynamics,"In this work, we investigate task planning for mobile robots under linear temporal logic (LTL) specifications. This problem is particularly challenging when robots navigate in continuous workspaces due to the high computational com- plexity involved. Sampling-based methods have emerged as a promising avenue for addressing this challenge by incrementally constructing random trees, thereby sidestepping the need to ex- plicitly explore the entire state-space. However, the performance of this sampling-based approach hinges crucially on the chosen sampling strategy, and a well-informed heuristic can notably enhance sample efficiency. In this work, we propose a novel neural-network guided (NN-guided) sampling strategy tailored for LTL planning. Specifically, we employ a multi-modal neural network capable of extracting features concurrently from both the workspace and the BÌˆuchi automaton. This neural network generates predictions that serve as guidance for random tree construction, directing the sampling process toward more optimal directions. Through numerical experiments, we com- pare our approach with existing methods and demonstrate its superior efficiency, requiring less than 15% of the time of the existing methods to find a feasible solution."
Synthesis of Temporally-Robust Policies for Signal Temporal Logic Tasks Using Reinforcement Learning,"Siqi Wang, Shaoyuan Li, Li Yin, Xiang Yin","Shanghai Jiao Tong University,Macau University of Science and Technology,Shanghai Jiao Tong Univ",Dynamics,"This paper investigates the problem of designing control policies that satisfy high-level specifications described by signal temporal logic (STL) in unknown, stochastic environments. While many existing works concentrate on optimizing the spatial robustness of a system, our work takes a step further by also considering temporal robustness as a critical metric to quantify the tolerance of time uncertainty in STL. To this end, we formulate two relevant control objectives to enhance the temporal robustness of the synthesized policies. The first objective is to maximize the probability of being temporally robust for a given threshold. The second objective is to maximize the worst-case spatial robustness value within a bounded time shift. We use reinforcement learning to solve both control synthesis problems for unknown systems. Specifically, we approximate both control objectives in a way that enables us to apply the standard Q-learning algorithm. Theoretical bounds in terms of the approximations are also derived. We present case studies to demonstrate the feasibility of our approach."
A Deep Learning Framework for Non-Symmetrical Coulomb Friction Identification of Robotic Manipulators,"Marcel Gabriel Lahoud, Gabriele Marchello, Mariapaola D'Imperio, Andreas Mueller, Ferdinando Cannella","Italian Institute of Technology,Istituto Italiano di Tecnologia,Johannes Kepler University",Dynamics,"The determination of the dynamic properties of a robot is especially important for designing highly accurate and efficient control systems. Conventional methods for dynamic model identification have proven to be effective, where deep learning (DL) approaches have shown limits due to data inefficiencies. However, thanks to novel physics-informed DL architectures, such as Deep Lagrangian Networks (DeLaN) [1], it is possible to control and extract interpretable physical information of a robot. This paper introduces an augmented DeLaN architecture for linear viscous and non-symmetrical Coulomb friction identification, which also learns motor parameters such as rotor inertia. An approach is proposed for comparing this method with the conventional dynamic identification and previous DeLaN implementations. Moreover, our friction and rotor inertia identification is validated, and the performance of our model is analyzed with a real robot (UR5e)."
Implicit Time Integration Simulation of Robots with Rigid Bodies and Cosserat Rods Based on a Newton-Euler Recursive Algorithm,"Frédéric Boyer, Andrea Gotelli, Philipp Tempel, Vincent Lebastard, Federico Renda, Sébastien Briot","IMT atlantique,École Centrale Nantes,Ecole Centrale de Nantes,Khalifa University of Science and Technology,LS,N",Dynamics,"In this paper, we propose a new algorithm for solving the forward dynamics of multibody systems consisting of rigid bodies connected in arbitrary topologies by localised joints and/or soft links, possibly actuated or not. The simulation is based on the implicit time-integration of the Lagrangian model of these systems, where the soft links are modelled by Cosserat rods parameterised by assumed strain modes. This choice imposes a predictor-corrector structure on the approach, and requires computing both the residual vector and the Jacobian of the residual vector of the dynamics constrained by the time integrator. These additional calculations are handled here with a new Newton-Euler recursive inverse dynamics algorithm and its linearized tangent version. The approach is illustrated with numerical examples from the Cosserat rod literature and from recent robotic applications."
Efficient Constrained Dynamics Algorithms Based on an Equivalent LQR Formulation Using Gauss' Principle of Least Constraint,"Ajay Suresha Sathya, Herman Bruyninckx, Wilm Decré, Goele Pipeleers","Inria,KU Leuven,Katholieke Universiteit Leuven",Dynamics,"We derive a family of efficient constrained dynamics algorithms by formulating an equivalent linear quadratic regulator (LQR) problem using Gauss' principle of least constraint and solving it using dynamic programming. Our approach builds upon the pioneering (but largely unknown) O(n+m^2d+m^3) solver by Popov and Vereshchagin (PV), where n,m and d are the number of joints, number of constraints and the kinematic tree depth respectively. We provide an expository derivation for the original PV solver and extend it to floating-base kinematic trees with constraints allowed on any link. We make new connections between the LQR's dual Hessian and the inverse operational space inertia matrix (OSIM), permitting efficient OSIM computation, which we further accelerate using matrix inversion lemma. We generalize the elimination ordering and support MuJoCo-type soft constraint models to obtain O(n+m) complexity solvers. Our numerical results indicate that significant simulation speed-up can be achieved for high dimensional robots like quadrupeds and humanoids using our algorithms as they scale better than the widely used O(nd^2+m^2d+d^2m+m^3) LTL algorithm of Featherstone."
Model-Based Co-Simulation of Flexible Mechanical Systems with Contacts Using Reduced Interface Models,"Xu Dai, Ali Raoofian, Jozsef Kovecses, Marek Teichmann","McGill University,CMLabs Simulations Inc",Dynamics,"Co-simulation is a useful approach in the modelling of robotic systems composed of multiple parts. In co-simulation, the subsystems only exchange information at communication points. The time delay of information exchange may cause error and instability. Thus, an appropriate way to determine the interface variables between the communication points is essential for efficient and stable performance, especially for real-time applications. Reduced interface models (RIMs) can be used to represent the dynamic behaviour of the subsystems at the interface in co-simulation. Such a model-based co-simulation scheme was limited to systems consisting of rigid bodies in previous studies. In this work, we introduce the formulation of RIMs for flexible multibody systems and based on that propose a general co-simulation scheme for systems consisting of both rigid body components and elements with structural flexibility. A robotic model is employed as an example to demonstrate the co-simulation scheme, where a non-smooth subsystem with contact interactions is present. The advantages of constructing RIM using flexible mechanical system models over rigid body models are also addressed by comparing the effective mass properties and the simulation results."
RIDER: Reinforcement-Based Inferred Dynamics Via Emulating Rehearsals for Robot Navigation in Unstructured Environments,"Sriram Siva, Maggie Wigness","Army Research Laboratory,U.S. Army Research Laboratory",Dynamics,"Autonomous navigation in unstructured environments is a challenging task due to the complex and dynamic nature of robot terrain interactions. Existing approaches often struggle to generalize amidst the complexities of real-world settings. They tend to rely on hand-engineered, rule-based robot models or static weightings assigned to obstacles, semantics, and other perceptual cues to estimate traversability. To address these challenges, we propose a novel approach called Reinforcement-Based Inferred Dynamics via Emulating Rehearsals (RIDER), that learns the dynamics of robot-terrain interactions within a compact latent space, capturing robot's traversability. Operating within a reinforcement learning paradigm, RIDER learns to infer its own dynamics by predicting how future robot observations and states evolve within this latent space in response to navigational behaviors. Furthermore, our approach leverages emulated rehearsals, where the robot learns within the latent space to predict its rewards and generate navigational behaviors, even when real observations have not been updated. Accordingly, RIDER equips robots with the ability to generate navigational behaviors by predicting environmental changes, and plan beyond the speed at which observations from sensors are available. Experimental results and comparisons with baseline methods establish that our proposed method outperforms other approaches in cluttered and unstructured environments and demonstrates an enhanced capacity for autonomous navigation in real-world settings."
Dynamic Multi-Agent Deep Deterministic Policy Gradient for Autonomous Navigation of Reconfigurable Unmanned Aerial Vehicle,"Xin Lu, Zegui Wu, Ruqing Zhao, Fusheng Li","University of Electronic Science and technology,University of Electronic Science and Technology of China",Distributed Robot Systems,"The reconfigurable unmanned aerial vehicle (RUAV) has the ability to create and break physical links to self-assemble and self-disassemble in midair. For the changes in task or environment, this system can dynamically disassemble the rectangular structure into multiple individual UAV modules or integrate these UAV modules into a whole. For practical applications, the R-UAV requires collaborative decision-making for autonomous navigation in complex environments. However, the navigation problem of the R-UAV has not been investigated. In this paper, we propose a dynamic multi-agent deep deterministic policy gradient (DMADDPG) algorithm for autonomous navigation of R-UAV. This algorithm introduces the leader agent assignment mechanism and a collaborative experience reward. The former deals with the action conflict problem caused by the disappearance of the UAV agent when multiple UAV modules are assembled. The latter provides guidance for the UAV agent to plan a collision-free and efficient trajectory. We validate our strategy in both simulation and practical scenarios, and experimental results demonstrate that the proposed scheme can generate reasonable and efficient paths for R-UAV in the presence of obstacles. The experiment video is available at https://youtu.be/mVm0qCvB7HY."
FogROS2-LS: A Location-Independent Fog Robotics Framework for Latency Sensitive ROS2 Applications,"Kaiyuan Eric Chen, Michael Wang, Marcus Gualtieri, Nan Tian, Christian Juette, Liu Ren, John Kubiatowicz, Ken Goldberg","University of California, Berkeley,Bosch,Bosch Research,Robert Bosch North America Research Technology Center,UC Berkeley",Distributed Robot Systems,"Limiting latency is essential for critical robot applications such as collision avoidance or target tracking and is challenging for Cloud or Fog robotics applications due to network congestion and failures. We introduce FogROS2-Latency-Sensitive(LS), a Fog Robotics framework that offers secure, location-independent connections between robots and latency-sensitive robotic services. FogROS2-LS offloads conventional on-board state estimators and feedback controllers to Cloud and Edge compute hardware without modifying the existing application in ROS2. In presence of multiple identical services, it dynamically identifies and transitions to the optimal service deployment that fulfills the application's latency requirement, thereby empowering robots with restricted on-board computing capacity to safely and efficiently navigate dynamic, human-dense environments. We evaluate FogROS2-LS with two latency sensitive case studies: (1) Collision Avoidance: a robot arm guided by visual feedback from consistent distance estimation and collision checking on Cloud and Edge. FogROS2-LS reduces collision failures by up to 8.5 times by selecting the best available machine (2) Target Tracking: FogROS2-LS also enables robust and continuous target following and can recover from network failures."
Leveraging Tethers for Distributed Formation Control of Simple Robots,"Sadie Cutler, Kirstin Hagelskjaer Petersen",Cornell University,Distributed Robot Systems,"Tethers have great potential in multi-robot systems from enabling retrieval of deployed robots and facilitating power transfer, to use by the robots as a net or partition. In this paper, we show in simulation that tethers can also be used to do distributed formation control on very simple robots. Specifically, our simulated agents are connected in series by un-actuated, flexible, fixed-length tethers and use tether angle and strain, in conjunction with the physical constraints of the tethers, to adjust their position with respect to their neighbors. This presents a significant simplification over traditional formation control which, at a minimum, requires exteroceptive sensors to perceive bearing and/or distance to nearby agents. We present and evaluate an algorithm on a large set of transitions between formations with 5 agents and an example transition with 35 agents. The convergence time grows with the number of agents, however, the memory and computation time per agent remain constant. Future work will investigate the ability to use tethers and strain for reactive behaviors and more diverse tasks."
Distributed Differential Dynamic Programming Architectures for Large-Scale Multi-Agent Control,"Augustinos Saravanos, Yuichiro Aoyama, Hongchang Zhu, Evangelos Theodorou",Georgia Institute of Technology,Distributed Robot Systems,"This paper proposes two decentralized multi-agent optimal control methods that combine the computational efficiency and scalability of Differential Dynamic Programming (DDP) and the distributed nature of the Alternating Direction Method of Multipliers (ADMM). The first one, Nested Distributed DDP (ND-DDP), is a three-level architecture which employs ADMM for consensus, an augmented Lagrangian layer for local constraints and DDP as the local optimizer. The second one, Merged Distributed DDP (MD-DDP), is a two-level architecture that addresses both consensus and local constraints with ADMM, further reducing computational complexity. Both frameworks are fully decentralized since all computations are parallelizable among the agents and only local communication is necessary. Simulation results that scale up to thousands of cars and hundreds of drones demonstrate the effectiveness of the algorithms. Superior scalability to large-scale systems against other DDP and sequential quadratic programming methods is also illustrated. Finally, hardware experiments on a multi-robot platform verify the applicability of the methods. A video with all results is provided in the supplementary material."
Accelerated K-Serial Stable Coalition for Dynamic Capture and Resource Defense,"Junfeng Chen, Zili Tang, Meng Guo",Peking University,Distributed Robot Systems,"Coalition is an important mean of multi-robot systems to collaborate on common tasks. An adaptive coalition strategy is essential for the online performance in dynamic and unknown environments. In this work, the problem of territory defense by large-scale heterogeneous robotic teams is considered. The tasks include exploration, capture of dynamic targets, and perimeter defense over valuable resources. Since each robot can choose among many tasks, it remains a challenging problem to coordinate jointly these robots such that the overall utility is maximized. This work proposes a generic coalition strategy called K-serial stable coalition algorithm. Different from centralized approaches, it is distributed and complete, meaning that only local communication is required and a K-serial Stable solution is ensured. Furthermore, to accelerate adaptation to dynamic targets and resource distribution that are only perceived online, a heterogeneous graph attention network based heuristic is learned to select more appropriate parameters and promising initial solutions during local optimization. Compared with manual heuristics or end-to-end predictors, it is shown to both improve online adaptability and retain the quality guarantee. The proposed methods are validated via large-scale simulations with 170 robots and hardware experiments of 13 robots, against several strong baselines such as GreedyNE and FastMaxSum."
Sensor-Based Multi-Robot Coverage Control with Spatial Separation in Unstructured Environments,"Xinyi Wang, Jiwen Xu, Chuanxiang Gao, Yizhou Chen, Jihan Zhang, Chenggang Wang, Yulong Ding, Ben M. Chen","The Chinese University of Hong Kong,Chinese University of Hong Kong,Shanghai Jiao Tong University,Tongji University",Distributed Robot Systems,"Multi-robot systems have increasingly become instrumental in tackling coverage problems. However, the challenge of optimizing task efficiency without compromising task success still persists, particularly in expansive, unstructured scenarios with dense obstacles. This paper presents an innovative, decentralized Voronoi-based coverage control approach to reactively navigate these complexities while guaranteeing safety. This approach leverages the active sensing capabilities of multi-robot systems to supplement GIS (Geographic Information System), offering a more comprehensive and real-time understanding of environments like post-disaster. Based on point cloud data, which is inherently non-convex and unstructured, this method efficiently generates collision-free Voronoi regions using only local sensing information through spatial decomposition and spherical mirroring techniques. Then, deadlock-aware guided map integrated with a gradient-optimized, centroid Voronoi-based coverage control policy, is constructed to improve efficiency by avoiding exhaustive searches and local sensing pitfalls. The effectiveness of our algorithm has been validated through extensive numerical simulations in high-fidelity environments, demonstrating significant improvements in task success rate, coverage ratio, and task execution time compared with others."
Localized and Incremental Probabilistic Inference for Large-Scale Networked Dynamical Systems,"Kai Matsuka, Soon-Jo Chung","California Institute of Technology,Caltech",Distributed Robot Systems,"We present new algorithms for Distributed Factor Graph Optimization (DFGO) problems that arise in the probabilistic inference of large-scale networked robotic systems. First, for the batch DFGO problem, we derive the Local Consensus ADMM (LC-ADMM) algorithm. LC-ADMM is fully localized; therefore, the computational effort, communication bandwidth, and memory for each agent scale like o(1) with respect to the network size. We establish two new theoretical results for LC-ADMM: (1) exponential convergence when the objective is strongly convex and has a Lipschitz continuous subdifferential, and (2) o(1/k) convergence when the objective is convex and has a unique solution. Second, we also develop the Incremental DFGO algorithm (iDFGO) for real-time problems by combining the ideas from LC-ADMM and the Bayes tree. The iDFGO algorithm incrementally recomputes estimates when new factors are added to the graph and is scalable with respect to both network size and time. We validate LC-ADMM and iDFGO in simulations with examples from multi-agent Simultaneous Localization and Mapping (SLAM) and power grids."
Comparison of Distributed Task Allocation Algorithms Considering Non-Ideal Communication Factors for Multi-UAV Collaborative Visit Missions,"Yan Cao, Teng Long, Jingliang Sun, Zhu Wang, Guangtong Xu","Beijing Institute of Technology,North China Electric Power University,Zhejiang University",Distributed Robot Systems,"This letter comprehensively investigates the performance of six state-of-art distributed task allocation algorithms (i.e., CBAA, CBBA, HIPC, PI, DHBA, and DGA) subject to non-ideal communication factors. The package loss, bit error, and time delay factors are considered in the distributed task allocation process. The performance of the algorithms for multi-UAV collaborative visit missions is compared under pre-allocation and dynamic allocation scenarios. The synchronous and asynchronous communication manners are separately utilized in different allocation scenarios for analyzing the effects of non-ideal communication factors. Comparison results show that bit error factors cause conflicted allocations. For the pre-allocation scenario, CBBA outperforms the competitors in terms of reliability, communication overhead, and efficiency. For the dynamic scenario, CBBA performs best optimality, while DHBA exhibits better reliability and lower overhead in harsh communication conditions."
JSTR: Joint Spatio-Temporal Reasoning for Event-Based Moving Object Detection,"Hanyu Zhou, Zhiwei Shi, Hao Dong, Shihan Peng, Yi Chang, Luxin Yan",Huazhong University of Science and Technology,Sensor Fusion II,"Event-based moving object detection is a challenging task, where static background and moving object are mixed together. Typically, existing methods mainly align the background events to the same spatial coordinate system via motion compensation to distinguish the moving object. However, they neglect the potential spatial tailing effect of moving object events caused by excessive motion, which may affect the structure integrity of the extracted moving object. We discover that the moving object has a complete columnar structure in the point cloud composed of motion-compensated events along the timestamp. Motivated by this, we propose a novel joint spatio-temporal reasoning method for event-based moving object detection. Specifically, we first compensate the motion of background events using inertial measurement unit. In spatial reasoning stage, we project the compensated events into the same image coordinate, discretize the timestamp of events to obtain a time image that can reflect the motion confidence, and further segment the moving object through adaptive threshold on the time image. In temporal reasoning stage, we construct the events into a point cloud along timestamp, and use RANSAC algorithm to extract the columnar shape in the cloud for peeling off the background. Finally, we fuse the results from the two reasoning stages to extract the final moving object region. This joint spatio-temporal reasoning framework can effectively detect the moving object from motion confidence and geometric structure. Moreover, we conduct extensive experiments on various datasets to verify that the proposed method can improve the moving object detection accuracy by 13%."
AYDIV: Adaptable Yielding 3D Object Detection Via Integrated Contextual Vision Transformer,"Tanmoy Dam, Sanjay Bhargav Dharavath, Sameer Alam, Nimrod Lilith, Supriyo Chakraborty, Mir Feroskhan","Saab NTU Joint Lab, Nanyang Technological University, Singapore,Indian Institute of Technology, Kharagpur, India,Saab-NTU Joint Lab, Nanyang Technological University, Singapore,Nanyang Technological University",Sensor Fusion II,"Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems. Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR's sparse data and the dense resolution of cameras. Besides, discrepancies in the two data representations further complicate fusion methods. We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies. AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion. AYDIV's performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset with a performance improvement of 7.40% in AP value demonstrates its efficacy in comparison to other existing fusion-based methods. Our code is publicly available at https://github.com/sanjay-810/AYDIV2."
RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale,"Han Li, Yukai Ma, Yaqing Gu, Kewei Hu, Yong Liu, Xingxing Zuo","Zhejiang University,zhejiang unicersity,Caltech",Sensor Fusion II,"We present a novel approach for metric dense depth estimation based on the fusion of a single-view image and a sparse, noisy Radar point cloud. The direct fusion of heterogeneous Radar and image data, or their encodings, tends to yield dense depth maps with significant artifacts, blurred boundaries, and suboptimal accuracy. To circumvent this issue, we learn to augment versatile and robust monocular depth prediction with the dense metric scale induced from sparse and noisy Radar data. We propose a Radar-Camera framework for highly accurate and fine-detailed dense depth estimation with four stages, including monocular depth prediction, global scale alignment of monocular depth with sparse Radar points, quasi-dense scale estimation through learning the association between Radar points and image patches, and local scale refinement of dense depth using a scale map learner. Our proposed method significantly outperforms the state-of-the-art Radar-Camera depth estimation methods by reducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2% on the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam dataset, respectively. Our code and dataset will be released at url{https://github.com/MMOCKING/RadarCam-Depth}."
HabitatDyn 2.0: Dataset for Spatial Anticipation and Dynamic Object Localization,"Zhengcheng Shen, Linh Kästner, Yi Gao, Jens Lambrecht","TU Berlin,T-Mobile, TU Berlin,Technische Universität Berlin",Sensor Fusion II,"The ability of a robot to perceive and understand its environment is crucial for its actions and behavior. Humans are adept at using semantic information for object localization and path planning, a skill that robots need to emulate for intelligent adaptation in dynamic settings. Training of the spatial anticipation ability, which can enhance spatial perception through semantic understanding, necessitates the availability of appropriate data. Although extensive research has been conducted on datasets for outdoor environments, especially in the context of autonomous driving, there is still a notable lack of datasets specifically designed for indoor environments, with a focus on dynamic object localization. This paper introduces HabitatDyn 2.0, a dataset specifically designed for enhancing object localization capabilities with semantic information from a robot's perspective. Besides RGB videos, semantic annotations, and depth information, HabitatDyn 2.0 also features top-down view labels for dynamic objects, which is required for training the spatial anticipation ability based on semantic information. Additionally, an algorithm that leverages spatial anticipation for dynamic object localization is presented, trained, and evaluated on the dataset."
Attentive Multimodal Fusion for Optical and Scene Flow,"Youjie Zhou, Guofeng Mei, Yiming Wang, Fabio Poiesi, Yi Wan","Shandong university,University of Technology Sydney,Fondazione Bruno Kessler,Shandong University",Sensor Fusion II,"This paper presents an investigation into the estimation of optical and scene flow using RGBD information in scenarios where the RGB modality is affected by noise or captured in dark environments. Existing methods typically rely solely on RGB images or fuse the modalities at later stages, which can result in lower accuracy when the RGB information is unreliable. To address this issue, we propose a novel deep neural network approach called FusionRAFT, which enables early-stage information exchange between sensor modalities (RGB and depth). Our approach incorporates self- and cross-attention layers at different network levels to fuse these modalities and construct informative features that leverage the strengths of both modalities. Through comparative experiments, we demonstrate that our approach surpasses recent methods in terms of performance on the synthetic dataset Flythings3D, as well as generalization on the real-world dataset KITTI. We illustrate that our approach exhibits enhanced robustness in the presence of noise and low-lighting conditions affecting the RGB images."
LiDAR-Camera Calibration Using Intensity Variance Cost,"Ryoichi Ishikawa, Shuyi Zhou, Yoshihiro Sato, Takeshi Oishi, Katsushi Ikeuchi","The University of Tokyo,Kyoto University of Advanced Science,Microsoft",Sensor Fusion II,"We propose an extrinsic calibration method for LiDAR-camera fusion systems using variations in intensities projected from camera images to the LiDAR point cloud. As the input, the proposed method uses a sequence of LiDAR data and camera images captured while moving the system. Once the camera motion is calculated, camera images are projected onto the point cloud. The variations in the projected intensities at each point are large in the presence of errors in the estimated motion or calibration parameters. Consequently, the extrinsic parameters are optimized for cost minimization based on the intensity variance. In addition, a suitable geometry is proposed for the calibration and verified using simulations. Our experimental results showed that the proposed method accurately performed calibrations using a camera and a sparse multi-beam LiDAR or one-dimensional LiDAR."
SRFNet: Monocular Depth Estimation with Fine-Grained Structure Via Spatial Reliability-Oriented Fusion of Frames and Events,"Tianbo Pan, Zidong Cao, Lin Wang","Hong Kong University of Science and Technology(Guangzhou),HKUST",Sensor Fusion II,"Monocular depth estimation is a crucial task to measure distance relative to a camera, which is important for applications, such as robot navigation and self-driving. Traditional frame-based methods suffer from performance drops due to the limited dynamic range and motion blur. Therefore, recent works leverage novel event cameras to complement or guide the frame modality via frame-event feature fusion. However, event streams exhibit spatial sparsity, leaving some areas unperceived, especially in regions with marginal light changes. Therefore, direct fusion methods, e.g., RAMNet, often ignore the contribution of the most confident regions of each modality. This leads to structural ambiguity in the modality fusion process, thus degrading the depth estimation performance.In this paper, we propose a novel Spatial Reliability-oriented Fusion Network (SRFNet), that can estimate depth with fine-grained structure at both daytime and nighttime. Our method consists of two key technical components. Firstly, we propose an attention-based interactive fusion (AIF) module that applies spatial priors of events and frames as the initial masks and learns the consensus regions to guide the inter-modal feature fusion. The fused feature are then fed back to enhance the frame and event feature learning. Meanwhile, it utilizes an output head to generate a fused mask, which is iteratively updated for learning consensual spatial priors. Secondly, we propose the Reliability-oriented Depth Refinement (RDR) module to estimate dense depth with the fine-grained structure based on the fused features and masks. We evaluate the effectiveness of our method on the synthetic and real-world datasets, which shows that, even without pretraining, our method outperforms the prior methods, e.g., RAMNet, especially in night scenes."
Bayesian Filtering for Homography Estimation,"Arturo Del Castillo Bernal, Philippe Decoste, James Richard Forbes",McGill University,Sensor Fusion II,"This paper considers homography estimation in a Bayesian filtering framework using rate gyro and camera measurements. The use of rate gyro measurements facilitates a more reliable estimate of homography in the presence of occlusions, while a Bayesian filtering approach generates both a homography estimate along with an uncertainty. Uncertainty information opens the door to adaptive filtering approaches, post-processing procedures, and safety protocols. In particular, herein an iterative extended Kalman filter and an interacting multiple model (IMM) filter are tested using both simulated and experimental datasets. The IMM is shown to have good consistency properties and better overall performance when compared to the state-of-the-art homography nonlinear deterministic observer in both simulations and experiments."
Saturation-Aware Angular Velocity Estimation: Extending the Robustness of SLAM to Aggressive Motions,"Simon-pierre Deschênes, Dominic Baril, Matej Boxan, Johann Laconte, Philippe Giguere, Francois Pomerleau","Université Laval,Norlab, Université Laval,French National Research Institute for Agriculture, Food and Env",Sensor Fusion II,"We propose a novel angular velocity estimation method to increase the robustness of Simultaneous Localization And Mapping (SLAM) algorithms against gyroscope saturations induced by aggressive motions. Field robotics expose robots to various hazards, including steep terrains, landslides, and staircases, where substantial accelerations and angular velocities can occur if the robot loses stability and tumbles. These extreme motions can saturate sensor measurements, especially gyroscopes, which are the first sensors to become inoperative. While the structural integrity of the robot is at risk, the robustness of the SLAM framework is oftentimes given little consideration. Consequently, even if the robot is physically capable of continuing the mission, its operation will be compromised due to a corrupted representation of the world. Regarding this problem, we propose a method to estimate the angular velocity using accelerometers during extreme rotations caused by tumbling. We show that our method reduces the median localization error by 71.5 % in translation and 65.5 % in rotation and is robust to mapping failures, which occurred in 37.5 % of the experiments without our method. We also propose the Tumbling-Induced Gyroscope Saturation (TIGS) dataset, which consists of outdoor experiments recording the motion of a mechanical lidar subject to angular velocities four times higher than other similar datasets available. The dataset is available online at https://github.com/norlab-ulaval/Norlab_wiki/wiki/TIGS-Dataset."
ContourPose: Monocular 6D Pose Estimation Method for Reflective Texture-Less Metal Parts,"Zaixing He, Quanzhi Li, Xinyue Zhao, Jin Wang, Huarong Shen, Shuyou Zhang, Jianrong Tan","Zhejiang University,Zhejiang Feihang Intelligent Technology Co., LTD",Visual Perception Applications,"Pose estimation is an essential technology for industrial robots to perform precise gripping and assembly. The state-of-the-art deep learning-based approach uses an indirect strategy, i.e., first finding local correspondence between the 2D image and 3D model, and then using the PnP and RANSAC methods to calculate the poses of ordinary objects. However, the metal parts in industry are reflective and texture-less, making it difficult to identify distinguishable point features to establish 2D-3D correspondences. To address this problem, in this paper, we propose a novel deep learning based two-stage method for pose estimation of reflective texture-less metal parts, which accurately estimates the target pose using monocular RGB images. Since contours play an important role in both keypoints prediction and pose estimation stages, our method is named ContourPose. First, an additional contour decoder is adopted to implicitly constrain the keypoints prediction in the former stage, which improves the accuracy of the keypoints prediction. Then, the predicted contour of the previous stage is taken as geometric prior that is used to iteratively solve for the optimal pose. Experiments indicate that the proposed approach for reflective texture-less metal parts has a significant improvement over the state-of-the-art approaches."
Soft Acoustic End-Effector,"Zhiyuan Zhang, Michael Koch, Daniel Ahmed","Acoustic Robotics Systems Laboratory, Institute of Robotics and ,ETH Zurich",Visual Perception Applications,"Acoustic techniques have been developed as multifunctional tools for various microscale manipulations. In prevalent design paradigms, a position-fixed piezoelectric transducer (PZT) is utilized to generate ultrasound waves. However, the immobility of the PZT restricts the modulation of the acoustic field's position and orientation, consequently diminishing the adaptability and effectiveness of subsequent acoustic micromanipulation tasks. Here, we proposed a miniaturized soft acoustic end-effector and demonstrated acoustic field modulation and microparticle manipulation by adjusting PZT position and orientation. The PZT is mounted on the end of a soft robotic arm that has three individual degrees of freedom and can be deformed in 3D space by inflating or deflating each chamber. Experiments showed that the soft acoustic end-effector can change the traveling direction of microparticles and modulate the location of a standing wave field. Our approach is simple, flexible, and controllable. We envision that the soft acoustic end-effector will facilitate multiscale acoustic manipulation in interdisciplinary applications, especially, for in vivo acoustic therapies."
Deep Learning Based 6-DoF Antipodal Grasp Planning from Point Cloud in Random Bin-Picking Task Using Single-View,"Tat Hieu Bui, Yeong Gwang Son, Seung Jae Moon, Quang Huy Nguyen, Issac Rhee, Juyong Hong, Hyouk Ryeol Choi","Sungkyunkwan University,SungKyunKwan University,Sungkyunkwan, mechanical engineering, Robottory,Sungkyunkwan University/ Robotics Innovatory,Sungkyunkwan univ",Visual Perception Applications,"Random bin picking is a crucial task in logistic centers, which is driven by E-Commerce growth. In this paper, we present an end-to-end method for 6-DoF antipodal grasps from cluttered scenes. Our approach includes two main steps: finding Potential Grasp Areas (PGAs) from depth image of the bin and detecting suitable parallel grasps in PGAs from point cloud data. To support our work, the training datasets are generated automatically in Pybullet simulation environment including 5000 depth images and above 30,000 point clouds of cluttered scenes with different number of the objects, which save time significantly for collecting and labeling. We implemented real grasping experiments with a robot arm UR10, 2-finger gripper, depth camera L515, and 10 objects arranged randomly in the bin to evaluate the efficiency of this method. It is simple, fast, and efficient to deal with many kinds of object which are random in shape, dimension, pose, and material. Video of the real robotic experiments is available at https://www.youtube.com/watch?v=cx5THPyIKjA"
Sim-To-Real Object Pose Estimation for Random Bin Picking,"Boyoung Kim, Junhong Min",Samsung Electronics,Visual Perception Applications,"In industry, random bin picking is a complex and difficult task where instance segmentation and object pose estimation based on point clouds are key processes. Recently, learning-based segmentation and pose estimation methods for 3D point clouds have been proposed. However, many of them require supervised learning with datasets with annotations of objects. Since it is difficult to annotate all stacked instances in bin picking dataset, learning without real-world datasets has become a major interest. In this paper, we introduce an instance-level object pose estimation method for bin picking, which is trained using only simulated data and seamlessly applied to real-world scenarios without additional adaptation. To enable this, we introduce a method for generating a comprehensive synthetic dataset using a physics simulator, which incorporates 3D CAD models of objects and automatically generates annotations for both segmentation and pose estimation. Our experiments, conducted on synthetic datasets, highlight the competitive performance of our method in terms of recall and accuracy. Furthermore, we demonstrate the successful integration of our approach with real robot random bin picking, resulting in significantly improved picking success rates."
Action-By-Detection: Efficient Forklift Action Detection for Autonomous Mobile Robots in Warehouses,"Alexander Prutsch, Horst Possegger, Horst Bischof",Graz University of Technology,Visual Perception Applications,"Understanding actions of other agents increases the efficiency of autonomous mobile robots (AMRs) since they encompass intention and indicate future movements. We propose a new method that allows us to infer vehicle actions using a shallow image-based classification model. The actions are classified via bird's-eye view scene crops, where we project the detections of a 3D object detection model onto a context map. We learn map context information and aggregate temporal sequence information without requiring object tracking. This results in a highly efficient classification model that can easily be deployed on embedded AMR hardware. To evaluate our approach, we create new large-scale synthetic datasets showing warehouse traffic based on real vehicle models and geometry."
RIDE: Self-Supervised Learning of Rotation-Equivariant Keypoint Detection and Invariant Description for Endoscopy,"Mert Asim Karaoglu, Viktoria Markova, Nassir Navab, Benjamin Busam, Alexander Ladikos","Technical University of Munich, ImFusion GmbH,ImFusion GmbH,TU Munich,Technical University of Munich,ImFusion",Visual Perception Applications,"Unlike in natural images in endoscopy there is no clear notion of an up-right camera orientation. Endoscopic videos therefore often contain large rotational motions, which require keypoint detection and description algorithms to be robust to these conditions. While most classical methods achieve rotation-equivariant detection and invariant description by design, many learning-based approaches learn to be robust only up to a certain degree. At the same time learning-based methods under moderate rotations often outperform classical approaches. In order to address this shortcoming, in this paper we propose RIDE, a learning-based method for rotation-equivariant detection and invariant description. Following recent advancements in group-equivariant learning, RIDE models rotation equivariance implicitly within its architecture. Trained in a self-supervised manner on a large curation of endoscopic images, RIDE requires no manual labeling of training data. We test RIDE in the context of surgical tissue tracking on the SuPeR dataset as well as in the context of relative pose estimation on a repurposed version of the SCARED dataset. In addition we perform explicit studies showing its robustness to large rotations. Our comparison against recent learning-based and classical approaches shows that RIDE sets a new state-of-the-art performance on matching and relative pose estimation tasks and scores competitively on surgical tissue tracking."
LLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery,"Kexin Chen, Du Yuyang, Tao You, Mobarakol Islam, Ziyu Guo, Yueming Jin, Guangyong Chen, Pheng Ann Heng","Department of Computer Science Engineering, the Chinese Universi,the Chinese University of Hong Kong,National University of Singapore,University College London,The Chinese University of Hong Kong,Shenzhen Institute of Advanced Technology, Chinese Academy of Sc",Visual Perception Applications,"Visual question answering (VQA) can be fundamentally crucial for promoting robotic-assisted surgical education. In practice, the needs of trainees are constantly evolving, such as learning more surgical types and adapting to new surgical instruments/techniques. Therefore, continually updating the VQA system by a sequential data stream from multiple resources is demanded in robotic surgery to address new tasks. In surgical scenarios, the privacy issue of patient data often restricts the availability of old data when updating the model, necessitating an exemplar-free continual learning (CL) setup. However, prior studies overlooked two vital problems of the surgical domain: i) large domain shifts from diverse surgical operations collected from multiple departments or clinical centers, and ii) severe data imbalance arising from the uneven presence of surgical instruments or activities during surgical procedures. This paper proposes to address these two problems with a multimodal large language model (LLM) and an adaptive weight assignment methodology. We first develop a new multi-teacher CL framework that leverages a multimodal LLM as the additional teacher. The strong generalization ability of the LLM can bridge the knowledge gap when domain shifts and data imbalances occur. We then put forth a novel data processing method that transforms complex LLM embeddings into logits compatible with our CL framework. We also design an adaptive weight assignment approach that balances the generalization ability of the LLM and the domain expertise of the old CL model. Finally, we construct a new dataset for surgical VQA tasks. Extensive experimental results demonstrate the superiority of our method to other advanced CL models."
Procedure Recognition by Knowledge-Driven Segmentation in Robotic-Assisted Vitreoretinal Surgery,"Zhen Li, Ya-wen Deng, Qiang Ye, Weihong Yu, Haoxiang Qi, Yaliang Liu, Zhangguo Yu, Gui-Bin Bian","Institute of Automation, Chinese Academy of Sciences,Beijing Institute of Technology,Peking Union Medical College Hospital",Visual Perception Applications,"Internal limiting membrane (ILM) peeling is a vital vitreoretinal surgery procedure. However, due to the thickness of just 1-2 micrometers and the intricacies associated with its varying density and adhesion, the difficulty of manipulation exceeds the physiological limits of human perception and operation. Surgical robot is characterized by high precision and stability. However, navigating intricate intraocular environments and handling minuscule high-precision areas remain enormous challenges. These include issues of uneven lighting, field-of-view loss, and motion blur. This paper proposed a perception method named 'Multimodal Surgical Process Recognition based on Domain Knowledge and Segmentation (MSPR-DKS),' designed to address these challenges and provide input for the precise control of robots. Moreover, a comprehensive dataset focused on ILM peeling during macular hole surgeries was established. Experimental results underscore the efficacy of this approach, with segmentation accuracies exceeding 99.27% for instruments and macular holes and an average accuracy of 98.97% in recognizing surgical processes. This study paves the way for leveraging domain knowledge and image segmentation to improve robot-assisted manipulation of soft tissues in ophthalmology."
Weighting Online Decision Transformer with Episodic Memory for Offline-To-Online Reinforcement Learning,"Xiao Ma, Wu-jun Li",Nanjing University,Learning in Control II,"Offline reinforcement learning (RL) has been shown to be successfully modeled as a sequence modeling problem, drawing inspiration from the success of Transformers. Offline RL is often limited by the quality of the offline dataset, so offline-to-online RL is a more realistic setting. Online decision transformer (ODT) is an effective and representative sequence modeling-based offline-to-online RL method. Despite its effectiveness, ODT still suffers from the sample inefficiency problem during the online fine-tuning phase. This sample inefficiency problem arises because the agent treats all state-action pairs in the replay buffer equally when trying to learn from the replay buffer. In this paper, we propose a simple yet effective method, called weighting online decision transformer with episodic memory (WODTEM), to improve sample efficiency. We first attempt to introduce an episodic memory (EM) mechanism into the sequence modeling-based RL methods. By utilizing the EM mechanism, we propose a novel training objective with a weighting function, based on ODT, to improve sample efficiency. Experimental results on multiple tasks show that WODTEM can improve sample efficiency."
COMPOSER: Scalable and Robust Modular Policies for Snake Robots,"Yuyou Zhang, Yaru Niu, Xingyu Liu, Ding Zhao","Carnegie Mellon University,Carnegie mellon university",Learning in Control II,"Snake robots have showcased remarkable compliance and adaptability in their interaction with environments as their natural counterparts. While their hyper-redundant and high-dimensional characteristics add to this adaptability, they also pose great challenges to robot control. Instead of perceiving the hyper-redundancy and flexibility of snake robots as mere challenges, there lies an unexplored potential in leveraging these traits to enhance robustness and generalizability at the control policy level. We seek to develop a control policy that effectively breaks down the high dimensionality of snake robots while harnessing their redundancy. In this work, we consider the snake robot as a modular robot and formulate the control of the snake robot as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. Each segment of the snake robot is an agent, using its local observation to independently determine its actions. Specifically, we incorporate a self-attention mechanism to enhance the cooperative behavior between agents. A high-level imagination policy is trained to provide additional rewards to guide the low-level control policy. We validate the proposed method COMPOSER with five snake robot tasks, including goal reaching, wall climbing, shape formation, tube crossing, and block pushing. COMPOSER achieves the highest success rate across all tasks when compared to a centralized baseline and four modular policy baselines. Additionally, we show enhanced robustness against module corruption and significantly superior zero-shot generalizability in our proposed method. The videos of this work are available on our project page: https://sites.google.com/view/composer-snake/."
Barrier Functions Inspired Reward Shaping for Reinforcement Learning,"Nilaksh Nilaksh, Abhishek Ranjan, Shreenabh Agrawal, Aayush Jain, Pushpak Jagtap, Shishir Kolathaya","Indian Institue of Technology, Kharagpur,Indian Institute of Science Bangalore,Indian Institute of Science, Bangalore,Indian Institute of Technology Kharagpur,Indian Institute of Science",Learning in Control II,"Reinforcement Learning (RL) has progressed from simple control tasks to complex real-world challenges with large state spaces. While RL excels in these tasks, training time remains a limitation. Reward shaping is a popular solution, but existing methods often rely on value functions, which face scalability issues. This paper presents a novel safety-oriented reward-shaping framework inspired by barrier functions, offering simplicity and ease of implementation across various environments and tasks. To evaluate the effectiveness of the proposed reward formulations, we conduct simulation experiments on CartPole, Ant, and Humanoid environments, along with real-world deployment on the Unitree Go1 quadruped robot. Our results demonstrate that our method leads to 1.4-2.8 times faster convergence and as low as 50-60% actuation effort compared to the vanilla reward. In a sim-to-real experiment with the Go1 robot, we demonstrated better control and dynamics of the bot with our reward framework."
AdaptAUG: Adaptive Data Augmentation Framework for Multi-Agent Reinforcement Learning,"Xin Yu, Yongkai Tian, Li Wang, Pu Feng, Wenjun Wu, Rongye Shi",Beihang University,Learning in Control II,"Multi-agent reinforcement learning has emerged as a promising approach for the control of multi-robot systems. Nevertheless, the low sample efficiency of MARL poses a significant obstacle to its broader application in robotics. While data augmentation appears to be a straightforward solution for improving sample efficiency, it usually incurs training instability, making the sample efficiency worse. Moreover, manually choosing suitable augmentations for a variety of tasks is a tedious and time-consuming process. To mitigate these challenges, our research theoretically analyzes the implications of data augmentation on MARL algorithms. Guided by these insights, we present AdaptAUG, an adaptive framework designed to selectively identify beneficial data augmentations, thereby achieving superior sample efficiency and overall performance in multi-robot tasks. Extensive experiments in both simulated and real-world multi-robot scenarios validate the effectiveness of our proposed framework."
HyperPPO: A Scalable Method for Finding Small Policies for Robotic Control,"Shashank Hegde, Zhehui Huang, Gaurav Sukhatme",University of Southern California,Learning in Control II,"Models with fewer parameters are necessary for the neural control of memory-limited, performant robots. Finding these smaller neural network architectures requires repetitive experimentation and can be time-consuming. We propose HyperPPO, an on-policy reinforcement learning algorithm that utilizes graph hypernetworks to estimate the weights of multiple architectures simultaneously. Our method is capable of estimating weights for policies that are much smaller than commonly used networks yet can represent high-performing policies. We obtain multiple trained policies at the same time while maintaining sample efficiency and provide the user the choice of picking a network architecture that satisfies their inference compute constraints. We show that our method scales well - more training resources produce faster convergence to higher-performing architectures. We also demonstrate that the neural policies estimated by HyperPPO are capable of decentralized control of a Crazyflie2.1 quadrotor. Project website: https://sites.google.com/usc.edu/hyperppo"
Grow Your Limits: Continuous Improvement with Real-World RL for Robotic Locomotion,"Laura Smith, Yunhao Cao, Sergey Levine",UC Berkeley,Learning in Control II,"Deep reinforcement learning can enable robots to autonomously acquire complex behaviors such as legged locomotion. However, RL in the real world is complicated by constraints on efficiency, safety, and overall training stability, which limits its practical applicability. We present APRL, a policy regularization framework that modulates the robot's exploration throughout training, striking a balance between flexible improvement potential and focused, efficient exploration. APRL enables a quadrupedal robot to efficiently learn to walk entirely in the real world within minutes and continue to improve with more training where prior work saturates in performance. We demonstrate that continued training with APRL results in a policy that is substantially more capable of navigating challenging situations and adapts to changes in dynamics. Videos and code to reproduce our results are available at: https://sites.google.com/berkeley.edu/aprl"
Torque-Based Deep Reinforcement Learning for Task-And-Robot Agnostic Learning on Bipedal Robots Using Sim-To-Real Transfer,"Donghyeon Kim, Glen Berseth, Mathew Schwartz, Jaeheung Park","Graduate School of Convergence Science and Technology, Seoul Nat,Université de Montréal,New Jersey Institute of Technology,Seoul National University",Learning in Control II,"In this paper, we review the question of which action space is best suited for controlling a real biped robot in combination with Sim2Real training. Position control has been popular as it has been shown to be more sample efficient and intuitive to combine with other planning algorithms. However, for position control gain tuning is required to achieve the best possible policy performance. We show that instead, using a torque-based action space enables task-and-robot agnostic learning with less parameter tuning and mitigates the sim-to-reality gap by taking advantage of torque control's inherent compliance. Also, we accelerate the torque-based-policy training process by pre-training the policy to remain upright by compensating for gravity. The paper showcases the first successful sim-to-real transfer of a torque-based deep reinforcement learning policy on a real human-sized biped robot."
Decentralized Motor Skill Learning for Complex Robotic Systems,"Yanjiang Guo, Zheyuan Jiang, Yen-jen Wang, Jingyue Gao, Jianyu Chen","Tsinghua university,Tsinghua University",Learning in Control II,"Reinforcement learning (RL) has achieved remarkable success in complex robotic systems (eg. quadruped locomotion). In previous works, the RL-based controller was typically implemented as a single neural network with concatenated observation input. However, the corresponding learned policy is highly task-specific. Since all motors are controlled in a centralized way, out-of-distribution local observations can impact global motors through the single coupled neural network policy. In contrast, animals and humans can control their limbs separately. Inspired by this biological phenomenon, we propose a Decentralized motor skill (DEMOS) learning algorithm to automatically discover motor groups that can be decoupled from each other while preserving essential connections and then learn a decentralized motor control policy. Our method improves the robustness and generalization of the policy without sacrificing performance. Experiments on quadruped and humanoid robots demonstrate that the learned policy is robust against local motor malfunctions and can be transferred to new tasks."
Toward Optimal Tabletop Rearrangement with Multiple Manipulation Primitives,"Baichuan Huang, Xujia Zhang, Jingjin Yu","Rutgers University,Southern University of Science and Technology",Manipulation Planning,"In practice, many types of manipulation actions (e.g., pick-n-place and push) are needed to accomplish real-world manipulation tasks. Yet, limited research exists that explores the synergistic integration of different manipulation actions for optimally solving long-horizon task-and-motion planning problems. In this study, we propose and investigate planning high-quality action sequences for solving long-horizon tabletop rearrangement tasks in which multiple manipulation primitives are required. Denoting the problem rearrangement with multiple manipulation primitives (REMP), we develop two algorithms, hierarchical best-first search (HBFS) and parallel Monte Carlo tree search for multi-primitive rearrangement (PMMR) toward optimally resolving the challenge. Extensive simulation and real robot experiments demonstrate that both methods effectively tackle REMP, with HBFS excelling in planning speed and PMMR producing human-like, high-quality solutions with a nearly 100% success rate. Source code and supplementary materials will be available at https://github.com/arc-l/remp."
ReorientDiff: Diffusion Model Based Reorientation for Object Manipulation,"Utkarsh Aashu Mishra, Yongxin Chen",Georgia Institute of Technology,Manipulation Planning,"The ability to manipulate objects in desired configurations is a fundamental requirement for robots to complete various practical applications. While certain goals can be achieved by picking and placing the objects of interest directly, object reorientation is needed for precise placement in most of the tasks. In such scenarios, the object must be reoriented and re-positioned into intermediate poses that facilitate accurate placement at the target pose. To this end, we propose a reorientation planning method, ReorientDiff, that utilizes a diffusion model-based approach. The proposed method employs both visual inputs from the scene, and goal-specific language prompts to plan intermediate reorientation poses. Specifically, the scene and language-task information are mapped into a joint scene-task representation feature space, which is subsequently leveraged to condition the diffusion model. The diffusion model samples intermediate poses based on the representation using classifier-free guidance and then uses gradients of learned feasibility-score models for implicit iterative pose-refinement. The proposed method is evaluated using a set of YCB-objects and a suction gripper, demonstrating a success rate of 95.2% in simulation. Overall, we present a promising approach to address the reorientation challenge in manipulation by learning a conditional distribution, which is an effective way to move towards generalizable object manipulation. More results can be found on our website: utkarshmishra04.github.io/ReorientDiff"
ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals,"Jeremy Collins, Cody Houff, You Liang Tan, Charlie Kemp","Georgia Institute of Technology,Hello Robot Inc.",Manipulation Planning,"We present ForceSight, a system for text-guided mobile manipulation that predicts visual-force goals using a text-conditioned vision transformer. Given a single RGBD image and a text prompt, ForceSight determines a target end-effector pose in the camera frame (kinematic goal) and the associated forces (force goal). Together, these two components form a visual-force goal. Prior work has demonstrated that deep models outputting human-interpretable kinematic goals can enable dexterous manipulation by real robots. Forces are critical to manipulation, yet have typically been relegated to low-level execution in these systems. When deployed on a mobile manipulator equipped with an eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps, drawer opening, and object handovers with an 81% success rate in unseen environments with object instances that differed significantly from the training data. In a separate experiment, relying exclusively on visual servoing and ignoring force goals dropped the success rate from 90% to 45%, demonstrating that force goals can significantly enhance performance. The appendix, videos, code, and trained models are available at https://force-sight.github.io/."
Unknown Object Retrieval in Confined Space through Reinforcement Learning with Tactile Exploration,"Xinyuan Zhao, Wenyu Liang, Xiaoshi Zhang, Chee Meng Chew, Yan Wu","Agency for Science, Technology and Research,Institute for Infocomm Research, A*STAR,National University of Singapore,A*STAR Institute for Infocomm Research",Manipulation Planning,"The potential of tactile sensing for dexterous robotic manipulation has been demonstrated by its ability to enable nuanced real-world interactions. In this study, the retrieval of unknown objects from confined spaces, which is unsuitable for conventional visual perception and gripper-based manipulation, is identified and addressed. Specifically, a tactile-sensorized tool stick that well fits in the narrow space is utilized to provide multi-point contact sensing in object manipulation. A reinforcement learning (RL) agent with a hybrid action space is then proposed to acquire the optimal policy for manipulating the objects without prior knowledge of their physical properties. To accelerate on-hardware training, a focused training strategy is adopted with the hypothesis that an agent trained on a small set of representative shapes can be generalized to a wide range of everyday objects. Additionally, a curriculum on terminal goals is designed to further accelerate the hardware-based training process. Comparative experiments and ablation studies have been conducted to evaluate the effectiveness and robustness of the proposed approach, which highlights the high success rate of our solution for retrieving everyday objects."
The Grasp Loop Signature: A Topological Representation for Manipulation Planning with Ropes and Cables,"Peter Mitrano, Dmitry Berenson",University of Michigan,Manipulation Planning,"Robotic manipulation of deformable, one-dimensional objects (DOOs) like ropes or cables has important potential applications in manufacturing, agriculture, and surgery. In such environments, the task may involve threading through or avoiding becoming tangled with objects like racks or frames. Grasping with multiple grippers can create closed loops between the robot and DOO, and If an obstacle lies within this loop, it may be impossible to reach the goal. However, prior work has only considered the topology of the DOO in isolation, ignoring the arms that are manipulating it. Searching over possible grasps to accomplish the task without considering such topological information is very inefficient, as many grasps will not lead to progress on the task due to topological constraints. Therefore, we propose a grasp loop signature which categorizes the topology of these grasp loops and show how it can be used to guide planning. We perform experiments in simulation on two DOO manipulation tasks to show that using the signature is faster and succeeds more often than methods that rely on local geometry or finite-horizon planning. Finally, we demonstrate using the signature in the real world to manipulate a cable in a scene with obstacles using a dual-arm robot."
Articulated Object Manipulation with Coarse-To-Fine Affordance for Mitigating the Effect of Point Cloud Noise,"Suhan Ling, Yian Wang, Ruihai Wu, Shiguang Wu, Yuzheng Zhuang, Tianyi Xu, Yu Li, Chang Liu, Hao Dong","Peking University,Umass Amherst,Chinese Academy of Sciences Beijing, China,Huawei Technologies Company,BUPT",Manipulation Planning,"3D articulated objects are inherently challenging for manipulation due to the varied geometries and intricate functionalities associated with articulated objects. Point-level affordance, which predicts the per-point actionable score and thus proposes the best point to interact with, has demonstrated excellent performance and generalization capabilities in articulated object manipulation. However, a significant challengeremains: while previous works use perfect point cloud generated in simulation, the models cannot directly apply to the noisy point cloud in the real-world. To tackle this challenge, we leverage the property of real-world scanned point cloud that, the point cloud becomes less noisy when the camera is closer to the object. Therefore, we propose a novel coarse-to-fine affordance learning pipeline to mitigate the effect of point cloud noise in two stages. In the first stage, we learn the affordance on the noisy far point cloud which includes the whole object to propose the approximated place to manipulate. Then, we move the camera in the front of the approximated place, scan a less noisy point cloud containing precise local geometries for manipulation, and learn affordance on such point cloud to propose fine-grained final actions. The proposed method is thoroughly evaluated both using large-scale simulated noisy point clouds mimicing real-world scans, and in the real world scenarios, with superiority over existing methods, demonstrating the effectiveness in tackling the noisy real-world point cloud problem."
Improved M4M: Faster and Richer Planning for Manipulation among Movable Objects in Cluttered 3D Workspaces,"Dhruv Saxena, Maxim Likhachev","The Robotics Institute, Carnegie Mellon University,Carnegie Mellon University",Manipulation Planning,"We are interested in enabling robots to solve difficult pick-and-place manipulation tasks in cluttered and constrained environments. If the robot does not have collision-free access to the object-of-interest (OoI) which it intends to grasp and extract from the workspace, it must reason about which movable objects to rearrange, where to move them, and how it may do so. In recent work we introduced E-M4M, a graph search-based solver for solving such Manipulation tasks Among Movable Objects (MAMO). In this paper we make several improvements to E-M4M - we introduce the use of prehensile or pick-and-place rearrangement actions in addition to pushes; we show that by running it as a depth-first search improves performance; we show how the search can be run ``eagerly lazily'' to only simulate actions in a physics-based simulator when necessary; finally we relax the assumption that we require perfect knowledge of the physical properties of objects (mass and coefficient of friction in particular). The improved version of E-M4M presented in this paper, I-M4M, is a faster and more versatile MAMO solver with a rich action space. We discuss the impact of the improvements we make in an extensive simulation study and show previously unachievable results on a real-world PR2 robot."
Preprocessing-Based Kinodynamic Motion Planning Framework for Intercepting Projectiles Using a Robot Manipulator,"Ramkumar Natarajan, Hanlan Yang, Qintong Xie, Yash Oza, Manash Pratim Das, Fahad Islam, Muhammad Suhail Saleem, Howie Choset, Maxim Likhachev","Robotics Institute, Carnegie Mellon University,Carnegie Mellon University,University of Oxford,Amazon Robotics",Manipulation Planning,"We are interested in studying sports with robots and starting with the problem of intercepting a projectile moving toward a robot manipulator equipped with a shield. To successfully perform this task, the robot needs to (i) detect the incoming projectile, (ii) predict the projectile's future motion, (iii) plan a minimum-time rapid trajectory that can evade obstacles and intercept the projectile, and (iv) execute the planned trajectory. These four steps must be performed under the manipulator's dynamic limits and extreme time constraints ("
MIM: Indoor and Outdoor Navigation in Complex Environments Using Multi-Layer Intensity Maps,"Adarsh Jagan Sathyamoorthy, Kasun Weerakoon, Mohamed Elnoor, Mason Russell, Jason Pusey, Dinesh Manocha","University of Maryland,University of Maryland, College Park,Army Research Laboratory,U.S. Army Research Laboratory (ARL)",Collision Avoidance III,"We present MIM (Multi-Layer Intensity Map), a novel 3D object representation for robot perception and autonomous navigation. MIMs consist of multiple stacked layers of 2D grid maps each derived from reflected point cloud intensities corresponding to a certain height interval. The different layers of MIMs can be used to simultaneously estimate obstacles' height, solidity/density, and opacity. We demonstrate that MIMs' can help accurately differentiate obstacles that are safe to navigate through (e.g. beaded/string curtains, pliable tall grass), from ones that must be avoided (e.g. transparent surfaces such as glass walls, bushes, trees, etc.) in indoor and outdoor environments. Further, to handle narrow passages, and navigate through non-solid obstacles in dense environments, we propose an approach to adaptively inflate or enlarge the obstacles detected on MIMs based on their solidity, and the robot's preferred velocity direction. We demonstrate these improved navigation capabilities in real-world narrow, dense environments using a real Turtlebot and Boston Dynamics Spot. We observe significant increases in success rates to more than 50%, up to a 9.5% decrease in normalized trajectory length, and up to a 22.6% increase in the F-score compared to current navigation methods using other sensor modalities."
Gaussian Process-Based Traversability Analysis for Terrain Mapless Navigation,"Abraham Abe Leininger, Mahmoud Ali, Hassan Jardali, Lantao Liu",Indiana University,Collision Avoidance III,"Efficient navigation through uneven terrain remains a challenging endeavor for autonomous robots. We propose a new geometric-based uneven terrain mapless navigation framework combining a Sparse Gaussian Process (SGP) local map with a Rapidly-Exploring Random Tree* (RRT*) planner. Our approach begins with the generation of a high-resolution SGP local map, providing an interpolated representation of the robot's immediate environment. This map captures crucial environmental variations, including height, uncertainties, and slope characteristics. Subsequently, we construct a traversability map based on the SGP representation to guide our planning process. The RRT* planner efficiently generates real-time navigation paths, avoiding untraversable terrain in pursuit of the goal. This combination of SGP-based terrain interpretation and RRT* planning enables ground robots to safely navigate environments with varying elevations and steep obstacles. We evaluate the performance of our proposed approach through robust simulation testing, highlighting its effectiveness in achieving safe and efficient navigation compared to existing methods. See the project GitHub for source code and supplementary materials, including a video demonstrating experimental results."
Active Collision-Based Navigation for Wheeled Robots,"Jingjing Li, Jialin Ji, Qianhao Wang, Huan Yu, Yu Pan, Fei Gao",Zhejiang University,Collision Avoidance III,"Collision is typically avoided in robot navigation for safety guarantee. However, when a robot's exteroceptive sensors fail, which means it becomes ""blind"", collision can actually be leveraged to improve localization performance. Our research demonstrates the informative nature of collisions in this context.Moreover, we show that a robot is able to navigate in a known environment with only proprioceptive sensors by actively colliding with its surroundings for more reliable localization. Firstly, we design a collision-based observation model, which is differentiable and can be easily applied to various estimators. Secondly, we integrate this model into a collision-aided localization framework and implement it in two widely used estimators, the Kalman filter and the particle filter. Thirdly, we propose an active collision path planning method, which effectively reduces localization uncertainty."
Graph-Based 3D Collision-Distance Estimation Network with Probabilistic Graph Rewiring,"Minjae Song, Yeseung Kim, Min Jun Kim, Daehyung Park","KAIST,Korea Advanced Institute of Science and Technology, KAIST",Collision Avoidance III,"We aim to solve the problem of data-driven collision-distance estimation given 3-dimensional (3D) geometries. Conventional algorithms suffer from low accuracy due to their reliance on limited representations, such as point clouds. In contrast, our previous graph-based model, GraphDistNet, achieves high accuracy using edge information but incurs higher message-passing costs with growing graph size, limiting its applicability to 3D geometries. To overcome these challenges, we propose GDN-R, a novel 3D graph-based estimation network.GDN-R employs a layer-wise probabilistic graph-rewiring algorithm leveraging the differentiable Gumbel-top-K relaxation. Our method accurately infers minimum distances through iterative graph rewiring and updating relevant embeddings. The probabilistic rewiring enables fast and robust embedding with respect to unforeseen categories of geometries. Through 41,412 random benchmark tasks with 150 pairs of 3D objects, we show GDN-R outperforms state-of-the-art baseline methods in terms of accuracy and generalizability. We also show that the proposed rewiring improves the update performance reducing the size of the estimation model. We finally show its batch prediction and auto-differentiation capabilities for trajectory optimization in both simulated and real-world scenarios."
Jump Over Block (JOB): An Efficient Line-Of-Sight Checker for Grid/voxel Maps with Sparse Obstacles,"Zhuo Yao, Wei Wang, Jiadong Zhang, Yan Wang, Jinjiang Li","Beihang University,Beihang university, school of mechanical engineering and automat,Beihang",Collision Avoidance III,"Line-Of-Sight (LOS) check plays a crucial role in collision avoidance and time comsuming, particularly in scenarios involving large-scale maps with sparse obstacles, as it necessitates a grid-by-grid state check. Specifically, LOS check consumes more than half of the computational time in any-angle path planning algorithms, such as Theta*, Visibility Graph, and RRT. To address this issue, we propose an efficient LOS checker for maps of arbitrary dimensions with sparse obstacles. Our approach involves a two-step process. Firstly, we partition the passable space into blocks until there is no vacancy for a minimum-sized block. When the adapted Bresenham algorithm reaches a surface of a block, it bypasses grid-by-grid traversal within the block and directly jumps to the opposing surface. This method significantly reduces the number of grids examined, resulting in higher efficiency compared to traditional LOS checks. We refer to our approach as Jump Over Block (JOB). To demonstrate the advantages of JOB, we compare its performance against traditional LOS checks using a widely recognized public dataset. The results indicate that JOB incurs only 1/6 to 1/5 of the computational cost associated with raw LOS checks, making it a valuable tool for both researchers and practitioners in the field. In order to facilitate further research within the community, we have made the source code of the proposed algorithm publicly available."
Conformal Predictive Safety Filter for RL Controllers in Dynamic Environments,"Kegan Strawn, Nora Ayanian, Lars Lindemann","University of Southern California,Brown University",Collision Avoidance III,"The interest in using reinforcement learning (RL) controllers in safety-critical applications such as robot navigation around pedestrians motivates the development of additional safety mechanisms. Running RL-enabled systems among uncertain dynamic agents may result in high counts of collisions and failures to reach the goal. The system could be safer if the pre-trained RL policy was uncertainty-informed. For that reason, we propose conformal predictive safety filters that: 1) predict the other agentsâ€™ trajectories, 2) use statistical techniques to provide uncertainty intervals around these predictions, and 3) learn an additional safety filter that closely follows the RL controller but avoids the uncertainty intervals. We use conformal prediction to learn uncertainty-informed predictive safety filters, which make no assumptions about the agentsâ€™ distribution. The framework is modular and outperforms the existing controllers in simulation. We demonstrate our approach with multiple experiments in a collision avoidance gym environment and show that our approach minimizes the number of collisions without making overly conservative predictions."
GrASPE: Graph Based Multimodal Fusion for Robot Navigation in Outdoor Environments,"Kasun Weerakoon, Adarsh Jagan Sathyamoorthy, Jing Liang, Tianrui Guan, Utsav Patel, Dinesh Manocha","University of Maryland, College Park,University of Maryland",Collision Avoidance III,"We present a novel trajectory traversability estimation and planning algorithm for robot navigation in complex outdoor environments. We incorporate multimodal sensory inputs from an RGB camera, 3D LiDAR, and the robot's odometry sensor to train a prediction model to estimate candidate trajectories' success probabilities based on partially reliable multi-modal sensor observations. We encode high-dimensional multi-modal sensory inputs to low-dimensional feature vectors using encoder networks and represent them as a connected graph. The graph is then used to train an attention-based Graph Neural Network (GNN) to predict trajectory success probabilities. We further analyze the number of features in the image (corners) and point cloud data (edges and planes) separately to quantify their reliability to augment the weights of the feature graph representation used in our GNN. During runtime, our model utilizes multi-sensor inputs to predict the success probabilities of the trajectories generated by a local planner to avoid potential collisions and failures. Our algorithm demonstrates robust predictions when one or more sensor modalities are unreliable or unavailable in complex outdoor environments. We evaluate our algorithm's navigation performance using a Spot robot in real-world outdoor environments. We observe an increase of 10-30% in terms of navigation success rate and up to 15% increase in AU-ROC compared to the state-of-the-art navigation methods."
E-RRT*: Path Planning for Hyper-Redundant Manipulators,"Hongcheng Ji, Haibo Xie, Cheng Wang, Huayong Yang","Zhejiang University,ZheJiang University",Collision Avoidance III,"A hyper-redundant manipulator(HRM) can flexibly accomplish tasks in narrow spaces. However, its excessive degrees of freedom pose challenges for path planning. In this article, an ellipsoid-shape rapidly-exporing random tree (E-RRT*) method is proposed for path planning of HRMs in workspace, particularly those with angle limits. This method replaces line segments with ellipsoids to connect adjacent nodes. Firstly, an analysis of angle constraints of the HRM is conducted, providing restrictions on node selection during path planning. Secondly, a slow-speed informed guiding approach is introduced to optimize the sampling process. Finally, the obtained path is enhanced by adding control points and applying cubic polynomial interpolation to achieve path smoothing. Simulations demonstrate that the proposed E-RRT* method effectively solves the path planning problem for HRMs. Especially in narrow environments, appropriate informed guiding speeds enable E-RRT* to outperform other methods."
LiDAR-Based Online Control Barrier Function Synthesis for Safe Navigation in Unknown Environments,"Shaghayegh Keyumarsi, Made Widhi Surya Atman, Azwirman Gusrialdi",Tampere University,Collision Avoidance III,"This paper presents a novel extension of the Control Barrier Function (CBF) as the low-level safety controller for autonomous mobile robots navigating in unknown environments. The main challenges of implementing CBF in real-world situations arise from the absence of a model or the lack of an exact one for the environment. Additionally, online learning is needed for the robot to maneuver in an unknown environment which leads to dealing with the sampled data set size, memory, and computational complexity. We address these challenges by designing an online non-parametric Lidar-based safety function using the Gaussian process (GP). It is both efficient in data size and eliminates the requirement to store previous data. Then, a CBF is synthesized using the proposed safety function to rectify the safe control input. The effectiveness of the Lidar-based CBF synthesis for navigation in unknown environments was validated by conducting experiments on unicycle-type robots."
"A Soft, Lightweight Flipping Robot with Versatile Motion Capabilities for Wall-Climbing Applications","Rui Chen, Xinrui Tao, Changyong (chase) Cao, Pei Jiang, Jun Luo, Yu Sun","Chongqing University,Case Western Reserve University,University of Toronto",Soft Robot Applications II,"Soft wall-climbing robots have been limited in their ability to perform complex locomotion in diverse environments due to their structure and weight. Thus far, soft wall-climbing robots with integrated functions that can locomote in complex 3D environments are yet to be developed. This study addresses this challenge by presenting a lightweight (2.57 g) soft wall-climbing robot with integrated linear, turning, and transitioning motion capabilities. The soft robot employs three pneumatic bending actuators and two adaptive electroadhesion (EA) pads, which enable it to flip forward, transition between two walls, turn in two directions, and adhere to various surfaces. Different motion and control strategies are proposed based on a theoretical model. The experimental results demonstrate that the robot can move at an average speed of 3.85 mm/s on horizontal, vertical, and inverted walls and make transitions between walls with different pinch angles within 180Â°. Additionally, the soft robot can carry a miniature camera on vertical walls to perform detection and surveillance tasks. This work provides a reliable structure and control strategy to enhance the multifunctionalit"
Tetraflex: A Multigait Soft Robot for Object Transportation in Confined Environments,"Peter Wharton, Tsam Lung You, George Jenkinson, Richard Suphapol Diteesawat, Nguyen Hao Le, Edith-Clare Hall, Martin Garrad, Andrew Conn, Jonathan Rossiter",University of Bristol,Soft Robot Applications II,"Unstructured environments call for versatile robots with adaptable morphology that can perform multiple goal-directed actions including locomotion in confined spaces, environmental mapping, object retrieval and object manipulation. In response to these challenges, we present the Polyflex design concept for fabrication of modular, soft truss robots and demonstrate its varied capabilities in a tetrahedral robot (Tetraflex). Tetraflex is composed of six pneumatically actuated bellows joined at four points by rigid nodes. By extending or contracting the bellows, Tetraflex is capable of large size and shape change, and rolling, crawling and bounding gaits. Furthermore, Tetraflex is able to roll onto and engulf objects then subsequently transport them with the crawling gait. The rolling gait discretises Tetraflexâ€™s locomotion into predictable steps on a triangular grid, simplifying odometry and allowing the use of path planning to attain a desired position. The size of rolling step can be changed at any time by dynamically varying the size of the robot. The crawling and bounding gaits enable Tetraflex to move in smaller incremental steps or through narrow passages (80 mm wide). The maximum speed was attained with a bounding locomotion gait at 19.6 mm/s (0.15 body lengths per second, or BL/s). Rolling locomotion attained between 15.6 and 19.4 mm/s (0.12-0.15 BL/s), and crawling 7.8 mm/s (0.06 BL/s). The rolling gait was the most accurate gait, achieving 2.3% linear deviation."
A Strong Underwater Soft Manipulator with Planarly-Bundled Actuators and Accurate Position Control,"Kailuan Tang, Chenghua Lu, Yishan Chen, Yin Xiao, Shijian Wu, Shaowu Tang, Hexiang Wang, Binbin Zhang, Zhong Shen, Juan Yi, Sicong Liu, Zheng Wang","Harbin Institute of Technology,University of Bristol,SOUTHERN UNIVERSITY OF SCIENCE AND TECHNOLOGY,Southern University of Science and Technology,The University of Sydney,The University of Hong Kong",Soft Robot Applications II,"Soft robotic manipulators have inherent advantages in underwater applications, as they generate motion by deforming seamless muscles rather than having rotational joints or sliding cylinders, as well as having excellent passive adaptability. However, limited by insufficient structural stiffness, achieving high payload and positioning accuracy remains challenging in existing soft manipulator designs. In this work, we propose an innovative approach to underwater soft manipulator design: 1) by constraining high- power optimized actuators with densely spaced lateral supporting plates, we could significantly enhance structural stiffness as well as improve the model accuracy drastically; 2) paired with a novel flow-controllable open-circuit hydraulic actuation, we could keep the manipulator smoothly operated and depth-compensation-free; 3) in result, the manipulator could be modelled kinematically in a simplified way for position control. The entire workflow from mechanical design to actuation and control is presented. A prototype soft manipulator was developed to validate the proposed design experimentally."
Learning-Based Object Recognition Via a Eutectogel Electronic Skin Enabled Soft Robotic Gripper,"Mo Deng, Fengya Fan, Xi Wei",University of Science and Technology of China,Soft Robot Applications II,"Compared to the traditional robot, which is rigidly structured, the soft robot, usually made of soft material, or following a continuous movement pattern, has attracted extensive attention due to its unique features, such as high adaptivity to various unstructured environments and safe interaction with living beings through the deformable interface. However, mechanical and morphological requirements limit the design and implementation of a compatible sensing module, which restricts the further development of robotic functionality. Here, we designed a flexible soft sensing Wire with the piezoresistive Eutectogel packed in an Ecoflex tube (WEE), which is sensitive, stable, and easily manipulated. The wire and its array facilitated the perception function of the soft gripper and acted as the Electronic skin (E-skin) to acquire information from grasped objects. With the built-in E-skin, the gripper achieved object recognition at an accuracy of 93.78% for standard geometric objects in 9 categories based on a machine learning model. In addition, our design successfully demonstrated its application in fruit sorting, which proves its robustness and versatility. The proposed WEE-based E-skin can be easily applied to other soft robots with facile integration and further expedites advanced functionalization in robot-object interaction."
Design and Validation of Slender Extensible Continuum Robot for Solar Wing Re-Unfolding in Aerospace,"Pengyuan Wang, Zheng Zheng, Jiazhen Sun, Yuqiang Liu, Zongbo He, Zhiguang Xing, Jianwen Zhao","Harbin Institute of Technology,Yangtze River Delta HIT Robot Technology Research Institute,Beijing Institute of Sapcecraft System Engineering,The Beijing Institute of Spacecraft System Engineering,Harbin Institute of Technology, Weihai",Soft Robot Applications II,"The solar array wing deployment of orbiting satellites cannot be performed due to power failure of the connector caused by uncertain loads such as high temperature or vibration in the launching process of the spacecraft. There is currently a lack of suitable unlocking solutions for solar wing re-unfolding. This paper proposes a solution in which an extensible continuum robot (ECR) carrying the unlocking device enters the gap between the satellite and the solar wing, re-unlocking the solar wing. This solution effectively leverages the advantages of ECR collision buffering and adaptable maneuverability within confined space. In response to the proposed solution, the designed ECR with two segments helical spring structure features scalability, hollowness, lightweight, and a big length-diameter ratio. To perform the critical unlocking task, an end effector with the function of loosening and unplugging the aerospace connector for communication is designed based on the drive device away from itself to reduce the inertia of the manipulator. The information from the cameras and force sensors is used to estimate the extent of task execution. We establish an experimental setup to simulate the process of unlocking. The results validate that the ECR successfully accesses the gap (65mm) and accomplishes the unlocking task. The ECR has great application potential for on-orbit service."
Bio-Inspired Pupal-Mode Actuator with Ultra-Crossing Capability for Soft Robots,"Zhenxing Wang, Xiao He, Yuhang Zhang, Cheng Zhang, Lei Sun, Zhidong Wang, Shun Xu, Hao Liu","Chinese Academy of Sciences,Shenyang Institute of Automation, Chinese Academy of Sciences,Shenyang Institute of Automation,The First Affiliated Hospital of China Medical University,Chiba Institute of Technology",Soft Robot Applications II,"Robot-assisted Natural Orifice Transluminal Endoscopic Surgery (NOTES) represents a paradigm shift in surgical practice, significantly minimizing patient morbidity. However, the variability of inner diameter and the inter-luminal crossing within the luminal tracts lead to challenge for effective robotic intervention. Inspired by the motion of the chrysalis during its transformation, we designed an innovative pupal-mode actuator for NOTES robots. Through the manipulation of its internal air chambers, this actuator is capable of replicating wrigglelike movements. Through experimental analysis, we have acquired the constitutive characteristics of this actuator. Subsequently, an innovative gastric endoscopy robot is developed base the actuator and tested in a phantom. The results of the task simulations substantiate that the pupal-mode actuator has the capability to reduce resistance and enhance the safety of the endoscopic intervention."
Crawling Soft Robot Exploiting Wheel-Legs and Multimodal Locomotion for High Terrestrial Maneuverability,"Xinpei Ai, Hengmao Yue, Wei Wang",Hanyang University,Soft Robot Applications II,"How to efficiently traverse complex terrain remains an unresolved challenge for mobile soft robots, because their deformable bodies limit the magnitude of the forces they can exert on the environment. To achieve high maneuverability, this study demonstrates a pneumatic soft crawling robot equipped with wheel-legs capable of multimodal locomotion to negotiate various obstacles. The soft robot consists of a pneumatic soft actuator capable of multiple modes of bending deformation as the body and four identical multi-spoked wheel-legs with passive unidirectional forward rotation as limbs. The synergy of the body actuator and wheel-legs enables the robot to achieve multiple crawling gaits, including gecko-like crawling and inchworm-like crawling. A single gait or a combination of multiple gaits, as well as shape-morphing of the body, enables the robot to navigate obstacles as diverse as confined spaces, inclined surfaces, gaps, and stairs, or to avoid obstacles by circumventing them. Our study substantially improves the maneuverability of pneumatic soft crawling robots, thereby providing new routes for the potential applications of soft robots in obstacle-filled scenarios, including search and rescue, exploration, and inspection."
Compliant Robotic Gripper with Integrated Ripeness Sensing for Blackberry Harvesting,"Arvyn De, Divyam Kumar, Ian Kwuan, Alex Qiu, Ai-Ping Hu","Georgia Institute of Technology,Georgia Tech Research Institute",Soft Robot Applications II,"Global blackberry demand has been surging due to their antioxidant and nutritional value in a traditional diet. However, blackberries have extreme fragility (resulting in up to 85% of harvest batches sustaining damage) and near-ripe and ripe blackberries are difficult to distinguish in normal lighting conditions. These challenges in maintaining the blackberry supply motivate the development of an autonomous robotic solution to harvest fully ripe blackberries with minimal damage. The present paper details the mechanical design, methodology, analysis, and experimental results of a compliant robotic gripper created for this purpose. The gripper has a compact form factor and retractable fingers with specialized TPU finger pads for gentle picks, a near-infrared (NIR) reflectance-based probe for detecting full ripeness and a standardized harvesting sequence for effectively picking berries. In an outdoor harvesting experiment, the gripper attempted picking 26 berries without ripeness sensing, with 65.4% (17) being successfully picked and 38.5% (10) sustaining damage. The movements of the robot arm in the harvesting sequence were accordingly adjusted and finalized for following in-lab experiments, in which the gripper was also outfitted with ripeness sensing. Out of 40 berries, 62.5% (25) were successfully picked, with 0% of them sustaining damage. The ripeness probe classified 56 ripe and 11 near-ripe berries, with 89% (50) of the ripe and 64% (7) of the near-ripe berries being correctly classified. In a second in-lab experiment, 16 of 20 berries were successfully picked, with 2 sustaining damage."
SG-RoadSeg: End-To-End Collision-Free Space Detection Sharing Encoder Representations Jointly Learned Via Unsupervised Deep Stereo,"Zhiyuan Wu, Jiaqi Li, Yi Feng, Chengju Liu, Wei Ye, Qijun Chen, Rui Fan",Tongji University,Semantic Scene Understanding III,"Collision-free space detection is of utmost importance for autonomous robot perception and navigation. State-of-the-art (SoTA) approaches generally extract features from RGB images and an additional source or modality of 3-D information, such as depth or disparity images, using a pair of independent encoders. The extracted features are subsequently fused and decoded to yield semantic predictions of collision-free spaces. Such feature-fusion approaches become infeasible in scenarios, where the sensor for 3-D information acquisition is unavailable, or just when multi-sensor calibration falls short of the necessary precision. To overcome these limitations, this paper introduces a novel end-to-end collision-free space detection network, referred to as SG-RoadSeg, built upon our previous work SNE-RoadSeg. A key contribution of this paper is a strategy for sharing encoder representations that are co-learned through both semantic segmentation and unsupervised stereo matching tasks, enabling the features extracted from RGB images to contain both semantic and spatial geometric information. The unsupervised deep stereo serves as an auxiliary functionality, capable of generating accurate disparity maps that can be used by other perception tasks that require depth-related data. Comprehensive experimental results on the KITTI road and semantics datasets validate the effectiveness of our proposed architecture and encoder representation sharing strategy. SG-RoadSeg also demonstrates superior performance than other SoTA collision-free space detection approaches. Our source code, demo video, and supplement are publicly available at mias.group/SG-RoadSeg."
Robust Few-Shot 3D Point Cloud Scene Segmentation,"Hao Huang, Shuaihang Yuan, Congcong Wen, Yu Hao, Yi Fang","New York University,New York University Abu Dhabi",Semantic Scene Understanding III,"In the domain of 3D point cloud scene semantic segmentation, a preponderance of methodology predominantly adopts a fully supervised framework. Such paradigms exhibit an intrinsic dependency on extensive labeled datasets, presenting challenges in acquisition and exhibiting incapacity to segment novel classes, especially when the training data are contaminated by noisy samples. To addressing these limitations, this study introduces a novel meta-learning-based few-shot segmentation approach to robustly segment 3D point cloud scenes. Specifically, we first build a multi-prototype graph and then suppress noisy samples based on the graph structure. A subgraph voting scheme is proposed to conduct transductive semi-supervised learning to propagate labels. To optimize the graph structure to learn discriminative prototype features, we design a triplet contrastive loss to increase the compactness of the graph. We eveluate our method on two widely used 3D point cloud scene segmentation benchmarks within specific few-shot segmentation (i.e., 2/3-way 5-shot) settings. Experimental results demonstrate improvement over the compared baseline methods, illustrating the robustness of our method in few-shot 3D scene segmentation against noisy samples."
Radar Instance Transformer: Reliable Moving Instance Segmentation in Sparse Radar Point Clouds,"Matthias Zeller, Vardeep Singh Sandhu, Benedikt Mersch, Jens Behley, Michael Heidingsfeld, Cyrill Stachniss","CARIAD SE,University of Bonn, CARIAD,University of Bonn",Semantic Scene Understanding III,"The perception of moving objects is crucial for autonomous robots performing collision avoidance in dynamic environments. LiDARs and cameras tremendously enhance scene interpretation but do not provide direct motion information and face limitations under adverse weather. Radar sensors overcome these limitations and provide Doppler velocities, delivering direct information on dynamic objects. In this paper, we address the problem of moving instance segmentation in radar point clouds to enhance scene interpretation for safety-critical tasks. Our Radar Instance Transformer enriches the current radar scan with temporal information without passing aggregated scans through a neural network.We propose a full-resolution backbone to prevent information loss in sparse point cloud processing. Our instance transformer head incorporates essential information to enhance segmentation but also enables reliable, class-agnostic instance assignments. In sum, our approach shows superior performance on the new moving instance segmentation benchmarks, including diverse environments, and provides model-agnostic modules to enhance scene interpretation. The benchmark is based on the RadarScenes dataset and is available at https://doi.org/10.5281/zenodo.10203864."
On the Overconfidence Problem in Semantic 3D Mapping,"Joao Marcos Correia Marques, Albert Zhai, Shenlong Wang, Kris Hauser","University of Illinois at Urbana-Champaign,UIUC",Semantic Scene Understanding III,"Semantic 3D mapping, the process of fusing depth and image segmentation information between multiple views to build 3D maps annotated with object classes in real-time, is a recent topic of interest. This paper highlights the fusion overconfidence problem, in which conventional mapping methods assign high confidence to the entire map even when they are incorrect, leading to miscalibrated outputs. Several methods to improve uncertainty calibration at different stages in the fusion pipeline are presented and compared on the ScanNet dataset. We show that the most widely used Bayesian fusion strategy is among the worst calibrated, and propose a learned pipeline that combines fusion and calibration, GLFS, which achieves simultaneously higher accuracy and 3D map calibration while retaining real-time capability and adding only 525 learned parameters to the pipeline. We further illustrate the importance of map calibration on a downstream task by showing that incorporating proper semantic fusion to an indoor object search agent improves its success rates."
Complementing Onboard Sensors with Satellite Maps: A New Perspective for HD Map Construction,"Wenjie Gao, Jiawei Fu, Yanqing Shen, Haodong Jing, Shitao Chen, Nan-Ning Zheng","Xi'an Jiaotong University,Institute of Artificial Intelligence and Robotics,xi'an jiaotong university",Semantic Scene Understanding III,"High-definition (HD) maps play a crucial role in autonomous driving systems. Recent methods have attempted to construct HD maps in real-time using vehicle onboard sensors. Due to the inherent limitations of onboard sensors, which include sensitivity to detection range and susceptibility to occlusion by nearby vehicles, the performance of these methods significantly declines in complex scenarios and long-range detection tasks. In this paper, we explore a new perspective that boosts HD map construction through the use of satellite maps to complement onboard sensors. We initially generate the satellite map tiles for each sample in nuScenes and release a complementary dataset for further research. To enable better integration of satellite maps with existing methods, we propose a hierarchical fusion module, which includes feature-level fusion and BEV-level fusion. The feature-level fusion, composed of a mask generator and a masked cross-attention mechanism, is used to refine the features from onboard sensors. The BEV-level fusion mitigates the coordinate differences between features obtained from onboard sensors and satellite maps through an alignment module. The experimental results on the augmented nuScenes showcase the seamless integration of our module into three existing HD map construction methods. The satellite maps and our proposed module notably enhance their performance in both HD map semantic segmentation and instance detection tasks. Our code will be available at https://github.com/xjtu-cs-gao/SatforHDMap."
Complementary Random Masking for RGB-Thermal Semantic Segmentation,"Ukcheol Shin, Kyunghyun Lee, In So Kweon, Jean Oh","CMU(Carnegie Mellon University),KAIST,Carnegie Mellon University",Semantic Scene Understanding III,"RGB-thermal semantic segmentation is one potential solution to achieve reliable semantic scene understanding in adverse weather and lighting conditions. However, the previous studies mostly focus on designing a multi-modal fusion module without consideration of the nature of multi-modality inputs. Therefore, the networks easily become over-reliant on a single modality, making it difficult to learn complementary and meaningful representations for each modality. This paper proposes 1) a complementary random masking strategy of RGB-T images and 2) self-distillation loss between clean and masked input modalities. The proposed masking strategy prevents over-reliance on a single modality. It also improves the accuracy and robustness of the neural network by forcing the network to segment and classify objects even when one modality is partially available. Also, the proposed self-distillation loss encourages the network to extract complementary and meaningful representations from a single modality or complementary masked modalities. We achieve state-of-the-art performance over three RGB-T semantic segmentation benchmarks. Our source code is available at https://github.com/UkcheolShin/CRM_RGBTSeg."
Collaborative Dynamic 3D Scene Graphs for Automated Driving,"Elias Greve, Martin Büchner, Niclas Vödisch, Wolfram Burgard, Abhinav Valada","University of Freiburg,University of Technology Nuremberg",Semantic Scene Understanding III,"Maps have played an indispensable role in enabling safe and automated driving. Although there have been many advances on different fronts ranging from SLAM to semantics, building an actionable hierarchical semantic representation of urban dynamic scenes and processing information from multiple agents are still challenging problems. In this work, we present Collaborative URBan Scene Graphs (CURB-SG) that enable higher-order reasoning and efficient querying for many functions of automated driving. CURB-SG leverages panoptic LiDAR data from multiple agents to build large-scale maps using an effective graph-based collaborative SLAM approach that detects inter-agent loop closures. To semantically decompose the obtained 3D map, we build a lane graph from the paths of ego agents and their panoptic observations of other vehicles. Based on the connectivity of the lane graph, we segregate the environment into intersecting and non-intersecting road areas. Subsequently, we construct a multi-layered scene graph that includes lane information, the position of static landmarks and their assignment to certain map sections, other vehicles observed by the ego agents, and the pose graph from SLAM including 3D panoptic point clouds. We extensively evaluate CURB-SG in urban scenarios using a photorealistic simulator. We release our code at http://curb.cs.uni-freiburg.de."
BroadBEV: Collaborative LiDAR-Camera Fusion for Broad-Sighted Bird's Eye View Map Construction,"Minsu Kim, Giseop Kim, Kyong Hwan Jin, Sunwook Choi","Korea Institute of Science and Technology,NAVER LABS,Korea University,NAVER LABS Corp.",Semantic Scene Understanding III,"A recent sensor fusion in a Bird's Eye View (BEV) space has shown its utility in various tasks such as 3D detection, map segmentation, etc. However, the approach struggles with inaccurate camera BEV estimation, and a perception of distant areas due to the sparsity of LiDAR points. In this paper, we propose a BEV fusion (BroadBEV) that aims to enhance camera BEV estimation for broad perception in the pre-defined BEV range, while simultaneously improving the completion of LiDAR's sparsity in the entire BEV space. Toward that end, we devise Point-scattering that scatters LiDAR BEV distribution to camera depth distribution. The method boosts the learning of depth estimation of the camera branch and induces accurate location of dense camera features in BEV space. For an effective BEV fusion between the spatially synchronized features, we suggest ColFusion that applies self-attention weights of LiDAR and camera BEV features to each other. Our extensive experiments demonstrate that the suggested methods enable a broad BEV perception with remarkable performance gains."
AGRNav: Efficient and Energy-Saving Autonomous Navigation for Air-Ground Robots in Occlusion-Prone Environments,"Junming Wang, Zekai Sun, Xiuxian Guan, Tianxiang Shen, Zongyuan Zhang, Tianyang Duan, Dong Huang, Shixiong Zhao, Heming Cui","The University of Hong Kong,the University of Hong Kong,Huawei Technologies Co., Ltd",Semantic Scene Understanding III,"The exceptional mobility and long endurance of air-ground robots are raising interest in their usage to navigate complex environments (e.g., forests and large buildings). However, such environments often contain occluded and unknown regions, and without accurate prediction of unobserved obstacles, the movement of the air-ground robot often suffers a suboptimal trajectory under existing mapping-based and learning-based navigation methods. In this work, we present AGRNav, a novel framework designed to search for safe and energy-saving air-ground hybrid paths. AGRNav contains a lightweight semantic scene completion network (SCONet) with self-attention to enable accurate obstacle predictions by capturing contextual information and occlusion area features. The framework subsequently employs a query-based method for low-latency updates of prediction results to the grid map. Finally, based on the updated map, the hierarchical path planner efficiently searches for energy-saving paths for navigation. We validate AGRNav's performance through benchmarks in both simulated and real-world environments, demonstrating its superiority over classical and state-of-the-art methods. The open-source code is available at https://github.com/jmwang0117/AGRNav."
ATPPNet: Attention Based Temporal Point Cloud Prediction Network,"Kaustab Pal, Aditya Sharma, Avinash Sharma, Madhava Krishna","International Institute of Information Technology, Hyderabad,Robotics Research Center, IIIT Hyderabad,International Institute of Information Technology,,IIIT Hyderabad",Deep Learning Methods,"Point cloud prediction is an important yet challenging task in the field of autonomous driving. The goal is to predict future point cloud sequences that maintain object structures while accurately representing their temporal motion. These predicted point clouds help in other subsequent tasks like object trajectory estimation for collision avoidance or estimating locations with the least odometry drift. In this work, we present ATPPNet, a novel architecture that predicts future point cloud sequences given a sequence of previous time step point clouds obtained with LiDAR sensor. ATPPNet leverages Conv-LSTM along with channel-wise and spatial attention dually complemented by a 3D-CNN branch for extracting an enhanced spatio-temporal context to recover high quality fidel predictions of future point clouds. We conduct extensive experiments on publicly available datasets and report impressive performance outperforming the existing methods. We also conduct a thorough ablative study of the proposed architecture and provide an application study that highlights the potential of our model for tasks like odometry estimation."
Transformer-CNN Cohort: Semi-Supervised Semantic Segmentation by the Best of Both Students,"Xu Zheng, Yunhao Luo, Chong Fu, Kangcheng Liu, Lin Wang","The Hong Kong University of Science and Technology,Brown University,Northeastern University,ETH Zurich,HKUST",Deep Learning Methods,"The popular methods for semi-supervised semantic segmentation mostly adopt a unitary network model using convolutional neural networks (CNNs) and enforce consistency of the modelâ€™s predictions over perturbations applied to the inputs or model. However, such a learning paradigm suffers from two critical limitations: a) learning the discriminative features for the unlabeled data; b) learning both global and local information from the whole image. In this paper, we propose a novel Semi-supervised Learning (SSL) approach, called Transformer-CNN Cohort (TCC), that consists of two students with one based on the vision transformer (ViT) and the other based on the CNN. Our method subtly incorporates the multi-level consistency regularization on the predictions and the heterogeneous feature spaces via pseudo labeling for the unlabeled data. First, as the inputs of the ViT student are image patches, the feature maps extracted encode crucial class-wise statistics. To this end, we propose class-aware feature consistency distillation (CFCD) that first leverages the outputs of each student as the pseudo labels and generates class-aware feature (CF) maps for knowledge transfer between the two students. Second, as the ViT student has more uniform representations for all layers, we propose consistency-aware cross distillation (CCD) to transfer knowledge between the pixel-wise predictions from the cohort. We validate the TCC framework on Cityscapes and Pascal VOC 2012 datasets, which outperforms existing SSL methods by a large margin. Project page: url{https://vlislab22.github.io/TCC/}."
CrackNex: A Few-Shot Low-Light Crack Segmentation Model Based on Retinex Theory for UAV Inspections,"Zhen Yao, Jiawei Xu, Shuhang Hou, Mooi Choo Chuah",Lehigh University,Deep Learning Methods,"Routine visual inspections of concrete structures are imperative for upholding the safety and integrity of critical infrastructure. Such visual inspections sometimes happen under low-light conditions, e.g., checking for bridge health. Crack segmentation under such conditions is challenging due to the poor contrast between cracks and their surroundings. However, most deep learning methods are designed for well-illuminated crack images and hence their performance drops dramatically in low-light scenes. In addition, conventional approaches require many annotated low-light crack images which is time-consuming. In this paper, we address these challenges by proposing CrackNex, a framework that utilizes reflectance information based on Retinex Theory to help the model learn a unified illumination-invariant representation. Furthermore, we utilize few-shot segmentation to solve the inefficient training data problem. In CrackNex, both a support prototype and a reflectance prototype are extracted from the support set. Then, a prototype fusion module is designed to integrate the features from both prototypes. CrackNex outperforms the SOTA methods on multiple datasets. Additionally, we present the first benchmark dataset, LCSD, for low-light crack segmentation. LCSD consists of 102 well-illuminated crack images and 41 low-light crack images. The dataset and code are available at https://github.com/zy1296/CrackNex."
FBPT: A Fully Binary Point Transformer,"Zhixing Hou, Yuzhang Shang, Yan Yan","Nanjing University of Science and Technology,Illinois Institute of Technology",Deep Learning Methods,"This paper presents a novel Fully Binary Point Cloud Transformer (FBPT) network model which has the potential to be widely applied and expanded in the fields of robotics and mobile devices. By compressing the weights and activations of a 32-bit full-precision network to 1-bit binary values, the proposed binary point cloud Transformer network significantly reduces the storage footprint and computational resource requirements of neural network models for point cloud processing tasks, compared to full-precision point cloud networks. However, achieving a fully binary point cloud Transformer network, where all parts except the modules specific to the task are binary, poses challenges and bottlenecks in quantizing the activations of Q, K, V and self-attention in the attention module, as they do not adhere to simple probability distributions and can vary with input data. Furthermore, in our network, the binary attention module undergoes a degradation of the self-attention module due to the uniform distribution that occurs after the softmax operation. The primary focus of this paper is on addressing the performance degradation issue caused by the use of binary point cloud Transformer modules. We propose a novel binarization mechanism called dynamic-static hybridization. Specifically, our approach combines static binarization of the overall network model with fine granularity dynamic binarization of data-sensitive components. Furthermore, we make use of a novel hierarchical training scheme to obtain the optimal model and binarization parameters. These above improvements allow the proposed binarization method to outperform binarization methods applied to convolution neural networks when used in point cloud Transformer structures. To demonstrate the superiority of our algorithm, we conducted experiments on two different tasks: point cloud classification and place recognition."
Diffusion-Based Point Cloud Super-Resolution for mmWave Radar Data,"Kai Luan, Chenghao Shi, Neng Wang, Yuwei Cheng, Huimin Lu, Xieyuanli Chen","Intelligence Science and Technologyï¼ŒNational University o,NUDT,National University of Defense Technology,Tsinghua university",Deep Learning Methods,"The millimeter-wave radar sensor maintains stable performance under adverse environmental conditions, making it a promising solution for all-weather perception tasks, such as outdoor mobile robotics. However, the radar point clouds are relatively sparse and contain massive ghost points, which greatly limits the development of mmWave radar technology. In this paper, we propose a novel point cloud super-resolution approach for 3D mmWave radar data, named Radar-diffusion. Our approach employs the diffusion model defined by mean-reverting stochastic differential equations (SDE). Using our proposed new objective function with supervision from corresponding LiDAR point clouds, our approach efficiently handles radar ghost points and enhances the sparse mmWave radar point clouds to dense LiDAR-like point clouds. We evaluate our approach on two different datasets, and the experimental results show that our method outperforms the state-of-the-art baseline methods in 3D radar super-resolution tasks. Furthermore, we demonstrate that our enhanced radar point cloud is capable of downstream radar point-based registration tasks."
Visual Noun Modifiers: The Problem of Binding Visual and Linguistic Cues,"Mohamadreza Faridghasemnia, Jennifer Renoux, Alessandro Saffiotti","Orebro University,Örebro University",Deep Learning Methods,"In many robotic applications, especially those involving humans and the environment, linguistic and visual information must be processed jointly and bound together. Existing works either encode the image or the language into a subsymbolic space, like the CLIP model, or create a symbolic space of extracted information, like the object detection models. In this paper, we propose to describe images by nouns and modifiers and introduce a new embedded binding space where the linguistic and visual cues can effectively be bound. We investigate how state-of-the-art models perform in recognizing nouns and modifiers from images, and propose our method by introducing a dataset and CLIP-like recognition techniques based on transfer learning and metric learning. We show real-world experiments that demonstrate the practical applicability of our approach to robotics applications. Our results indicate that our method can surpass the state-of-the-art in recognizing nouns and modifiers from images. Interestingly, our method exhibits a language characteristic related to context sensitivity."
Cycle-Correspondence Loss: Learning Dense View-Invariant Visual Features from Unlabeled and Unordered RGB Images,"David Benjamin Adrian, Andras Kupcsik, Markus Spies, Heiko Neumann","Bosch Corporate Research & Ulm University,Bosch Center for Artificial Intelligence,Ulm University",Deep Learning Methods,"Robot manipulation relying on learned object-centric descriptors became popular in recent years. Visual descriptors can easily describe manipulation task objectives, they can be learned efficiently using self-supervision, and they can encode actuated and even non-rigid objects. However, learning robust, view-invariant keypoints in a self-supervised approach requires a meticulous data collection approach involving precise calibration and expert supervision. In this paper we introduce Cycle-Correspondence Loss (CCL) for view-invariant dense descriptor learning, which adopts the concept of cycle-consistency, enabling a simple data collection pipeline and training on unpaired RGB camera views. The key idea is to autonomously detect valid pixel correspondences by attempting to use a prediction over a new image to predict the original pixel in the original image, while scaling error terms based on the estimated confidence. Our evaluation shows that we outperform other self-supervised RGB-only methods, and approach performance of supervised methods, both with respect to keypoint tracking as well as for a robot grasping downstream task."
End-To-End RGB-D SLAM with Multi-MLPs Dense Neural Implicit Representations,"Mingrui Li, Jiaming He, Yangyang Wang, Hongyu Wang","Dalian University of Technology,Dalian Maritime University",Deep Learning Methods,"An accurate and generalizable dense 3D reconstruction system has attracted much attention. However, existing 3D dense reconstruction systems are limited by the requirement for pre-training, and there is a demand for improved reconstruction of feature details. We propose an end-to-end 3D reconstruction system which achieves fine scene reconstruction without prior information by utilizing neural implicit encoding. Our proposed system successfully achieves the goal through improved multi-MLP decoders (MLM) and effective keyframe selection strategy. Experiments conducted on the commonly used Replica and TUM RGB-D datasets demonstrate that our approach can compete with widely adopted NeRF-based SLAM methods in terms of 3D reconstruction accuracy. Moreover, our approach shows 40.8%(except Completion Ratio) improvement in accuracy compared to NICE-SLAM that does not use prior information."
Closing the Visual Sim-To-Real Gap with Object-Composable NeRFs,"Nikhil Mishra, Maximilian Sieb, Pieter Abbeel, Xi Chen","UC Berkeley,covariant.ai,CovariantAI,UC Berkeley,Embodied Intelligence, UC Berkeley",Deep Learning Methods,"Deep learning methods for perception are the cornerstone of many robotic systems. Despite their potential for impressive performance, obtaining real-world training data is expensive, and can be impractically difficult for some tasks. Sim-to-real transfer with domain randomization offers a potential workaround, but often requires extensive manual tuning and results in models that are brittle to distribution shift between sim and real. In this work, we introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF model that is the centerpiece of a real-to-sim pipeline for synthesizing training data targeted to scenes and objects from the real world. COV-NeRF extracts objects from real images and composes them into new scenes, generating photorealistic renderings and many types of 2D and 3D supervision, including depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the rendering quality of modern NeRF methods, and can be used to rapidly close the sim-to-real gap across a variety of perceptual modalities."
Usability Evaluation Framework for Close-Proximity Collaboration with Large Industrial Manipulators,"Kasper Hald, Matthias Rehm",Aalborg University,Human-Robot Collaboration III,"Our goal is to design a framework for holistic evaluation of human-robot collaboration systems. To this end we utilize several standardized questionnaires administered while participants perform collaborative tasks in robot work cells. We used Standard Usability Scale and the Usability metric for user experience questionnaires to access usability, NASA Task-Load for workload, two questionnaires for human-robot trust as well as the Unified theory of acceptance and use of technology questionnaires. We performed two pilot tests of our framework with human-robot collabotation work cells at two test sites as part of the DrapeBot project. The goal of the project is to enable human-robot collaboration in the process of carbon fiber draping the production of outer parts. After utilizing the evaluation framework at the two test sites we found that the collection of questionnaires were easy to adapt to each work cell and the practical limitation around running the experiments. Both work cells scored high in usability, expected increase of productivity, as well as high trust and low anxiety, but both work cells scored low on expectancy of use for work in the future at their current state of development."
MyoPassivity Map: Does Multi-Channel sEMG Correlate with the Energetic Behavior of Upper-Limb Biomechanics During Physical Human-Robot Interaction?,"Suzanne Oliver, Peter Paik, Xingyuan Zhou, S. Farokh Atashzar","New York University,New York University (NYU), US",Human-Robot Collaboration III,"The human arm has an intrinsic capacity to absorb energy during physical human-robot interaction (pHRI), which can be identified as biomechanical excess of passivity (EoP). This can be used as a central factor in the development of passivity-based pHRI controllers, securing haptic transparency while guaranteeing pHRI stability. Despite its significance, the real-time estimation of EoP remains an under-investigated topic. For the first time, we investigate the relationship between the EoP and muscle activity of the forearm at the wrist joint while analyzing sixteen surface electromyography (sEMG) sensors. The study explores optimal sensor placement for maximizing the correlation between muscle activity and the estimated EoP. Ten subjects participated in this study. The EoP of the wrist was identified through high-frequency perturbations in four directions, and two instructed co-contraction levels. The results uncover a strong correlation between sEMG and EoP. This paper also reports the effect of the direction of pHRI interaction on the EoP of the wrist, with increased energetic passivity in the abduction-adduction direction compared to supination-pronation. The findings of this paper indicate that sEMG encodes significant potential for real-time estimation of EoP in the design of next-generation pHRI controllers supporting concurrent transparency and stability."
"Language-Guided Active Sensing of Confined, Cluttered Environments Via Object Rearrangement Planning","Weihan Chen, Hanwen Ren, Ahmed H. Qureshi",Purdue University,Human-Robot Collaboration III,"Language-guided active sensing is a robotics subtask where a robot with an onboard sensor interacts efficiently with the environment via object manipulation to maximize perceptual information, following given language instructions. These tasks appear in various practical robotics applications, such as household service, search and rescue, and environment monitoring. Despite many applications, the existing works do not account for language instructions and have mainly focused on surface sensing, i.e., perceiving the environment from the outside without rearranging it for dense sensing. Therefore, in this paper, we introduce the first language-guided active sensing approach that allows users to observe specific parts of the environment via object manipulation. Our method spatially associates the environment with language instructions, determines the best camera viewpoints for perception, and then iteratively selects and relocates the best view-blocking objects to provide the dense perception of the region of interest. We evaluate our method against different baseline algorithms in simulation and also demonstrate it in real-world confined cabinet-like settings with multiple unknown objects. Our results show that the proposed method exhibits better performance across different metrics and successfully generalizes to real-world complex scenarios."
Feedforward Control of Lower Limb Exoskeletons: Which Torque Profile Should We Use?,"Hannah Dinovitzer, Mohammad Shushtari, Arash Arami",University of Waterloo,Human-Robot Collaboration III,"Despite the increased use of lower limb exoskeletons as gait training and mobility assistive devices, their controllers often lack the ability to synchronize and adapt to meet individual users' needs. This paper investigates two control approaches for lower limb exoskeletons: a real-time kinematic state-dependent estimation of desired torques with an inverse dynamics model and a data-driven component in the first approach, and a pre-defined torque control based on gait speed and phase in the second approach. These controllers are linearly combined to shift the controller behavior between pure kinematic state-dependent and pure gait phase-dependent control.} These combinations were tested during overground and treadmill walking with nine able-bodied participants. The linearly combined controller with a greater emphasis on kinematic state-dependent control produced a more natural gait in terms of spatiotemporal metrics. This is reflected by 0.1m/s increases in overground walking speed and 5% decrease in percent stance compared to walking with a passive exoskeleton. This controller also decreases the overall activity of lower limb muscles by up to 25% and thigh co-contractions by up to 40%. Participant feedback through a questionnaire, in terms of perceived effort, walking naturalness, and stability, also favored the aforementioned controller."
"Design of Two Morphing Robot Surfaces and Results from a User Study on What People Want and Expect of Them, towards a â€œRobot-Roomâ€","Nithesh Kumar, Hsin-ming Chao, Bruno Dantas Da Silva Tassari, Elena Sabinson, Ian Walker, Keith Evan Green","Clemson,Cornell University,Clemson University",Human-Robot Collaboration III,"We propose, examine prototypes of, and collect user input on morphing robotic surface, â€œrobot-roomâ€ elements that, individually or in combination, change the functionality of the rooms we live in, directly controlled by the roomâ€™s occupants engaging with it. Robot-rooms represent an advance in human-robot interaction whereby human interaction is within a machine that physically envelops us. We discuss the motivation for such robot-rooms, present initial work aimed at their physical realization, and report on a user study of 80 participants to learn what people might want of and expect from robot rooms, the results of which will inform both the iterative design of the robot room and the thinking of our community as it grapples with how we want to live with (and â€œinâ€) robots. Keywords: Robot surfaces, User studies"
Automatic Trust Estimation from Movement Data in Industrial Human-Robot Collaboration Based on Deep Learning,"Matthias Rehm, Ioannis Pontikis, Kasper Hald","Aalborg University,AALBORG UNIVERSITY",Human-Robot Collaboration III,"Trust in automation is usually assessed with post-interaction questionnaires. For human robot collaboration it would be beneficial to assess the trust level during the interaction to adjust the robot's collaboration behavior to the user expectations. In this paper we investigate if trust can be estimated from observable behavior like movements during the interaction with a large industrial manipulator. To this end, we report on a data collection for two tasks during collaborative draping, the transport of large cut pieces and the actual draping process in close proximity to the robot. The data is used to train and compare different deep learning models. Results show that automatic trust estimation is feasible, which opens up to using trust as a parameter for informing the interaction with robots."
A Dual Closed-Loop Control Strategy for Human-Following Robots Respecting Social Space,"Jianwei Peng, Zhelin Liao, Zefan Su, Hanchen Yao, Yadan Zeng, Houde Dai","University of Chinese Academy of Sciences,Fujian Agriculture and Forestry University,Fuzhou University,Haixi Institutes, Chinese Academy of Sciences,Nanyang Technology University",Human-Robot Collaboration III,"Human following for mobile robots has emerged as a promising technique with widespread applications. To ensure psychological comfort while collaborating, coexisting, and interacting with humans, robots need to respect the social space of the target person. In this study, we propose a dual closed-loop human-following control strategy that combines model predictive control (MPC) and impedance control. The outer-loop MPC ensures precise control of the robot's posture while tracking the target person's velocity and direction to coordinate the motion between them. The inner-loop impedance controller is employed to regulate the robot's motion and interaction force with the target person, enabling the robot to maintain a respectful and comfortable distance from the target person. Concretely, the social interaction dynamics characteristics between the robot and the target person are described by human-robot interaction dynamics, which considers the rules of social space. Furthermore, an obstacle avoidance component constructed using behavioral dynamics is integrated into the impedance controller. Experimental results demonstrate the effectiveness of the proposed method in achieving human following and obstacle avoidance without intruding into the intimate zone of the target person."
A Bayesian Optimization Framework for the Automatic Tuning of MPC-Based Shared Controllers,"Anne Van Der Horst, Bastiaan Guillermo Lorenzo Meere, Dinesh Krishnamoorthy, Saray Bakker, Bram Van De Vrande, Henry Stoutjesdijk, Marco Alonso, Elena Torta","Eindhoven University of Technology,TU Eindhoven,Delft University of Technology,Philips,Philips Medical Systems,Company",Human-Robot Collaboration III,This paper presents a Bayesian optimization framework for the automatic tuning of shared controllers which are defined as a Model Predictive Control (MPC) problem. The proposed framework includes the design of performance metrics as well as the representation of user inputs for simulation-based optimization. The framework is applied to the optimization of a shared controller for an Image Guided Therapy robot. VR-based user experiments confirm the increase in performance of the automatically tuned MPC shared controller with respect to a hand-tuned baseline version as well as its generalization ability.
Robust Body Exposure (RoBE): A Graph-Based Dynamics Modeling Approach to Manipulating Blankets Over People,"Kavya Puthuveetil, Sasha Wald, Atharva Pusalkar, Pratyusha Karnati, Zackory Erickson","Carnegie Mellon University,Google X, Everyday Robots",Human-Robot Interaction II,"Robotic caregivers could potentially improve the quality of life of many who require physical assistance. However, in order to assist individuals who are lying in bed, robots must be capable of dealing with a significant obstacle: the blanket or sheet that will almost always cover the person's body. We propose a method for targeted bedding manipulation over people lying supine in bed where we first learn a model of the cloth's dynamics. Then, we optimize over this model to uncover a given target limb using information about human body shape and pose that only needs to be provided at run-time. We show how this approach enables greater robustness to variation relative to geometric and reinforcement learning baselines via a number of generalization evaluations in simulation and in the real world. We further evaluate our approach in a human study with 12 participants where we demonstrate that a mobile manipulator can adapt to real variation in human body shape, size, pose, and blanket configuration to uncover target body parts without exposing the rest of the body. Source code and supplementary materials are available online."
Recency Bias in Task Performance History Affects Perceptions of Robot Competence and Trustworthiness,"Matthew Luebbers, Aaquib Tabrez, Kanaka Samagna Talanki, Bradley Hayes",University of Colorado Boulder,Human-Robot Interaction II,"Human memory of a robot's competence, and resulting subjective perceptions of that robot, are influenced by numerous cognitive biases. One class of cognitive bias deals with the ordering of items or interactions: information presented last among a grouping is most salient in memory formation (recency bias), followed by information presented first (primacy bias), followed by information in the middle, collectively known as the serial-position effect. For example, if a human's last observation of a robot involves a task failure, this will disproportionately negatively alter their perception of the robot's competence, as well as their trust in the robot moving forward. It is valuable to characterize the effect of these biases within human-robot interactions to inform strategies for risk-aware planning that cultivate appropriate levels of human trust. We conducted a human-subjects study (n=53) testing the influence of the serial-position effect on recalled competence (see overview at https://youtu.be/BgH2zhh1s48). Participants viewed videos of a robot performing the same tasks at the same level of competence, with task order differing by experimental condition (rising competence, falling competence, or failures at the midpoint), asking participants to rate robot competence in between every video as well at the very end of the experiment. We found that while the average between-video rating of robot competence remained stable across conditions, the recalled, post-experiment ratings of competence and trust were significantly lower in the condition with decreasing competence than in either of the other two conditions, suggesting a notable recency bias. We conclude with implications for human-subjects experiment design (i.e., how subjective measures are influenced by ordering effects) and provide design recommendations to minimize them. We further discuss practical applications of these results in creating risk-aware robotic planners capable of trust calibration."
LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction by Enhancing Laminar Characteristics in Human Flow,"Yufei Zhu, Han Fan, Andrey Rudenko, Martin Magnusson, Erik Schaffernicht, Achim J. Lilienthal","Örebro University,Robert Bosch GmbH,Örebro University, AASS Research Center,Orebro University",Human-Robot Interaction II,"Long-term human motion prediction (LHMP) is essential for safely operating autonomous robots and vehicles in populated environments. It is fundamental for various applications, including motion planning, tracking, human-robot interaction and safety monitoring. However, accurate prediction of human trajectories is challenging due to complex factors, including, for example, social norms and environmental conditions. The influence of such factors can be captured through Maps of Dynamics (MoDs), which encode spatial motion patterns learned from (possibly scattered and partial) past observations of motion in the environment and which can be used for data-efficient, interpretable motion prediction (MoD-LHMP). To address the limitations of prior work, especially regarding accuracy and sensitivity to anomalies in long-term prediction, we propose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach is inspired by data-driven airflow modelling, which estimates laminar and turbulent flow components and uses predominantly the laminar components to make flow predictions. Based on the hypothesis that human trajectory patterns also manifest laminar flow (that represents predictable motion) and turbulent flow components (that reflect more unpredictable and arbitrary motion), LaCE-LHMP extracts the laminar patterns in human dynamics and uses them for human motion prediction. We demonstrate the superior prediction performance of LaCE-LHMP through benchmark comparisons with state-of-the-art LHMP methods, offering an unconventional perspective and a more intuitive understanding of human movement patterns."
Interactive Continual Learning Architecture for Long-Term Personalization of Home Service Robots,"Ali Ayub, Chrystopher L. Nehaniv, Kerstin Dautenhahn",University of Waterloo,Human-Robot Interaction II,"For robots to perform assistive tasks in unstructured home environments, they must learn and reason on the semantic knowledge of the environments. Despite a resurgence in the development of semantic reasoning architectures, these methods assume that all the training data is available a priori. However, each user's environment is unique and can continue to change over time, which makes these methods unsuitable for personalized home service robots. Although research in continual learning develops methods that can learn and adapt over time, most of these methods are tested in the narrow context of object classification on static image datasets. In this paper, we combine ideas from continual learning, semantic reasoning, and interactive machine learning literature and develop a novel interactive continual learning architecture for continual learning of semantic knowledge in a home environment through human-robot interaction. The architecture builds on core cognitive principles of learning and memory for efficient and real-time learning of new knowledge from humans. We integrate our architecture with a physical mobile manipulator robot and perform extensive system evaluations in a laboratory environment over two months. Our results demonstrate the effectiveness of our architecture to allow a physical robot to continually adapt to the changes in the environment from limited data provided by the users (experimenters), and use the learned knowledge to perform object fetching tasks."
Human-Robot Interactive Creation of Artistic Portrait Drawings,"Fei Gao, Dai Lingna, Jingjie Zhu, Mei Du, Zhang Yiyuan, Maoying Qiao, Chenghao Xia, Nannan Wang, Peng Li","Xidian University,AiSketcher Technology Co.Ltd,Aisketcher Technology Co.Ltd.,Hangzhou Danzi University,UTS,The University of Sydney,Institute of Software, Chinese Academy of Sciences",Human-Robot Interaction II,"In this paper, we present a novel system for Human-Robot Interactive Creation of Artworks (HRICA). Different from previous robot painters, HRICA allows a human user and a robot to alternately draw strokes on a canvas, to collaboratively create a portrait drawing through frequent interactions. The key is to enable the robot to understand human intentions, during the interactive creation process. We here formulate this as a mask-free image inpainting problem, and propose a novel method to estimate the complete version of a portrait drawing, after the human user has drawn some initial strokes. In this way, the robot can select some complementary strokes and draw them on the canvas. To train and evaluate our inpainting method, we construct a novel large-scale portrait drawing dataset, CelebLine, which composes of high-quality portrait line-drawings, with dense labels of both 2D semantic parsing masks and 3D depth maps. Finally, we develop a humanrobot interactive drawing system with low-cost hardware, user-friendly interface, and interesting creation experience. Experiments show that our robot can stably cooperate with human users to create diverse styles of portrait drawings. In addition, our portrait drawing inpainting method significantly outperforms previous advanced methods. The code and dataset have been released at: https://github.com/fei-aiart/HRICA."
High Stimuli Virtual Reality Training for a Brain Controlled Robotic Wheelchair,"Alexander Thomas, Jianan Chen, Anna Hella-szabo, Merlin Kelly, Tom Carlson","University College London,Univeristy College London,University College London, UK",Human-Robot Interaction II,"Smart robotic wheelchairs, as well as other assistive robotic devices, can provide an effective form of independent mobility for those who suffer with motor disabilities. Although many control interfaces exist to operate these devices, brain computer interfaces (BCI) offer a control modality for those who have little to no motor function, as well as being able to re-associate movement with brain functionality. Although BCIs have been designed for robotic wheelchairs, more research and development is required before they can be adopted for use in the â€˜real worldâ€™. One key challenge on that journey is the user training required to achieve an acceptable accuracy of the control. In this paper, we aim to identify the best training method by comparing users trained on a simple task, in a simulated environment on a 2D display (VR-2DD) and in a virtual environment using a virtual reality headset (VR-HMD). We trained 15 participants in mix of high and low noise virtual environments or on a simple training task, and found a significant improvement in the classification accuracies of the participants who trained using the VR-2DD task compared with those who were trained with the simple task. We also carried out active (online) tests across all participants in the same virtual training environment, with a varying level of external stimuli, and found a significant improvement in the performance of participants in both VR groups compared to participants in the simple task group."
Automatic Captioning Based on Visible and Infrared Images,"Yan Wang, Shuli Lou, Kai Wang, Xiaohu Yuan, Huaping Liu","Yantai University,Yantai Univ.,Tsinghua Univerisity,Tsinghua University",Human-Robot Interaction II,"In this paper, we tackle the task of image captioning with the complementarity of visible light images and infrared images. To address this problem, we propose an RGB-IR image fusion captioning model, which can take full advantage of visible light images and infrared images under different conditions. Meanwhile, we develop a wearable environment-assisted system. In addition, we collect and annotate a new dataset containing 3510 pairs of RGB-IR images to support model training. Finally, we conduct extensive experiments to evaluate the model and system. Experimental results show that our new method and system significantly outperform baselines on multiple metrics and have potential practical value."
A Semi-Automatic Oriental Ink Painting Framework for Robotic Drawing from 3D Models,"Hao Jin, Minghui Lian, Shicheng Qiu, Xuxu Han, Xizhi Zhao, Long Yang, Zhiyi Zhang, Haoran Xie, Kouichi Konno, Shaojun Hu","Northwest A&F University,Japan Advanced Institute of Science and Technology,Iwate University",Human-Robot Interaction II,"Creating visually pleasing stylized ink paintings from 3D models is a challenge in robotic manipulation. We propose a semi-automatic framework that can extract expressive strokes from 3D models and draw them in oriental ink painting styles by using a robotic arm. The framework consists of a simulation stage and a robotic drawing stage. In the simulation stage, geometrical contours were automatically extracted from a certain viewpoint and a neural network was employed to create simplified contours. Then, expressive digital strokes were generated after interactive editing according to user's aesthetic understanding. In the robotic drawing stage, an optimization method was presented for drawing smooth and physically consistent strokes to the digital strokes, and two oriental ink painting styles termed as Noutan (shade) and Kasure (scratchiness) were applied to the strokes by robotic control of a brush's translation, dipping and scraping. Unlike existing methods that concentrate on generating paintings from 2D images, our framework has the advantage of rendering stylized ink paintings from 3D models by using a consumer-grade robotic arm. We evaluate the proposed framework by taking 3 standard models and a user-defined model as examples. The results show that our framework is able to draw visually pleasing oriental ink paintings with expressive strokes."
A 3D Mixed Reality Interface for Human-Robot Teaming,"Jiaqi Chen, Boyang Sun, Hermann Blum, Marc Pollefeys",ETH Zurich,Human-Robot Interaction II,"This paper presents a mixed-reality human-robot teaming system. It allows human operators to see in real-time where robots are located, even if they are not in line of sight. The operator can also visualize the map that the robots create of their environment and can easily send robots to new goal positions. The system mainly consists of a mapping and a control module. The mapping module is a real-time multi-agent visual SLAM system that co-localizes all robots and mixed-reality devices to a common reference frame. Visualizations in the mixed-reality device then allow operators to see a virtual life-sized representation of the cumulative 3D map overlaid onto the real environment. As such, the operator can effectively â€œsee throughâ€ walls into other rooms. To control robots and send them to new locations, we propose a drag-and-drop interface. An operator can grab any robot hologram in a 3D mini map and drag it to a new desired goal pose. We validate the proposed system through a user study and real-world deployments. We make the mixed-reality application publicly available at github.com/cvg/hololens_ros."
Online Camera Orientation Calibration Aided by a High-Speed Ground-View Camera,"Junzhe Su, Masahiro Hirano, Yuji Yamakawa",The University of Tokyo,Wheeled Robots,
Fast Wheeled Driving to Legged Leaping Onto a Step in a Leg-Wheel Transformable Robot,"Zhi-ren Chen, Wei-shun Yu, Pei-Chun Lin","NTU,National Taiwan University",Wheeled Robots,"The leg-wheel transformable robot has the advantage of smooth, fast, and power-efficient motion on flat terrain and negotiability on rough terrain. This study presents a highly dynamic maneuver of the robot to leap onto a step using its legged form from its original form of wheeled driving, taking full advantage of the rapid switching capabilities of the leg-wheel design of the robot. The robot motion is designed based on a reduced-order model and is planned using an optimization method with multiple constraints. In addition, both position and impedance control strategies are investigated. The proposed strategy is experimentally evaluated. The results show that the robot can leap onto a step higher than itself and then smoothly transition back to the wheeled mode after leaping. The dynamic driving-to-leaping maneuver endows the robot with an alternative and time-efficient approach to negotiate the step obstacles."
Body Velocity Estimation in a Legâ€“Wheel Transformable Robot without a Priori Knowledge of Legâ€“Wheel Ground Contacts,"Pei-chun Huang, I-Chia Chang, Wei-shun Yu, Pei-Chun Lin","National Taiwan University,Purdue University",Wheeled Robots,"The state estimation of legged robots often relies on ground contact detection. However, due to complex mechanisms and other factors, ground contact detection can be challenging to obtain in certain situations. This paper presents a velocity estimation method that combines inertia measurement unit (IMU) and encoders, allowing estimation without using the ground contact state as the a priori. In this paper, the initial estimate derived from IMU integration is refined. Following the computation of velocity and ground contact state probabilities using encoder data, these probabilities are employed to modify particle weights within the particle filter framework. Subsequent resampling ensures that the contact status converges toward the correct result. This paper tests the algorithm through simulations and validates the method with physical experiments, showcasing the feasibility of concurrent ground contact state and velocity estimation."
Rolling with Planar Parametric Curves for Real-Time Robot Locomotion Algorithms,"Adwait Mane, Christian Hubicki","FAMU-FSU College of Engineering, Florida State University, Talla,Florida State University",Wheeled Robots,"Robots routinely encounter obstacles and rough terrain, but terrain curvature is seldom included in models for real-time algorithms. We present a closed-form dynamic model for rolling with two planar smooth curves, and apply it to sagittal-plane locomotion problems. We assumed that the body rolls without slip and maintains a single point of contact. Using an auxiliary coordinate system to define the rolling body and terrain as parametric curves, we derived rolling constraints and dynamic equations of motion for model-based control algorithms - specifically Operational Space Control. The formulation was used to simulate an arbitrarily curved rock rolling on undulating terrain and to generate control signals to stabilize it on parabolic terrain. The stabilization problem was solved as a quadratic program in < 3 ms which shows that our formulation is suitable for real-time control algorithms. We also applied this framework to dynamically balance an underactuated 2 degree-of-freedom leg on parabolic terrain and achieve prescribed locomotion tasks for a wheel-leg vehicle on sinusoidal terrain in simulation."
Non-Smooth Trajectory Optimization for Wheeled Balancing Robots with Contact Switches and Impacts,"Victor Klemm, Yvain De Viragh, David Rohr, Roland Siegwart, Marco Tognon","ETH Zurich,Inria Rennes",Wheeled Robots,"Recent years have seen a steady rise in the abilities of wheeled-legged balancing robots. Yet, their use is still severely restricted by the lack of efficient control algorithms for overcoming obstacles such as stairs. We take a considerable step towards closing this gap by presenting a fast trajectory optimizer for generating trajectories over a large class of challenging terrains. By limiting the underlying modeling to the planar, nonlinear rigid-body dynamics and subdividing the terrain into contact-phases, a tractable nonlinear programming problem is obtained. The model explicitly accounts for contact switches and impacts, traction limits, and actuation bounds. By introducing an arc-length-related parametrization, the trajectories are rendered inherently contact constraint-consistent. We apply our method to the specific case of the wheeled bipedal robot Ascento, for which we derive closed-form expressions of the dynamics equations, including the kinematic loops. To track the trajectories, we propose a simple LQR-based controller. The approach is validated in real-world experiments where we show the execution of trajectories for traversing steps, driving up ramps, jumping, standing up, and driving up entire stairways. To the authorsâ€™ best knowledge, enabling the latter by means of trajectory optimization is a novelty for wheeled-legged robots."
Design and Central Pattern Generator Control of a New Transformable Wheel-Legged Robot,"Tyler Bishop, Keran Ye, Konstantinos Karydis","University of California, Riverside",Wheeled Robots,"This paper introduces a new wheel-legged robot and develops motion controllers based on central pattern generators (CPGs) for the robot to navigate over a range of terrains. A transformable leg-wheel design is considered and characterized in terms of key locomotion characteristics as a function of the design. Kinematic analysis is conducted based on a generalized four-bar mechanism driven by a coaxial hub arrangement. The analysis is used to inform the design of a central pattern generator to control the robot by mapping oscillator states to wheel-leg trajectories and implementing differential steering within the oscillator network. Three oscillator models are used as the basis of the CPGs, and their performance is compared over a range of inputs. The CPG-based controller is used to drive the developed robot prototype on level ground and over obstacles. Additional simulated tests are performed for uneven terrain negotiation and obstacle climbing. Results demonstrate the effectiveness of CPG control in transformable wheel-legged robots."
Planned Trajectory Classification for Wheeled Mobile Robots to Prevent Rollover and Slip,"Sang-Yun Jeon, Rakjoon Chung, Dongjun Lee","Seoul National University,Samsung Electronics",Wheeled Robots,"In this paper, a novel planned trajectory classification method (PTCM) is proposed to evaluate the safety of the car-like four-wheeled mobile robots (4-WMRs) with Ackermann steering. To classify a planned trajectory to be safe or unsafe before the 4-WMR actually follows it, the conditions of the wheel forces (WFs: longitudinal, lateral, and normal forces for each wheel) necessary to execute the planned trajectory without rollover and slip are calculated using the passive decomposition of the WMR dynamics with the Pfaffian constraints of the no-rollover and no-slip conditions. Similar to the case of Navierâ€™s table problem, only nine-dimensional WFs projected onto the constrained space are identifiable among the twelve-dimensional WFs. This indeterminacy turns out not to affect the rollover prediction, yet does so for the slip prediction. For this, we propose novel optimistic and pessimistic methods, together upper and lower bounding the exact slip prediction. The proposed PTCM classifies the planned trajectory as safe if rollover and slip are not predicted and unsafe otherwise. The proposed PTCM is demonstrated and validated by simulations and outdoor experiments."
Mechanical Design and Kinematics of a Multimodal Two-Wheeled Robot,"Botian Sun, Qinglin Lang, Minghe Li, Xuefeng Wang",Peking University,Wheeled Robots,"A two-wheeled vehicle has a compact structure and high mobility in crowded and complex environments. The bicycle and self-balancing vehicle are two main modes of the two-wheeled vehicle, and their combination allows for good balance-control stability at both high and low speeds. Four control inputs by two steerable driving wheels are required to implement transformations between the two modes due to the difference of their configuration spaces. However, the control inputs are redundant for planar motions, which results in an over constraint of the vehicle. In this work, a two-wheeled robot with an addition structural deformation is designed to balance inputs and degrees of freedom (DOFs), so that the over constraint is avoided. A transition mode based on oblique vehicle motions is used to bridge the transformation of the bicycle and self-balancing vehicle modes. A general kinematic model is developed for planar motions of the two-wheeled robot, and kinematics of the three modes are special cases with particular servo constraints. Structural deformation control laws are developed and experimentally validated on a prototype robot. Smooth transformations of the multimodal motions are also validated by the prototype."
Global Tracking Control for Car-Like Mobile Robots with Zero-Crossing Driving Velocity,Kai Yan,Beihang University,Wheeled Robots,"This work proposes a smooth time-varying controller to address the trajectory tracking problems of car-like mobile robots. Currently, literature does not suggest globally asymptotically stable controllers solving this problem. Unlike the prototypical method of transforming the model into a nonholonomic chained-form system, the proposed method is designed based on the original tracking error equation, and therefore our approach does not have singularities of chained-form transformations. Opposing current methods, our control law satisfactorily addresses the cases where the vehicle's velocity passes through zero. In general, our redesigned control law has no singularities, which can satisfy the requirement that the vehicle's velocity can cross zero and at the same time have a global attraction region. The design of the controller is mainly divided into two steps. Firstly, the linear velocity and steering angle of the robot are regarded as control inputs, which are designed by making the derivative of a positive definite Lyapunov-like function semi-negative definite. In the next step, another control input is designed by the backstepping approach. Furthermore, the global convergence of the state trajectory to the reference one is strictly proved by Barbalat's Lemma. Finally, simulated and actual experiments on a car-like robot demonstrate the effectiveness of the proposed control scheme."
Cascaded Compositional Residual Learning for Complex Interactive Behaviors,"Niranjan Kumar Kannabiran, Irfan Essa, Sehoon Ha",Georgia Institute of Technology,Legged Robots and Learning II,"Real-world autonomous missions often require rich interaction with nearby objects, such as doors or switches, along with effective navigation. However, such complex behaviors are difficult to learn because they involve both high-level planning and low-level motor control. We present a novel framework, Cascaded Compositional Residual Learning (CCRL), which learns composite skills by recursively leveraging a library of previously learned control policies. Our framework combines multiple levels of pre-learned skills by using multiplicative skill composition and residual action learning. We also introduce a goal synthesis network and an observation selector to support combination of heterogeneous skills, each with its unique goals and observation space. Finally, we develop residual regularization for learning policies that solve a new task, while preserving the style of the motion enforced by the skill library. We show that our framework learns joint-level control policies for a diverse set of motor skills ranging from basic locomotion to complex interactive navigation, including navigating around obstacles, pushing objects, crawling under a table, pushing a door open with its leg, and holding it open while walking through it. The proposed CCRL framework leads to policies with consistent styles and lower joint torques, which we successfully transfer to a real Unitree A1 robot without any additional fine-tuning."
Deep Compliant Control for Legged Robots,"Adrian Hartmann, Dongho Kang, Fatemeh Zargarbashi, Miguel Zamora, Stelian Coros","ETH Zürich,ETH Zurich",Legged Robots and Learning II,"Control policies trained using deep reinforcement learning often generate stiff, high-frequency motions in response to unexpected disturbances. To promote more natural and compliant balance recovery strategies, we propose a simple modification to the typical reinforcement learning training process. Our key insight is that stiff responses to perturbations are due to an agentâ€™s incentive to maximize task rewards at all times, even as perturbations are being applied. As an alternative, we introduce an explicit recovery stage where tracking rewards are given irrespective of the motions generated by the control policy. This allows agents a chance to gradually recover from disturbances before attempting to carry out their main tasks. Through an in-depth analysis, we highlight both the compliant nature of the resulting control policies, as well as the benefits that compliance brings to legged locomotion. In our simulation and hardware experiments, the compliant policy achieves more robust, energy-efficient, and safe interactions with the environment."
Imitating and Finetuning Model Predictive Control for Robust and Symmetric Quadrupedal Locomotion,"Dong Hoon Youm, Hyunyoung Jung, Hyeongjun Kim, Jemin Hwangbo, Hae-Won Park, Sehoon Ha","Korea Advanced Institute of Science and Technology,Georgia Institute of Technology,Korean Advanced Institute of Science and Technology",Legged Robots and Learning II,"Control of legged robots is a challenging problem that has been investigated by different approaches, such as model-based control and learning algorithms. This work proposes a novel Imitating and Finetuning Model Predictive Control (IFM) framework to take the strengths of both approaches. Our framework first develops a conventional model predictive controller (MPC) using Differential Dynamic Programming and Raibert heuristic, which serves as an expert policy. Then we train a clone of the MPC using imitation learning to make the controller learnable. Finally, we leverage deep reinforcement learning with limited exploration for further finetuning the policy on more challenging terrains. By conducting comprehensive simulation and hardware experiments, we demonstrate that the proposed IFM framework can significantly improve the performance of the given MPC controller on rough, slippery, and conveyor terrains that require careful coordination of footsteps. We also showcase that IFM can efficiently produce more symmetric, periodic, and energy-efficient gaits compared to Vanilla RL with a minimal burden of reward shaping."
Learning Agile Locomotion and Adaptive Behaviors Via RL-Augmented MPC,"Yiyu Chen, Quan Nguyen",University of Southern California,Legged Robots and Learning II,"In the context of legged robots, adaptive behavior involves adaptive balancing and adaptive swing foot reflection. While adaptive balancing counteracts perturbations to the robot, adaptive swing foot reflection helps the robot to navigate intricate terrains without foot entrapment. In this paper, we manage to bring both aspects of adaptive behavior to quadruped locomotion by combining RL and MPC while improving the robustness and agility of blind legged locomotion. This integration leverages MPC's strength in predictive capabilities and RL's adeptness in drawing from past experiences.Unlike traditional locomotion controls that separate stance foot control and swing foot trajectory, our innovative approach unifies them, addressing their lack of synchronization. At the heart of our contribution is the synthesis of stance foot control with swing foot reflection, improving agility and robustness in locomotion with adaptive behavior. A hallmark of our approach is robust blind stair climbing through swing foot reflection. Moreover, we intentionally designed the learning module as a general plugin for different robot platforms. We trained the policy and implemented our approach on the Unitree A1 robot, achieving impressive results: a peak turn rate of 8.5 rad/s, a peak running speed of 3 m/s, and steering at a speed of 2.5 m/s. Remarkably, this framework also allows the robot to maintain stable locomotion while bearing an unexpected load of 10 kg, or 83% of its body mass. We further demonstrate the generalizability and robustness of the same policy where it realizes zero-shot transfer to different robot platforms like Go1 and AlienGo robots for load carrying. Code is made available for the use of the research community at https://github.com/DRCL-USC/RL_augmented_MPC.git"
Extreme Parkour with Legged Robots,"Xuxin Cheng, Kexin Shi, Ananye Agarwal, Deepak Pathak","University of California, San Diego,Carnegie Mellon University",Legged Robots and Learning II,"Humans can perform parkour by traversing obstacles in a highly dynamic fashion requiring precise eye-muscle coordination and movement. Getting robots to do the same task requires overcoming similar challenges. Classically, this is done by independently engineering perception, actuation, and control systems to very low tolerances. This restricts them to tightly controlled settings such as a predetermined obstacle course in labs. In contrast, humans are able to learn parkour through practice without significantly changing their underlying biology. In this paper, we take a similar approach to developing robot parkour on a small low-cost robot with imprecise actuation and a single front-facing depth camera for perception which is low-frequency, jittery, and prone to artifacts. We show how a single neural net policy operating directly from a camera image, trained in simulation with large-scale RL, can overcome imprecise sensing and actuation to output highly precise control behavior end-to-end. We show our robot can perform a high jump on obstacles 2x its height, long jump across gaps 2x its length, do a handstand and run across tilted ramps, and generalize to novel obstacle courses with different physical properties. Parkour videos at https://extreme-parkour.github.io/."
Learning Risk-Aware Quadrupedal Locomotion Using Distributional Reinforcement Learning,"Lukas Schneider, Jonas Frey, Takahiro Miki, Marco Hutter",ETH Zurich,Legged Robots and Learning II,"Deployment in hazardous environments requires robots to understand the risks associated with their actions and movements to prevent accidents. Despite its importance, these risks are not explicitly modeled by currently deployed locomotion controllers for legged robots. In this work, we propose a risk sensitive locomotion training method employing distributional reinforcement learning to consider safety explicitly. Instead of relying on a value expectation, we estimate the complete value distribution to account for uncertainty in the robot's interaction with the environment. The value distribution is consumed by a risk metric to extract risk sensitive value estimates. These are integrated into Proximal Policy Optimization (PPO) to derive our method, Distributional Proximal Policy Optimization (DPPO). The risk preference, ranging from risk-averse to risk-seeking, can be controlled by a single parameter, which enables to adjust the robot's behavior dynamically. Importantly, our approach removes the need for additional reward function tuning to achieve risk sensitivity. We show emergent risk sensitive locomotion behavior in simulation and on the quadrupedal robot ANYmal. Videos of the experiments and code are available at https://sites.google.com/leggedrobotics.com/risk-aware-locomotion."
Robust Quadrupedal Locomotion Via Risk-Averse Policy Learning,"Jiyuan Shi, Chenjia Bai, Haoran He, Lei Han, Dong Wang, Bin Zhao, Mingguo Zhao, Xiu Li, Xuelong Li","Tsinghua University,Shanghai Artificial Intelligence Laboratory,Shanghai Jiao Tong University,Tencent Robotics X,Northwestern Polytechnical University",Legged Robots and Learning II,"The robustness of legged locomotion is crucial for quadrupedal robots in challenging terrains. Recently, Rein- forcement Learning (RL) has shown promising results in legged locomotion and various methods try to integrate privileged distillation, scene modeling, and external sensors to improve the generalization and robustness of locomotion policies. However, these methods are hard to handle uncertain scenarios such as abrupt terrain changes or unexpected external forces. In this paper, we consider a novel risk-sensitive perspective to enhance the robustness of legged locomotion. Specifically, we employ a distributional value function learned by quantile regression to model the aleatoric uncertainty of environments, and perform risk-averse policy learning by optimizing the worst-case scenarios via a risk distortion measure. Extensive experiments in both simulation environments and a real Aliengo robot demonstrate that our method is efficient in handling various external disturbances, and the resulting policy exhibits improved robustness in harsh and uncertain situations in legged locomotion. Videos are available at https://risk-averse- locomotion.github.io/."
Maximizing Quadruped Velocity by Minimizing Energy,"Srinath Mahankali, Chi-chang Lee, Gabriel Margolis, Zhang-wei Hong, Pulkit Agrawal","Massachusetts Institute of Technology,Research Center for Information Technology Innovation, Academia ,National Tsing Hua University,MIT",Legged Robots and Learning II,"Reinforcement Learning (RL) has been a powerful tool for training robots to acquire agile locomotion skills. To learn locomotion, it is commonly necessary to introduce additional reward-shaping terms, such as an energy minimization term, to guide an algorithm like Proximal Policy Optimization (PPO) to good performance. Prior works rely on hyper-parameter tuning on the weight of the reward shaping terms to obtain satisfactory task performance. To save the efforts of tuning these weights, we adopt the Extrinsic-Intrinsic Policy Optimization (EIPO) framework. The key idea of EIPO is to establish a constrained optimization framework for the primary objective of enhancing task performance and the secondary objective of minimizing energy consumption. It seeks a policy that minimizes the energy consumption objective within the optimal policy space for task performance. This guarantees that the learned policy excels in task performance while conserving energy, all without requiring manual weight adjustments for both objectives. Our experiments evaluate EIPO on various quadruped locomotion tasks, revealing that policies trained with EIPO consistently achieve higher task performance than PPO comparisons while maintaining comparable energy consumption levels. Furthermore, EIPO exhibits superior task performance in real-world evaluations compared to PPO."
Dexterous Legged Locomotion in Confined 3D Spaces with Reinforcement Learning,"Zifan Xu, Amir Hossain Raj, Xuesu Xiao, Peter Stone","University of Texas at Austin,George Mason University",Legged Robots and Learning II,"Recent advances of locomotion controllers utilizing deep reinforcement learning (RL) have yielded impressive results in terms of achieving rapid and robust locomotion across challenging terrain, such as rugged rocks, non-rigid ground, and slippery surfaces. However, while these controllers primarily address challenges underneath the robot, relatively little research has investigated legged mobility through confined 3D spaces, such as narrow tunnels or irregular voids, which impose all-around constraints. The cyclic gait patterns resulted from existing RL-based methods to learn parameterized locomotion skills characterized by motion parameters, such as velocity and body height, may not be adequate to navigate robots through challenging confined 3D spaces, requiring both agile 3D obstacle avoidance and robust legged locomotion. Instead, we propose to learn locomotion skills end-to-end from goal-oriented navigation in confined 3D spaces. To address the inefficiency of tracking distant navigation goals, we introduce a hierarchical locomotion controller that combines a classical planner tasked with planning waypoints to reach a faraway global goal location, and an RL-based policy trained to follow these waypoints by generating low-level motion commands. This approach allows the policy to explore its own locomotion skills within the entire solution space and facilitates smooth transitions between local goals, enabling long-term navigation towards distant goals. In simulation, our hierarchical approach succeeds at navigating through demanding confined 3D environments, outperforming both pure end-to-end learning approaches and parameterized locomotion skills. We further demonstrate the successful real-world deployment of our simulation-trained controller on a real robot."
Optimal Control for Clutched-Elastic Robots: A Contact-Implicit Approach,"Dennis Ossadnik, Vasilije Rakcevic, Mehmet Can Yildirim, Edmundo Pozo FortuniÄ‡, Hugo Tadashi Kussaba, Abdalla Swikir, Sami Haddadin",Technical University of Munich,Optimization and Optimal Control II,"Intrinsically elastic robots surpass their rigid counterparts in a range of different characteristics. By temporarily storing potential energy and subsequently converting it to kinetic energy, elastic robots are capable of highly dynamic motions even with limited motor power. However, the time-dependency of this energy storage and release mechanism remains one of the major challenges in controlling elastic robots. A possible remedy is the introduction of locking elements (i.e. clutches and brakes) in the drive train. This gives rise to a new class of robots, so-called clutched-elastic robots (CER), with which it is possible to precisely control the energy-transfer timing. A prevalent challenge in the realm of CERs is the automatic discovery of clutch sequences. Due to complexity, many methods still rely on pre-defined modes. In this paper, we introduce a novel contact-implicit scheme designed to optimize both control input and clutch sequence simultaneously. A penalty in the objective function ensures the prevention of unnecessary clutch transitions. We empirically demonstrate the effectiveness of our proposed method on a double pendulum equipped with two of our newly proposed clutch-based Bi-Stiffness Actuators (BSA)."
Optimal Control for Articulated Soft Robots,"Saroj Prasad Chhatoi, Michele Pierallini, Franco Angelini, Carlos Mastalli, Manolo Garabini","University of Pisa,Centro di Ricerca E. Piaggio - Università di Pisa,Heriot-Watt University,Università di Pisa",Optimization and Optimal Control II,"Soft robots can execute tasks with safer interactions. However, control techniques that can effectively exploit the systemsâ€™ capabilities are still missing. Differential dynamic programming (DDP) has emerged as a promising tool for achieving highly dynamic tasks. But most of the literature deals with applying DDP to articulated soft robots by using numerical differentiation, in addition to using pure feed-forward control to perform explosive tasks. Further, underactuated compliant robots are known to be difficult to control and the use of DDP-based algorithms to control them is not yet addressed. We propose an efficient DDP-based algorithm for trajectory optimization of articulated soft robots that can optimize the state trajectory, input torques, and stiffness profile. We provide an efficient method to compute the forward dynamics and the analytical derivatives of series elastic actuators /variable stiffness actuators and underactuated compliant robots. We present a state-feedback controller that uses locally optimal feedback policies obtained from DDP. We show through simulations and experiments that the method can generate motion plans and control for robo"
Force Feedback Model-Predictive Control Via Online Estimation,"Armand Jordana, Sebastien Kleff, Justin Carpentier, Nicolas Mansard, Ludovic Righetti","New York University,INRIA,CNRS",Optimization and Optimal Control II,"Nonlinear model-predictive control has recently shown its practicability in robotics. However it remains limited in contact interaction tasks due to its inability to leverage sensed efforts. In this work, we propose a novel model-predictive control approach that incorporates direct feedback from force sensors while circumventing explicit modeling of the contact force evolution. Our approach is based on the online estimation of the discrepancy between the force predicted by the dynamics model and force measurements, combined with high-frequency nonlinear model-predictive control. We report an experimental validation on a torque-controlled manipulator in challenging tasks for which accurate force tracking is necessary. We show that a simple reformulation of the optimal control problem combined with standard estimation tools enables to achieve state-of-the-art performance in force control while preserving the benefits of model-predictive control, thereby outperforming traditional force control techniques. This work paves the way toward a more systematic integration of force sensors in model predictive control."
Geometric Algebra for Optimal Control with Applications in Manipulation Tasks,"Tobias Löw, Sylvain Calinon","Idiap Research Institute, EPFL,Idiap Research Institute",Optimization and Optimal Control II,"Many problems in robotics are fundamentally problems of geometry, which lead to an increased research effort in geometric methods for robotics in recent years. The results were algorithms using the various frameworks of screw theory, Lie algebra and dual quaternions. A unification and generalization of these popular formalisms can be found in geometric algebra. The aim of this paper is to showcase the capabilities of geometric algebra when applied to robot manipulation tasks. In particular the modelling of cost functions for optimal control can be done uniformly across different geometric primitives leading to a low symbolic complexity of the resulting expressions and a geometric intuitiveness. We demonstrate the usefulness, simplicity and computational efficiency of geometric algebra in several experiments using a Franka Emika robot. The presented algorithms were implemented in c++20 and resulted in the publicly available library gafro. The benchmark shows faster computation of the kinematics than state-of-the-art robotics libraries."
Trajectory Tracking Runtime Assurance for Systems with Partially Unknown Dynamics,"Michael Enqi Cao, Samuel Coogan","Georgia Institute of Technology,Georgia Tech",Optimization and Optimal Control II,"We consider the problem of tracking a reference trajectory for dynamical systems subject to a priori unknown state-dependent disturbance behavior. We propose a formulation that embeds the uncertain system into a higher dimensional deterministic system that accounts for worst case disturbances. Our main insight is that a single controlled trajectory of this embedding system corresponds to a controlled forward invariant interval tube around the reference trajectory. By taking observations of the system, we then propose to estimate the state-dependent uncertainty with Gaussian Process regression, which improves the accuracy of the forward invariant tube as data is collected. Given a safety objective, we also provide conditions on when an additional observation of the unknown disturbance behavior needs to be collected to maintain safety. We demonstrate our formulation on a case study of a planar multirotor attempting a safe landing in an unknown wind field."
How to Train Your Neural Control Barrier Function: Learning Safety Filters for Complex Input-Constrained Systems,"Oswin So, Zachary Serlin, Makai Mann, Jake Gonzales, Kwesi Rutledge, Nicholas Roy, Chuchu Fan","Massachusetts Institute of Technology,Boston University,MIT Lincoln Laboratory,University of Washington,University of Michigan",Optimization and Optimal Control II,"Control barrier functions (CBFs) have become popular as a safety filter to guarantee the safety of nonlinear dynamical systems for arbitrary inputs. However, it is difficult to construct functions that satisfy the CBF constraints for high relative degree systems with input constraints. To address these challenges, recent work has explored learning CBFs using neural networks via neural CBFs (NCBFs). However, such methods face difficulties when scaling to higher dimensional systems under input constraints. In this work, we first identify challenges that NCBFs face during training. Next, to address these challenges, we propose policy neural CBFs (PNCBFs), a method of constructing CBFs by learning the value function of a nominal policy, and show that the value function of the maximum-over-time cost is a CBF. We demonstrate the effectiveness of our method in simulation on a variety of systems ranging from toy linear systems to a jet aircraft with a 16-dimensional state space. Finally, we validate our approach on a two-agent quadcopter system on hardware under tight input constraints."
"Stable, Safe, and Passive Teleoperation of Multi-Robot Systems",Gennaro Notomista,University of Waterloo,Optimization and Optimal Control II,"In this paper, we present a unified framework to ensure the stability, safety, and passivity of a multi-robot teleoperation system in a holistic fashion. The proposed approach consists of encoding these three properties as constraints in an optimization-based controller using control Lypaunov and (integral) control barrier functions. The result is a stability-safety-passivity (SSP) filter implemented as a convex optimization control policy, which can be efficiently evaluated in an online fashion. The developed filter minimally modifies the teleoperation input in order to ensure that the robotic system remains stable, safe, and passive. The effectiveness of the developed approach is showcased using a team of mobile robots in a human-multi-robot teleoperation scenario."
Approximate Optimal Controller Synthesis for Cart-Poles and Quadrotors Via Sums-Of-Squares,"Lujie Yang, Hongkai Dai, Alexandre Amice, Russ Tedrake","MIT,Toyota Research Institute,Massachusetts Institute of Technology",Optimization and Optimal Control II,"Sums-of-squares (SOS) optimization is a promising tool to synthesize certifiable controllers for nonlinear dynamical systems. Building upon prior works, we demonstrate that SOS can synthesize dynamic controllers with bounded suboptimal performance for various underactuated robotic systems by finding good approximations of the value function. We summarize a unified SOS framework to synthesize both under- and over- approximations of the value function for continuous-time, control-affine systems, use these approximations to generate approximate optimal controllers, and perform regional analysis on the closed-loop system driven by these controllers. We then extend the formulation to handle hybrid systems with contacts. We demonstrate that our method can generate tight under- and over- approximations of the value function with low-degree polynomials, which are used to provide stabilizing controllers for continuous-time systems including the inverted pendulum, the cart-pole, and the quadrotor as well as a hybrid system, the planar pusher. To the best of our knowledge, this is the first time that a SOS-based time-invariant controller can swing up and stabilize a cart-pole, and push the planar slider to the desired pose."
Online Multi-Contact Feedback Model Predictive Control for Interactive Robotic Tasks,"Seo Wook Han, Maged Iskandar, Jinoh Lee, Min Jun Kim","KAIST,German Aerospace Center - DLR,German Aerospace Center (DLR)",Optimization and Optimal Control II,"In this paper, we propose a model predictive control (MPC) that accomplishes interactive robotic tasks, in which multiple contacts may occur at unknown locations. To address such scenarios, we made an explicit contact feedback loop in the MPC framework. An algorithm called Multi-Contact Particle Filter with Exploration Particle (MCP-EP) is employed to establish real-time feedback of multi-contact information. Then the interaction locations and forces are accommodated in the MPC framework via a spring contact model. Moreover, we achieved real-time control for a 7 degrees of freedom robot without any simplifying assumptions by employing a Differential-Dynamic-Programming algorithm. We achieved 6.8kHz, 1.9kHz, and 1.8kHz update rates of the MPC for 0, 1, and 2 contacts, respectively. This allows the robot to handle unexpected contacts in real time. Real-world experiments show the effectiveness of the proposed method in various scenarios."
Learning-Based Efficient Phase-Amplitude Modulation and Hybrid Control for MRI-Guided Focused Ultrasound Treatment,"Jing Dai, Bohao Zhu, Xiaomei Wang, Zhiyi Jiang, Mengjie Wu, Liyuan Liang, Xiaochen Xie, James Lam, Hing-chiu Chang, Ka-Wai Kwok","The University of Hong Kong,University of Hong Kong,THE UNIVERSITY OF HONG KONG,Harbin Institute of Technology, Shenzhen",Medical Robots VI,"Magnetic resonance-guided focused ultrasound (MRg-FUS) has become attractive, accrediting to its non-invasive nature. However, ultrasound beams focusing and steering is still challenging owing to aberrations induced by soft tissue heterogeneity. In particular for beam motion control to ensure real-time and precise tracking in the deep-seated region over abdominal organs, while considering full-wave propagation. To this end, we proposed a closed-loop hybrid control scheme and a learning-based modulation model for robot-assisted MRg-FUS treatments. By introducing a rapid phase estimator to provide an efficient ("
Co-Axial Slender Tubular Robot (CAST): Towards Robotized Operation for Transorbital Neurosurgery with Minimal Invasiveness,"Shuai Wang, Qing Xiang Zhao, Jian Chen, Mingcong Chen, Guanglin Cao, Jian Hu, Runfeng Zhu, Hongbin Liu","HKPU(The Hong Kong Polytechnic University),Hong Kong Institute of Science & Innovation, Centre for Artifici,University of Chinese Academy of Sciences,City University of Hong Kong,Institute of Automation, Chinese Academy of Sciences,The Hong Kong Polytechnic University,Hong Kong Institute of Science & Innovation, Chinese Academy of ",Medical Robots VI,"Transorbital Neuro Surgery (TNS) offers a novel treatment towards the lesion inside skull pursuing minimal invasiveness. Most conventional TNS tools are rigid and straight, limiting the dexterity and accessibility in passing a small port. Bendable and steerable surgical tools provides an alternative for this issue. In this work, we proposed a dual-segment slender surgical robot arm for TNS, which is a Co-Axial Slender Tubular robot (CAST), and modelled it using novel approaches. Another contribution is tendon-mortise shaped slits along the axial direction, enhancing the overall stiffness. The bending of CAST is actuated by pushing/pulling distance, and the maximum diameter is only 1.7mm with high dexterity after mounting on a rigid robot arm. Experiments demonstrates that the proposed the slit design doubles the stiffness properties compared to traditional rectangle slit designs. The path-following task shows that the position error was maximally 3mm in open-looped control. Test on a skull model demonstrates that the whole system could successfully perform electrocoagulation procedure inside the depth of skull in a robotized manner effectively."
Vascular Centerline-Guided Autonomous Navigation Methods for Robot-Lead Endovascular Interventions,"Naner Li, Yiwei Wang, Haoyuan Cheng, Huan Zhao, Han Ding",Huazhong University of Science and Technology,Medical Robots VI,"In minimally invasive endovascular interventional surgery, guidewire navigation is an indispensable process. However, even experienced physicians often encounter difficulties in manually manipulating the guidewire for branch selection, while also facing the risk of radiation exposure. In this study, we investigated robotic autonomous guidewire navigation methods. An electromagnetic system was used to track the real-time position and orientation of the guidewire tip, and a state space representing the guidewire within the vascular environment was constructed to guide the robot in precise guidewire manipulation. Experimental results demonstrated that the proposed trial-and-error and centerline-guided methods successfully completed navigation tasks in a static environment, outperforming human navigation performance in terms of trajectory smoothness, trajectory length, and incorrect branch entry counts. For dynamic environment navigation, dynamic time warping (DTW), a technique for measuring the similarity between two temporal sequences, was integrated into the centerline-guided method. The proposed approaches eliminate the need for visual feedback and thereby minimizing the risk of radiation exposure for both patients and medical staff present in the operating room during the procedure."
A Soft Micro-Robotic Catheter for Aneurysm Treatment: A New Design and Enhanced Euler-Bernoulli Model with Cross-Section Optimization,"Nicotra Emanuele, Nguyen Chi Cong, James J. Davies, Phuoc Thien Phan, Trung Thien Hoang, Sharma Bibhu, Adrienne Ji, Kefan Zhu, Trung-dung Ngo, Van Ho, Hung La, Nigel Lovell, Thanh Nho Do","UNSW Sydney,University of New South Wales,University of Prince Edward Island,Japan Advanced Institute of Science and Technology,University of Nevada at Reno",Medical Robots VI,"Aneurysms, balloon-like bulges in blood vessels, present a signifi cant health risk due to their potential to rupture, leading to life-threatening internal bleeding. Current treatments often involve delivering embolic materials or metal coils to fill these bulges, occluding them from the pressure of blood flow. However, clinical micro-catheters that deploy embolic materials used today face limitations, primarily their rigidity and the lack of active control over the bending tip of the catheter. This paper introduces a new soft micro-robotics catheter, with diameter of only 0.8 mm, equipped with a hollow channel. With this new design, the new device can induce bending motions at its tip for active steerability to reach desired aneurysm targets and then perform the delivery of embolic materials and tools. To enhance the control and precise navigation during procedures, a robust mathematical model and image processing techniques are also introduced and validated. Experiments are also performed to characterise and validate the modelâ€™s accuracy and the steerability and navigation capabilities of the new micro-catheter."
A Generic Modeling Framework for the Design of Tendon-Driven Continuum Manipulators with Flexure Patterns,"Yang Liu, Hansoul Kim, Yash Kulkarni, Farshid Alambeigi","The University of Texas at Austin,The University of California, Berkeley,University of Texas at Austin",Medical Robots VI,"In this paper, a novel mathematical framework is introduced for modeling deformation behavior of Tendon- Driven Continuum Manipulators (TD-CMs) featuring discontinuous cross-sectional geometries (i.e., having flexural patterns). Leveraging this framework, we also introduce the concept of design space by which the deformation-behavior space of a TD-CM can intuitively be analyzed via its geometrical design parameters. To thoroughly evaluate the performance of the proposed modeling framework, we have conducted various simulation studies and experiments"
"Bevel-Tip Needle Deflection Modeling, Simulation, and Validation in Multi-Layer Tissues","Yanzhou Wang, Lidia Al-zogbi, Guanyun Liu, Jiawei Liu, Tokuda Junichi, Axel Krieger, Iulian Iordachita","Johns Hopkins University,University of Florida,Brigham and Women's Hospital and Harvard Medical School",Medical Robots VI,"Percutaneous needle insertions are commonly performed for diagnostic and therapeutic purposes as an effective alternative to more invasive surgical procedures. However, the outcome of needle-based approaches relies heavily on the accuracy of needle placement, which remains a challenge even with robot assistance and medical imaging guidance due to needle deflection caused by contact with soft tissues. In this paper, we present a novel mechanics-based 2D bevel-tip needle model that can account for the effect of nonlinear strain-dependent behavior of biological soft tissues under compression. Real-time finite element simulation allows multiple control inputs along the length of the needle with full three-degree-of-freedom (DOF) planar needle motions. Cross-validation studies using custom-designed multi-layer tissue phantoms as well as heterogeneous chicken breast tissues result in less than 1mm in-plane errors for insertions reaching depths of up to 61 mm, demonstrating the validity and generalizability of the proposed method."
Excitation Trajectory Optimization for Dynamic Parameter Identification Using Virtual Constraints in Hands-On Robotic System,"Tian Huanyu, Martin Huber, Christopher Edwin Mower, Zhe Han, Changsheng Li, Xingguang Duan, Christos Bergeles","Beijing Institution of Technology,King's College London,Huawei Technologies Research & Development,Beijing Institute of Technology",Medical Robots VI,"This paper proposes a novel, more computationally efficient method for optimizing robot excitation trajectories for dynamic parameter identification, emphasizing self-collision avoidance. This addresses the system identification challenges for getting high-quality training data associated with co-manipulated robotic arms that can be equipped with a variety of tools, a common scenario in industrial but also clinical and research contexts. Utilizing the Unified Robotics Description Format (URDF) to implement a symbolic Python implementation of the Recursive Newton-Euler Algorithm (RNEA), the approach aids in dynamically estimating parameters such as inertia using regression analyses on data from real robots. The excitation trajectory was evaluated and achieved on par criteria when compared to state-of-the-art reported results which didn't consider self-collision and tool calibrations. Furthermore, physical Human-Robot Interaction (pHRI) admittance control experiments were conducted in a surgical context to evaluate the derived inverse dynamics model showing a 30.1% workload reduction by the NASA TLX questionnaire."
Learning-Based Inverse Perception Contracts and Applications,"Dawei Sun, Benjamin Yang, Sayan Mitra","UIUC,University of Illinois at Urbana Champaign,University of Ilinois, Urbana Champagne",Robot Safety II,"Perception modules are integral in many modern autonomous systems, but their accuracy can be subject to the vagaries of the environment. In this paper, we propose a learning-based approach that can automatically characterize the error of a perception module from data and use this for safe control. The proposed approach constructs an inverse perception contract (IPC) which generates a set that contains the ground-truth value that is being estimated by the perception module, with high probability. We apply the proposed approach to study a vision pipeline deployed on a quadcopter. With the proposed approach, we successfully constructed an IPC for the vision pipeline. We then designed a control algorithm that utilizes the learned IPC, with the goal of landing the quadcopter safely on a landing pad. Experiments show that with the learned IPC, the control algorithm safely landed the quadcopter despite the error from the perception module, while the baseline algorithm without using the learned IPC failed to do so."
Safe Multi-Robot Exploration Using Symbolic Control,"Manas Sashank Juvvi, David Smith Sundarsingh, Ratnangshu Das, Pushpak Jagtap","Indian Institute of Science, Bengaluru,Indian Institute of Science, Bangalore,Indian Institute of Science",Robot Safety II,"Multi-robot exploration is a complex problem that involves multiple robots working in a shared unknown environment. In such scenarios, the safety of the robots is of paramount importance alongside the completion of the exploration task. In this paper, we propose a modular exploration framework that (i) identifies safe frontier targets for multiple robots while taking into account the system dynamics of each robot to ensure collision avoidance with previously unknown obstacles and (ii) ensures that the robots reach their exploration targets while avoiding any obstacles discovered and each other. We employ a scalable approach to generate symbolic controllers for the multi-robot system, utilizing distance functions. We also provide formal guarantees on the safety of the exploration targets and the completion of each exploration run, with the robots avoiding collisions with each other and the obstacles. We test our approach on simulation experiments and a real-world implementation to validate it."
Receding-Constraint Model Predictive Control Using a Learned Approximate Control-Invariant Set,"Gianni Lunardi, Asia La Rocca, Matteo Saveriano, Andrea Del Prete",University of Trento,Robot Safety II,"In recent years, advanced model-based and data-driven control methods are unlocking the potential of complex robotics systems, and we can expect this trend to continue at an exponential rate in the near future. However, ensuring safety with these advanced control methods remains a challenge. A well-known tool to make controllers (either Model Predictive Controllers or Reinforcement Learning policies) safe, is the so-called control-invariant set (a.k.a. safe set). Unfortunately, for nonlinear systems, such a set cannot be exactly computed in general. Numerical algorithms exist for computing approximate control-invariant sets, but classic theoretic control methods break down if the set is not exact. This paper presents our recent efforts to address this issue. We present a novel Model Predictive Control scheme that can guarantee recursive feasibility and/or safety under weaker assumptions than classic methods. In particular, recursive feasibility is guaranteed by making the safe-set constraint move backward over the horizon, and assuming that such set satisfies a condition that is weaker than control invariance. Safety is instead guaranteed under an even weaker assumption on the safe set, triggering a safe task-abortion strategy whenever a risk of constraint violation is detected. We evaluated our approach on a simulated robot manipulator, empirically demonstrating that it leads to less constraint violations than state-of-the-art approaches, while retaining reasonable performance in terms of tracking cost, number of completed tasks, and computation time."
VBOC: Learning the Viability Boundary of a Robot Manipulator Using Optimal Control,"Asia La Rocca, Matteo Saveriano, Andrea Del Prete",University of Trento,Robot Safety II,"Safety is often the most important requirement in robotics applications. Nonetheless, control techniques that can provide safety guarantees are still extremely rare for nonlinear systems, such as robot manipulators. A well-known tool to ensure safety is the Viability kernel, which is the largest set of states from which safety can be ensured. Unfortunately, computing such a set for a nonlinear system is extremely challenging in general. Several numerical algorithms for approximating it have been proposed in the literature, but they suffer from the curse of dimensionality. This paper presents a new approach for numerically approximating the viability kernel of robot manipulators. Our approach solves optimal control problems to compute states that are guaranteed to be on the boundary of the set. This allows us to learn directly the set boundary, therefore learning in a smaller dimensional space. Compared to the state of the art on systems up to dimension 6, our algorithm resulted to be more than 2 times as accurate for the same computation time, or 6 times as fast to reach the same accuracy."
Closing the Perception-Action Loop for Semantically Safe Navigation in Semi-Static Environments,"Jingxing Qian, Siqi Zhou, Nicholas Ren, Veronica Chatrath, Angela P. Schoellig","University of Toronto,Technical University of Munich,University of Waterloo,Vector Institute,TU Munich",Robot Safety II,"Autonomous robots navigating in changing environments demand adaptive navigation strategies for safe long-term operation. While many modern control paradigms offer theoretical guarantees, they often assume known extrinsic safety constraints, overlooking challenges when deployed in real-world environments where objects can appear, disappear, and shift over time. In this paper, we present a closed-loop perception-action pipeline that bridges this gap. Our system encodes an online-constructed dense map, along with object-level semantic and consistency estimates into a control barrier function (CBF) to regulate safe regions in the scene. A model predictive controller (MPC) leverages the CBF-based safety constraints to adapt its navigation behaviour, which is particularly crucial when potential scene changes occur. We test the system in simulations and real-world experiments to demonstrate the impact of semantic information and scene change handling on robot behavior, validating the practicality of our approach."
"Magnetorheological-Actuators: An Enabling Technology for Fast, Safe and Practical Collaborative Robots","Alexandre St-Jean, Francis Dorval, Jean-Sebastien Plante, Alexis Lussier Desbiens",Université de Sherbrooke,Robot Safety II,"Collaborative robots are more and more used in applications requiring robots and humans to work in proximity or direct contact. However, conventional collaborative robots powered by servo-geared actuators are intrinsically dangerous due to their high reflected inertia. Recent studies have shown that low inertia and high bandwidth (> 30 Hz) magnetorheological (MR) actuators have the potential to improve the safety of collaborative robots without reducing their force and speed capabilities. The main contribution of this paper is to provide a quantitative assessment of how MR actuators can contribute to reducing the impact forces with humans, and thus increase the safety of collaborative robots. Dynamic models, validated with simplified 1 DOF experiments, show that the safety level of collaborative robots can be increased by a factor up to 3 only by changing the conventional servo-geared actuator architectures for MR actuators with no other changes. The paper also presents a simple, reliable, and fast collision detection method based on joint angular velocity band-pass filtering, a method exploiting the unique low inertia and clean dynamics properties of MR actuators."
Risk-Aware Control for Robots with Non-Gaussian Belief Spaces,"Matti Vahs, Jana Tumova","KTH Royal Institute of Technology, Stockholm,KTH Royal Institute of Technology",Robot Safety II,"This paper addresses the problem of safety-critical control of autonomous robots, considering the ubiquitous uncertainties arising from unmodeled dynamics and noisy sensors. To take into account these uncertainties, probabilistic state estimators are often deployed to obtain a belief over possible states. Namely, Particle Filters (PFs) can handle arbitrary non-Gaussian distributions in the robot's state. In this work, we define the belief state and belief dynamics for continuous-discrete PFs and construct safe sets in the underlying belief space. We design a controller that provably keeps the robot's belief state within this safe set. As a result, we ensure that the risk of the unknown robot's state violating a safety specification, such as avoiding a dangerous area, is bounded. We provide an open-source implementation as a ROS2 package and evaluate the solution in simulations and hardware experiments involving high-dimensional belief spaces."
Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions,"Jordan Lekeufack Sopze, Anastasios Angelopoulos, Andrea Bajcsy, Michael I. Jordan, Jitendra Malik","University of California, Berkeley,Carnegie Mellon University,UC Berkeley",Robot Safety II,"We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to be high throughput but low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans and robot manufacturing."
A Learning-Based Framework for Safe Human-Robot Collaboration with Multiple Backup Control Barrier Functions,"Neil Janwani, Ersin Das, Thomas Touma, Skylar Wei, Tamas G. Molnar, Joel Burdick","California Institute of Technology,Caltech,Wichita State University",Robot Safety II,"Ensuring robot safety in complex environments is a difficult task due to actuation limits, such as torque bounds. This paper presents a safety-critical control framework that leverages learning-based switching between multiple backup controllers to formally guarantee safety under bounded control inputs while satisfying driver intention. By leveraging {em backup controllers} designed to uphold safety and input constraints, textit{backup control barrier functions} (BCBFs) construct implicitly defined control invariant sets via a feasible quadratic program (QP). However, BCBF performance largely depends on the design and conservativeness of the chosen backup controller, especially in our setting of human-driven vehicles in complex, e.g, off-road, conditions. While conservativeness can be reduced by using multiple backup controllers, determining when to switch is an open problem. Consequently, we develop a broadcast scheme that estimates driver intention and integrates BCBFs with multiple backup strategies for human-robot interaction. An LSTM classifier uses data inputs from the robot, human, and safety algorithms to continually choose a backup controller in real-time. We demonstrate our method's efficacy on a dual-track robot in obstacle avoidance scenarios. Our framework guarantees robot safety while adhering to driver intention."
Geared Rod-Driven Continuum Robot with Woodpecker-Inspired Extension Mechanism and IMU-Based Force Sensing,"Ujjal Mavinkurve, Ayato Kanada, Amir Tafrishi, Koki Honda, Yasutaka Nakashima, Motoji Yamamoto","Kyushu University,Cardiff Univerity,The University of Tokyo",Bioinspired Robot Abilities,"Continuum robot arms that can access confined spaces are useful in many applications, such as invasive surgery, search and rescue, and inspection. However, their reach is often limited because their extension mechanism relies on elastic deformation or folding structures. To address this challenge, we propose a continuum robot with a novel extension mechanism inspired by the impressive ability of woodpeckers to extend and bend their long tongues to catch insects in tree holes. The proposed mechanism can change the effective length of the robot from almost zero to any length by moving the robot's body back and forth. Our prototype robot demonstrated a maximum extension of 450 mm and a minimum bending radius of 125 mm. In addition, we developed a Gaussian process regression model to predict an external force applied to the robot's tip using inertial measurement units. This enabled us to determine the magnitude and direction of the force with an error rate of 4.8 percent and 11.1 percent, even when the robot's length was varied between the training and test data. The unrestricted extension capability of the proposed approach has the potential to increase the application prospects of continuum robots."
A Multi-Modal Hybrid Robot with Enhanced Traversal Performance,"Zhipeng He, Na Zhao, Yudong Luo, Sian Long, Xi Luo, Hongbin Deng","Beijing Institute of Technology,Dalian Maritime University,Yichang Testing Tech. Research Institution",Bioinspired Robot Abilities,"Current multi-modal hybrid robots with flight and wheeled modes have fallen into the dilemma that they can only avoid obstacles by re-taking off when encountering obstacles due to the poor performance of wheeled obstacle-crossing. To tackle this problem, this paper presents a novel multi-modal hybrid robot with the ability to actively adjust the wheel's size, which is inspired by the behavior of the turtle's legs when it encounters obstacles, to enhance the traversal performance. In detail, we first describe the hardware design that allows the robot to achieve a modal switch between flight and wheeled modes through foldable structures and variable wheel diameters; then, we present the architecture to control these two morphing mechanisms. After that, we establish the theoretical kinematic models for both the foldable arm and variable wheel, and carry out extensive experiments to test the performance of the foldable arm, the variable-diameter wheel, as well as the traversal performance of the robot. Experimental results show that the proposed multimodal robot can realize the function of a quadrotor, respond quickly with full-scale folding within 0.9 s, climb a maximum slope of 36 deg, and traverse narrow passageways, which exhibit superior mobility and environmental adaptability."
Anisotropic Body Compliance Facilitates Robotic Sidewinding in Complex Environments,"Velin Kojouharov, Tianyu Wang, Matthew Fernandez, Jiyeon Maeng, Daniel Goldman",Georgia Institute of Technology,Bioinspired Robot Abilities,"Sidewinding, a locomotion strategy characterized by the coordination of lateral and vertical body undulations, is frequently observed in rattlesnakes and has been successfully implemented by limbless robotic systems for effective movement across diverse terrestrial terrains. However, the integration of compliant mechanisms into sidewinding limbless robots remains less explored, posing challenges for navigation in complex, rheologically diverse environments. Inspired by a notable control simplification via mechanical intelligence in lateral undulation, which offloads feedback control to passive body mechanics and interactions with the environment, we present an innovative design of a mechanically intelligent limbless robot for sidewinding. This robot features a decentralized bilateral cable actuation system that resembles organismal muscle actuation mechanisms. We develop a feedforward controller that incorporates programmable body compliance into the sidewinding gait template. Our experimental results highlight the emergence of mechanical intelligence when the robot is equipped with an appropriate level of body compliance. This allows the robot to 1) locomote more energetically efficiently, as evidenced by a reduced cost of transport, and 2) navigate through terrain heterogeneities, all achieved in an open-loop manner, without the need for environmental awareness."
Combining Tail and Reaction Wheel for Underactuated Spatial Reorientation in Robot Falling with Quadratic Programming,"Xiangyu Chu, Shengzhi Wang, Raymond Ng, Chun Yin Fan, Jiajun An, Samuel Au","The Chinese University of Hong Kong,Chinese University of Hong Kong",Bioinspired Robot Abilities,"Inertial appendages (e.g., tails and reaction wheels) have shown their reorientation capability to enhance robots' mobility while airborne or improve robots' safety in falling. The tail, especially with two Degrees of Freedom (DoFs), is normally subject to its limited Range of Motion (RoM). Although the reaction wheel circumvents this limitation, its efficiency has been shown lower than the tail in terms of inducing Moment of Inertia (MoI). In literature, only one type of inertial appendages has been used on terrestrial robots in the air, e.g., either using a tail on the hexapedal robot RHex or using a reaction wheel on the jumping quadruped robot SpaceBok. In this paper, to benefit from both unlimited RoM and efficient MoI-inducing, we propose combining a 1-DoF tail and a reaction wheel together for spatial reorientation (regulating robot body's 3D orientation). Inspired by this, a hybrid tail-wheel robot is built, i.e., the tail that creates roll motion is attached to a wheel-equipped robot whose wheels act like a reaction wheel and generate pitch rotation; however, the robot is underactuated on the yaw rotation. To achieve its real-time spatial reorientation, we propose a novel quadratic programming algorithm based on a geometric metric for the underactuated hybrid tail-wheel robot. Within the proposed algorithm, the physical limitations on tail and wheel velocities are automatically accommodated. Numerical comparisons among wheel-wheel, tail-wheel, and 2-DoF tail robots s"
Environment-Modulated Self-Assembly by Changes in Modules' Buoyancy,"Xiao Chen, Junyi Han, Xin Jin, Shuhei Miyashita","University of Sheffield,university of sheffield",Bioinspired Robot Abilities,"While many inkjet printers employ only four types of ink (i.e. CKMY) to produce a wide range of colors, numerous technical challenges still exist for contemporary 3D printers to fabricate various materials and generate composite products such as electric devices. Conversely, there have been attempts and endeavors to make things through self-assembly of parts, analogous to the autonomous and decentralized development process of the human body from just 20 types of amino acids. In our previous work, we proposed a method for the rapid production of 3D objects using the centimeter-sized modules (referred to as Roblets) capable of generating a 2D structure and subsequently self-folding themselves into a 3D configuration, akin to origami. To further leverage the capability of generating a wide variety of different types of structures by combining different modules, this research studies a method of automatically selecting and supplying modules using environmental cues. More precisely, we developed a mechanism to couple different modules corresponding to three different environments (on a flat surface, on low-dense saline, and on saturated saline) and yielded different module configurations. The process of self-assembly necessitated the application of perturbation, which was realized by imparting magnetic torque originating from an external magnetic field onto the magnets embedded in the modules."
Analysis and Validation of Stiffness and Payload of Nematode-Inspired Cable Routing Method for Cable Driven Redundant Manipulator,"Hoyoung Kim, Jungwon Yoon","GIST,Gwangju Institutue of Science and Technology",Bioinspired Robot Abilities,"The cable-driven redundant manipulator (CDRM) has significant potential for applications in narrow and hazardous spaces. However, traditional CDRMs have limited stiffness and load capacity due to their cable routing method. To address these limitations, several scholars have proposed new mechanisms and control strategies. Nevertheless, the cable routing method has not changed, and CDRMs continue to suffer from their limitations. Recently, a nematode-inspired cable routing method was proposed; however, stiffness calculations, derivation of inverse kinematics, and validation of stiffness and load capacity were incomplete. In this paper, we calculate the analytic equivalent stiffness of the nematode-inspired cable routing method and compare it with other cable routing methods. Additionally, we derived and simulate the kinematics and an effective inverse kinematics algorithm. Finally, we validate the stiffness and load capacity using a developed prototype."
"Design, Implementation, and Observer-Based Output Control of a Super-Coiled Polymer-Driven Two Degree-Of-Freedom Robotic Eye","Sunil Kumar Rajendran, Qi Wei, Ningshi Yao, Feitian Zhang","BSS Technologies Inc.,George Mason University,Peking University",Bioinspired Robot Abilities,"The prevalence of ineffective corrective surgeries for ocular motor disorders calls for a robotic eye platform in aiding ophthalmologists to better understand the biomechanisms of human eye movement. This letter presents the first hardware design and implementation of a 2-DOF robotic eye driven by super-coiled polymer (SCP) artificial muscles. While our previous work designed and simulated a deep deterministic policy gradient (DDPG) learning-based controller that requires full-state feedback of the SCP-driven robotic eye, measuring the temperature states of the slender SCPs is generally impractical for the ubiquitously aimed robot. To address this predicament, this letter proposes a reduced-order state observer to estimate the temperature of SCPs given the kinematic measurements. Combining the designed observer and the learning-based controller, the closed-loop output feedback control is implemented on the robotic eye prototype to examine its performance on three classical types of eye movements: visual fixation, saccadic pursuit, and smooth pursuit. The experimental results are presented which successfully validate the observer-based output control of the SCP-driven robotic eye."
Super-Resolution of Lunar Satellite Images for Enhanced Robotic Traverse Planning,"José Ignacio Delgado-centeno, Paula Harder, Valentin Bickel, Ben Moseley, Freddie Kalaitzis, Siddha Ganju, Miguel Olivares-Mendez","Universite du Luxembourg,Mila - Quebec AI Institute,ETH Zurich,University of Oxford,NVIDIA,Interdisciplinary Centre for Security, Reliability and Trust - U",Space Robotics I,"Lunar exploration missions require detailed and accurate planning to ensure their safety. Remote sensing data, such as optical satellite imagery acquired by lunar orbiters, is key for the identification of future landing and mission sites. Here, robot- and astronaut-scale obstacles are the most relevant to resolve, however, the spatial resolution of the available image data is often insufficient - particularly in the poorly illuminated polar regions of the Moon -, leading to uncertainty. This work shows how a novel single-image Super-Resolution (SR) application - ANUBIS, Adversarial Network for Uncertainty Based Image Super-resolution - can enhance lunar surface imagery by improving their resolution by a factor of 2, outperforming other approaches and benchmarks. The enhanced images improve the reliability and detail of lunar traverse planning and topographic reconstruction, while providing an estimate of the uncertainty associated with the enhancement process, vital to ensure mission planning integrity. This work demonstrates how machine learning-driven processing can enhance existing data products to maximize their value for science and exploration of the Moon and other celestial"
PPO-Based Dynamic Control of Uncertain Floating Platforms in Zero-G Environment,"Mahya Ramezani, Mohammadamin Alandihallaj, Andreas Hein",University of Luxembourg,Space Robotics I,"Abstractâ€” In the realm of space exploration, floating platforms play a crucial role in scientific investigations and technological advancements. However, controlling these platforms in zero-gravity environments presents unique challenges, including uncertainties and disturbances. This paper introduces an innovative approach that combines Proximal Policy Optimization (PPO) with Model Predictive Control (MPC) in the zero-gravity laboratory (Zero-G Lab) at the University of Luxembourg. This approach leverages PPOâ€™s reinforcement learning power and MPCâ€™s precision to navigate the complex control dynamics of floating platforms. Unlike traditional control methods, this PPO-MPC approach learns from MPC predictions, adapting to unmodeled dynamics and disturbances, resulting in a resilient control framework tailored to the zero-gravity environment. Simulations and experiments in the Zero-G Lab validate this approach, showcasing the adaptability of the PPO agent. This research opens new possibilities for controlling floating platforms in zero-gravity settings, promising advancements in space exploration."
Learning-Aided Control of Robotic Tether-Net with Maneuverable Nodes to Capture Large Space Debris,"Achira Boonrath, Feng Liu, Eleonora Botta, Souma Chowdhury","University at Buffalo, SUNY,The State University of New York, University at Buffalo,University at Buffalo,University at Buffalo, State University of New York",Space Robotics I,"Maneuverable tether-net systems launched from an unmanned spacecraft offer a promising solution for the active removal of large space debris. Guaranteeing the successful capture of such space debris is dependent on the ability to reliably maneuver the tether-net system -- a flexible, many-DoF (thus complex) system -- for a wide range of launch scenarios. Here, scenarios are defined by the relative location of the debris with respect to the chaser spacecraft. This paper represents and solves this problem as a hierarchically decentralized implementation of robotic trajectory planning and control and demonstrates the effectiveness of the approach when applied to two different tether-net systems, with 4 and 8 maneuverable units (MUs), respectively. Reinforcement learning (policy gradient) is used to design the centralized trajectory planner that, based on the relative location of the target debris at the launch of the net, computes the final aiming positions of each MU, from which their trajectory can be derived. Each MU then seeks to follow its assigned trajectory by using a decentralized PID controller that outputs the MU's thrust vector and is informed by noisy sensor feedback (for realism) of its relative location. System performance is assessed in terms of capture success and overall fuel consumption by the MUs. Reward shaping and surrogate models are used to respectively guide and speed up the RL process. Simulation-based experiments show that this approach allows the successful capture of debris at fuel costs that are notably lower than nominal baselines, including in scenarios where the debris is significantly off-centered compared to the approaching chaser spacecraft."
Online Supervised Training of Spaceborne Vision During Proximity Operations Using Adaptive Kalman Filtering,"Tae Ha Park, Simone D’amico",Stanford University,Space Robotics I,"This work presents an Online Supervised Training (OST) method to enable robust vision-based navigation about a non-cooperative spacecraft. Spaceborne Neural Networks (NN) are susceptible to domain gap as they are primarily trained with synthetic images due to the inaccessibility of space. OST aims to close this gap by training a pose estimation NN online using incoming flight images during Rendezvous and Proximity Operations (RPO). The pseudo-labels are provided by an adaptive unscented Kalman filter where the NN is used in the loop as a measurement module. Specifically, the filter tracks the target's relative orbital and attitude motion, and its accuracy is ensured by robust on-ground training of the NN using only synthetic data. The experiments on real hardware-in-the-loop trajectory images show that OST can improve the NN performance on the target image domain given that OST is performed on images of the target viewed from a diverse set of directions during RPO."
Towards Real-World Efficiency: Domain Randomization in Reinforcement Learning for Pre-Capture of Free-Floating Moving Targets by Autonomous Robots,"Bahador Beigomi, Zhenghong (george) Zhu",York University,Space Robotics I,"In this research, we introduce a deep reinforcement learning-based control approach to address the intricate challenge of the robotic pre-grasping phase under microgravity conditions. Leveraging reinforcement learning eliminates the necessity for manual feature design, therefore simplifying the problem and empowering the robot to learn pre-grasping policies through trial and error. Our methodology incorporates an off-policy reinforcement learning framework, employing the soft actor-critic technique to enable the gripper to proficiently approach a free-floating moving object, ensuring optimal pre-grasp success. For effective learning of the pre-grasping approach task, we developed a reward function that offers the agent clear and insightful feedback. Our case study examines a pre-grasping task where a Robotiq 3F gripper is required to navigate towards a free-floating moving target, pursue it, and subsequently position itself at the desired pre-grasp location. We assessed our approach through a series of experiments in both simulated and real-world environments. The source code, along with recordings of real-world robot grasping, is available at Fanuc_Robotiq_Grasp."
SPADES: A Realistic Spacecraft Pose Estimation Dataset Using Event Sensing,"Arunkumar Rathinam, Haytam Qadadri, Djamila Aouada","University of Luxembourg,University of Strasbourg,SnT, University of Luxembourg",Space Robotics I,"In recent years, there has been a growing demand for improved autonomy for in-orbit operations such as rendezvous, docking, and proximity manoeuvres, leading to increased interest in employing Deep Learning-based Spacecraft Pose Estimation techniques. However, due to limited access to real target datasets, algorithms are often trained using synthetic data and applied in the real domain, resulting in a performance drop due to the domain gap. State-of-the-art approaches employ Domain Adaptation techniques to mitigate this issue. In the search for viable solutions, event sensing has been explored in the past and shown to reduce the domain gap between simulations and real-world scenarios. Event sensors have made significant advancements in hardware and software in recent years. Moreover, the characteristics of the event sensor offer several advantages in space applications compared to RGB sensors. To facilitate further training and evaluation of DL-based models, we introduce a new dataset, SPADES, comprising real event data acquired in a controlled laboratory environment and simulated event data using the same camera intrinsics. Furthermore, we introduce an image-based event representation that performs better than existing representations. In addition, we propose an effective data filtering method to improve the quality of training data, thus enhancing model performance. A multifaceted baseline evaluation was conducted using different event representations, event filtering strategies, and algorithmic frameworks, and the results are summarized. The dataset will be made available at http://cvi2.uni.lu/spades."
Covariance Based Terrain Mapping for Autonomous Mobile Robots,"Lennart Werner, Pedro Proença, Andreas Nuechter, Roland Brockers","ETH Zürich,California Institute of Technology,University of Würzburg",Space Robotics I,"In this paper, we present a local, robot-centric navigation map optimized for autonomous mobile robots operating in unknown environments, enhancing their onboard perception systems for collision-free operation with far look-ahead distances. Utilizing a novel converging covariance cell representation, our approach effectively analyzes hazards such as obstacles and hazardous slopes in both terrestrial and aerial navigation contexts. The new technique specifically targets mapping from stereo scenarios with ultra short baseline and highly oblique viewpoints close to the ground. Our methodology surpasses traditional window-based hazard analysis by resolving sub-cell size obstacles and terrain gradients at the individual cell level, thereby avoiding the computational overhead typically associated with such analyses. It leverages a multi-resolution strategy adaptive to the range errors common in stereo vision systems, making it particularly suitable for embedded systems with computational limitations. Functionality includes constant-time queries for height, obstacle presence, and slope details, boasting improvements in run time, memory usage, precision, and resolvable obstacle size compared to existing grid-based mapping algorithms. We validate our approach through rigorous simulation and real-world testing. This technique will be used for the local mapping and collision avoidance on NASA's CADRE lunar rovers."
VINSat: Solving the Lost-In-Space Problem with Visual-Inertial Navigation,"Kyle McCleary, Swaminathan Gurumurthy, Paulo Fisch, Saral Tayal, Zachary Manchester, Brandon Lucia",Carnegie Mellon University,Space Robotics I,"Rapid growth in the number of nanosatellite deployments has heightened the need for rapid, cost-effective, and accurate orbit determination (OD). This paper introduces a solution to this â€œlost-in-spaceâ€ problem that we call Visual-Inertial Navigation for Satellites (VINSat). VINSat performs OD using data from an inertial measurement unit (IMU) and a low-cost RGB camera. Machine learning techniques are used to identify known landmarks in images captured by the spacecraft. These landmark locations are then combined with IMU data and a dynamics model in a batch nonlinear least-squares state estimator to determine the full state of the spacecraft. We validate VINSat in simulation using real nadir-pointing imagery and find that 85% of simulated satellites are localized to under 5 km within 6 hours (4 orbits). This performance substantially surpasses that of ground radar, demonstrating significantly faster and more precise localization without any reliance on ground infrastructure."
Spatial Assisted Human-Drone Collaborative Navigation and Interaction through Immersive Mixed Reality,"Luca Morando, Giuseppe Loianno",New York University,Applications,"Aerial robots have the potential to play a crucial role in assisting humans with complex and dangerous tasks. Nevertheless, the future industry demands innovative solutions to streamline the interaction process between humans and drones to enable seamless collaboration and efficient co-working. In this paper, we present a novel tele-immersive framework that facilitates cognitive and physical collaboration between humans and robots through Mixed Reality (MR). This includes a novel bi-directional spatial awareness and a multi-modal virtual-physical interaction approaches. The former seamlessly integrates the physical and virtual worlds, providing a bidirectional egocentric and exocentric environment representations. The latter, leveraging the proposed spatial representation, further enhances the collaboration combining a robot planning algorithm for obstacle avoidance with a variable admittance control. This enables the user to generate commands based on virtual forces while ensuring compatibility with the environment map. We validate the proposed approach by conducting several collaborative planning and exploration tasks involving a drone and a user equipped with a MR headset."
A Compiler Framework for Proactive UAV Regulation Enforcement,"Huaxin Tang, John Henry Burns, Alexander Strong, Yu David Liu","Binghamton University,SUNY Binghamton",Applications,"In the rapidly evolving landscape of Unmanned Aerial Vehicles (UAVs), regulation enforcement is critical. Unfortunately, existing practices are largely manual and reactive in nature. In this paper, we present THEMIS, a novel compiler-directed approach for automated and proactive regulation enforcement. By expressing regulations through a specification language and integrating their enforcement into the compilation process, THEMIS enables safe and regulation-compliant UAV flights by enforcing prohibited and restricted areas, avoiding flights over humans, and managing maximum limits of altitude and speed. Our framework features a bi-directional interface that allows the concrete algorithms used for enforcement to be customized. Our evaluation shows THEMIS-compiled autopilots can adhere to regulatory constraints amidst complex flight conditions, while significantly reducing the burden of UAV operators."
Real-Time Dynamic-Consistent Motion Planning for Over-Actuated UAVs,"Yao Su, Jingwen Zhang, Ziyuan Jiao, Hang Li, Meng Wang, Hangxin Liu","Beijing Institute for General Artificial Intelligence （BIGAI）,University of California, Los Angeles,Beijing Institute for General Artificial Intelligence,Beijing Institute for General Artificial Intelligence (BIGAI)",Applications,"Existing motion planning approaches for over-actuated unmanned aerial vehicle (UAV) platforms can achieve online planning without considering dynamics. However, in many envisioned application areas such as aerial manipulation, payload delivery, and moving target tracking, it is critical to ensure dynamic consistency in the generated trajectory. The dynamics of these platforms introduce a high nonlinearity, leading to a substantial increase in computational burden. This paper presents an efficient method to plan motions that are consistent with the dynamics of over-actuated UAVs. With a hierarchical control structure, the dimension of the optimization problem is greatly reduced with synthesized wrench commands. Additionally, by exploring the dynamics of over-actuated UAVs, the complex planning process is decoupled into two simpler sub-problems. As a result, the proposed planner can be solved as two small quadratic programmings (QPs) and deployed in real-time. The computational efficiency and dynamic consistency of the proposed method are verified through both simulations and experiments, including comparison with other approaches and dynamic target tracking."
Trajectory Optimization for Cooperatively Localizing Quadrotor UAVs,"H S Helson Go, Hugh Hong-Tao Liu",University of Toronto,Applications,"In this paper, an Active Cooperative Localization system for Quadrotor Unmanned Aerial Vehicles is developed. The optimal trajectories are determined by minimizing the uncertainty in position estimation by Extended Kalman Filter. In this system, a piecewise-polynomial parameterization of trajectories is adopted for the optimizer, and the underlying state estimator is updated with appropriate models of sensors and quadrotor dynamics. This system is verified in extensive simulations in the scenario of a team of quadrotors with heterogeneous GNSS capabilities. These simulations answer an open question, showing that solving for trajectories by minimizing Kalman covariance computed in a noiseless environment is reasonable and that the optimized trajectories offers visible reductions in positioning uncertainty in the presence of noise."
Extending Guiding Vector Field to Track Unbounded UAV Paths,"Mael Feurgard, Gautier Hattenberger, Simon Lacroix","Ecole Nationale de l'Aviation Civile,ENAC, French Civil Aviation University,LAAS/CNRS",Applications,"A recent advance in vector field path following is the introduction of the Parametric Guiding Vector Field method. It allows for singularity-free vector fields with strong convergence guarantees, usable even for self-intersecting paths. However, the method requires significant gain tuning for practical use. In particular, for unbounded paths, the gains will inevitably become ill-suited for efficient path following. We propose a method to overcome this issue by introducing a dynamic step adaptation strategy, which provides additional normalization properties to the field. This allows the following of unbounded curves and reduces the number of gains to tune. The proposed improvements are verified in simulations using the PaparazziUAV software."
Tethered Lifting-Wing Multicopter Landing Like Kite,"Haoyu Wei, Shuai Wang, Quan Quan",Beihang University,Applications,"Automatic landing of tethered unmanned aerial vehicles (UAVs) is an important issue. Typically, UAVs rely on location sensors such as global navigation satellite system (GNSS) and external cameras to obtain location data. However, harsh environments such as denial GNSS or strong winds make it difficult for UAVs to approach the landing area, and common solutions cannot be used for automatic landing. A tethered lifting-wing multicopter has a structure and static stability similar to a kite. Inspired by kites, this paper proposes a new landing method for tethered lifting-wing multicopters, which can be used without location or velocity sensors. During the landing phase, the tethered lifting-wing multicopter only needs to keep the rotor thrust to actively straighten the tethered cable and a constant attitude similar to that of a kite to keep position stability and increase damping. Meanwhile, the winch only needs to recover the cable at a constant speed until the tethered lifting-wing multicopter returns to its base. The feasibility and practicability of this method are demonstrated by real flight experiments."
AirFisheye Dataset: A Multi-Model Fisheye Dataset for UAV Applications,"Pravin Kumar Jaisawal, Stephanos Papakonstantinou, Volker Gollnick",Hamburg University of Technology,Applications,"Drone applications require perception all around the vehicle for obstacle avoidance during drone navigation. Due to the weight and computation limitations on UAVs, using a large number of sensors e.g. a large amount of cameras could be prohibitive. In such scenarios, usage of fisheye camera with a wider field of view is very beneficial. Despite the usefulness of fisheye camera for UAV applications, not much work has been carried out to develop perception algorithm for fisheye camera. One of the main problems being the lack of publicly available omnidirectional datasets in relation to drone flight. With this paper, we address this gap by presenting AirFisheye dataset, which is applicable for tasks such as segmentation, depth estimation and depth completion, among other tasks required for autonomous drone navigation. Also, a generic framework for creating synthetic fisheye images is provided. Furthermore, we propose a novel occlusion correction algorithm that removes incorrectly projected LiDAR point clouds into the camera image due to the viewpoint variation of both sensors. We release about 26K images and LiDAR scans along with annotations. Baseline code and supporting scripts are available at https:// collaborating.tuhh.de/ilt/airfisheye-dataset"
Bio-Inspired Visual Relative Localization for Large Swarms of UAVs,"Martin Krizek, Matous Vrba, Antonella Barisic, Stjepan Bogdan, Martin Saska","Czech Technical University in Prague,Faculty of Electrical Engineering, Czech Technical University in,University of Zagreb, Faculty of Electrical Engineering and Comp,University of Zagreb",Applications,"We propose a new approach to visual perception for relative localization of agents within large-scale swarms of UAVs. Inspired by biological perception utilized by schools of sardines, swarms of bees, and other large groups of animals capable of moving in a decentralized yet coherent manner, our method does not rely on detecting individual neighbors by each agent and estimating their relative position, but rather we propose to regress a neighbor density over distance. This allows for a more accurate distance estimation as well as better scalability with respect to the number of neighbors. Additionally, a novel swarm control algorithm is proposed to make it compatible with the new relative localization method. We provide a thorough evaluation of the presented methods and demonstrate that the regressing approach to distance estimation is more robust to varying relative pose of the targets and that it is suitable to be used as the main source of relative localization for swarm stabilization."
Heuristic-Based Incremental Probabilistic Roadmap for Efficient UAV Exploration in Dynamic Environments,"Zhefan Xu, Christopher Suzuki, Xiaoyang Zhan, Kenji Shimada",Carnegie Mellon University,Applications,"Autonomous exploration in dynamic environments necessitates a planner that can proactively respond to changes and make efficient and safe decisions for robots. Although plenty of sampling-based works have shown success in exploring static environments, their inherent sampling randomness and limited utilization of previous samples often result in sub-optimal exploration efficiency. Additionally, most of these methods struggle with efficient replanning and collision avoidance in dynamic settings. To overcome these limitations, we propose the Heuristic-based Incremental Probabilistic Roadmap Exploration (HIRE) planner for UAVs exploring dynamic environments. The proposed planner adopts an incremental sampling strategy based on the probabilistic roadmap constructed by heuristic sampling toward the unexplored region next to the free space, defined as the heuristic frontier regions. The heuristic frontier regions are detected by applying a lightweight vision-based method to the different levels of the occupancy map. Moreover, our dynamic module ensures that the planner dynamically updates roadmap information based on the environment changes and avoids dynamic obstacles. Simulation and physical experiments prove that our planner can efficiently and safely explore dynamic environments. Our software is available on GitHub with the experiment video."
Dynamic Evaluation of a Suction Based Gripper for Fruit Picking Using a Physical Twin,"Alejandro Velasquez, Cindy Grimm, Joseph Davidson",Oregon State University,Robotics and Automation in Agriculture and Forestry I,"We present and evaluate a novel suction-based gripper designed for fruit picking. This work is motivated by common problems observed in field trials of robotic harvesting: Calibration/perception errors, workspace obstacles, fruit swinging/moving when contacted, and varying stem and branch stiffnesses. The gripper consists of three suction-cups located on the palm, along with in-hand perception. To evaluate the gripper, we developed a physical proxy that approximates a realistic apple-stem-branch dynamic system. We performed 756 apple picks on the proxy with varying branch stiffness, stem strength and gripper pose (yaw, roll and offset w.r.t. the apple). Our results show that grasping performance improves when the gripper yaw w.r.t. the apple has two suction cups on the bottom of the apple and one suction cup on top. Even with Â±15mm offset, at least two suction cups engaged with the apple 80% of the time, regardless of branch stiffness. Moreover, the gripper withstands Â±20mm offset when it approaches the apple near its equator."
Strawberry Weight Estimation Based on Plane-Constrained Binary Division Point Cloud Completion,"Yanjiang Huang, Jiepeng Liu, Xianmin Zhang","Guangdong Provincial key lab. of precision equipment and manufac,South China University of Technology",Robotics and Automation in Agriculture and Forestry I,"Labor shortages and the development of digital technology both impose requirements on the fruit industry. Modern agricultural competition has shifted from competition between products to competition between supply chains. Enhancing the digitization of production lines is crucial for gaining a competitive advantage. Strawberries, as fruits with a short shelf life, require sorting and packaging of fruits of different weights after being harvested. Estimating strawberry weight through visual technology can save time and labor costs. Common methods include methods based on feature size and learning-based methods, with the former having larger errors and the latter requiring a large amount of data. To address these issues, we propose a dataset for estimating strawberry weight, which includes strawberries with different heights and angles. Additionally, we propose a strawberry weight estimation method based on plane-constrained binary division point cloud completion. This method separates the plane point cloud and strawberry point cloud, constructs a coordinate system on the strawberry point cloud, generates an axis-aligned bounding box (AABB), and estimates the strawberry weight based on the bounding box and placement plane as constraints. Through comparison with different methods, we achieved a maximum improvement of 12.38% in prediction accuracy, demonstrating that our method provides the best estimation accuracy."
AV4GAInsp: An Efficient Dual-Camera System for Identifying Defective Kernels of Cereal Grains,"Lei Fan, Dongdong Fan, Yiwen Ding, Yong Wu, Hongxia Chu, Maurice Pagnucco, Yang Song","The University of New South Wales,Gaozhe Technology, Hefei, Anhui, CN,Gauture Technology,GaoZhe technology,gaozhe technology,University of New South Wales",Robotics and Automation in Agriculture and Forestry I,"Grain Appearance Inspection (GAI) is a pre-requisite for grain quality determination, providing guidance for grain processing, storage and trade. GAI is routinely performed by trained inspectors who are required to visually inspect cereal grains for each individual kernel. Since grain kernels (e.g., wheat, rice) are tiny with heterogeneous shapes and appearance, manually performing GAI is time-consuming and error-prone. This paper presents a machine vision-based customization of an automated system for grain appearance inspection, called AV4GAInsp, which consists of a device and an analysis framework. The device is equipped with an elaborate feeding module and a capturing module for automatically pre-processing grain kernels and efficiently acquiring high-quality images for these kernels. The framework employs deep convolutional neural networks to process these captured images to classify the kernels as normal or defective. We also built and released a large-scale dataset, named GrainDet, that includes over $140K$ images for three types of grains: wheat, sorghum and rice. Comprehensive experiments are conducted to validate the efficacy and performance of our AV4GAInsp system, achieving an average F1-score of 98.4% and excelling at inspection efficiency by over 20x speedup. Kappa statistic tests are performed to confirm the consistency between our system and human experts. It is expected that AV4GAInsp will alleviate inspectors' workloads and inspire further r"
Aerial Image-Based Inter-Day Registration for Precision Agriculture,"Chen Gao, Franz Daxinger, Lukas Roth, Fabiola Maffra, Paul Beardsley, Margarita Chli, Lucas Teixeira","ETH Zürich,ETH Zurich,Unity Technologies,ETH Zurich & University of Cyprus",Robotics and Automation in Agriculture and Forestry I,"Satellite imagery has traditionally been used to collect crop statistics, but its low resolution and registration accuracy limit agricultural analytics to plant stand levels and large areas. Precision agriculture seeks analytic tools at near single plant level, and this work explores how to improve aerial photogrammetry to enable inter-day precision agriculture analytics for intervals of up to a month. Our work starts by presenting an accurately registered image time series, captured up to twice a week, by an unmanned aerial vehicle over a wheat crop field. The dataset is registered using photogrammetry aided by fiducial ground control points (GCPs). Unfortunately, GCPs severely disrupt crop management activities. To address this, we propose a novel inter-day registration approach that only relies once on GCPs, at the beginning of the season. The method utilises LoFTR, a state-of-the-art image-matching transformer. The original LoFTR network was trained using imagery of outdoor urban areas. One of our contributions is to extend LoFTR's training method, which uses matching images of a static scene, to a dynamic scene of plants undergoing growth. Another contribution is a thorough evaluation of our registration method that integrates intra-day crop reconstruction with earlier-day scans in a seven degree-of-freedom alignment. Experimental results show the advantage of our approach over other matching algorithms and demonstrate the importance of retraining using crop scenes, and a training method customised for growing crops, with an average registration error of 27 cm across a season."
"Inexpensive, Automated Pruning Weight Estimation in Vineyards","Jonathan Jaramillo, Aaron Wilhelm, Nils Napp, Justine Vanden Heuvel, Kirstin Hagelskjaer Petersen",Cornell University,Robotics and Automation in Agriculture and Forestry I,"Pruning weight is indicative of a vineâ€™s ability to produce crop the following year, informing vineyard man- agement. Current methods for estimating pruning weight are costly, laborious, and/or require specialized know-how and equipment. In this paper we demonstrate an affordable, simple, computer vision-based method to measure pruning weight using a smartphone camera and structured light which produces results better than state-of-the-art techniques for vertical shoot position (VSP) vines and demonstrate initial steps towards estimating pruning weight in high cordon procumbent (HC) vines such as Concord. The simplicity and affordability of this technique lends its self to deployment by farmers today or on future viticulture robotics platforms. We achieved an R2=.80 for VSP vines (better than state-of-the-art computer vision-based methods) and R2=.29 for HC vines (not previously attempted with computer vision-based methods)."
High Precision Leaf Instance Segmentation for Phenotyping in Point Clouds Obtained under Real Field Conditions,"Elias Ariel Marks, Matteo Sodano, Federico Magistri, Louis Wiesmann, Dhagash Desai, Rodrigo Marcuzzi, Jens Behley, Cyrill Stachniss","University of Bonn,Photogrammetry and Robotics Lab, University of Bonn",Robotics and Automation in Agriculture and Forestry I,"Measuring plant traits with a high throughput allows breeders to monitor and select the best cultivars to be used in following breeding generations. This in turn can enable farmers to improve yield to produce more food, feed, and fiber. Current breeding practices involve extracting leaf parameters on a small subset of the leaves present in the breeding plots, while still requiring substantial manual labor. To automate this process an important step is the precise distinction between separate leaves, which is the problem we address in this paper. To this end, we exploit recent advancements in 3D deep learning to build a convolutional neural network that learns to segment individual leaves. As done in current breeding practices, we select a subset of leaves to be used for phenotypic trait evaluation as this allows us to alleviate the influence of segmentation errors on the phenotypic trait estimation. To achieve this we propose to use an additional neural network to predict the quality of each segmented leaf and discard inaccurate leaf instances. The experiments show that our network yields higher segmentation accuracy on sugar beet breeding plots planted under the supervision of the German federal office for plant varieties. Furthermore, we show that our neural network helps in filtering out leaves with lower segmentation accuracy."
Towards Robotic Tree Manipulation: Leveraging Graph Representations,"Chung Hee Kim, Moonyoung Lee, Oliver Kroemer, George Kantor",Carnegie Mellon University,Robotics and Automation in Agriculture and Forestry I,"There is growing interest in automating agricultural tasks that require intricate and precise interaction with specialty crops, such as trees and vines. However, developing robotic solutions for crop manipulation remains a difficult challenge due to complexities involved in modeling their deformable behavior. In this study, we present a framework for learning the deformation behavior of tree-like crops under contact interaction. Our proposed method involves encoding the state of a spring-damper modeled tree crop as a graph. This representation allows us to employ graph networks to learn both a forward model for predicting resulting deformations, and a contact policy for inferring actions to manipulate tree crops. We conduct a comprehensive set of experiments in a simulated environment and demonstrate generalizability of our method on previously unseen trees. Videos can be found on the project website: https://kantor-lab.github.io/tree_gnn"
Field Evaluation of a Prioritized Path-Planning Algorithm for Heterogeneous Agricultural Tasks of Multi-UGVs,"Yuseung Jo, Hyoung Il Son",Chonnam National University,Robotics and Automation in Agriculture and Forestry I,"This paper introduces a prioritized path-planning algorithm for heterogeneous tasks performed by multiple unmanned ground vehicles (UGVs) in agricultural environments. The algorithm considers varying robot priorities, thereby extending the traditional multi-agent path finding (MAPF) approach. The proposed algorithm is evaluated in scenarios occurring during representative agricultural operations: harvesting and transportation. An experimental validation is conducted in agriculture-like settings by using multiple simultaneous localization and mapping systems and navigation systems. The results revealed that the path of agent1, which was assigned the highest priority in both the indoor and outdoor environments, was shortened considerably (3.38 m, 3.6 m, and 5.6 m, respectively). Especially in the face scenario, the sum of changes in distance, calculated using the proposed algorithm was negative, meaning that traffic congestion in the multi-robot system used in the experiment was alleviated without the need for inter-robot communication."
Unsupervised Pre-Training for 3D Leaf Instance Segmentation,"Gianmarco Roggiolani, Federico Magistri, Tiziano Guadagnino, Jens Behley, Cyrill Stachniss",University of Bonn,Robotics and Automation in Agriculture and Forestry I,"Crops for food, feed, fiber, and fuel are key resources for our society. Monitoring plants and measuring their traits is an important task in agriculture often referred to as plant phenotyping. Traditionally, this task is done manually, which is time- and labor-intensive. Robots can automate phenotyping providing reproducible and high-frequency measurements. Today's perception systems use deep learning to interpret these measurements, but require a substantial amount of annotated data to work well. Obtaining such labels is challenging as it often requires background knowledge on the side of the labelers.This paper addresses the problem of reducing the labeling effort required to perform leaf instance segmentation on 3D point clouds, which is a first step toward phenotyping in 3D. Separating all leaves allows us to count them and compute relevant traits as their areas, lengths, and widths.We propose a novel self-supervised task-specific pre-training approach to initialize the backbone of a network for leaf instance segmentation. We also introduce a novel automatic postprocessing that considers the difficulty of correctly segmenting the points close to the stem, where all the leaves petiole overlap. The experiments presented in this paper suggest that our approach boosts the performance over all the investigated scenarios. We also evaluate the embeddings to assess the quality of the fully unsupervised approach and see a higher performance of our domain-specific postprocessing."
GPS-VIO Fusion with Online Rotational Calibration,"Junlin Song, Pedro J Sanchez-cuevas, Antoine Richard, Raj Thilak Rajan, Miguel Olivares-Mendez","University of Luxembourg,Advanced Center for Aerospace Technologies,Delft university of technology,Interdisciplinary Centre for Security, Reliability and Trust - U",Localization VI,"Accurate global localization is crucial for autonomous navigation and planning. To this end, various GPS-aided Visual-Inertial Odometry (GPS-VIO) fusion algorithms are proposed in the literature. This paper presents a novel GPS-VIO system that is able to significantly benefit from the online calibration of the rotational extrinsic parameter between the GPS reference frame and the VIO reference frame. The behind reason is this parameter is observable. This paper provides novel proof through nonlinear observability analysis. We also evaluate the proposed algorithm extensively on diverse platforms, including flying UAV and driving vehicle. The experimental results support the observability analysis and show increased localization accuracy in comparison to state-of-the-art (SOTA) tightly-coupled algorithms."
Fully Onboard Low-Power Localization with Semantic Sensor Fusion on a Nano-UAV Using Floor Plans,"Nicky Zimmerman, Hanna Müller, Michele Magno, Luca Benini","University of Lugano,ETH Zürich,ETH Zurich,University of Bologna",Localization VI,"Nano-sized unmanned aerial vehicles (UAVs) are well-fit for indoor applications and for close proximity to humans. To enable autonomy, the nano-UAV must be able to self-localize in its operating environment. This is a particularly challenging task due to the limited sensing and compute resources on board. This work presents an online and onboard approach for localization in floor plans annotated with semantic information. Unlike sensor-based maps, floor plans are readily-available, and do not increase the cost and time of deployment. To overcome the difficulty of localizing in sparse maps, the proposed approach fuses geometric information from miniaturized Time-of-Flight~(ToF) sensors and semantic cues. The semantic information is extracted from images by deploying a state-of-the-art object detection model on a high-performance multi-core microcontroller onboard the drone, consuming only 2.5mJ per frame and executing in 38ms. In our evaluation, we globally localize in a real-world office environment, achieving 90% success rate. We also release an open-source implementation of our work."
"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization","Ilayda Yaman, Guoda Tian, Martin Larsson, Patrik Persson, Michiel Sandra, Alexander Dürr, Erik Tegler, Nikhil Challa, Henrik Garde, Fredrik Tufvesson, Kalle Åström, Ove Edfors, Steffen Malkowsky, Liang Liu","Lund University,LTH, Lund University",Localization VI,"We present a synchronized multisensory dataset for accurate and robust indoor localization: the Lund University Vision, Radio, and Audio (LuViRA) Dataset. The dataset includes color images, corresponding depth maps, inertial measurement unit (IMU) readings, channel response between a 5G massive multiple-input and multiple-output (MIMO) testbed and user equipment, audio recorded by 12 microphones, and accurate six degrees of freedom (6DOF) pose ground truth of 0.5 mm. We synchronize these sensors to ensure that all data is recorded simultaneously. A camera, speaker, and transmit antenna are placed on top of a slowly moving service robot, and 88 trajectories are recorded. Each trajectory includes 20 to 50 seconds of recorded sensor data and ground truth labels. Data from different sensors can be used separately or jointly to perform localization tasks, and data from the motion capture (mocap) system is used to verify the results obtained by the localization algorithms. The main aim of this dataset is to enable research on sensor fusion with the most commonly used sensors for localization tasks. Moreover, the full dataset or some parts of it can also be used for other research areas such as channel estimation, image classification, etc. Our dataset is available at: https://github.com/ilaydayaman/LuViRA_Dataset."
Dual-IMU State Estimation for Relative Localization of Two Mobile Agents,"Wenqian Lai, Ruonan Guo, Kejian Wu",XREAL,Localization VI,"In this paper, we address the problem of relative localization of two mobile agents. Specifically, we consider the Dual-IMU system, where each agent is equipped with one IMU, and employs relative pose observations between them. Previous works, however, typically assumed known ego motion and ignored biases of the IMUs. Instead, we study the most general case of unknown biases for both IMUs. Besides the derivation of dynamic model equations of the proposed system, we focus on the observability analysis, for the observability under general motion and the unobservable directions arising from various special motions. Through numerical simulations, we validate our key observability findings and examine their impact on the estimation accuracy and consistency. Finally, the system is implemented to achieve effective relative localization of an HMD with respect to a vehicle moving in the real world."
Accurate Prior-Centric Monocular Positioning with Offline LiDAR Fusion,"Jinhao He, Huaiyang Huang, Shuyang Zhang, Jianhao Jiao, Chengju Liu, Ming Liu","The Hong Kong University of Science and Technology (Guangzhou),the Hong Kong University of Science and Technology,The Hong Kong University of Science and Technology,University College London,Tongji University,Hong Kong University of Science and Technology (Guangzhou)",Localization VI,"Unmanned vehicles usually rely on Global Positioning System (GPS) and Light Detection and Ranging (LiDAR) sensors to achieve high-precision localization results for navigation purpose. However, this combination with their associated costs and infrastructure demands, poses challenges for widespread adoption in mass market applications. In this paper, we aim to use only a monocular camera to achieve comparable onboard localization performance by tracking deep-learning visual features on a LiDAR-enhanced visual prior map. Experiments show that the proposed algorithm can provide centimeter-level global positioning results with scale, which is effortlessly integrated and favorable for low-cost robot system deployment in real-world applications."
A Nonlinear Estimator for Dead Reckoning of Aquatic Surface Vehicles Using an IMU and a Doppler Velocity Log,"Jessica Paterson, Bruno Vilhena Adorno, Barry Lennox, Keir Groves","University of Manchester,The University of Manchester",Localization VI,"Aquatic robots require an accurate and reliable localization system to navigate autonomously and perform practical missions. Kalman filters (KFs) and their variants are typically used in aquatic robots to combine sensor data. The two critical drawbacks of KFs are the requirement for skilled tuning of several filter parameters and the fact that changes to how the Inertial Measurement Unit (IMU) is oriented necessitate modifying the filter. To overcome those problems, this paper presents a novel method of fusing sensor data from a Doppler Velocity Log (DVL) and IMU using an adaptive nonlinear estimator to provide dead reckoning localization for a small autonomous surface vehicle. The proposed method has only one insensitive tuning parameter and is agnostic to the configuration of the IMU. The system was validated using a small ASV in a 2.4x3.6x2.4 m water tank, with a motion capture system as ground truth, and was evaluated against a state-of-the-art method based on KFs. Experiments showed that the average drift error of the nonlinear filter was 0.16 m (s.d. 0.06 m) compared to 0.15 m (s.d. 0.05 m) for the state of the art, meaning that the benefits in terms of tuning and flexible configuration do not come at the expense of performance."
Range-Visual-Inertial Sensor Fusion for Micro Aerial Vehicle Localization and Navigation,"Abhishek Goudar, Zhao Wenda, Angela P. Schoellig","University of Toronto,TU Munich",Localization VI,We propose a fixed-lag smoother-based sensor fusion architecture to leverage the complementary benefits of range-based sensors and visual-inertial odometry (VIO) for localization. We use two fixed-lag smoothers (FLS) to decouple accurate state estimation and high-rate pose generation for closed-loop control. The first FLS combines ultrawideband (UWB)-based range measurements and VIO to estimate the robot trajectory and any systematic biases that affect the range measurements in cluttered environments. The second FLS estimates smooth corrections to VIO to generate pose estimates at a high rate for online control. The proposed method is lightweight and can run on a computationally constrained micro-aerial vehicle (MAV). We validate our approach through closed-loop flight tests involving dynamic trajectories in multiple real-world cluttered indoor environments. Our method achieves decimeter-to-sub-decimeter-level positioning accuracy using off-the-shelf sensors and decimeter-level tracking accuracy with minimally-tuned open-source controllers.
An Equivariant Approach to Robust State Estimation for the ArduPilot Autopilot System,"Alessandro Fornasier, Yixiao Ge, Pieter Van Goor, Martin Scheiber, Andrew Tridgell, Robert Mahony, Stephan Weiss","University of Klagenfurt,Australian National University,The Australian National University,Universität Klagenfurt",Localization VI,"The majority of commercial and open-source autopilot software for uncrewed aerial vehicles rely on the tried and tested extended Kalman filter (EKF) to provide the state estimation solution for the inertial navigation system (INS). While modern implementations achieve remarkable robustness, it is often due to the careful implementation of exception code for a multitude of corner cases along with significant skilled tuning effort. In this paper, we use the data wealth of the ArduPilot community to identify and highlight the most common real-world challenges in INS state estimation, including sensor self-calibration, robustness in static conditions, global navigation satellite system (GNSS) outliers and shifts, and robustness to faulty inertial measurement units (IMUs). We propose a novel equivariant filter (EqF) formulation for the INS solution that exploits a Semi-Direct-Bias symmetry group for multi-sensor fusion with self-calibration capabilities and incorporates equivariant velocity-type measurements. We augment the filter with a simple innovation-covariance inflation strategy that seamlessly handles GNSS outliers and shifts without requiring coding of a whole set of exception cases. We use real-world data from the Ardupilot community to demonstrate the performance of the proposed filter on known cases where existing filters fail without careful exception handling or case-specific tuning and benchmark against the ArduPilotâ€™s EKF3, the most sophisticated EKF implementation currently available."
Robust Indoor Localization with Ranging-IMU Fusion,"Fan Jiang, David Caruso, Ashutosh Dhekne, Qi Qu, Jakob Engel, Jing Dong","Georgia Institute of Technology,Facebook Reality Lab,Meta,Facebook",Localization VI,"Indoor wireless ranging localization is a promising approach for low-power and high-accuracy localization of wearable devices. A primary challenge in this domain stems from non-line of sight propagation of radio waves. This study tackles a fundamental issue in wireless ranging: the unpredictability of real-time multipath determination, especially in challenging conditions such as when there is no direct line of sight. We achieve this by fusing range measurements with inertial measurements obtained from a low cost Inertial Measurement Unit (IMU). For this purpose, we introduce a novel asymmetric noise model crafted specifically for non-Gaussian multipath disturbances. Additionally, we present a novel Levenberg-Marquardt (LM)-family trust-region adaptation of the iSAM2 fusion algorithm, which is optimized for robust performance for our ranging-IMU fusion problem. We evaluate our solution in a densely occupied real office environment. Our proposed solution can achieve temporally consistent localization with an average absolute accuracy of ~0.3m in real-world settings. Furthermore, our results indicate that we can achieve comparable accuracy even with infrequent range measurements down to 1Hz."
Efficient Pose Prediction with Rational Regression Applied to VSLAM,"George Terzakis, Manolis Lourakis","Varjo Technologies,Foundation for Research and Technology -- Hellas",SLAM III,"Compared to polynomial splines, rational functions are known to be more efficient and well-behaved data fitting models. However, due to the potential presence of zeros in their denominator, rational functions tend to yield notoriously hard optimization problems. In this work, we present a novel least squares method for 6D pose prediction that employs rational regression. Our method can accommodate fixed data points and is able to circumvent the occurrence of zeros for rational quadratic interpolants. We demonstrate the suitability of rational quadratics for pose prediction by applying our approach to real data from the feature tracking stage of a real-time visual SLAM system and showing that it yields far more stable predictions when compared to state-of-the-art rational and polynomial spline methods."
IMU-Aided Event-Based Stereo Visual Odometry,"Junkai Niu, Sheng Zhong, Yi Zhou",Hunan University,SLAM III,"Direct methods for event-based visual odometry solve the mapping and camera pose tracking sub-problems by establishing implicit data association in a way that the generative model of events is exploited. The main bottlenecks faced by state-of-the-art work in this field include the high computational complexity of mapping and the limited accuracy of tracking. In this paper, we improve our previous direct pipeline Event-based Stereo Visual Odometry in terms of accuracy and efficiency. To speed up the mapping operation, we propose an efficient strategy of edge-pixel sampling according to the local dynamics of events. The mapping performance in terms of completeness and local smoothness is also improved by combining the temporal stereo results and the static stereo results. To circumvent the degeneracy issue of camera pose tracking in recovering the yaw component of general 6-DoF motion, we introduce as a prior the gyroscope measurements via pre-integration. Experiments on publicly available datasets justify our improvement. We release our pipeline as an open-source software for future research in this field."
S-Graphs+: Real-Time Localization and Mapping Leveraging Hierarchical Representations,"Hriday Bavle, Jose Luis Sanchez-Lopez, Muhammad Shaheer, Javier Civera, Holger Voos","University of Luxembourg,Interdisciplinary Center for Security, Reliability and Trust (Sn,Universidad de Zaragoza",SLAM III,"In this paper, we present an evolved version of Situational Graphs, which jointly models in a single optimizable factor graph (1) a pose graph, as a set of robot keyframes comprising associated measurements and robot poses, and (2) a 3D scene graph, as a high-level representation of the environment that encodes its different geometric elements with semantic attributes and the relational information between them. Specifically, our S-Graphs+ is a novel four-layered factor graph that includes: (1) a keyframes layer with robot pose estimates, (2) a walls layer representing wall surfaces, (3) a rooms layer encompassing sets of wall planes, and (4) a floors layer gathering the rooms within a given floor level. The above graph is optimized in real-time to obtain a robust and accurate estimate of the robotâ€™s pose and its map, simultaneously constructing and leveraging high-level information of the environment. To extract this high-level information, we present novel room and floor segmentation algorithms utilizing the mapped wall planes and free-space clusters. We tested S-Graphs+ on multiple datasets, including simulated and real data of indoor environments from varying construction sites, and on a real public dataset of several indoor office areas. On average over our datasets, S-Graphs+ outperforms the accuracy of the second-best method by a margin of 10.67%, while extending the robot situational awareness by a richer scene model. Moreover, we make the software available as a d"
Visual Place Recognition: A Tutorial,"Stefan Schubert, Peer Neubert, Sourav Garg, Michael Milford, Tobias Fischer","Chemnitz University of Technology,University of Koblenz,University of Adelaide,Queensland University of Technology",SLAM III,"Localization is an essential capability for mobile robots. A rapidly growing field of research in this area is Visual Place Recognition (VPR), which is the ability to recognize previously seen places in the world based solely on images. This present work is the first tutorial paper on visual place recognition. It unifies the terminology of VPR and complements prior research in two important directions: 1) It provides a systematic introduction for newcomers to the field, covering topics such as the formulation of the VPR problem, a general-purpose algorithmic pipeline, an evaluation methodology for VPR approaches, and the major challenges for VPR and how they may be addressed. 2) As a contribution for researchers acquainted with the VPR problem, it examines the intricacies of different VPR problem types regarding input, data processing, and output. The tutorial also discusses the subtleties behind the evaluation of VPR algorithms, e.g., the evaluation of a VPR system that has to find all matching database images per query, as opposed to just a single match. Practical code examples in Python illustrate to prospective practitioners and researchers how VPR is implemented and evaluated."
Multi-Radar Inertial Odometry for 3D State Estimation Using mmWave Imaging Radar,"Jui-te Huang, Ruoyang Xu, Akshay Hinduja, Michael Kaess",Carnegie Mellon University,SLAM III,"State estimation is a crucial component for the successful implementation of robotic systems, relying on sensors such as cameras, LiDAR, and IMUs. However, in real-world scenarios, the performance of these sensors is degraded by challenging environments, e.g. adverse weather conditions and low-light scenarios. The emerging 4D imaging radar technology is capable of providing robust perception in adverse conditions. Despite its potential, challenges remain for indoor settings where noisy radar data does not present clear geometric features. Moreover, disparities in radar data resolution and field of view(FOV) can lead to inaccurate measurements. While prior research has explored radar-inertial odometry based on Doppler velocity information, challenges remain for the estimation of 3D motion because of the discrepancy in the FOV and resolution of the radar sensor. In this paper, we address Doppler velocity measurement uncertainties. We present a method to optimize body frame velocity while managing Doppler velocity uncertainty. Based on our observations, we propose a dual imaging radar configuration to mitigate the challenge of discrepancy in radar data. To attain high-precision 3D state estimation, we introduce a strategy that seamlessly integrates radar data with a consumer-grade IMU sensor using fixed-lag smoothing optimization. Finally, we evaluate our approach using real-world 3D motion data."
Semantically Guided Feature Matching for Visual SLAM,"Oguzhan Ilter, Iro Armeni, Marc Pollefeys, Daniel Barath","ETH Zürich,Stanford University,ETH Zurich,MTA SZTAKI; Visual Recognition Group in CTU Prague",SLAM III,"We introduce a new algorithm that utilizes semantic information to enhance feature matching in visual SLAM pipelines. The proposed method constructs a high-dimensional semantic descriptor for each detected ORB feature. When integrated with traditional visual ones, these descriptors aid in establishing accurate tentative point correspondences between consecutive frames. Additionally, our semantic descriptors enrich 3D map points, enhancing loop closure detection by providing deeper insights into the underlying map regions. Experiments on public large-scale datasets demonstrate that our technique surpasses the accuracy of established methods. Importantly, given its detector-agnostic nature, our algorithm also amplifies the efficacy of modern keypoint detectors, such as SuperPoint. The implementation of our algorithm can be found on Github."
DVI-SLAM: A Dual Visual Inertial SLAM Network,"Xiongfeng Peng, Zhihua Liu, Weiming Li, Ping Tan, Soonyong Cho, Qiang Wang","Samsung R&D Institute China-Beijing,Samsung Research Center, Beijing, China,Samsung Advanced Institute of Technology (SAIT),Simon Fraser University,Samsung Advanced Institute of Technology,Samsung",SLAM III,"Recent deep learning based visual simultaneous localization and mapping (SLAM) methods have made significant progress. However, how to make full use of visual information as well as better integrate with inertial measurement unit (IMU) in visual SLAM has potential research value. This paper proposes a novel deep SLAM network with dual visual factors. The basic idea is to integrate both photometric factor and re-projection factor into the end-to-end differentiable structure through multi-factor data association module. We show that the proposed network dynamically learns and adjusts the confidence maps of both visual factors and it can be further extended to include the IMU factors as well. Extensive experiments validate that our proposed method significantly outperforms the state-of-the-art methods on several public datasets, including TartanAir, EuRoC and ETH3D-SLAM. Specifically, when dynamically fusing the three factors together, the absolute trajectory error for both monocular and stereo configurations on EuRoC dataset has reduced by 45.3% and 36.2% respectively."
DMSA - Dense Multi Scan Adjustment for LiDAR Inertial Odometry and Global Optimization,"David Skuddis, Norbert Haala",University of Stuttgart,SLAM III,"We propose a new method for fine registering multiple point clouds simultaneously. The approach is characterized by being dense, therefore point clouds are not reduced to pre-selected features in advance. Furthermore, the approach is robust against small overlaps and dynamic objects, since no direct correspondences are assumed between point clouds. Instead, all points are merged into a global point cloud, whose scattering is then iteratively reduced. This is achieved by dividing the global point cloud into uniform grid cells whose contents are subsequently modeled by normal distributions. We show that the proposed approach can be used in a sliding window continuous trajectory optimization combined with IMU measurements to obtain a highly accurate and robust LiDAR inertial odometry estimation. Furthermore, we show that the proposed approach is also suitable for large scale keyframe optimization to increase accuracy. We provide the source code and some experimental data on https://github.com/davidskdds/DMSA_LiDAR_SLAM.git."
CTA-LO: Accurate and Robust LiDAR Odometry Using Continuous-Time Adaptive Estimation,"Yuezhang Lv, Yunzhou Zhang, Xiaoyu Zhao, Wu Li, Jian Ning, Yang Jin","Northeastern University,Northeastern University, China",SLAM III,"Accurate and robust LiDAR odometry is a crucial technology for robot localization. However, motion distortion and ranging error make it a bottleneck. Most existing methods are limited in accuracy and robustness because they simply compensate for motion distortion by constant velocity motion assumption without accurate model of ranging error. In this paper, we propose a high-precision and robust LiDAR odometry (LO), which utilizes continuous-time estimation to remove LiDAR distortion and builds the spot uncertainty model to quantify the ranging error. Generally, the number of variables in continuous-time estimation is several times higher than that in discrete-time ones, leading to insufficient constraints on the LiDAR odometry. To solve this problem, we propose a marginalization method to retain prior scans' constraints by exploiting the local support property of the B-spline. To further improve the odometry accuracy, we propose a residual adaptive weighting method and a probabilistic point cloud map based on the spot uncertainty model of LiDAR points. The experimental results show that our method outperforms state-of-the-art LiDAR odometry in accuracy and robustness."
Coordinated Landing Control for Cross-Domain UAV-USV Fleets Using Heterogeneous-Feature Matching,"Jianing Ding, Hai-Tao Zhang, Binbin Hu","Huazhong University of Science and Technology,Huazhong University of Science andTechnology,Nanyang Technological University",Networked and Cooperating Robots,"Abstractâ€”Coordinated landing control for multiple unmanned aerial vehicles (UAVs) on appropriate multiple unmanned surface vehicles (USVs) is an urgent yet challenging mission with the tremendous development of modern marine industry. To this end, we propose a coordinated multiple UAV-USV landing control algorithm via heterogeneous-feature matching. Specifically, the heterogeneous landing features of different UAVs and USVs are extracted to establish a dynamic UAV-USV cooperative landing ability mapping for the cross domain UAV-USV fleets (CDUUFs). Then, by incorporating suitable allocation with UAV-USV landing convergence and collision avoidance among UAVs into constraints with the assistance of both control Lyapunov functions (CLFs) and control barrier functions (CBFs), the multiple UAV-USV landing control problem is formulated as a constraint-based optimization one. Therein, slack variables are introduced to fulfill the assignment and facilitate the searching of a balanced solution between control performance and landing safety. Finally, extensive simulations are conducted to substantiate the effectiveness of the present multiple UAV-USV landing control law."
Distributed Control Barrier Functions for Global Connectivity Maintenance,"Nicola De Carli, Paolo Salaris, Paolo Robuffo Giordano","CNRS,University of Pisa,IRISA CNRS UMR,,,,",Networked and Cooperating Robots,"In this work, we propose a framework for the distributed implementation of Quadratic Programs-based controllers, building upon and rectifying a significant limitation in a previously presented approach. The proposed framework is primarily motivated by the distributed implementation of Control Barrier Functions (CBFs), whose primary objective is to make minimal adjustments to a nominal controller while ensuring constraint satisfaction. By improving over some limitations in the current state-of-the-art, we are able to apply distributed CBFs to the problem of global connectivity maintenance in presence of communication and sensing constraints. Specifically, we consider the problem of preserving connectivity for a group of quadrotors with onboard sensors under distance and field of view constraints. Leveraging distributed control barrier functions, our approach maintains global graph connectivity while optimizing the performance of the desired task. Numerical simulations validate its effectiveness."
Stability Analysis of Distance-Angle Leader-Follower Formation Control,"Manao Machida, Masumi Ichien","NEC,NEC Corporation",Networked and Cooperating Robots,"Necessary and sufficient conditions are described for stable distance-angle leader-follower formation control of first- and second-order holonomic and non-holonomic mobile robots. The distance-angle leader-follower formation is a problem of maintaining the desired relative distance and orientation of robots in a group. Our analysis shows that the input constraints on the leader are necessary for stable formation control. These constraints are summarized as follows: 1) In a team of first (second) order holonomic mobile robots, the leader has to be controlled as a first (second) order non-holonomic mobile robot; 2) In a team of first (second) order non-holonomic mobile robots, the control input of the leader must be limited so that the curvature is first (second) order differentiable. We further show that these constraints are sufficient for the followers to maintain formation. Moreover, we present globally asymptotically stable controllers and describe simulation experiments that demonstrate the effectiveness of these controllers."
"A Distributed Multi-Robot Framework for Exploration, Information Acquisition and Consensus","Aalok Patwardhan, Andrew J Davison",Imperial College London,Networked and Cooperating Robots,"The distributed coordination of robot teams performing complex tasks is challenging to formulate. The different aspects of a complete task such as local planning for obstacle avoidance, global goal coordination and collaborative mapping are often solved separately, when clearly each of these should influence the others for the most efficient behaviour. In this paper we use the example application of distributed information acquisition as a robot team explores a large space to show that we can formulate the whole problem as a single factor graph with multiple connected layers representing each aspect. We use Gaussian Belief Propagation (GBP) as the inference mechanism, which permits parallel, on-demand or asynchronous computation for efficiency when different aspects are more or less important. This is the first time that a distributed GBP multi-robot solver has been proven to enable intelligent collaborative behaviour rather than just guiding robots to individual, selfish goals. We encourage the reader to view our demos at https://aalpatya.github.io/gbpstack."
Environmental Awareness Dynamic 5G QoS for Retaining Real Time Constraints in Robotic Applications,"Gerasimos Damigos, Akshit Saradagi, Sara Sandberg, George Nikolakopoulos","Ericsson AB - Ericsson Research,Luleå University of Technology, Luleå, Sweden,Luleå University of Technology",Networked and Cooperating Robots,"The fifth generation (5G) cellular network technology is mature and increasingly utilized in many industrial and robotics applications, while an important functionality is the advanced Quality of Service (QoS) features. Despite the prevalence of 5G QoS discussions in the related literature, there is a notable absence of real-life implementations and studies concerning their application in time-critical robotics scenarios. This article considers the operation of time-critical applications for 5G-enabled unmanned aerial vehicles (UAVs) and how their operation can be improved by the possibility to dynamically switch between QoS data flows with different priorities. As such, we introduce a robotics oriented analysis on the impact of the 5G QoS functionality on the performance of 5G-enabled UAVs. Furthermore, we introduce a novel framework for the dynamic selection of distinct 5G QoS data flows that is autonomously managed by the 5G-enabled UAV. This problem is addressed in a novel feedback loop fashion utilizing a probabilistic finite state machine (PFSM). Finally, the efficacy of the proposed scheme is experimentally validated with a 5G-enabled UAV in a real-world 5G stand-alone (SA) network."
"CloudGripper: An Open Source Cloud Robotics Testbed for Robotic Manipulation Research, Benchmarking and Data Collection at Scale","Muhammad Zahid, Florian T. Pokorny",KTH Royal Institute of Technology,Networked and Cooperating Robots,"We present CloudGripper, an open source cloud robotics testbed, consisting of a scalable, space and cost-efficient design constructed as a rack of 32 small robot arm work cells. Each robot work cell is fully enclosed and features individual lighting, a low-cost Cartesian robot arm with an attached rotatable parallel jaw gripper and a dual camera setup for experimentation. The system design is focused on continuous operation and features a 10 Gbit/s network connectivity allowing for high throughput remote-controlled experimentation and data collection for robotic manipulation. Furthermore, CloudGripper is intended to form a community testbed to study the challenges of large scale machine learning and cloud and edge-computing in the context of robotic manipulation. In this work, we describe the mechanical design of the system, its initial software stack and evaluate the repeatability of motions executed by the proposed robot arm design. A local network API throughput and latency analysis is also provided. CloudGripper-Rope-100, a dataset of more than a hundred hours of randomized rope pushing interactions and approximately 4 million camera images is collected and serves as a proof of concept demonstrating data collection capabilities. A project website with more information is available at https://cloudgripper.org."
FogROS2-Config: A Toolkit for Choosing Server Configurations for Cloud Robotics,"Kaiyuan Eric Chen, Kush Hari, Rohil Khare, Charlotte Le, Trinity Chung, Jaimyn Drake, Jeffrey Ichnowski, John Kubiatowicz, Ken Goldberg","University of California, Berkeley,UC Berkeley,Carnegie Mellon University",Networked and Cooperating Robots,"Cloud service providers provide over 50,000 distinct and dynamically changing set of cloud server options. To help roboticists make cost-effective decisions, we present FogROS2-Config, an open toolkit that takes ROS2 nodes as input and automatically runs relevant benchmarks to quickly return a menu of cloud compute services that tradeoff latency and cost. Because it is infeasible to try every hardware configuration, FogROS2-Config quickly samples tests a small set of edge-case servers. We evaluate FogROS2-Config on three robotics application tasks: visual SLAM, grasp planning. and motion planning. FogROS2-Config can reduce the cost by up to 20x. By comparing with a Pareto frontier for cost and latency by running the application task on all available server configurations, we evaluate cost and latency models and confirm that FogROS2-Config selects efficient hardware configurations to balance cost and latency."
Opportunistic Communication in Robot Teams,"Daniel Mox, Kashish Garg, Alejandro Ribeiro, Vijay Kumar",University of Pennsylvania,Networked and Cooperating Robots,"In this paper we present a new approach to Mobile Infrastructure on Demand (MID) where a dedicated team of robots creates and sustains a wireless network that satisfies the communication requirements of a different team of task-oriented robots seeking to coordinate their actions in the absence of existing communication infrastructure. Different from previous works, our approach forgoes heuristics for network performance such as algebraic-connectivity or network flow optimizations and instead positions communication support robots to directly maximize the probability of packet delivery by the underlying opportunistic routing protocol. Our system is task agnostic and practical to implement and operate on robots equipped with off-the-shelf WiFi radios. We demonstrate this through a set of experiments showing our MID system maintaining the delivery of critical mission data in a situational awareness setting and enabling foraging robots to effectively coordinate their actions during multi-robot exploration."
Enhancing the Tracking Performance of Passivity-Based High-Frequency Robot Cloud Control,"Fabian Jakob, Xiao Chen, Hamid Sadeghian, Sami Haddadin",Technical University of Munich,Networked and Cooperating Robots,"This paper addresses the migration of high-frequency robot controllers to remote computing services, which are connected via a communication channel prone to delays and packet loss. The stability of the networked system is guaranteed by ensuring passivity of each subcomponent in the interconnection, as well as the Time-Domain-Passivity-Approach (TDPA) for the communication channel. We reduce conservatism of the TDPA using the model knowledge on both sides of the communication system to identify passivity excesses. This is further used to avoid over-dissipation of energy in the passivity controller by augmentation of a tolerable passivity-shortage. Tracking offsets are eliminated with a position drift compensation algorithm, for which convergence guarantees are provided. The experimental validation of the results conducted on a 7-DoF Franka Research 3 robot demonstrates a substantial enhancement in tracking performance due to the proposed modifications, particularly in scenarios with high communication delays."
Solving Rearrangement Puzzles Using Path Defragmentation in Factored State Spaces,"Servet Bora Bayraktar, Andreas Orthey, Zachary Kingston, Marc Toussaint, Lydia Kavraki","TU Berlin,Realtime Robotics Inc.,Rice University",Motion Planning I,"Rearrangement puzzles are variations of rearrangement problems in which the elements of a problem are potentially logically linked together. To efficiently solve such puzzles, we develop a motion planning approach based on a new state space that is logically factored, integrating the capabilities of the robot through factors of simultaneously manipulatable joints of an object. Based on this factored state space, we propose less-actions RRT (LA-RRT), a planner which optimizes for a low number of actions to solve a puzzle. At the core of our approach lies a new path defragmentation method, which rearranges and optimizes consecutive edges to minimize action cost. We solve six rearrangement scenarios with a Fetch robot, involving planar table puzzles and an escape room scenario. LA-RRT significantly outperforms the next best asymptotically-optimal planner by 4.01 to 6.58 times improvement in final action cost."
Leveraging Opportunism in Sample-Based Motion Planning,"Michael Lanighan, Oscar Youngquist","TRACLabs, Inc,University of Massachusetts Amherst",Motion Planning I,"Sample-based motion planning approaches, such as RRT*, have been widely adopted in robotics due to their support for high-dimensional state spaces and guarantees of completeness and optimality. This paper introduces an RRT* approach (ORRT*) that leverages opportunism to (1) find solutions quickly, (2) reduce wasted compute, and (3) improve data efficiency. The key insight of the approach is to make the most of compute when expanding the search tree by adding the last viable configurations found when connecting new nodes rather than rejecting the sampled nodes outright, allowing for more productive exploration of the space. We evaluate the proposed approach in a set of mobility and manipulator postural control domains, contrasting the performance of the opportunistic approach with state-of-the-art RRT* variants. Our analysis shows that such an approach has desirable characteristics and warrants further exploration."
SE(2) Assembly Planning for Magnetic Modular Cubes,"Kjell Keune, Aaron T. Becker","Technische Universität Braunschweig,University of Houston",Motion Planning I,"Magnetic modular cubes are cube-shaped bodies with embedded permanent magnets. The cubes are uniformly controlled by a global time-varying magnetic field. A 2D physics simulator is used to simulate global control and the resulting continuous movement of magnetic modular cube structures. We develop local plans, closed-loop control algorithms for planning the connection of two structures at desired faces. The global planner generates a building instruction graph for a target structure that we traverse in a depth-first-search approach by repeatedly applying local plans. We analyze how structure size and shape affect planning time. The planner solves 80% of the randomly created instances with up to 12 cubes in an average time of about 200 seconds."
A Constrained Path Following Method for Snake-Like Manipulators Via Controlled Winding Uncoiling Strategy,"Mingrui Luo, Yunong Tian, Yinghua Cao, Minghao Chen, Yanfeng Zhang, En Li, Min Tan","Institute of Automation, Chinese Academy of Sciences,Institute of Automationï¼ŒChinese Academy of Sciences,Institute of Automation,Chinese Academy of Sciences",Motion Planning I,"Benefiting from its hyper-redundant structure, the biomimetic snake-like manipulator retains its remarkable flexibility even within confined spaces. However, its motion planning and control pose significant challenges. This paper imitates the winding uncoiling behavior of snakes to achieve controllable constrained path following. Firstly, based on control points, a recursive computational model and an equivalent planning angle model are established, enabling efficient and analytical determination of joint positions, collision regions, and motion parameters during the path following. Subsequently, the sliding control point algorithm and motion smoothing restriction algorithm are designed. The former ensures that the remaining segments during following strictly remain within the collision-free regions defined by the base and path controls, while the latter smooths the control parameters based on velocity and acceleration limitations. Finally, simulation and practical experiments demonstrate the feasibility of the proposed methods. The prototype that applied our method can reach targets and accomplish tasks, further validating the applicability of the snake-like manipulator."
Multi-Profile Quadratic Programming (MPQP) for Optimal Gap Selection and Speed Planning of Autonomous Driving,"Alexandre Miranda Anon, Sangjae Bae, Manish Saroya, David Isele","Honda Research Institute USA,Honda Research Institute, USA,Honda Research Institute USA, Inc.,University of Pennsylvania, Honda Research Institute USA",Motion Planning I,"Smooth and safe speed planning is imperative for the successful deployment of autonomous vehicles. This paper presents a mathematical formulation for the optimal speed planning of autonomous driving, which has been validated in high-fidelity simulations and real-road demonstrations with practical constraints. The algorithm explores the inter-traffic gaps in the time and space domain using a breadth-first search. For each gap, quadratic programming finds an optimal speed profile, synchronizing the time and space pair along with dynamic obstacles. Qualitative and quantitative analysis in Carla is reported to discuss the smoothness and robustness of the proposed algorithm. Finally, we present a road demonstration result for urban city driving."
IKLink: End-Effector Trajectory Tracking with Minimal Reconfigurations,"Yeping Wang, Carter Sifferman, Michael Gleicher","University of Wisconsin-Madison,University of Wisconsin - Madison",Motion Planning I,"Many applications require a robot to accurately track reference end-effector trajectories. Certain trajectories may not be tracked as single, continuous paths due to the robot's kinematic constraints or obstacles elsewhere in the environment. In this situation, it becomes necessary to divide the trajectory into shorter segments. Each such division introduces a reconfiguration, in which the robot deviates from the reference trajectory, repositions itself in configuration space, and then resumes task execution. The occurrence of reconfigurations should be minimized because they increase time and energy usage. In this paper, we present IKLink, a method for finding joint motions to track reference end-effector trajectories while executing the minimum number of reconfigurations. Our graph-based method generates a diverse set of Inverse Kinematics (IK) solutions for every waypoint on the reference trajectory and utilizes a dynamic programming algorithm to find the optimal motion by linking the IK solutions. We demonstrate the effectiveness of IKLink through a simulation experiment and an illustrative demonstration using a physical robot."
A Control Barrier Function-Based Motion Planning Scheme for a Quadruped Robot,"Halil Utku Unlu, Vinicius Mariano Gonçalves, Dimitris Chaikalis, Antonios Tzes, Farshad Khorrami","New York University,New York University Abu Dhabi, United Arab Emirates,New York University Abu Dhabi,New York University Tandon School of Engineering",Motion Planning I,"A Control Barrier Function (CBF)-based motion planning algorithm is proposed. The algorithm explores an unknown environment to reach a target point, providing velocity commands to the robot controller module. CBFs, along with a circulation inequality are used to generate safe paths toward the goal while preventing collisions with obstacles. The proposed global navigation scheme is experimentally verified on a quadruped platform to demonstrate safe, collision-free exploration over long distances."
Physics-Informed Neural Motion Planning on Constraint Manifolds,"Ruiqi Ni, Ahmed H. Qureshi",Purdue University,Motion Planning I,"Constrained Motion Planning (CMP) aims to find a collision-free path between the given start and goal configurations on the kinematic constraint manifolds. These problems appear in various scenarios ranging from object manipulation to legged-robot locomotion. However, the zero-volume nature of manifolds makes the CMP problem challenging, and the state-of-the-art methods still take several seconds to find a path and require a computationally expansive path dataset for imitation learning. Recently, physics-informed motion planning methods have emerged that directly solve the Eikonal equation through neural networks for motion planning and do not require expert demonstrations for learning. Inspired by these approaches, we propose the first physics-informed CMP framework that solves the Eikonal equation on the constraint manifolds and trains neural function for CMP without expert data. Our results show that the proposed approach efficiently solves various CMP problems in both simulation and real-world, including object manipulation under orientation constraints and door opening with a high-dimensional 6-DOF robot manipulator. In these complex settings, our method exhibits high success rates and finds paths in sub-seconds, which is many times faster than the state-of-the-art CMP methods."
Practical and Safe Navigation Function Based Motion Planning of UAVs,"Himani Sinhmar, Marcus Carl Greiff, Stefano Di Cairano","Cornell University,Mitsubishi Electric Research Laboratories",Motion Planning I,"This paper offers a practical method for certifiably safe operations of an unmanned aerial vehicle (UAV) with limited power and computation, useful for real-time operations where the UAV is exposed to significant disturbances in non- convex free space. We propose a motion planning method based on the Explicit Reference Governor (ERG) framework to ensure the safety of a flying quadrotor UAV. From a small set of experiment data and assumptions on modeling errors, a Lyapunov function is synthesized by which an ERG is constructed to modify the UAV set-points. The method can handle polyhedral obstacles and constraints imposed on the maximum thrust of the UAV and its maximum tilt. We demonstrate the approach with extensive simulations and experiments using a Crazyflie 2.1."
Flock-Formation Control of Multi-Agent Systems Using Imperfect Relative Distance Measurements,"Andreas Brandstätter, Scott Smolka, Scott Stoller, Ashish Tiwari, Radu Grosu","Technische Universität Wien,Stony Brook University,Microsoft Corp,TU Wien",Swarm Robotics,"We present distributed distance-based control (DDC), a novel approach for controlling a multi-agent system, such that it achieves a desired formation, in a resource-constrained setting. Our controller is fully distributed and only requires local state-estimation and scalar measurements of inter-agent distances. It does not require an external localization system or inter-agent exchange of state information. Our approach uses spatial-predictive control (SPC), to optimize a cost function given strictly in terms of inter-agent distances and the distance to the target location. In DDC, each agent continuously learns and updates a very abstract model of the actual system, in the form of a dictionary of three independent key-value pairs (delta s, delta d), where delta d is the partial derivative of the distance measurements along a spatial direction delta s. This is sufficient for an agent to choose the best next action. We validate our approach by using DDC to control a collection of Crazyflie drones to achieve formation flight and reach a target while maintaining flock formation."
From Shadows to Light: A Swarm Robotics Approach with Onboard Control for Seeking Dynamic Sources in Constrained Environments,"Tugay Alperen Karagüzel, Victor Retamal Guiberteau, Nicolas Cambier, Eliseo Ferrante",Vrije Universiteit Amsterdam,Swarm Robotics,"In this paper, we present a swarm robotics control and coordination approach that can be used for locating a moving target or source in a GNSS-denied indoor setting. The approach is completely on-board and can be deployed on nano-drones such as the Crazyflies. The swarm acts on a simple set of rules to identify and trail a dynamically changing source gradient. To validate the effectiveness of our approach, we conduct experiments to detect the maxima of the dynamic gradient, which was implemented with a set of lights turned on and off with a time-varying pattern. Additionally, we introduce also a minimalistic fully onboard obstacle avoidance method, and assess the flexibility of our method by introducing an obstacle into the environment. The strategies rely on local interactions among UAVs, and the sensing of the source happens only at the individual level and is scalar, making it a viable option for UAVs with limited capabilities. Our method is adaptable to other swarm platforms with only minor parameter adjustments. Our findings demonstrate the potential of this approach as a flexible solution to tackle such tasks in constrained GNSS-denied indoor environments successfully."
Multi-Swarm Interaction through Augmented Reality for Kilobots,"Luigi Feola, Andreagiovanni Reina, Mohamed S. Talamali, Vito Trianni","University of Rome ""La Sapienza"" - National Research Council IST,Université Libre de Bruxelles,University College London (UCL),Consiglio Nazionale delle Ricerche",Swarm Robotics,"Research with swarm robotics systems can be complicated, time-consuming, and often expensive in terms of space and resources. The situation is even worse for studies involving multiple, possibly heterogeneous robot swarms. Augmented reality can provide an interesting solution to these problems, as demonstrated by the ARK system (Augmented Reality for Kilobots), which enhanced the experimentation possibilities with Kilobots, also relieving researchers from demanding tracking and logging activities. However, ARK is limited in mostly enabling experimentation with a single swarm. In this paper, we introduce M-ARK, a system to support studies on multi-swarm interaction. M-ARK is based on the synchronisation over a network connection of multiple ARK systems, whether real or simulated, serving a twofold purpose: (i) to study the interaction of multiple, possibly heterogeneous swarms, and (ii) to enable a gradual transition from simulation to reality. Moreover, M-ARK enables the interaction between swarms dislocated across multiple labs worldwide, encouraging scientific collaboration and advancement in multi-swarm interaction studies."
Morphobot: A Platform for Morphogenesis in Robot Swarm,"Xiaoyang Qin, Yongliang Yang, Mengyun Pan, Long Cui, Lianqing Liu","Shenyang Institute of Automation, CAS,Shenyang Institute of Automation, Chinese Academy of Sciences,Shenyang Institute of Automation",Swarm Robotics,"Various robot platforms have been developed for investigating new algorithms for swarm robotics. Morphogenetic engineering in robot swarms, however, proposes new requirements for platforms: precise motion control, physical interactions with the environment or neighbor robots, and functionalized shells. Few current platforms fulfill all the above characteristics. Here, we present Morphobot, a robot platform for morphogenetic engineering in swarm robotics. Its direct current coreless motors provide physical support and strong power, meanwhile, these needle-like motors also enhance the physical interactions among robots. Each Morphobot has a changeable shell. It is functionalized for programming local interactions, through physical contact or communication, among Morphobots. We characterized the mobility of Morphobot to test its capability of moving and physically interacting with its neighbors. To demonstrate its advantages in the morphogenesis of robot swarms, we designed two morphogenetic engineering experiments. The results revealed that swarms of Morphobots can form patterns via physical interactions and optical communications."
FireAntV3: A Modular Self-Reconfigurable Robot towards Free-Form Self-Assembly Using Attach-Anywhere Continuous Docks,"Petras Swissler, Michael Rubenstein","New Jersey Institute of Technology,Northwestern University",Swarm Robotics,"FireAntV3 uses a refined version of the 3D Continuous Docks to attach to other such docks at any location at any orientation with simple control and without alignment. The robot improves upon previous FireAnt-series robots by redesigning the locomotion drive system to improve mechanical and attachment reliability while also reducing the number of motors from six to three. We also expand the sensory capabilities of FireAntV3 to enable the robot to sense forces, sense the direction to a light source, and to sense contacting neighbors using vibrations. We validate this robot through full-robot tests demonstrating phototaxis and neighbor-detecting behavior. This paper also describes the method for manufacturing the continuous docks in a variety of geometries."
Reciprocal and Non-Reciprocal Swarmalators with Programmable Locomotion and Formations for Robot Swarms,"Steven Ceron, Wei Xiao, Daniela Rus","Massachusetts Institute of Technology,MIT",Swarm Robotics,"Natural and robotic swarms often exhibit non-reciprocal interactions; agents do not exhibit equal and opposite forces on each other. By studying the effects of reciprocal and non-reciprocal interactions we are better able to design emergent behaviors in robot collectives composed of agents that exert attractive and repulsive forces on each other. Moreover, by controlling agent-specific coupling forces on-demand, we can enable a collective to exhibit desired behaviors previously not possible. We use a general form of the swarming oscillator, swarmalator, model to study reciprocal and non-reciprocal interactions among agents that affect each other's motions over long and short distances, we use non-reciprocal coupling to elicit collective locomotion toward or away from target sites, and we use the control barrier function method to optimize the non-reciprocal interactions for a desired spatial formation. This work addresses the interests of the active matter, swarm robotics, and control barrier functions communities and demonstrates various collective behaviors with strong potential to be realized in macro- and micro- length scale robot swarms."
Automatically Designing Robot Swarms in Environments Populated by Other Robots: An Experiment in Robot Shepherding,"David Garzon Ramos, Mauro Birattari",Université Libre de Bruxelles,Swarm Robotics,"Automatic design is a promising approach to realizing robot swarms. Given a mission to be performed by the swarm, an automatic method produces the required control software for the individual robots. Automatic design has concentrated on missions that a swarm can execute independently, interacting only with a static environment and without the involvement of other active entities. In this paper, we investigate the design of robot swarms that perform their mission by interacting with other robots that populate their environment. We frame our research within robot shepherding: the problem of using a small group of robotsâ€”the shepherdsâ€”to coordinate a relatively larger groupâ€”the sheep. In our study, the group of shepherds is the swarm that is automatically designed, and the sheep are pre-programmed robots that populate its environment. We use automatic modular design and neuroevolution to produce the control software for the swarm of shepherds to coordinate the sheep. We show that automatic design can leverage mission-specific interaction strategies to enable an effective coordination between the two groups."
CrazySim: A Software-In-The-Loop Simulator for the Crazyflie Nano Quadrotor,"Christian Llanes, Zahi Kakish, Kyle Williams, Samuel Coogan","The Georgia Institute of Technology,Sandia National Laboratories,Sandia National Labs,Georgia Tech",Swarm Robotics,"In this work we develop a software-in-the-loop simulator platform for Crazyflie nano quadrotor drone fleets. One of the challenges in maintaining a large fleet of drones is ensuring that the fleet performs its task as expected without collision, and this becomes more challenging as the number of drones scales, possibly into the hundreds. Software-in-the-loop simulation is an important component in verifying that drone fleets operate correctly and can significantly reduce development time. The simulator interface that we develop runs an instance of the Crazyflie flight stack firmware for each individual drone on a commercial, desktop machine along with a sensors and communication plugin on Gazebo Sim. The plugin transmits simulated sensor information to the firmware along with a socket link interface to run external scripts that would be run on a ground station during hardware deployment. The plugin simulates a radio communication delay between the drones and the ground station to test offboard control algorithms and high-level fleet commands. To validate the proposed simulator, we provide a case study of decentralized model predictive control (MPC) that is run on a ground station to command a fleet of sixteen drones to follow a specified trajectory. We first run the controller on the simulator interface to verify performance and robustness of the algorithm before deployment to a Crazyflie hardware experiment in the Georgia Tech Robotarium."
Geometry-Informed Distance Candidate Selection for Adaptive Lightweight Omnidirectional Stereo Vision with Fisheye Images,"Conner Pulling, Je Hon Tan, Yaoyu Hu, Sebastian Scherer","Carnegie Mellon University,Defence Science and Technology Agency",Range Sensing,"Multi-view stereo omnidirectional distance estimation usually needs to build a cost volume with many hypothetical distance candidates. The cost volume building process is often computationally heavy considering the limited resources a mobile robot has. We propose a new geometry-informed way of distance candidates selection method which enables the use of a very small number of candidates and reduces the computational cost. We demonstrate the use of the geometry-informed candidates in a set of model variants. We find that by adjusting the candidates during robot deployment, our geometry-informed distance candidates also improve a pre-trained model's accuracy if the extrinsics or the number of cameras changes. Without any re-training or fine-tuning, our models outperform models trained with evenly distributed distance candidates. Models are also released as hardware-accelerated versions with a new dedicated large-scale dataset. The project page, code, and dataset can be found at https://theairlab.org/gicandidates/."
On Camera Model Conversions,"Eva Goichon, Guillaume Caron, Pascal Vasseur, Fumio Kanehiro","UPJV,CNRS,Université de Picardie Jules Verne,National Inst. of AIST",Range Sensing,"On the one hand, cameras of conventional field-of-view usually considered in computer vision and robotics are very often modeled as a pinhole plus possibly a distortion model. On the other hand, there is a large variety of models for panoramic cameras. Many camera models have been proposed for fisheye cameras, catadioptric cameras, and super fisheye cameras. But in both cases, few models offer the possibility of converting them into another model. This paper contributes to filling this gap in, to allow an algorithm designed with a projection model to accept data of a camera calibrated with another model. So, a pre-existing data set can be used without having to recalibrate the camera. We provide the methodology and mathematical developments for three conversions considering three different types of cameras that are evaluated with respect to calibration and within a visual Simultaneous Localization And Mapping benchmark. The source code of the camera model conversions studied in this paper is shared within the libPeR library for Perception in Robotics: https://github.com/PerceptionRobotique/libPeR_base."
RadCloud: Real-Time High-Resolution Point Cloud Generation Using Low-Cost Radars for Aerial and Ground Vehicles,"David Hunt, Shaocheng Luo, Amir Khazraei, Xiao Zhang, Robert Hallyburton, Tingjun Chen, Miroslav Pajic","Duke University,Duke university",Range Sensing,"Abstractâ€” In this work, we present RadCloud, a novel real time framework for directly obtaining higher-resolution lidar-like 2D point clouds from low-resolution radar frames on resource-constrained platforms commonly used in unmanned aerial and ground vehicles (UAVs and UGVs, respectively); such point clouds can then be used for accurate environmental mapping, navigating unknown environments, and other robotics tasks. While high-resolution sensing using radar data has been previously reported, existing methods cannot be used on most UAVs, which have limited computational power and energy; thus, existing demonstrations focus on offline radar processing. RadCloud overcomes these challenges by using a radar configuration with 1/4th of the range resolution and employing a deep learning model with 2.25Ã— fewer parameters. Additionally, RadCloud utilizes a novel chirp-based approach that makes obtained point clouds resilient to rapid movements (e.g., aggressive turns or spins), that commonly occur during UAV flights. In real-world experiments, we demonstrate the accuracy and applicability of RadCloud on commercially available UAVs and UGVs, with off-the-shelf radar platforms on-board."
That's My Point: Compact Object-Centric LiDAR Pose Estimation for Large-Scale Outdoor Localisation,"Georgi Pramatarov, Matthew Gadd, Paul Newman, Daniele De Martini","University of Oxford,Oxford University",Range Sensing,"This paper is about 3D pose estimation on LiDAR scans with extremely minimal storage requirements to enable scalable mapping and localisation. We achieve this by clustering all points of segmented scans into semantic objects and representing them only with their respective centroid and semantic class. In this way, each LiDAR scan is reduced to a compact collection of four-number vectors. This abstracts away important structural information from the scenes, which is crucial for traditional registration approaches. To mitigate this, we introduce an object-matching network based on self- and cross-correlation that captures geometric and semantic relationships between entities. The respective matches allow us to recover the relative transformation between scans through weighted Singular Value Decomposition (SVD) and RANdom SAmple Consensus (RANSAC). We demonstrate that such representation is sufficient for metric localisation by registering point clouds taken under different viewpoints on the KITTI dataset, and at different periods of time localising between KITTI and KITTI-360. We achieve accurate metric estimates comparable with state-of-the-art methods with almost half the representation size, specifically 1.33 kB on average."
A Point-To-Distribution Degeneracy Detection Factor for LiDAR SLAM Using Local Geometric Models,"Sehua Ji, Weinan Chen, Zerong Su, Yisheng Guan, Jiehao Li, Hong Zhang, Haifei Zhu","Guangdong University of Technology,Institute of Intelligent Manufacturing, Guangdong Academy of Sci,South China University of Technology,SUSTech",Range Sensing,"Limited by the working principles, LiDAR-SLAM systems suffer from the degeneration phenomenon in environments such as long corridors and tunnels, due to the lack of sufficient geometric features for frame-to-frame matching. The accuracy and sensitivity of existing degeneracy detection methods need to be further improved. In this paper, we propose a novel method for degeneracy detection using local geometric models based on point-to-distribution matching. To obtain an accurate description of local geometric models, an adaptive adjustment of voxel segmentation according to the point cloud distribution and density is designed. The codes of the proposed method is open-source and available at https://github.com/jisehua/Degenerate-Detection.git. Experiments with public datasets and self-build robots were conducted to evaluate the methods. The results exhibit that our proposed method achieves higher accuracy than the other existing approaches. Applying our proposed method is beneficial for improving the robustness of the LiDAR-SLAM systems."
"I-Octree: A Fast, Lightweight, and Dynamic Octree for Proximity Search","Jun Zhu, Hongyi Li, Zhepeng Wang, Shengjie Wang, Tao Zhang","Tsinghua University,Lenovo Research",Range Sensing,"Establishing the correspondences between newly acquired points and historically accumulated data (i.e., map) through nearest neighbors search is crucial in numerous robotic applications. However, static tree data structures are inadequate to handle large and dynamically growing maps in real-time. To address this issue, we present the i-Octree, a dynamic octree data structure that supports both fast nearest neighbor search and real-time dynamic updates, such as point insertion, deletion, and on-tree down-sampling. The i-Octree is built upon a leaf-based octree and has two key features: a local spatially continuous storing strategy that allows for fast access to points while minimizing memory usage, and local on-tree updates that significantly reduce computation time compared to existing static or dynamic tree structures. The experiments show that i-Octree outperforms contemporary state-of-the-art approaches by achieving, on average, a 19% reduction in runtime on real-world open datasets."
Self-Supervised Depth Correction of Lidar Measurements from Map Consistency Loss,"Ruslan Agishev, Tomas Petricek, Karel Zimmermann","Czech Technical University in Prague, FEE,Czech Technical University in Prague,Ceske vysoke uceni technicke v Praze, FEL",Range Sensing,"Depth perception is considered an invaluable source of information in the context of 3D mapping and various robotics applications. However, point cloud maps acquired using consumer-level light detection and ranging sensors (lidars) still suffer from bias related to local surface properties such as measuring beam-to-surface incidence angle. This fact has recently motivated researchers to exploit traditional filters, as well as the deep learning paradigm, in order to suppress the aforementioned depth sensors error while preserving geometric and map consistency details. Despite the effort, depth correction of lidar measurements is still an open challenge mainly due to the lack of clean 3D data that could be used as ground truth. In this paper, we introduce two novel point cloud map consistency losses, which facilitate self-supervised learning on real data of lidar depth correction models. Specifically, the models exploit multiple point cloud measurements of the same scene from different view-points in order to learn to reduce the bias based on the constructed map consistency signal. Complementary to the removal of the bias from the measurements, we demonstrate that the depth correction models help to reduce localization drift. Additionally, we release a data set that contains point cloud scans captured in an indoor corridor environment with precise localization and ground truth mapping information."
GroundGrid: LiDAR Point Cloud Ground Segmentation and Terrain Estimation,"Nicolai Steinke, Daniel Goehring, Raúl Rojas",Freie Universität Berlin,Range Sensing,"The precise point cloud ground segmentation is a crucial prerequisite of virtually all perception tasks for LiDAR sensors in autonomous vehicles. Especially the clustering and extraction of objects from a point cloud usually relies on an accurate removal of ground points. The correct estimation of the surrounding terrain is important for aspects of the drivability of a surface, path planning, and obstacle prediction. In this article, we propose our system GroundGrid which relies on 2D elevation maps to solve the terrain estimation and point cloud ground segmentation problems. We evaluate the ground segmentation and terrain estimation performance of GroundGrid and compare it to other state-of-the-art methods using the SemanticKITTI dataset and a novel evaluation method relying on airborne LiDAR scanning. The results show that GroundGrid is outperforming other state-of-the-art systems with an average IoU of 94.78% while maintaining a high run-time performance of 171Hz. The source code is available at https://github.com/dcmlr/groundgrid"
Morphable-SfS: Enhancing Shape-From-Silhouette Via Morphable Modeling,Guoyu Lu,University of Georgia,Range Sensing,"Reconstructing accurate object shapes based on single image inputs is still a critical and challenging task, mainly due to the potential shape ambiguity and occlusion. Most existing single image 3D reconstruction approaches, either trained on stereo setting or structure-from-motion, estimate 2.5D visible models which generally reconstruct one viewpoint of objects. We propose a method to leverage both the general Morphable Model on common objects and a multi-view synthesis-based shape-from-silhouette model to reconstruct complete object shapes. We use the proposed method to exploit strong geometric and perceptual cues in 3D shape reconstruction. During the inference, the trained model is able to produce high-quality and complete meshes with finely detailed structures from a 2D image captured from arbitrary perspectives. The proposed method is evaluated on both large-scale synthetic ShapeNet and real-world Pascal 3D+ and Pix3D datasets. The proposed work achieves state-of-the-art results compared with other recent self-supervised methods. Moreover, it shows a good capability of being applied in the unseen object reconstruction tasks."
Collision Detection and Avoidance for Black Box Multi-Robot Navigation,"Sara Ayoubi, Ilija Hadzic, Lou Salaun, Antonio Massaro",Nokia Bell Labs,Path Planning for Multiple Mobile Robots or Agents I,"To date, commercial industrial robots only provide multi-robot coordination for their own fleet of robots and treat robots from other vendors as general obstacles. The ability to enable robots from different vendors to co-exist in the same space is crucial to prevent vendor lock-in. We present the first decentralized system that achieves coordination between a heterogeneous fleet of black box robots for which the internals of the navigation stack are presumed unmodifiable. Our system, which we call CODAK, achieves the coordination by relying on minimum set of interfaces that are commonly available on most industrial and service robots. For each robot, CODAK uses a trained recurrent neural network to anticipate collisions from externally observable metrics. Anticipated collisions are avoided using a simple, but yet effective, concurrency control scheme. We run a series of experiments in simulation and with real robots to demonstrate CODAKâ€™s ability to enable safe navigation in different environments. We also experimentally compare CODAK with previously published white-box solutions to evaluate the penalty of black-box constraint."
Stackelberg Game-Theoretic Trajectory Guidance for Multi-Robot Systems with Koopman Operator,"Yuhan Zhao, Quanyan Zhu",New York University,Path Planning for Multiple Mobile Robots or Agents I,"Guided trajectory planning involves a leader robot strategically directing a follower robot to collaboratively reach a designated destination. However, this task becomes notably challenging when the leader lacks complete knowledge of the follower's decision-making model. There is a need for learning-based methods to effectively design the cooperative plan. To this end, we develop a Stackelberg game-theoretic approach based on the Koopman operator to address the challenge. We first formulate the guided trajectory planning problem through the lens of a dynamic Stackelberg game. We then leverage Koopman operator theory to acquire a learning-based linear system model that approximates the follower's feedback dynamics. Based on this learned model, the leader devises a collision-free trajectory to guide the follower using receding horizon planning. We use simulations to elaborate on the effectiveness of our approach in generating learning models that accurately predict the follower's multi-step behavior when compared to alternative learning techniques. Moreover, our approach successfully accomplishes the guidance task and notably reduces the leader's planning time to nearly half when contrasted with the model-based baseline method."
Optimal Path Planning for a Convoy-Support Vehicle Pair through a Repairable Network,"Abhay Singh Bhadoriya, Christopher Montez, Sivakumar Rathinam, Swaroop Darbha, David Casbeer, Satyanarayana Gupta Manyam","Texas A&M University,Texas A&M University - College Station,TAMU,AFRL,Infoscitex corp.",Path Planning for Multiple Mobile Robots or Agents I,"In this article, we consider a multi-agent path planning problem in a partially impeded environment. The impeded environment is represented by a graph with select road segments(edges) in disrepair impeding vehicular movement in the road network. A primary vehicle, which we refer to as a convoy, wishes to travel from a starting location to a destination while minimizing some accumulated cost. The convoy may traverse an impeded edge for an additional cost (associated with repairing the edge) than if it were unimpeded. A support vehicle, which we refer to as a service vehicle, is simultaneously deployed to assist the convoy by repairing edges, reducing the cost for the convoy to traverse those edges. The convoy is permitted to wait at any vertex to allow the service vehicle to repair an edge. The service vehicle is permitted to terminate its path at any vertex. The goal is then to find a pair of paths so the convoy reaches its destination while minimizing the total time (cost) the two vehicles are active, including any time the convoy waits. We refer to this problem as the Assisted Shortest Path Problem (ASPP). We present a generalized permanent labeling algorithm (GPLA) to find an optimal solution for the ASPP. We also introduce additional modifications to the labeling algorithm to significantly improve the computation time and refer to the modified labeling algorithm as GPLA*. Computational results are presented to illustrate the effectiveness of GPLA* in solving the ASPP."
Learning Heterogeneous Multi-Agent Allocations for Ergodic Search,"Ananya Rao, Guillaume Sartoretti, Howie Choset","Carnegie Mellon University,National University of Singapore (NUS)",Path Planning for Multiple Mobile Robots or Agents I,"Information-based coverage directs robots to move over an area to optimize a pre-defined objective function based on some measure of information. Our prior work determined that the spectral decomposition of an information map can be used to guide a set of heterogeneous agents, each with different sensor and motion models, to optimize coverage in a target region, based on a measure called ergodicity. In this paper, we build on this insight to construct a reinforcement learning formulation of the problem of allocating heterogeneous agents to different search regions in the frequency domain. We relate the spectral coefficients of the search map to each other in three different ways. The first method maps agents to pre-defined sets of spectral coefficients. In the second method, each agent learns a weight distribution over all spectral coefficients. Finally, in the third method, each agent learns weight distributions as parameterized curves over coefficients. Our numerical results demonstrate that distributing and assigning coverage responsibilities to agents depending on their sensing and motion models leads to 40%, 51%, and 46% improvement in coverage performance as measured by the ergodic metric, and 15%, 22%, and 20% improvement in time to find all targets in the search region, for the three methods respectively."
Multi-Robot Cooperative Socially-Aware Navigation Using Multi-Agent Reinforcement Learning,"Weizheng Wang, Le Mao, Ruiqi Wang, Byung-Cheol Min","Purdue University,Beijing University of Chemical Technology",Path Planning for Multiple Mobile Robots or Agents I,"In public spaces shared with humans, ensuring multi-robot systems navigate without collisions while respecting social norms is challenging, particularly with limited communication. Although current robot social navigation techniques leverage advances in reinforcement learning and deep learning, they frequently overlook robot dynamics in simulations, leading to a simulation-to-reality gap. In this paper, we bridge this gap by presenting a new multi-robot social navigation environment crafted using Dec-POSMDP and multi-agent reinforcement learning. Furthermore, we introduce SAMARL: a novel benchmark for cooperative multi-robot social navigation. SAMARL employs a unique spatial-temporal transformer combined with multi-agent reinforcement learning. This approach effectively captures the complex interactions between robots and humans, thus promoting cooperative tendencies in multi-robot systems. Our extensive experiments reveal that SAMARL outperforms existing baseline and ablation models in our designed environment. Demo videos for this work can be found at: https://sites.google.com/view/samarl"
Multi-Agent Path Finding for Cooperative Autonomous Driving,"Zhongxia Yan, Han Zheng, Cathy Wu","Massachusetts Institute of Technology,MIT",Path Planning for Multiple Mobile Robots or Agents I,"Anticipating possible future deployment of connected and automated vehicles (CAVs), cooperative autonomous driving at intersections has been studied by many works in control theory and intelligent transportation across decades. Simultaneously, recent parallel works in robotics have devised efficient algorithms for multi-agent path finding (MAPF), though often in environments with simplified kinematics. In this work, we hybridize insights and algorithms from MAPF with the structure and heuristics of optimizing the crossing order of CAVs at signal-free intersections. We devise an optimal and complete algorithm, Order-based Search with Kinematics Arrival Time Scheduling (OBS-KATS), which significantly outperforms existing algorithms, fixed heuristics, and prioritized planning with KATS. The performance is maintained under different vehicle arrival rates, lane lengths, crossing speeds, and control horizon. Through ablations and dissections, we offer insight on the contributing factors to OBS-KATS's performance. Our work is directly applicable to many similarly scaled traffic and multi-robot scenarios with directed lanes."
Communication-Aware Map Compression for Online Path-Planning,"Evangelos Psomiadis, Dipankar Maity, Panagiotis Tsiotras","Georgia Institute of Technology,University of North Carolina - Charlotte,Georgia Tech",Path Planning for Multiple Mobile Robots or Agents I,"This paper addresses the problem of the communication of optimally compressed information for mobile robot path-planning. In this context, mobile robots compress their current local maps to assist another robot in reaching a target in an unknown environment. We propose a framework that sequentially selects the optimal level of compression, guided by the robot's path, by balancing map resolution and communication cost. Our approach is tractable in close-to-real scenarios and does not necessitate prior environment knowledge. We design a novel decoder that leverages compressed information to estimate the unknown environment via convex optimization with linear constraints and an encoder that utilizes the decoder to select the optimal compression. Numerical simulations are conducted in a large close-to-real map and a maze map and compared with two alternative approaches. The results confirm the effectiveness of our framework in assisting the robot reach its target by reducing transmitted information, on average, by approximately 50%, while maintaining satisfactory performance."
Wind Field Modeling for Formation Planning in Multi-Drone Systems,"Minhyuk Park, Tsz-Chiu Au","UNIST,Ulsan National Institute of Science and Technology",Path Planning for Multiple Mobile Robots or Agents I,"In multi-drone systems such as drone light shows, drones move in formation while avoiding collisions. However, few existing formation planning algorithms consider the wind fields of drones during planning. Since the wind field effect is prominent when drones have to fly close to each other, we cannot ignore the effect during planning. In this paper, we extend the reservation system in autonomous intersection management for grid-based formation planning by including a new type of reservation called non-exclusive reservations specifically for handling wind fields. We train a deep learning model to predict the deviation of a drone's trajectory when the drone enters the wind field of another drone and then use the reservation grid to prevent collision. Based on the reservation system, we develop a new formation planning algorithm that focuses on adjusting the start times of motion plans to avoid collision. Our experimental results show that trajectory prediction can help make better decisions in task assignments for minimizing makespans."
Multi-Robot Informative Path Planning from Regression with Sparse Gaussian Processes,"Kalvik Jakkala, Srinivas Akella",University of North Carolina at Charlotte,Path Planning for Multiple Mobile Robots or Agents I,"This paper addresses multi-robot informative path planning (IPP) for environmental monitoring. The problem involves determining informative regions in the environment that should be visited by robots to gather the most information about the environment. We propose an efficient sparse Gaussian process-based approach that uses gradient descent to optimize paths in continuous environments. Our approach efficiently scales to both spatially and spatio-temporally correlated environments. Moreover, our approach can simultaneously optimize the informative paths while accounting for routing constraints, such as a distance budget and limits on the robot's velocity and acceleration. Our approach can be used for IPP with both discrete and continuous sensing robots, with point and non-point field-of-view sensing shapes, and for both single and multi-robot IPP. We demonstrate that the proposed approach is fast and accurate on real-world data."
ASP-LED: Learning Ambiguity-Aware Structural Priors for Joint Low-Light Enhancement and Deblurring,"Jing Ye, Yang Liu, Congjing Yu, Changzhen Qiu, Zhiyong Zhang","Sun Yat-Sen University,Sun Yat-sen University",Visual Learning I,"Low-light enhancement and deblurring is vital for high-level vision-related nighttime tasks. Most existing cascade and joint enhancement methods may provide undesirable results, suffering from severe artifacts, deteriorating blur, and unclear details. In this paper, we propose a novel ambiguity-aware network (ASP-LED) with structural priors, including high-frequency and edge, to enable effective image representation learning for joint low-light enhancement and deblurring. Specifically, we employ a Transformer backbone to explore the global clues of the image. To compensate for the inadequate local detail optimization, we propose a multi-patch perception pyramid block that models the correlation between different size patches and ambiguity, and identifies non-uniform deblurring spatial features, facilitating the reconstruction of potential high-frequency and edge information. Furthermore, a prior-guided reconstruction block based on the parallel attention mechanism is present to adaptively correct global image with statistical features, which helps guide the model to refine sharp texture and structure. Extensive experiments performed on simulated and real-world datasets demonstrate the efficacy of our proposed method in restoring low-light blurry images with increased visual perception compared to state-of-the-art methods."
AnyOKP: One-Shot and Instance-Aware Object Keypoint Extraction with Pretrained ViT,"Fangbo Qin, Taogang Hou, Shan Lin, Kaiyuan Wang, Michael Yip, Shan Yu","Institute of Automation, Chinese Academy of Sciences,Beijing Jiaotong University,University of California, San Diego",Visual Learning I,"Towards flexible object-centric visual perception, we propose a one-shot instance-aware object keypoint (OKP) extraction approach, AnyOKP, which leverages the powerful representation ability of pretrained vision transformer (ViT), and can obtain keypoints on multiple object instances of arbitrary category after learning from a support image. An off-the-shelf petrained ViT is directly deployed for generalizable and transferable feature extraction, which is followed by training-free feature enhancement. The best-prototype pairs (BPPs) are searched for in support and query images based on appearance similarity, to yield instance-unaware candidate keypoints. Then, the entire graph with all candidate keypoints as vertices are divided into sub-graphs according to the feature distributions on the graph edges. Finally, each sub-graph represents an object instance. AnyOKP is evaluated on real object images collected with the cameras of a robot arm, a mobile robot, and a surgical robot, which not only demonstrates the cross-category flexibility and instance awareness, but also show remarkable robustness to domain shift and viewpoint change."
RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision,"Mingjie Pan, Jiaming Liu, Renrui Zhang, Peixiang Huang, Xiaoqi Li, Bing Wang, Hongwei Xie, Li Liu, Shanghang Zhang","Peking University,CUHK,NTU,Nanjing University,Xiaomi Car",Visual Learning I,"3D occupancy prediction holds significant promise in the fields of robot perception and autonomous driving, which quantifies 3D scenes into grid cells with semantic labels. Recent works mainly utilize complete occupancy labels in 3D voxel space for supervision. However, the expensive annotation process and sometimes ambiguous labels have severely constrained the usability and scalability of 3D occupancy models. To address this, we present RenderOcc, a novel paradigm for training 3D occupancy models only using 2D labels. Specifically, we extract a NeRF-style 3D volume representation from multi-view images, and employ volume rendering techniques to establish 2D renderings, thus enabling direct 3D supervision from 2D semantics and depth labels. Additionally, we introduce an Auxiliary Ray method to tackle the issue of sparse viewpoints in autonomous driving scenarios, which leverages sequential frames to construct comprehensive 2D rendering for each object. To our best knowledge, RenderOcc is the first attempt to train multi-view 3D occupancy models only using 2D labels, reducing the dependence on costly 3D occupancy annotations. Extensive experiments demonstrate that RenderOcc achieves comparable performance to models fully supervised with 3D labels, underscoring the significance of this approach in real-world applications."
DRO: Deep Recurrent Optimizer for Video to Depth,"Xiaodong Gu, Weihao Yuan, Zuozhuo Dai, Siyu Zhu, Chengzhou Tang, Zilong Dong, Ping Tan","Alibaba Group,Hong Kong University of Science and Technology,Alibaba AI Lab,Simon Fraser University,Company",Visual Learning I,"There are increasing interests of studying the video-to-depth (V2D) problem with machine learning techniques. While earlier methods directly learn a mapping from images to depth maps and camera poses, more recent works enforce multi-view geometry constraints through optimization embedded in the learning framework. This paper presents a novel optimization method based on recurrent neural networks to further exploit the potential of neural networks in V2D. Specifically, our neural optimizer alternately updates the depth and camera poses through iterations to minimize a feature-metric cost, and two gated recurrent units iteratively improve the results by tracing historical information. In this way, our network is a gradient-free zeroth-order optimizer designed for V2D and can be applied to both supervised and self-supervised V2D. Extensive experimental results demonstrate that our method outperforms previous methods and is more efficient in computation and memory consumption than cost-volume-based methods. In particular, our self-supervised method outperforms previous supervised methods on the KITTI and ScanNet datasets. Our source code will be made public."
Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow,"Zhenyu Jiang, Hanwen Jiang, Yuke Zhu","The Unversity of Texas at Austin,UT Austin,The University of Texas at Austin",Visual Learning I,"Dense visual correspondence plays a vital role in robotic perception. This work focuses on establishing the dense correspondence between a pair of images that captures dynamic scenes undergoing substantial transformations. We introduce Doduo to learn general dense visual correspondence from in-the-wild images and videos without ground truth supervision. Given a pair of images, it estimates the dense flow field encoding the displacement of each pixel in one image to its corresponding pixel in the other image. Doduo uses flow-based warping to acquire supervisory signals for the training. Incorporating semantic priors with self-supervised flow training, Doduo produces accurate dense correspondence robust to the dynamic changes of the scenes. Trained on an in-the-wild video dataset, Doduo illustrates superior performance on point-level correspondence estimation over existing self-supervised correspondence learning baselines. We also apply Doduo to articulation estimation and zero-shot goal-conditioned manipulation, underlining its practical applications in robotics."
RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models,"Zijun Long, George Killick, Richard Mccreadie, Gerardo Aragon-Camarasa","University of Glasgow,School of Computing Science, University of Glasgow",Visual Learning I,"Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challengeâ€”a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. All the code used in this paper can be found in https://github.com/longkukuhi/RoboLLM."
CrossVideo: Self-Supervised Cross-Modal Contrastive Learning for Point Cloud Video Understanding,"Yunze Liu, Changxi Chen, Zifan Wang, Li Yi",Tsinghua University,Visual Learning I,"This paper introduces a novel approach named CrossVideo, which aims to enhance self-supervised cross-modal contrastive learning in the field of point cloud video understanding. Traditional supervised learning methods encounter limitations due to data scarcity and challenges in label acquisition. To address these issues, we propose a self-supervised learning method that leverages the cross-modal relationship between point cloud videos and image videos to acquire meaningful feature representations. Intra-modal and cross-modal contrastive learning techniques are employed to facilitate effective comprehension of point cloud video. We also propose a multi-level contrastive approach for both modalities. Through extensive experiments, we demonstrate that our method significantly surpasses previous state-of-the-art approaches, and we conduct comprehensive ablation studies to validate the effectiveness of our proposed designs."
FSNet: Redesign Self-Supervised MonoDepth for Full-Scale Depth Prediction for Autonomous Driving,"Yuxuan Liu, Zhenhua Xu, Huaiyang Huang, Lujia Wang, Ming Liu","Hong Kong University of Science and Technology,the Hong Kong University of Science and Technology,The Hong Kong University of Technology (Guangzhou),Hong Kong University of Science and Technology (Guangzhou)",Visual Learning I,"Predicting accurate depth with monocular images is important for low-cost robotic applications and autonomous driving. This study proposes a comprehensive self-supervised framework for accurate scale-aware depth prediction on autonomous driving scenes utilizing inter-frame poses obtained from inertial measurements. In particular, we introduce a Full-Scale depth prediction network named FSNet. FSNet contains four important improvements over existing self-supervised models: (1) a multichannel output representation for stable training of depth prediction in driving scenarios, (2) an optical-flow-based mask designed for dynamic object removal, (3) a self-distillation training strategy to augment the training process, and (4) an optimization-based post-processing algorithm in test time, fusing the results from visual odometry. With this framework, robots and vehicles with only one well-calibrated camera can collect sequences of training image frames and camera poses, and infer accurate 3D depths of the environment without extra labeling work or 3D data. Extensive experiments on the KITTI dataset, KITTI-360 dataset and the nuScenes dataset demonstrate the potential of FSNet. More visualizations are presented in url{https://sites.google.com/view/fsnet/home}"
V2CE: Video to Continuous Events Simulator,"Zhongyang Zhang, Shuyang Cui, Kaidong Chai, Haowen Yu, Subhasis Dasgupta, Upal Mahbub, Tauhidur Rahman","University of California San Diego,University of California, San Diego,University of Massachusetts Amherst,Qualcomm",Visual Learning I,"Dynamic Vision Sensor (DVS)-based solutions have recently garnered significant interest across various computer vision tasks, offering notable benefits in terms of dynamic range, temporal resolution, and inference speed. However, as a relatively nascent vision sensor compared to Active Pixel Sensor (APS) devices such as RGB cameras, DVS suffers from a dearth of ample labeled datasets. Prior efforts to convert APS data into events often grapple with issues such as a considerable domain shift from real events, the absence of quantified validation, and layering problems within the time axis. In this paper, we present a novel method for video-to-events stream conversion from multiple perspectives, considering the specific characteristics of DVS. A series of carefully designed losses helps enhance the quality of generated event voxels significantly. We also propose a novel local dynamic-aware timestamp inference strategy to accurately recover event timestamps from event voxels in a continuous fashion and eliminate the temporal layering problem. Results from rigorous validation through quantified metrics at all stages of the pipeline establish our method unquestionably as the current state-of-the-art (SOTA). The code can be found at bit.ly/v2ce."
Physically Grounded Vision-Language Models for Robotic Manipulation,"Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh","Stanford University,Google Inc,Google,Google Brain,Princeton University",Computer Vision for Automation,"Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PhysObjects, an object-centric dataset of 39.6K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, including generalization to held-out concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically grounded VLM in an interactive framework with a large language model-based robotic planner, and show improved planning performance on tasks that require reasoning about physical object concepts, compared to baselines that do not leverage physically grounded VLMs. We additionally illustrate the benefits of our physically grounded VLM on a real robot, where it improves task success rates. We release our dataset and provide further details and visualizations of our results at https://iliad.stanford.edu/pg-vlm/."
How Many Views Are Needed to Reconstruct an Unknown Object Using NeRF?,"Sicong Pan, Liren Jin, Hao Hu, Marija Popovic, Maren Bennewitz","University of Bonn,Fudan University",Computer Vision for Automation,"Neural Radiance Fields (NeRFs) are gaining significant interest for online active object reconstruction due to their exceptional memory efficiency and requirement for only posed RGB inputs. Previous NeRF-based view planning methods exhibit computational inefficiency since they rely on an iterative paradigm, consisting of (1) retraining the NeRF when new images arrive; and (2) planning a path to the next best view only. To address these limitations, we propose a non-iterative pipeline based on the Prediction of the Required number of Views (PRV). The key idea behind our approach is that the required number of views to reconstruct an object depends on its complexity. Therefore, we design a deep neural network, named PRVNet, to predict the required number of views, allowing us to tailor the data acquisition based on the object complexity and plan a globally shortest path. To train our PRVNet, we generate supervision labels using the ShapeNet dataset. Simulated experiments show that our PRV-based view planning method outperforms baselines, achieving good reconstruction quality while significantly reducing movement cost and planning time. We further justify the generalization ability of our approach in a real-world experiment."
Active Implicit Reconstruction Using One-Shot View Planning,"Hao Hu, Sicong Pan, Liren Jin, Marija Popovic, Maren Bennewitz","Fudan University,University of Bonn",Computer Vision for Automation,"Active object reconstruction using autonomous robots is gaining great interest. A primary goal in this task is to maximize the information of the object to be reconstructed, given limited on-board resources. Previous view planning methods exhibit inefficiency since they rely on an iterative paradigm based on explicit representations, consisting of (1) planning a path to the next-best view only; and (2) requiring a considerable number of less-gain views in terms of surface coverage. To address these limitations, we propose to integrate implicit representations into the One-Shot View Planning (OSVP). The key idea behind our approach is to use implicit representations to obtain the small missing surface areas instead of observing them with extra views. Therefore, we design a deep neural network, named OSVP, to directly predict a set of views given a dense point cloud refined from an initial sparse observation. To train our OSVP network, we generate supervision labels using dense point clouds refined by implicit representations and set covering optimization problems. Simulated experiments show that our method achieves sufficient reconstruction quality, outperforming several baselines under limited view and movement budgets. We further demonstrate the applicability of our approach in a real-world object reconstruction scenario."
Multimodal Object Query Initialization for 3D Object Detection,"Mathijs Ruben Van Geerenstein, Felicia Ruppel, Klaus Dietmayer, Dariu Gavrila","Delft University of Technology,Bosch Research and Ulm University,University of Ulm",Computer Vision for Automation,"3D object detection models that exploit both LiDAR and camera sensor features are top performers in large-scale autonomous driving benchmarks. A transformer is a popular network architecture used for this task, in which so-called object queries act as candidate objects. Initializing these object queries based on current sensor inputs is a common practice. For this, existing methods strongly rely on LiDAR data however, and do not fully exploit image features. Besides, they introduce significant latency. To overcome these limitations we propose EfficientQ3M, an efficient, modular, and multimodal solution for object query initialization for transformer-based 3D object detection models. The proposed initialization method is combined with a ""modality-balanced"" transformer decoder where the queries can access all sensor modalities throughout the decoder. In experiments, we outperform the state of the art in transformer-based LiDAR object detection on the competitive nuScenes benchmark and showcase the benefits of input-dependent multimodal query initialization, while being more efficient than the available alternatives for LiDAR-camera intitialization. The proposed method can be applied with any combination of sensor modalities as input, demonstrating its modularity."
SM^3: Self-Supervised Multi-Task Modeling with Multi-View 2D Images for Articulated Objects,"Haowen Wang, Zhen Zhao, Zhao Jin, Zhengping Che, Liang Qiao, Huang Yakun, Zhipeng Fan, Qiao Xiuquan, Jian Tang","Beijing University of Posts and Telecommunications,Midea Group,Midea Group (Shanghai) Co., Ltd.",Computer Vision for Automation,"Reconstructing real-world objects and estimating their movable joint structures are pivotal technologies within the field of robotics. Previous research has predominantly focused on supervised approaches, relying on extensively annotated datasets to model articulated objects within limited categories. However, this approach falls short of effectively addressing the diversity present in the real world. To tackle this issue, we propose a self-supervised interaction perception method, referred to as SM3, which leverages multi-view RGB images captured before and after interaction to model articulated objects, identify movable parts, and infer the parameters of their rotating joints. By constructing 3D geometries and textures from the captured 2D images, SM3 achieves integrated optimization of movable parts and joint parameters during the reconstruction process, obviating the need for annotations. Furthermore, we introduce the MMArt dataset, an extension of PartNet-Mobility, encompassing multi-view and multi-modal data of articulated objects spanning diverse categories. Evaluations demonstrate that UM3 surpasses existing benchmarks across various categories and objects, while its adaptability in real-world scenarios has been duly validated."
MF-MOS: A Motion-Focused Model for Moving Object Segmentation,"Jintao Cheng, Kang Zeng, Zhuoxu Huang, Xiaoyu Tang, Wu Jin, Chengxi Zhang, Xieyuanli Chen, Rui Fan","South China Normal University,Aberystwyth University,UESTC,Jiangnan Universtiy,National University of Defense Technology,Tongji University",Computer Vision for Automation,"Moving object segmentation (MOS) provides a reliable solution for detecting traffic participants and thus is of great interest in the autonomous driving field. Dynamic capture is always critical in the MOS problem. While previous methods capture motion features from the range images directly, we argue that the residual maps provide greater potential for motion information, and on the other hand, range images contain rich semantic guidance. Based on this intuition, we propose MF-MOS, a novel motion-focused model with a dual-branch structure for Lidar moving object segmentation. Novelly, we decouple the spatial-temporal information by capturing the motion from residual maps and generating semantic features from range images, which are used as movable object guidance for the motion branch. Our straightforward yet distinctive solution can make the most use of both range images and residual maps, thus greatly improving the performance of the Lidar-based MOS task. Remarkably, our MF-MOS achieved a leading IoU of 76.7% on the MOS leaderboard of the SemanticKITTI dataset upon submission, demonstrating the current state-of-the-art performance. The implementation of our MF-MOS has been released at https://github.com/SCNU-RISLAB/MF-MOS."
Improving Neural Indoor Surface Reconstruction with Mask-Guided Adaptive Consistency Constraints,"Xinyi Yu, Liqin Lu, Rong Jintao, Guangkai Xu, Linlin Ou","Zhejiang University of Technology,Zhejiang University",Computer Vision for Automation,"3D scene reconstruction from 2D images has been a long-standing task. Instead of estimating per-frame depth maps and fusing them in 3D, recent research leverages the neural implicit surface as a unified representation for 3D reconstruction. Equipped with data-driven pre-trained geometric cues, these methods have demonstrated promising performance. However, inaccurate prior estimation, which is usually inevitable, can lead to suboptimal reconstruction quality, particularly in some geometrically complex regions. In this paper, we propose a two-stage training process, decouple view-dependent and view-independent colors, and leverage two novel consistency constraints to enhance detail reconstruction performance without requiring extra priors. Additionally, we introduce an essential mask scheme to adaptively influence the selection of supervision constraints, thereby improving performance in a self-supervised paradigm. Experiments on synthetic and real-world datasets show the capability of reducing the interference from prior estimation errors and achieving high-quality scene reconstruction with rich geometric details."
NFL: Normal Field Learning for 6-DoF Grasping of Transparent Objects,"Junho Lee, Sang Min Kim, Yonghyeon Lee, Young Min Kim","Seoul National University,Korea Institute for Advanced Study",Computer Vision for Automation,"We present Normal Field Learning (NFL), a robust yet practical solution to perceive 3D layouts of transparent objects and grasp them quickly. Conventional input modalities for vision-based grasping do not provide sufficient information for transparent objects. However, with the recent advance on datasets and algorithms for transparent objects, we can at least obtain noisy estimates of normals and masks for various real-world conditions. Instead of directly using the RGB images, we propose to use the estimates to train a neural volume, which serves as an intermediate representation ignorant of challenging appearance variations. We formulate the training objective to account for in- herent uncertainty in individual estimation, and together with the volumetric aggregation, we can reliably extract useful geometric information for grasping. Our neural volume deploys a voxel- grid based representation, motivated by acceleration techniques of neural radiance fields. However, we directly store the normal and density values in the grid cells instead of latent features. Our modification allows direct access to the geometric values without additional inference or volume rendering, further enhancing the efficiency. Our results show over 85% success rates in grasping in cluttered scenes with only 40 seconds of training time."
TRTM: Template-Based Reconstruction and Target-Oriented Manipulation of Crumpled Cloths,"Wenbo Wang, Gen Li, Miguel Zamora, Stelian Coros",ETH Zurich,Computer Vision for Automation,"Precise reconstruction and manipulation of the crumpled cloths is challenging due to the high dimensionality of cloth models, as well as the limited observation at self-occluded regions. We leverage the recent progress in the field of single-view human reconstruction to template-based reconstruct crumpled cloths from their top-view depth observations only, with our proposed sim-real registration protocols. In contrast to previous implicit cloth representations, our reconstruction mesh explicitly describes the positions and visibilities of the entire cloth mesh vertices, enabling more efficient dual-arm and single-arm target-oriented manipulations. Experiments demonstrate that our TRTM system can be applied to daily cloths that have similar topologies as our template mesh, but with different shapes, sizes, patterns, and physical properties. Videos, datasets, pre-trained models, and code can be downloaded from our project website: https://wenbwa.github.io/TRTM/."
Active Learning of Discrete-Time Dynamics for Uncertainty-Aware Model Predictive Control,"Alessandro Saviolo, Jonathan Frey, Abhishek Rathod, Moritz Diehl, Giuseppe Loianno","New York University,University of Freiburg,University of Idaho,Univ. of Heidelberg",Model Learning for Control,"Model-based control requires an accurate model of the system dynamics for precisely and safely controlling the robot in complex and dynamic environments. Moreover, in presence of variations in the operating conditions, the model should be continuously refined to compensate for dynamics changes. In this paper, we present a self-supervised learning approach that actively models the dynamics of nonlinear robotic systems. We combine offline learning from past experience and online learning from current robot interaction with the unknown environment. These two ingredients enable a highly sample-efficient and adaptive learning process, capable of accurately inferring model dynamics in real-time even in operating regimes that greatly differ from the training distribution. Moreover, we design an uncertainty-aware model predictive controller that is heuristically conditioned to the aleatoric (data) uncertainty of the learned dynamics. This controller actively chooses the optimal control actions that (i) optimize the control performance and (ii) improve the efficiency of online learning sample collection. We demonstrate the effectiveness of our method through a series of cha"
SculptBot: Pre-Trained Models for 3D Deformable Object Manipulation,"Alison Bartsch, Charlotte Avra, Amir Barati Farimani",Carnegie Mellon University,Model Learning for Control,"Deformable object manipulation presents a unique set of challenges in robotic manipulation by exhibiting high degrees of freedom and severe self-occlusion. State representation for materials that exhibit plastic behavior, like modeling clay or bread dough, is also difficult because they permanently deform under stress and are constantly changing shape. In this work, we investigate each of these challenges using the task of robotic sculpting with a parallel gripper. We propose a system that uses point clouds as the state representation and leverages pre-trained point cloud reconstruction Transformer to learn a latent dynamics model to predict material deformations given a grasp action. We design a novel action sampling algorithm that reasons about geometrical differences between point clouds to further improve the efficiency of model-based planners. All data and experiments are conducted entirely in the real world. Our experiments show the proposed system is able to successfully capture the dynamics of clay, and is able to create a variety of simple shapes. Videos and additional figures are available on our project page at: https://sites.google.com/andrew.cmu.edu/sculptbot"
Learning-Based Model Predictive Control for an Autonomous Formula Student Racing Car,"David Gomes, Miguel Botto, Pedro U. Lima","Instituto Superior Técnico, University of Lisbon,DEM/IST,Instituto Superior Técnico - Institute for Systems and Robotics",Model Learning for Control,"Advancements in Automated Driving Systems (ADSs) have enabled the achievement of a certain level of autonomy while commuting in a car. However, emergency and high-speed maneuvers still arise as significant challenges for ADSs due to the intrinsic nonlinearity and fast-paced behavior of such events. These maneuvers are a distinctive feature within the recently established motorsport discipline of Autonomous Racing (AR). In this work, we explore the use of Learning-based Model Predictive Control (LMPC) to address possible model mismatches of the first principles model in high-speed racing. To this end, a Model Predictive Contouring Control (MPCC) (a specific formulation of the standard Model Predictive Control, MPC) is formulated, and a Neural Network (NN) that leverages the use of Feedforward and Recurrent layers is employed to learn the errors of the first principles model. By combining the NN with the first principles model, the LMPC is born, capable of accurately predicting the future with a computational effort compatible with real-time feasibility, effectively handling the vehicle at its limits. Furthermore, the controller can adapt to changing environments by training the NN during the race. The MPCC (formulation without the NN) is deployed on a real autonomous formula student racing car showing an improvement of 16% in mean lap times across the same track between a common geometric controller. The LMPC is analyzed in a high-fidelity simulator, achieving an improvement of 8.9% in mean lap times when compared to the MPCC."
Learning Terrain-Aware Kinodynamic Model for Autonomous Off-Road Rally Driving with Model Predictive Path Integral Control,"Hojin Lee, Taekyung Kim, Jungwi Mun, Wonsuk Lee",Agency for Defense Development,Model Learning for Control,"High-speed autonomous driving in off-road environments has immense potential for various applications, but it also presents challenges due to the complexity of vehicle-terrain interactions. In such environments, it is crucial for the vehicle to predict its motion and adjust its controls proactively in response to environmental changes, such as variations in terrain elevation. To this end, we propose a method for learning terrain-aware kinodynamic model which is conditioned on both proprioceptive and exteroceptive information. The proposed model generates reliable predictions of 6-degree-of-freedom motion and can even estimate contact interactions without requiring ground truth force data during training. This enables the design of a safe and robust model predictive controller through appropriate cost function design which penalizes sampled trajectories with unstable motion, unsafe interactions, and high levels of uncertainty derived from the model. We demonstrate the effectiveness of our approach through experiments on a simulated off-road track, showing that our proposed model-controller pair outperforms the baseline and ensures robust high-speed driving performance without control failure."
Adaptive Gait Modeling and Optimization for Principally Kinematic Systems,"Siming Deng, Noah J. Cowan, Brian Bittner","Johns Hopkins University,JHUAPL",Model Learning for Control,"Robotic adaptation to unanticipated operating conditions is crucial to achieving persistence and robustness in complex real world settings. For a wide range of cutting-edge robotic systems, such as micro- and nano-scale robots, soft robots, medical robots, and bio-hybrid robots, it is infeasible to anticipate the operating environment a priori due to complexities that arise from numerous factors including imprecision in manufacturing, chemo-mechanical forces, and poorly understood contact mechanics. Drawing inspiration from data-driven modeling, geometric mechanics (or gauge theory), and adaptive control, we employ an adaptive system identification framework and demonstrate its efficacy in enhancing the performance of principally kinematic locomotors (those governed by Rayleigh dissipation or zero momentum conservation). We showcase the capability of the adaptive model to efficiently accommodate varying terrains and iteratively modified behaviors within a behavior optimization framework. This provides both the ability to improve fundamental behaviors and perform motion tracking to precision.Notably, we are capable of optimizing the gaits of the Purcell swimmer using approximately 10 cycles per link, which for the nine-link Purcell swimmer provides a factor of ten improvement in optimization speed over the state of the art. Beyond simply a computational speed up, this ten-fold improvement may enable this method to be successfully deployed for in-situ behavior refinement, injury recovery, and terrain adaptation, particularly in domains where simulations provide poor guides for the real world."
Recursive Least Squares with Log-Determinant Divergence Regularisation for Online Inertia Identification,"Namhoon Cho, Taeyoon Lee, Hyo-Sang Shin","Cranfield University,Naver labs",Model Learning for Control,"This study presents a recursive algorithm for solving the regularised least squares problem for online identification of rigid body dynamic model parameters with emphasis on the physical consistency of estimated inertial parameters. One of the geometric approaches is to use a regulariser that represents how close the pseudo-inertia matrix is to a given reference on the feasible manifold in the regression problem. The proposed extension enables memory-efficient online learning in addition to the benefits of geometry-aware convex regularisation using the log-determinant divergence of the pseudo-inertia matrix. Also, the resursive version endows the estimator with the capability to deal with time-variation of parameters by introducing an optional forgetting mechanism. The characteristics of the recursive regularised least squares algorithm is demonstrated using the MIT Cheetah 3 leg swinging experiment dataset and compared to the existing batch optimisation method."
Sequential Manipulation of Deformable Linear Object Networks with Endpoint Pose Measurements Using Adaptive Model Predictive Control,"Tyler Toner, Vahidreza Molazadeh, Miguel Saez, Dawn Tilbury, Kira Barton","University of Michigan,Carnegie Mellon University,General Motors,University of Michigan at Ann Arbor",Model Learning for Control,"Robotic manipulation of deformable linear objects (DLOs) is an active area of research, though emerging applications, like automotive wire harness installation, introduce constraints that have not been considered in prior work. Confined workspaces and limited visibility complicate prior assumptions of multi-robot manipulation and direct measurement of DLO configuration (state). This work focuses on single-arm manipulation of stiff DLOs (StDLOs) connected to form a DLO network (DLON), for which the measurements (output) are the endpoint poses of the DLON, which are subject to unknown dynamics during manipulation. To demonstrate feasibility of output-based control without state estimation, direct input-output dynamics are shown to exist by training neural network models on simulated trajectories. Output dynamics are then approximated with polynomials and found to contain well-known rigid body dynamics terms. A composite model consisting of a rigid body model and an online data-driven residual is developed, which predicts output dynamics more accurately than either model alone, and without prior experience with the system. An adaptive model predictive controller is developed with the composite model for DLON manipulation, which completes DLON installation tasks, both in simulation and with a physical automotive wire harness."
Physics-Informed Neural Network for Multirotor Slung Load Systems Modeling,"Gil Serrano, Marcelo Jacinto, Jose Ribeiro-gomes, Joao Pinto, Bruno J. N. Guerreiro, Alexandre Bernardino, Rita Cunha","Instituto Superior Técnico, University of Lisbon,Instituto Superior Técnico,Instituto Superior Tecnico, University of Lisbon,Instituto Superior Tecnico, Universidade de Lisboa,NOVA School of Science and Technology,IST - Técnico Lisboa,Instituto Superior Tecnico",Model Learning for Control,"Recent advances in aerial robotics have enabled the use of multirotor vehicles for autonomous payload transportation. Resorting only to classical methods to reliably model a quadrotor carrying a cable-slung load poses significant challenges. On the other hand, purely data-driven learning methods do not comply by design with the problem's physical constraints, especially in states that are not densely represented in training data. In this work, we explore the use of physics-informed neural networks to learn an end-to-end model of the multirotor-slung-load system and, at a given time, estimate a sequence of the future system states. An LSTM encoder-decoder with an attention mechanism is used to capture the dynamics of the system. To guarantee the cohesiveness between the multiple predicted states of the system, we propose the use of a physics-based term in the loss function, which includes a discretized physical model derived from first principles together with slack variables that allow for a small mismatch between expected and predicted values. To train the model, a dataset using a real-world quadrotor carrying a slung load was curated and is made available. Prediction results are presented and corroborate the feasibility of the approach. The proposed method outperforms both the first principles physical model and a comparable neural network model trained without the physics regularization proposed."
A Probabilistic Motion Model for Skid-Steer Wheeled Mobile Robot Navigation on Off-Road Terrains,"Ananya Trivedi, Mark Zolotas, Adeeb Abbas, Sarvesh Prajapati, Salah Bazzi, Taskin Padir","Northeastern University,Northeastern Univeristy",Model Learning for Control,"Skid-Steer Wheeled Mobile Robots (SSWMRs) are increasingly being used for off-road autonomy applications. When turning at high speeds, these robots tend to undergo significant skidding and slipping. In this work, using Gaussian Process Regression (GPR) and Sigma-Point Transforms, we estimate the non-linear effects of tire-terrain interaction on robot velocities in a probabilistic fashion. Using the mean estimates from GPR, we propose a data-driven dynamic motion model that is more accurate at predicting future robot poses than conventional kinematic motion models. By efficiently solving a convex optimization problem based on the history of past robot motion, the GPR augmented motion model generalizes to previously unseen terrain conditions. The output distribution from the proposed motion model can be used for local motion planning approaches, such as stochastic model predictive control, leveraging model uncertainty to make safe decisions. We validate our work on a benchmark real-world multi-terrain SSWMR dataset. Our results show that the model generalizes to three different terrains while significantly reducing errors in linear and angular motion predictions. As shown in the attached video, we perform a separate set of experiments on a physical robot to demonstrate the robustness of the proposed algorithm."
TartanDrive 2.0: More Modalities and Better Infrastructure to Further Self-Supervised Learning Research in Off-Road Driving Tasks,"Matthew Sivaprakasam, Parv Maheshwari, Mateo Guaman Castro, Samuel Triest, Micah Nye, Steven Willits, Andrew Saba, Wenshan Wang, Sebastian Scherer","Carnegie Mellon University,Indian Institute of Technology Kharagpur,University of Pittsburgh",Learning in Field Robotics,"We present TartanDrive 2.0, a large-scale off-road driving dataset for self-supervised learning tasks. In 2021 we released TartanDrive 1.0, which is one of the largest datasets for off-road terrain. As a follow up to our original dataset, we collected seven hours of data at speeds of up to 15m/s with the addition of three new LiDAR sensors alongside the original camera, inertial, GPS, and proprioceptive sensors. We also release the tools we use for collecting, processing, and querying the data, including our metadata system designed to further the utility of our data. Custom infrastructure allows end users to reconfigure the data to cater to their own platforms. These tools and infrastructure alongside the dataset are useful for a variety of tasks in the field of off-road autonomy and, by releasing them, we encourage collaborative data aggregation. These resources lower the barrier to entry to utilizing large-scale datasets, thereyby helping facilitate the advancement of robotics in areas such as self-supervised learning, multi-modal perception, inverse reinforcement learning, and representation learning. The dataset is available at https://theAirLab.org/TartanDrive2."
EnYOLO: A Real-Time Framework for Domain-Adaptive Underwater Object Detection with Image Enhancement,"Junjie Wen, Jinqiang Cui, Benyun Zhao, Bingxin Han, Xuchen Liu, Zhi Gao, Ben M. Chen","The Chinese University of Hong Kong,Peng Cheng Laboratory,Temasek Laboratories @ NUS,Chinese University of Hong Kong",Learning in Field Robotics,"In recent years, significant progress has been made in the field of underwater image enhancement (UIE). However, its practical utility for high-level vision tasks, such as underwater object detection (UOD) in Autonomous Underwater Vehicles (AUVs), remains relatively unexplored. It may be attributed to several factors: (1) Existing methods typically employ UIE as a pre-processing step, which inevitably introduces considerable computational overhead and latency. (2) The process of enhancing images prior to training object detectors may not necessarily yield performance improvements. (3) The complex underwater environments can induce significant domain shifts across different scenarios, seriously deteriorating the UOD performance. To address these challenges, we introduce EnYOLO, an integrated real-time framework designed for simultaneous UIE and UOD with domain-adaptation capabilities. Specifically, both the UIE and UOD task heads share the same network backbone and utilize a lightweight design. Furthermore, to ensure balanced training for both tasks, we present a multi-stage training strategy aimed at consistently enhancing the performance of both functions. Additionally, we propose a novel domain-adaptation strategy to align feature embeddings originating from diverse underwater environments. Comprehensive experiments demonstrate that our framework not only achieves state-of-the-art (SOTA) performance in both UIE and UOD tasks, but also shows superior adaptability when applied to different underwater scenarios. Our efficiency analysis further highlights the substantial potential of our framework for onboard deployment."
GS-PKNN: An Efficient and High-Fidelity Mobility Prediction Method for Unmanned Ground Vehicles,"Chen Hua, Chunmao Jiang, Runxin Niu, Biao Yu, Hui Zhu, Bichun Li","University of Science and Technology of China,Hefei Institutes of Physical Science, Chinese Academy of Science",Learning in Field Robotics,"To avoid unmanned ground vehicles being obstructed by deformed terrain in off-road, effective vehicle mobility analysis is required. However, the computational complexity of existing mobility analysis methods, such as discrete element analysis, poses significant challenges when applied to largescale terrains. To address this problem, we propose an efficient and high-fidelity vehicle mobiliy prediction method for a largescale terrain. Initially, precise terrain models are constructed employing Gaussian sampling (GS), thereby serving as optimal inputs for the mobility simulation. Subsequently, we introduce a co-simulation method based on a multi-body dynamics model and discrete element analysis to obtain high-fidelity vehicle mobility data on sampled terrains. Following that, the mobility data is utilized to train a PSO-kriging neural network (PKNN), enabling accurate predictions of the global mobility map. Through rigorous simulation experiments, the proposed method (GS-PKNN) demonstrates its remarkable effectiveness."
UNRealNet: Learning Uncertainty-Aware Navigation Features from High-Fidelity Scans of Real Environments,"Samuel Triest, David D. Fan, Sebastian Scherer, Ali-Akbar Agha-Mohammadi","Carnegie Mellon University,Jet Propulsion Laboratory, California Institute of Technology, P,NASA-JPL, Caltech",Learning in Field Robotics,"Traversability estimation in rugged, unstructured environments remains a challenging problem in field robotics. Often, the need for precise, accurate traversability estimation is in direct opposition to the limited sensing and compute capability present on affordable, small-scale mobile robots. To address this issue, we present a novel method to learn [u]ncertainty-aware [n]avigation features from high-fidelity scans of [real]-world environments (UNRealNet). This network can be deployed on-robot to predict these high-fidelity features using input from lower-quality sensors. UNRealNet predicts dense, metric-space features directly from single-frame lidar scans, thus reducing the effects of occlusion and odometry error. Our approach is label-free, and is able to produce traversability estimates that are robot-agnostic. Additionally, we can leverage UNRealNetâ€™s predictive uncertainty to both produce risk-aware traversability estimates, and refine our feature predictions over time. We find that our method outperforms traditional local mapping and inpainting baselines by up to 40%, and demonstrate its efficacy on multiple legged platforms."
Geo-Localization with Transformer-Based 2D-3D Match Network,"Laijian Li, Yukai Ma, Kai Tang, Xiangrui Zhao, Chao Chen, Jianxin Huang, Jianbiao Mei, Yong Liu","Zhejiang University,zhejiang unicersity",Learning in Field Robotics,"This letter presents a novel method for geographical localization by registering satellite maps with LiDAR point clouds. This method includes a Transformer-based 2D-3D matching network called D-GLSNet that directly matches the LiDAR point clouds and satellite images through end-to-end learning. Without the need for feature point detection, D-GLSNet provides accurate pixel-to-point association between the LiDAR point clouds and satellite images. And then, we can easily calculate the horizontal offset and angular deviation between them, thereby achieving accurate registration. To demonstrate the potential of our network in localization, we have designed a Geo-localization Node (GLN) that implements geographical localization and can seamlessly integrate with SLAM systems. Compared to GPS, GLN is less susceptible to external interference, such as building occlusion. In urban scenarios, our proposed D-GLSNet can output high-quality matching, enabling GLN to function stably and deliver more accurate localization results. We conduct extensive experiments on the KITTI dataset to verify the robustness and feasibility of our method, and the experimental results show that our method outperforms the state-of-the-art LiDAR-based geo-localization methods."
Robot-Dependent Traversability Estimation for Outdoor Environments Using Deep Multimodal Variational Autoencoders,"Matthias Eder, Gerald Steinbauer-Wagner",Graz University of Technology,Learning in Field Robotics,"Efficient and reliable navigation in off-road environments poses a significant challenge for robotics, especially when factoring in the varying capabilities of robots across different terrains. To achieve this, the robot system's traversability is usually estimated to plan traversable routes through an environment. This paper presents a new approach that utilizes Deep Multimodal Variational Autoencoders (DMVAEs) for estimating the traversability of different robots in complex off-road terrains. Our method utilizes DMVAEs to capture essential environmental information and robot properties, effectively modeling factors that influence robotic traversability. The key contribution of this research is a two-stage traversability estimation framework for various robots in diverse off-road conditions that integrates robot properties in addition to environmental information to predict the traversability for various robots in a single model. We validate our method through real-world experiments involving four ground robots navigating an alpine environment. Comparative evaluations against state-of-the-art traversability estimation methods demonstrate the superior accuracy and robustness of our approach. Additionally, we investigate the transfer of trained models to new robots, enhancing their traversability estimation and extending the applicability of our framework."
F3DMP: Foresighted 3D Motion Planning of Mobile Robots in Wild Environments,"Andong Yang, Wei Li, Yu Hu","Institute of Computing Technology, Chinese Academy of Sciences,Institute of Computing Technology Chinese Academy of Sciences",Learning in Field Robotics,"In wild environments, motion planning for mobile robots faces the challenge of local optimal path traps due to limited sensor perception range and lack of spatial awareness. Existing approaches that avoid local optimum by designing heuristic functions or high-quality global paths in wild environments are time-consuming and unstable. This work proposes F3DMP, which consists of two parts to alleviate the local optimum solution and better utilize distant terrain information. First, the entire planning framework is adapted to the three-dimensional space so that the planning result conforms to the geometric characteristics of the terrain. Second, a time allocation function based on offline reinforcement learning is proposed. This function can anticipate potential challenges or opportunities based on semantic information for the image and proactively determine a time allocation. Our planner is integrated into a complete mobile robot system and deployed to a real robot. Experiments in simulation and the real world demonstrate that our method can improve the success rate by 28% and the trajectory smoothness by 27% compared with traditional methods."
MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts,"Zhuo Xu, Rui Zhou, Yida Yin, Huidong Gao, Masayoshi Tomizuka, Jiachen Li","UC Berkeley,University of California, Berkeley,University of California,University of California, Riverside",Learning in Field Robotics,"Data-driven methods have great advantages in modeling complicated human behavioral dynamics and dealing with many human-robot interaction applications. However, collecting massive and annotated real-world human datasets has been a laborious task, especially for highly interactive scenarios. On the other hand, algorithmic data generation methods are usually limited by their model capacities, making them unable to offer realistic and diverse data needed by various application users. In this work, we study trajectory-level data generation for multi-human or human-robot interaction scenarios and propose a learning-based automatic trajectory generation model, which we call Multi-Agent TRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of generating interactive human behaviors in realistic diverse contexts. We achieve this goal by modeling the explicit and interpretable objectives so that MATRIX can generate human motions based on diverse destinations and heterogeneous behaviors. We carried out extensive comparison and ablation studies to illustrate the effectiveness of our approach across various metrics. We also presented experiments that demonstrate the capability of MATRIX to serve as data augmentation for imitation-based motion planning."
Distributed Multi-Robot Online Sampling with Budget Constraints,"Azin Shamshirgaran, Sandeep Manjanna, Stefano Carpin","University of California, Merced,Plaksha University",Task Planning,"In multi-robot informative path planning the problem is to find a route for each robot in a team to visit a set of locations that can provide the most useful data to reconstruct an unknown scalar field. In the budgeted version, each robot is subject to a travel budget limiting the distance it can travel. Our interest in this problem is motivated by applications in precision agriculture, where robots are used to collect measurements to estimate domain-relevant scalar parameters such as soil moisture or nitrates concentrations. In this paper, we propose an online, distributed multi-robot sampling algorithm based on Monte Carlo Tree Search (MCTS) where each robot iteratively selects the next sampling location through communication with other robots and considering its remaining budget. We evaluate our proposed method for varying team sizes and in different environments, and we compare our solution with four different baseline methods. Our experiments show that our solution outperforms the baselines when the budget is tight by collecting measurements leading to smaller reconstruction errors."
Coupled Active Perception and Manipulation Planning for a Mobile Manipulator in Precision Agriculture Applications,"Shuangyu Xie, Chengsong Hu, Di Wang, Joe Johnson, Muthukumar Bagavathiannan, Dezhen Song","Texas A&M University,Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)",Task Planning,"A mobile manipulator often finds itself in an application where it needs to take a close-up view before performing a manipulation task. Named this as a coupled active perception and manipulation (CAPM) problem, we model the uncertainty in the perception process and devise a key state/task planning approach that considers reachability conditions as task constraints of both perception and manipulation tasks for the mobile platform. By minimizing the expected energy usage in body key state planning while satisfying task constraints, our algorithm achieves the best balance between the task success rate and energy usage. We have implemented the algorithm and tested it in both simulation and physical experiments. The results have confirmed that our algorithm has a lower energy consumption compared to a two-stage decoupled approach, while still maintaining a success rate of 100% for the task."
RAMP: A Benchmark for Evaluating Robotic Assembly Manipulation and Planning,"Jack Collins, Mark Robson, Jun Yamada, Mohan Sridharan, Karol Janik, Ingmar Posner","University of Oxford,University of Birmingham,University of Edinburgh,Manufacturing Technology Centre,Oxford University",Task Planning,"We introduce RAMP, an open-source robotics benchmark inspired by real-world industrial assembly tasks. RAMP consists of beams that a robot must assemble into specified goal configurations using pegs as fasteners. As such it assesses planning and execution capabilities, and poses challenges in perception, reasoning, manipulation, diagnostics, fault recovery, and goal parsing. RAMP has been designed to be accessible and extensible. Parts are either 3D printed or otherwise constructed from materials that are readily obtainable. The part design and detailed instructions are publicly available. In order to broaden community engagement, RAMP incorporates fixtures such as April Tags which enable researchers to focus on individual sub-tasks of the assembly challenge if desired. We provide a full digital twin as well as rudimentary baselines to enable rapid progress. Our vision is for RAMP to form the substrate for a community-driven endeavour that evolves as capability matures."
Towards Safe Robot Use with Edged or Pointed Objects: A Surrogate Study Assembling a Human Hand Injury Protection Database,"Robin Kirschner, Carina M. Micheler, Yangcan Zhou, Sebastian Julian Siegner, Mazin Hamad, Claudio Glowalla, Jan Neumann, Nader Rajaei, Rainer Burgkart, Sami Haddadin","TU Munich, Institute for Robotics and Systems Intelligence,Technical University of Munich, TUM School of Medicine, Klinikum,Technical University of Munich,TU Munich,Technical University of Munich (TUM),Department of Orthopaedics and Sports Orthopaedics, Klinikum rec,Technische Universität München",Task Planning,"The use of pointed or edged tools or objects is one of the most challenging aspects of today's application of physical human-robot interaction (pHRI). One reason for this is that the severity of harm caused by such edged or pointed impactors is less well studied than for blunt impactors. Consequently, the standards specify well-reasoned force and pressure thresholds for blunt impactors and advise avoiding any edges and corners in contacts. Nevertheless, pointed or edged impactor geometries cannot be completely ruled out in real pHRI applications. For example, to allow edged or pointed tools such as screwdrivers near human operators, the knowledge of injury severity needs to be extended so that robot integrators can perform well-reasoned, time-efficient risk assessments. In this paper, we provide the initial datasets on injury prevention for the human hand based on drop tests with surrogates for the human hand, namely pig claws and chicken drumsticks. We then demonstrate the ease and efficiency of robot use using the dataset for contact on two examples. Finally, our experiments provide a set of injuries that may also be expected for human subjects under certain robot mass-velocity constellations in collisions. To extend this work, testing on human samples and a collaborative effort from research institutes worldwide is needed to create a comprehensive human injury avoidance database for any pHRI scenario and thus for safe pHRI applications including edged and pointed geometries."
Accelerating Long-Horizon Planning with Affordance-Directed Dynamic Grounding of Abstract Strategies,"Khen Elimelech, Zachary Kingston, Wil Thomason, Vardi Moshe, Lydia Kavraki",Rice University,Task Planning,"Long-horizon task planning is important for robot autonomy, especially as a subroutine for frameworks such as Integrated Task and Motion Planning. However, task planning is computationally challenging and struggles to scale to realistic problem settings. We propose to accelerate task planning over an agent's lifetime by integrating abstract strategies: a generalizable planning experience encoding introduced in earlier work. In this work, we contribute a practical approach to planning with strategies by introducing a novel formalism of planning in a strategy-augmented domain. We also introduce and formulate the notion of a strategy's affordance, which indicates its predicted benefit to the solution, and use it to guide the planning and strategy grounding processes. Together, our observations yield an affordance-directed, lazy-search planning algorithm, which can seamlessly compose strategies and actions to solve long-horizon planning problems. We evaluate our planner in an object rearrangement domain, where we demonstrate performance benefits relative to a state-of-the-art task planner."
Long-Horizon Planning and Execution with Functional Object-Oriented Networks,"David Paulius, Alejandro Agostini, Dongheui Lee","Brown University,University of Innsbruck,Technische Universität Wien (TU Wien)",Task Planning,"Following work on joint object-action representations, functional object-oriented networks (FOON) were introduced as a knowledge graph representation for robots. A FOON contains symbolic concepts useful to a robotâ€™s understanding of tasks and its environment for object-level planning. Prior to this work, little has been done to show how plans acquired from FOON can be executed by a robot, as the concepts in a FOON are too abstract for execution. We thereby introduce the idea of exploiting object-level knowledge as a FOON for task planning and execution. Our approach automatically transforms FOON into PDDL and leverages off-the-shelf planners, action contexts, and robot skills in a hierarchical planning pipeline to generate executable task plans. We demonstrate our entire approach on long-horizon tasks in CoppeliaSim and show how learned action contexts can be extended to never-before-seen scenarios."
From Cooking Recipes to Robot Task Trees â€“ Improving Planning Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network,"Md Sadman Sakib, Yu Sun",University of South Florida,Task Planning,"Task planning for robotic cooking involves generating a sequence of actions for a robot to prepare a meal successfully. This paper introduces a novel task tree generation pipeline producing correct planning and efficient execution for cooking tasks. Our method first uses a large language model (LLM) to retrieve recipe instructions and then utilizes a fine-tuned GPT-3 to convert them into a task tree, capturing sequential and parallel dependencies among subtasks. The pipeline then mitigates the uncertainty and unreliable features of LLM outputs using task tree retrieval. We combine multiple LLM task tree outputs into a graph and perform a task tree retrieval to avoid questionable nodes and high-cost nodes to improve planning correctness and improve execution efficiency. Our evaluation results show its superior performance compared to previous works in task planning accuracy and efficiency."
Stepwise Large-Scale Multi-Agent Task Planning Using Neighborhood Search,"Fan Zeng, Shouhei Shirafuji, Changxiang Fan, Masahiro Nishio, Jun Ota","the University of Tokyo,Kansai University,Institute of Facility Agriculture, Guangdong Academy of Agricult,TOYOTA MOTOR CORPORATION,The University of Tokyo",Task Planning,"This paper presents a novel stepwise multi-agent task planning method that incorporates neighborhood search to address large-scale problems, thereby reducing computation time. With an increasing number of agents, the search space for task planning expands exponentially. Hence, conventional methods aiming to find globally optimal solutions, especially for some large-scale problems, incur extremely high computational costs and may even fail. In this paper, the proposed method easily achieves the goals of multi-agent task planning by solving an initial problem using a minimal number of agents. Subsequently, tasks are reallocated among all agents based on this solution and the solutions are iteratively optimized using a neighborhood search. While aiming to find a near-optimal solution rather than an optimal one, the method substantially reduces the time complexity of searching to a polynomial level. Moreover, the effectiveness of the proposed method is demonstrated by solving some benchmark problems and comparing the results obtained using the proposed method with those obtained using other state-of-the-art methods."
Bayesian-Guided Evolutionary Strategy with RRT for Multi-Robot Exploration,"Shuge Wu, Chunzheng Wang, Jiayi Pan, Dongming Han, Zhongliang Zhao",Beihang University,Task Planning,"With the increasing demand for multi-robot exploration of unknown environments, how to accomplish this problem efficiently has become a focus of research. However, in this kind of task, the formulation of strategies for frontier point detection and task allocation largely determines the overall efficiency of the system. In the task of multi-robot exploration of unknown environments, the strategies of frontier point detection and task assignment determine the overall efficiency of the system. Most of the existing methods implement frontier point detection based on the Rapidly-Exploring Random Tree (RRT) and use greedy algorithms for task allocation. However, the classical RRT algorithm is a fixed growth step, which leads to the difficulty of growing branches in narrow environments, making the efficiency and correctness of detecting frontier points lower. Meanwhile, the allocation strategy of the greedy algorithm causes each robot to consider only the exploration area with the largest gain for itself, which easily leads to repeated exploration and reduces the overall efficiency of the system. To solve these problems, we propose an adaptive RRT tree growth strategy for frontier point detection, which can adjust the step size according to the known map information and thus improve the efficiency and accuracy of detection; and introduce a Bayesian-guided evolutionary strategy(BGE) for efficient task allocation, which can utilize the current and historical information to find the optimal allocation scheme in a global perspective. We conduct a comprehensive test of the proposed strategy in the ROS system as well as in the real world, which proves the efficiency of our strategy."
A Novel Model for Layer Jamming-Based Continuum Robots,"Bowen Yi, Yeman Fan, Dikai Liu","Polytechnique Montreal,University of Technology Sydney,University of Technology, Sydney","Modeling, Control, and Learning for Soft Robots I","Continuum robots with variable stiffness have gained wide popularity in the last decade. Layer jamming (LJ) has emerged as a simple and efficient technique to achieve tunable stiffness for continuum robots. Despite its merits, the development of a control-oriented dynamical model1 tailored for this specific class of robots remains an open problem in the literature. This paper aims to present the first solution, to the best of our knowledge, to close the gap. We propose an energy-based model that is integrated with the LuGre frictional model for LJ-based continuum robots. Then, we take a comprehensive theoretical analysis for this model, focusing on two fundamental characteristics of LJ-based continuum robots: shape locking and adjustable stiffness. To validate the modeling approach and theoretical results, a series of experiments using our OctRobot-I continuum robotic platform was conducted. The results show that the proposed model is capable of interpreting and predicting the dynamical behaviors in LJ-based continuum robots."
"Lumped Parameter Dynamic Model of an Eversion Growing Robot: Analysis, Simulation and Experimental Validation","Panagiotis Vartholomeos, Zicong Wu, S.m.hadi Sadati, Christos Bergeles","University of Thessaly,King's College London","Modeling, Control, and Learning for Soft Robots I","This paper presents a lumped-parameter dynamic model of a pressure driven eversion robot carrying a catheter through its hollow core. A simulation framework based on the model is developed in MATLAB and is used for understanding the underlying physics, for identifying the regions of operation, and for demonstrating that, for a range of input commands, the catheter can be used as an actuation mechanism for propelling eversion; an approach especially useful for miniaturised systems. Simulations are experimentally validated on the MAMMOBOT system, which is a miniature steerable soft growing robot for early breast cancer detection. It was demonstrated that for most regions of operation experimental results compare well with simulation exhibiting an error less than 4%. Only one region of operation demonstrated larger deviations due possibly to unmodeled dynamics, which will be investigated in future work."
Trajectory Tracking Control of Dual-PAM Soft Actuator with Hysteresis Compensator,"Junyi Shen, Tetsuro Miyazaki, Shingo Ohno, Maina Sogabe, Kenji Kawashima","The University of Tokyo,Bridgestone Corporation,the University of Tokyo","Modeling, Control, and Learning for Soft Robots I","Soft robotics is an emergent and swiftly evolving field. Pneumatic actuators are suitable for driving soft robots because of their superior performance. However, their control is not easy due to their hysteresis characteristics. In response to these challenges, we propose an adaptive control method to compensate hysteresis of a soft actuator. Employing a novel dual pneumatic artificial muscle (PAM) bending actuator, the innovative control strategy abates hysteresis effects by dynamically modulating gains within a traditional PID controller corresponding with the predicted motion of the reference trajectory. Through comparative experimental evaluation, we found that the new control method outperforms its conventional counterparts regarding tracking accuracy and response speed. Our work reveals a new direction for advancing control in soft actuators."
Kinematic Modeling and Control of a Soft Robotic Arm with Non-Constant Curvature Deformation,"Zhanchi Wang, Vector G.T.Wang, Xiaoping Chen, Nikolaos Freris",University of Science and Technology of China,"Modeling, Control, and Learning for Soft Robots I","The passive compliance of soft robotic arms renders the development of accurate kinematic models and model-based controllers challenging. The most widely used model in soft robotic kinematics assumes Piecewise Constant Curvature (PCC). However, PCC introduces errors when the robot is subject to external forces or even gravity. In this paper, we establish a three-dimensional (3D) kinematic representation of a soft robotic arm with pseudo universal and prismatic joints that are capable of capturing non-constant curvature deformations of the soft segments. We theoretically demonstrate that this constitutes a more general methodology than PCC. Simulations and experiments on the real robot attest to the superior modeling accuracy of our approach in 3D motions with unknown load. The maximum position/rotation error of the proposed model is verified 6.7$times$/4.6$times$ lower than the PCC model considering gravity and external forces. Furthermore, we devise an inverse kinematic controller that is capable of positioning the tip, tracking trajectories, as well as performing interactive tasks in the 3D space."
CIDGIKc: Distance-Geometric Inverse Kinematics for Continuum Robots,"Hanna Zhang, Matthew Giamou, Filip Maric, Jonathan Kelly, Jessica Burgner-kahrs","University of Toronto,University of Toronto Institute for Aerospace Studies","Modeling, Control, and Learning for Soft Robots I","The small size, high dexterity, and intrinsic compliance of continuum robots (CRs) make them well suited for constrained environments. Solving the inverse kinematics (IK), that is finding robot joint configurations that satisfy desired position or pose queries, is a fundamental challenge in motion planning, control, and calibration for any robot structure. For CRs, the need to avoid obstacles in tightly confined workspaces greatly complicates the search for feasible IK solutions. Without an accurate initialization or multiple re-starts, existing algorithms often fail to find a solution. We present CIDGIKc (Convex Iteration for Distance-Geometric Inverse Kinematics for Continuum Robots), an algorithm that solves these nonconvex feasibility problems with a sequence of semidefinite programs whose objectives are designed to encourage low-rank minimizers. CIDGIKc is enabled by a novel distance-geometric parameterization of constant curvature segment geometry for CRs with extensible segments. The resulting IK formulation involves only quadratic expressions and can efficiently incorporate a large number of collision avoidance constraints. Our experimental results demonstrate >98% solve success rates within complex, highly cluttered environments which existing algorithms cannot account for."
Automatically-Tuned Model Predictive Control for an Underwater Soft Robot,"David Null, William Edwards, Dohun Jeong, Teodor Tchalakov, James Menezes, Kris Hauser, Y Z","University of Illinois Urbana-Champaign,University of Illinois at Urbana-Champaign,University of Illinois Urbana Champaign,University of Illinois at Urbana Champaign","Modeling, Control, and Learning for Soft Robots I","Soft robots have desirable qualities for use in underwater environments thanks to their inherent compliance and lack of need for exposed hardware. Nevertheless, these advantages come at the cost of considerable control challenges. Data-driven model predictive control (MPC) is an approach that has shown promise in controlling soft robots. However, manually tuning the many hyperparameters in the learned dynamics model and the optimizer can be extremely tedious. In this work, we explore using data-driven MPC to control an underwater soft robot, and employ the AutoMPC method to automatically tune the hyperparameters and generate the controller. In the process, we extend AutoMPCâ€™s capabilities to handle multi-task tuning and we add a barrier cost function to enforce actuator constraints. Our experiments show that the AutoMPC controller reaches targets with significantly higher accuracy and reliability than state-of-the-art baselines both in- and out-of-distribution of the training data."
Realtime Robust Shape Estimation of Deformable Linear Object,"Jiaming Zhang, Zhaomeng Zhang, Yihao Liu, Yaqian Chen, Amir Kheradmand, Mehran Armand",Johns Hopkins University,"Modeling, Control, and Learning for Soft Robots I","Realtime shape estimation of continuum objects and manipulators is essential for developing accurate planning and control paradigms. The existing methods that create dense point clouds from camera images, and/or use distinguishable markers on a deformable body have limitations in realtime tracking of large continuum objects/manipulators. The physical occlusion of markers can often compromise accurate shape estimation. We propose a robust method to estimate the shape of linear deformable objects in realtime using scattered and unordered key points. By utilizing a robust probability-based labeling algorithm, our approach identifies the true order of the detected key points and then reconstructs the shape using piecewise spline interpolation. The approach only relies on knowing the number of the key points and the interval between two neighboring points. We demonstrate the robustness of the method when key points are partially occluded. The proposed method is also integrated into a simulation in Unity for tracking the shape of a cable with a length of 1m and a radius of 5mm. The simulation results show that our proposed approach achieves an average length error of 1.07% over the continuum's centerline and an average cross-section error of 2.11mm. The real-world experiments of tracking and estimating a heavy-load cable prove that the proposed approach is robust under occlusion and complex entanglement scenarios."
Learning-Based Tracking Control of Soft Robots,"Jingting Zhang, Xiaotian Chen, Paolo Stegagno, Mingxi Zhou, Chengzhi Yuan","University of Electronic Science and Technology of China,University of Rhode Island","Modeling, Control, and Learning for Soft Robots I","This paper proposes an adaptive radial basis function neural network (RBF NN) based scheme for the dynamics learning and tracking control problems of a soft trunk robot. Specifically, a low-order approximate model describing the soft robot's dynamics is first derived with the finite element method and proper orthogonal decomposition technique. Based on this model, an adaptive learning control scheme is developed with RBF NN, which can not only provide stable and accurate tracking control for the soft robot, but also achieve accurate learning of the robot's dynamics during the online control process. The proposed controller can effectively handle the soft robot's complex nonlinear uncertain dynamics and external disturbances, it thus can guarantee desirable tracking accuracy and control adaptability. The learned knowledge of robot's dynamics can be obtained and stored in a constant RBF NN model. Based on this, a novel knowledge-based controller is further proposed to provide desirable control performance for the soft robot without needing to repeat any online parameter adaptations, which significantly improves the overall system's operational efficiency with reduced computational complexity and easier control implementation. Effectiveness and advantages of the proposed methods are validated through physical experiments."
A Provably Stable Iterative Learning Controller for Continuum Soft Robots,"Michele Pierallini, Francesco Stella, Franco Angelini, Bastian Deutschmann, Josie Hughes, Antonio Bicchi, Manolo Garabini, Cosimo Della Santina","Centro di Ricerca E. Piaggio - Università di Pisa,EPFL,University of Pisa,German Aerospace Center,Fondazione Istituto Italiano di Tecnologia,Università di Pisa,TU Delft","Modeling, Control, and Learning for Soft Robots I","Fully exploiting soft robotsâ€™ capabilities requires devising strategies that can accurately control their movements with the limited amount of control sources available. This task is challenging for reasons including the hard-to-model dynamics, the systemâ€™s underactuation, and the need of using a prominent feedforward control action to preserve the soft and safe robot behavior. To tackle this challenge, this letter proposes a purely feedforward iterative learning control algorithm that refines the torque action by leveraging both the knowledge of the model and data obtained from past experience. After presenting a 3D polynomial description of soft robots, we study their intrinsic properties, e.g., input-to-state stability, and we prove the convergence of the controller coping with locally Lipschitz nonlinearities. Finally, we validate the proposed approach through simulations and experiments involving multiple systems, trajectories, and in the case of external disturbances and model mismatches."
Stable Motion Primitives Via Imitation and Contrastive Learning,"Rodrigo Pérez-Dattari, Jens Kober","Delft University of Technology,TU Delft",Learning from Demonstration I,"Learning from humans allows non-experts to program robots with ease, lowering the resources required to build complex robotic solutions. Nevertheless, such data-driven approaches often lack the ability to provide guarantees regarding their learned behaviors, which is critical for avoiding failures and/or accidents. In this work, we focus on reaching/point-to-point motions, where robots must always reach their goal, independently of their initial state. This can be achieved by modeling motions as dynamical systems and ensuring that they are globally asymptotically stable. Hence, we introduce a novel Contrastive Learning loss for training Deep Neural Networks (DNN) that, when used together with an Imitation Learning loss, enforces the aforementioned stability in the learned motions. Differently from previous work, our method does not restrict the structure of its function approximator, enabling its use with arbitrary DNNs and allowing it to learn complex motions with high accuracy. We validate it using datasets and a real robot. In the former case, motions are 2 and 4 dimensional, modeled as first- and second-order dynamical systems. In the latter, motions are 3, 4, and 6 dimensional, of first and second order, and are used to control a 7DoF robot manipulator in its end effector space and joint space. More details regarding the real-world experiments are presented in: https://youtu.be/OM-2edHBRfc."
GAN-Based Editable Movement Primitive from High-Variance Demonstrations,"Xuanhui Xu, You Mingyu, Zhou Hongjun, Zhifeng Qian, Weisheng Xu, Bin He","TongJi University,Tongji,Tongji University",Learning from Demonstration I,"Movement Primitive (MP) is a promising Learning from Demonstration (LfD) framework, which is commonly used to learn movements from human demonstrations and adapt the learned movements to new task scenes. A major goal of MP research is to improve the adaptability of MP to various target positions and obstacles. MPs enable their adaptability by capturing the variability of demonstrations. However, current MPs can only learn from low-variance demonstrations. The low-variance demonstrations include varied target positions but leave various obstacles alone. These MPs can not adapt the learned movements to the task scenes with different obstacles, which limits their adaptability since obstacles are everywhere in daily life. In this paper, we propose a novel transformer and GAN-based Editable Movement Primitive (EditMP), which can learn movements from high-variance demonstrations. These demonstrations include the movements in the task scenes with various target positions and obstacles. After movement learning, EditMP can controllably and interpretably edit the learned movements for new task scenes. Notably, EditMP enables all robot joints rather than the robot end-effector to avoid hitting complex obstacles. The proposed method is evaluated on three tasks and deployed to a real-world robot. We compare EditMP with probabilistic-based MPs and empirically demonstrate the state-of-the-art adaptability of EditMP."
One-Shot Imitation Learning with Graph Neural Networks for Pick-And-Place Manipulation Tasks,"Francesco Di Felice, Salvatore D'avella, Alberto Remus, Paolo Tripicchio, Carlo Alberto Avizzano","Mechanical Intelligence Institute, Sant'Anna School of Advanced ,Sant'Anna School of Advanced Studies,Scuola Superiore Sant'Anna",Learning from Demonstration I,"The proposed work presents a framework based on Graph Neural Networks (GNN) that abstracts the task to be executed and directly allows the robot to learn task-specific rules from synthetic demonstrations given through imitation learning. A graph representation of the state space is considered to encode the task-relevant entities as nodes for a Pick-and-Place task declined at different levels of difficulty. During training, the GNN-based policy learns the underlying rules of the manipulation task focusing on the structural relevance and the type of objects and goals, relying on an external primitive to move the robot to accomplish the task. The GNN-policy has been trained as a node-classification approach by looking at the different configurations of the objects and goals present in the scene, learning the association between them with respect to their type for the Pick-and-Place task. The experimental results show a high generalization capability of the proposed model in terms of the number, positions, height distributions, and even configurations of the objects/goals. Thanks to the generalization, only a single image of the desired goal configuration is required at inference time."
Unsupervised Human Motion Segmentation Based on Characteristic Force Signals of Contact Events,"Keito Sugawara, Sho Sakaino, Toshiaki Tsuji","Saitama University,University of Tsukuba",Learning from Demonstration I,"Humans perform complex tasks involving force interactions daily. Learning from demonstration, a method for transferring such human manipulation skills to robots, requires techniques for segmenting the demonstrations into movement primitives. Therefore, we propose an unsupervised motion segmentation method that utilizes small characteristic fluctuations of 6-axis force/torque signals as features for motion segmentation. This method includes a feature extraction phase using a differentiation process and detects segmentation points based on the differentiated 6-axis force/torque signals obtained in the task demonstrations. The segmentation method was evaluated using a peg-in-hole task and bottle-lid opening task. The experimental results demonstrate the validity of using differentiated forces and torques for motion segmentation."
Robotic Skill Mutation in Robot-To-Robot Propagation During a Physically Collaborative Sawing Task,"Rosa Enna Sophia Maessen, Joseph Micah Prendergast, Luka Peternel",Delft University of Technology,Learning from Demonstration I,"Skill propagation among robots without human involvement can be crucial in quickly spreading new physical skills to many robots. In this respect, it is a good alternative to pure reinforcement learning, which can be time-consuming, or learning from human demonstration, which requires human involvement. In the latter case, there may not be enough humans to quickly spread skills to many robots. However, propagation among robots without direct human supervision can result in robotic skills mutating from the original source. This can be beneficial when better skills might emerge or when a new skill is obtained to be used for other similar tasks. However, it can also be dangerous in terms of task execution safety. This letter studies the mutation of a robotic skill when it is propagated from one robot to another during a physically collaborative task. We chose the collaborative sawing task as a study case since it involves complex two-agent physical interaction/coordination and because its periodic nature can facilitate repetitive learning. The study employs periodic Dynamic Movement Primitives and Locally Weight Regression to encode and learn the motion and impedance required to execute the task. To explore what influences mutation, we varied several control and environment conditions such as the maximum stiffness, robot base position, friction coefficient of the sawed object, and movement period."
Few-Shot Learning of Force-Based Motions from Demonstration through Pre-Training of Haptic Representation,"Marina Y Aoyama, Joao Moura, Namiko Saito, Sethu Vijayakumar","The University of Edinburgh,the University of Edinburgh,University of Edinburgh",Learning from Demonstration I,"In many contact-rich tasks, force sensing plays an essential role in adapting the motion to the physical properties of the manipulated object. To enable robots to capture the underlying distribution of object properties necessary for generalising learnt manipulation tasks to unseen objects, existing Learning from Demonstration (LfD) approaches require a large number of costly human demonstrations. Our proposed semi-supervised LfD approach decouples the learnt model into a haptic representation encoder and a motion generation decoder. This enables us to pre-train the first using a large amount of unsupervised data, easily accessible, while using few-shot LfD to train the second, leveraging the benefits of learning skills from humans. We validate the approach on the wiping task using sponges with different stiffness and surface friction. Our results demonstrate that pre-training significantly improves the ability of the LfD model to recognise physical properties and generate desired wiping motions for unseen sponges, outperforming the LfD method without pre-training. We validate the motion generated by our semi-supervised LfD model on the physical robot hardware using the KUKA iiwa robot arm. We also validate that the haptic representation encoder, pre-trained in simulation, captures the properties of real objects, explaining its contribution to improving the generalisation of the downstream task."
Learning-Based Risk-Bounded Path Planning under Environmental Uncertainty,"Fei Meng, Liangliang Chen, Han Ma, Jiankun Wang, Max Qing Hu Meng","The Chinese University of Hong Kong,Georgia Institute of Technology,Southern University of Science and Technology",Learning from Demonstration I,"Building a general and efficient path planning framework in uncertain nonconvex environments is challenging due to the safety constraints and complex configuration. Traditional avenues usually involve convexifying obstacles and presume Gaussian distribution, which are not universal. Meanwhile, the fast convergence of high-quality solutions is not guaranteed. Therefore, we develop a novel neural risk-bounded path planner to quickly find near-optimal solutions that have an acceptable collision probability in the complex environments. Firstly, we retrieve the nonconvex obstacles with arbitrary probabilistic uncertainties in the form of a deterministic point cloud map. A neural network sampler encodes it into a latent embedding and is trained with sufficient expert demonstrations, predicting states in the potential subspace. We construct a neural cost estimator to select the best informed state from those samples. Then, we recursively use the simple yet effective neural networks to march toward the start and goal bidirectionally. The collision risk of the intermediate connections is verified based on sum-of-squares optimization. Simulation results show that our approach significantly saves time and resources in finding comparable solutions over the state-of-the-art methods in the seen and unseen challenging environments."
ConBaT: Control Barrier Transformer for Safe Robot Learning from Demonstrations,"Yue Meng, Sai Vemprala, Rogerio Bonatti, Chuchu Fan, Ashish Kapoor","Massachusetts Institute of Technology,Scaled Foundations,Microsoft,MicroSoft",Learning from Demonstration I,"Large-scale self-supervised models have recently revolutionized our ability to perform a variety of tasks within the vision and language domains. However, using such models for autonomous systems is challenging because of safety requirements: besides executing correct actions, an autonomous agent must also avoid the high cost and potentially fatal critical mistakes. Traditionally, self-supervised training mainly focuses on imitating previously observed behaviors, and the training demonstrations carry no notion of which behaviors should be explicitly avoided. In this work, we propose Control Barrier Transformer (ConBaT), an approach that learns safe behaviors from demonstrations in a self-supervised fashion. ConBaT is inspired by the concept of control barrier functions in control theory and uses a causal transformer that learns to predict safe robot actions autoregressively using a critic that requires minimal safety data labeling. During deployment, we employ a lightweight online optimization to find actions that ensure future states lie within the learned safe set. We apply our approach to different simulated control tasks and show that our method results in safer control policies compared to other classical and learning-based methods such as imitation learning, reinforcement learning, and model predictive control."
Unsupervised Learning of Neuro-Symbolic Rules for Generalizable Context-Aware Planning in Object Arrangement Tasks,"Siddhant Sharma, Shreshth Tuli, Rohan Paul",Indian Institute of Technology Delhi,Learning from Demonstration I,"As robots tackle complex object arrangement tasks, it becomes imperative for them to be able to generalize to complex worlds and scale with number of objects. This work postulates that extracting action primitives, such as push operations, their pre-conditions and effects would enable strong generalization to unseen worlds. Hence, we factorize policy learning as inference of such generic rules, which act as strong priors for predicting actions given the world state. Learnt rules act as propositional knowledge and enable robots to reach goals in a zero-shot method by applying the rules independently and incrementally. However, obtaining hand-engineered rules, such as PDDL descriptions is hard, especially for unseen worlds. This work aims to learn generic, sparse, and context-aware rules that govern action primitives in robotic worlds through human demonstrations in simple domains. We demonstrate that our approach, namely RLAP, is able to extract rules without explicit supervision of rule labels and generate goal-reaching plans in complex Sokoban styled domains that scale with number of objects. RLAP furnishes significantly higher goal reaching rate and shorter planning times compared to the state-of-the-art techniques. The code, dataset, and videos are hosted at https://rule-learning-rlap.github.io/."
PCPNet: An Efficient and Semantic-Enhanced Transformer Network for Point Cloud Prediction,"Zhen Luo, Junyi Ma, Zijie Zhou, Guangming Xiong",Beijing Institute of Technology,Deep Learning I,"The ability to predict future structure features of environments based on past perception information is extremely needed by autonomous vehicles, which helps to make the following decision-making and path planning more reasonable. Recently, point cloud prediction (PCP) is utilized to predict and describe future environmental structures by the point cloud form. In this letter, we propose a novel efficient Transformer-based network to predict the future LiDAR point clouds exploiting the past point cloud sequences. We also design a semantic auxiliary training strategy to make the predicted LiDAR point cloud sequence semantically similar to the ground truth and thus improves the significance of the deployment for more tasks in real-vehicle applications. Our approach is completely self-supervised, which means it does not require any manual labeling and has a solid generalization ability toward different environments. The experimental results show that our method outperforms the state-of-the-art PCP methods on the prediction results and semantic similarity, and has a good real-time performance. Our open-source code and pre-trained models are available at https://github.com/Blurryface0814/PCPNet."
N2M2: Learning Navigation for Arbitrary Mobile Manipulation Motions in Unseen and Dynamic Environments,"Daniel Honerkamp, Tim Welschehold, Abhinav Valada","Albert Ludwigs Universität Freiburg,Albert-Ludwigs-Universität Freiburg,University of Freiburg",Deep Learning I,"Despite its importance in both industrial and service robotics, mobile manipulation remains a significant challenge as it requires seamless integration of end-effector trajectory generation with navigation skills as well as reasoning over long-horizons. Existing methods struggle to control the large configuration space and to navigate dynamic and unknown environments. In the previous work, we proposed to decompose mobile manipulation tasks into a simplified motion generator for the end-effector in task space and a trained reinforcement learning agent for the mobile base to account for the kinematic feasibility of the motion. In this work, we introduce Neural Navigation for Mobile Manipulation (N2 M2 ), which extends this decomposition to complex obstacle environments, extends the agent's control to the torso joint and the norm of the end-effector motion velocities, uses a more general reward function and, thereby, enables robots to tackle a much broader range of tasks in real-world settings. The resulting approach can perform unseen, long-horizon tasks in unexplored environments while instantly reacting to dynamic obstacles and environmental changes. At the same time, it provides a simple way to define new mobile manipulation tasks. We demonstrate the capabilities of our proposed approach in extensive simulation and real-world experiments on multiple kinematically diverse mobile manipulators."
The Foreseeable Future: Self-Supervised Learning to Predict Dynamic Scenes for Indoor Navigation,"Hugues Thomas, Jian Zhang, Timothy Barfoot","University of Toronto,Purdue University",Deep Learning I,"We present a method for generating, predicting, and using Spatiotemporal Occupancy Grid Maps (SOGM), which embed future semantic information of real dynamic scenes. We present an auto-labeling process that creates SOGMs from noisy real navigation data. We use a 3D-2D feedforward architecture, trained to predict the future time steps of SOGMs, given 3D lidar frames as input. Our pipeline is entirely self-supervised, thus enabling lifelong learning for real robots. The network is composed of a 3D back-end that extracts rich features and enables the semantic segmentation of the lidar frames, and a 2D front-end that predicts the future information embedded in the SOGM representation, potentially capturing the complexities and uncertainties of real-world multi-agent interactions. We also design a navigation system that uses these predicted SOGMs within planning, after they have been transformed into Spatiotemporal Risk Maps (SRMs). We verify our navigation system's abilities in simulation, validate it on a real robot, study SOGM predictions on real data in various circumstances, and provide a novel indoor 3D lidar dataset, collected during our experiments, which includes our automated annotations."
The Treachery of Images: Bayesian Scene Keypoints for Deep Policy Learning in Robotic Manipulation,"Jan Ole Von Hartz, Eugenio Chisari, Tim Welschehold, Wolfram Burgard, Joschka Boedecker, Abhinav Valada","University of Freiburg,Albert-Ludwigs-Universität Freiburg,University of Technology Nuremberg",Deep Learning I,"In policy learning for robotic manipulation, sample efficiency is of paramount importance. Thus, learning and extracting more compact representations from camera observations is a promising avenue. However, current methods often assume full observability of the scene and struggle with scale invariance. In many tasks and settings, this assumption does not hold as objects in the scene are often occluded or lie outside the field of view of the camera, rendering the camera observation ambiguous with regard to their location. To tackle this problem, we present BASK, a Bayesian approach to tracking scale-invariant keypoints over time. Our approach can successfully resolve inherent ambiguities in images, enabling keypoint tracking on symmetrical objects and occluded and out-of-view objects. We employ ourmethod to learn challenging multi-object robot manipulation tasks from wrist camera observations and demonstrate superior utility for policy learning compared to other representation learning techniques. Furthermore, we show outstanding robustness towards disturbances such as clutter, occlusions, and noisy depth measurements, as well as generalization to unseen objects both in simulation and real-world robotic experiments."
PBP: Path-Based Trajectory Prediction for Autonomous Driving,"Sepideh Afshar, Nachiket Deo, Akshay Bhagat, Titas Chakraborty, Yunming Shao, Balarama Raju Buddharaju, Adwait Deshpande, Henggang Cui","Motional,UC San Diego,Motional AD Inc,ZOOX,Motional AD,Georgia Institute of Technology",Deep Learning I,"Trajectory prediction plays a crucial role in the autonomous driving stack by enabling autonomous vehicles to anticipate the motion of surrounding agents. Goal-based prediction models have gained traction in recent years for addressing the multimodal nature of future trajectories. Goal-based prediction models simplify multimodal prediction by first predicting 2D goal locations of agents and then predicting trajectories conditioned on each goal. However, a single 2D goal location serves as a weak inductive bias for predicting the whole trajectory, often leading to poor map compliance, i.e., part of the trajectory going off-road or breaking traffic rules. In this paper, we improve upon goal-based prediction by proposing the Path-based prediction (PBP) approach. PBP predicts a discrete probability distribution over reference paths in the HD map using the path features and predicts trajectories in the path-relative Frenet frame. We applied the PBP trajectory decoder on top of the HiVT scene encoder and report results on the Argoverse dataset. Our experiments show that PBP achieves competitive performance on the standard trajectory prediction metrics, while significantly outperforming state-of-the-art baselines in terms of map compliance."
Sensorless Estimation of Contact Using Deep-Learning for Human-Robot Interaction,"Shilin Shan, Quang-Cuong Pham","Nanyang Technological University,NTU Singapore",Deep Learning I,"Physical human-robot interaction has been an area of interest for decades. Collaborative tasks, such as joint compliance, demand high-quality joint torque sensing. While external torque sensors are reliable, they come with the drawbacks of being expensive and vulnerable to impacts. To address these issues, studies have been conducted to estimate external torques using only internal signals, such as joint states and current measurements. However, insufficient attention has been given to friction hysteresis approximation, which is crucial for tasks involving extensive dynamic to static state transitions. In this paper, we propose a deep-learning-based method that leverages a novel long-term memory scheme to achieve dynamics identification, accurately approximating the static hysteresis. We also introduce modifications to the well-known Residual Learning architecture, retaining high accuracy while reducing inference time. The robustness of the proposed method is illustrated through a joint compliance and task compliance experiment."
Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies,"Daniel Lawson, Ahmed H. Qureshi",Purdue University,Deep Learning I,"Recent work has shown the promise of creating generalist, transformer-based, models for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in parameter space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also demonstrate the importance of various methodological choices when merging policies, such as utilizing common pre-trained initializations, increasing model capacity, and utilizing Fisher information for weighting parameter importance. In general, we believe research in this direction could help democratize and distribute the process that forms multi-task robotics policies. Our implementation is available at https://github.com/daniellawson9999/merging-decision-transf ormers."
Trust-Aware Motion Planning for Human-Robot Collaboration under Distribution Temporal Logic Specifications,"Pian Yu, Shuyang Dong, Shili Sheng, Lu Feng, Marta Kwiatkowska","University of Oxford,University of Virginia",Human-Robot Collaboration IV,"Recent work has considered trust-aware decision making for human-robot collaboration (HRC) with a focus on model learning. In this paper, we are interested in enabling the HRC system to complete complex tasks specified using temporal logic formulas that involve human trust. Since accurately observing human trust in robots with precision is challenging, we adopt the widely used partially observable Markov decision process (POMDP) framework for modelling the interactions between humans and robots. To specify the desired behaviour, we propose to use syntactically co-safe linear distribution temporal logic (scLDTL), a logic that is defined over predicates of states as well as belief states of partially observable systems. The incorporation of belief predicates in scLDTL enhances its expressiveness while simultaneously introducing added complexity. This also presents a new challenge as the belief predicates must be evaluated over the continuous (infinite) belief space. To address this challenge, we present an algorithm for solving the optimal policy synthesis problem. First, we enhance the belief MDP (derived by reformulating the POMDP) with a probabilistic labelling function. Then a product belief MDP is constructed between the probabilistically labelled belief MDP and the automaton translation of the scLDTL formula. Finally, we show that the optimal policy can be obtained by leveraging existing point-based value iteration algorithms with essential modifications. Human subject experiments with 21 participants on a driving simulator demonstrate the effectiveness of the proposed approach."
Towards Proactive Safe Human-Robot Collaborations Via Data-Efficient Conditional Behavior Prediction,"Ravi Pandya, Zhuoyuan Wang, Yorie Nakahira, Changliu Liu","Carnegie Mellon University,CMU",Human-Robot Collaboration IV,"We focus on the problem of how we can enable a robot to collaborate seamlessly with a human partner, specifically in scenarios where preexisting data is sparse. Much prior work in human-robot collaboration uses observational models of humans (i.e. models that treat the robot purely as an observer) to choose the robot's behavior, but such models do not account for the influence the robot has on the human's actions, which may lead to inefficient interactions. We instead formulate the problem of optimally choosing a collaborative robot's behavior based on a conditional model of the human that depends on the robot's future behavior. First, we propose a novel model-based formulation of conditional behavior prediction that allows the robot to infer the human's intentions based on its future plan in data-sparse environments. We then show how to utilize a conditional model for proactive goal selection and safe trajectory generation around human collaborators. Finally, we use our proposed proactive controller in a collaborative task with real users to show that it can improve users' interactions with a robot collaborator quantitatively and qualitatively."
Risk-Bounded Online Team Interventions Via Theory of Mind,"Yuening Zhang, Paul Robertson, Tianmin Shu, Sungkweon Hong, Brian Williams","Massachusetts Institute of Technology,Dynamic Object Language Labs Inc. (DOLL Inc.),MIT",Human-Robot Collaboration IV,"Despite advancements in human-robot teamwork, limited progress was made in developing AI assistants capable of advising teams online during task time, due to the challenges of modeling both individual and collective beliefs of the team members. Dynamic epistemic logic has proved to be a viable tool for representing a machine Theory of Mind and for modeling communication in epistemic planning, with applications to human-robot teamwork. However, this approach has yet to be applied in an online teaming assistance context and fails to account for the real-life probabilities of potential team beliefs. We propose a novel blend of epistemic planning and POMDP techniques to create a risk-bounded AI team assistant, that intervenes only when the team's expected likelihood of failure exceeds a predefined risk threshold or in the case of potential execution deadlocks. Our experiments and simulated demonstration on the Virtualhome testbed show that the assistant can effectively improve team performance."
Human-Robot Complementary Collaboration for Flexible and Precision Assembly,"Shichen Cao, Jing Xiao","Worcester Polytechnic Institute,Worcester Polytechnic Institute (WPI)",Human-Robot Collaboration IV,This paper addresses human-robot collaborative (HRC) precision assembly that complements natural human ability and the strength of an autonomous robot system. Our approach enables both flexibility and efficiency of tight-clearance assembly of various complex-shaped parts in the presence of uncertainty without requiring assembly skills and knowledge of robotics from the human operator. We demonstrated the effectiveness of our approach in a variety of experiments and comparisons with other HRC assembly approaches.
HAC-SLAM: Human Assisted Collaborative 3D-SLAM through Augmented Reality,"Malak Sayour, Mohammad Karim Yassine, Nadim Dib, Imad Elhajj, Boulos Asmar, Elie Khoury, Daniel Asmar","American University of Beirut,Idealworks",Human-Robot Collaboration IV,"Simultaneous Localization and Mapping (SLAM) has emerged as a prime autonomous mobile agent localization algorithm. Despite the global research effort to improve SLAM, its mapping component remains limited and serves little more than to satisfy the coupled localization problem. We present a collaborative 3D SLAM approach leveraging the power of augmented reality (AR). The system introduces a trio of diverse agents, each with its unique capability to become an active member in the mapping process: mobile robots, human operators, and AR head-mounted display (AR-HMD). A 3D complementary mapping pipeline is developed to utilize the built-in SLAM capabilities of the AR-HMD as shareable data. Our system aligns and merges the AR-HMD and the robotâ€™s local map automatically, triggered by a human-dictated initial guess. The created merged map proves advantageous in scenarios where the robot is restricted from navigating in certain areas. To correct map imperfections resulting from problematic objects such as transparent or reflective surfaces, the fused map is overlayed onto the environment, and hand gestures are used to add or delete 3D map features in real-time. Our system is implemented in both a lab and a real industrial warehouse setup. The results show a significant improvement in the map quality and mapping duration."
GAN-Based Semi-Supervised Training of LSTM Nets for Intention Recognition in Cooperative Tasks,"Matija Mavsar, Jun Morimoto, Aleš Ude","Jozef Stefan Institute,ATR Computational Neuroscience Labs",Human-Robot Collaboration IV,"The accumulation of a sufficient amount of data for training deep neural networks is a major hindrance in the application of deep learning in robotics. Acquiring real-world data requires considerable time and effort, yet it might still not capture the full range of potential environmental variations. The generation of new synthetic data based on existing training data has been enabled with the development of generative adversarial networks (GANs). In this paper, we introduce a training methodology based on GANs that utilizes a recurrent, LSTM-based architecture for intention recognition in robotics. The resulting networks predict the intention of the observed human or robot based on input RGB videos. They are trained in a semi-supervised manner, with the output classification networks predicting one of possible labels for the observed motion, while the recurrent generator networks produce fake RGB videos that are leveraged in the training process. We show that utilization of the generated data during the network training process increases the accuracy and generality of motion classification compared to using only real training data. The proposed method can be applied to a variety of dynamic tasks and different LSTM-based classification networks to supplement real data."
CoBT: Collaborative Programming of Behaviour Trees from One Demonstration for Robot Manipulation,"Aayush Jain, Philip Long, Valeria Villani, John D. Kelleher, Maria Chiara Leva","Irish Manufacturing Research and Technological University Dublin,Atlantic Technological University,University of Modena and Reggio Emilia,Trinity Colege Dublin,Technological University Dublin",Human-Robot Collaboration IV,"Mass customization and shorter manufacturing cycles are becoming more important among small and medium-sized companies. However, classical industrial robots struggle to cope with product variation and dynamic environments. In this paper, we present CoBT, a collaborative programming by demonstration framework for generating reactive and modular behavior trees. CoBT relies on a single demonstration and a combination of data-driven machine learning methods with logic-based declarative learning to learn a task, thus eliminating the need for programming expertise or long development times. The proposed framework is experimentally validated on 7 manipulation tasks and we show that CoBT achieves approx 93% success rate overall with an average of 7.5s programming time. We conduct a pilot study with non-expert users to provide feedback regarding the usability of CoBT. More videos and generated behavior trees are available at:https://github.com/jainaayush2006/CoBT.git."
A Dynamic Planner for Safe and Predictable Human-Robot Collaboration,"Andrea Pupa, Marco Minelli, Cristian Secchi","University of Modena and Reggio Emilia,Univ. of Modena & Reggio Emilia",Human-Robot Collaboration IV,"The new face of modern industrial scenarios involves shared workspaces where humans and robots work closely together. To ensure safe human-robot collaboration (HRC), regulations have been updated introducing the ISO/TS15066. However, complying with these regulations often leads to inefficient behavior, such as unnecessarily reducing robot speed or unpredictably changing the robot path, which may negatively affect the operator perception of the robot. In this work an optimal approach to address together these two issues is proposed. Starting from a desired final configuration, the framework plans a collision-free trajectory for the robot. Subsequently, predictability is taken into account and a set of virtual tubes into which the path of the robot can move is built. Lastly, an optimization problem is solved online to ensure that the robot stays within these tubes and the velocities are compliant with the ISO/TS 15066. The proposed approach has been experimentally validated in two different scenarios: one composed by a mobile manipulator, i.e. a UR10e mounted on a Neobotix MPO-500, and one composed by only a collaborative manipulator, i.e. a UR5e."
Towards Unifying Human Likeness: Evaluating Metrics for Human-Like Motion Retargeting on Bimanual Manipulation Tasks,"Andre Meixner, Mischa Carl, Franziska Krebs, Noémie Jaquier, Tamim Asfour",Karlsruhe Institute of Technology (KIT),Modeling and Simulating Humans,"Generating human-like robot motions is pivotal for achieving smooth human-robot interactions. Such motions contribute to better predictions of robot motions by humans, thus leading to more intuitive interaction and increased acceptability. Human likeness in robot motions has been conventionally measured and realized via the optimization of human-likeness metrics. However, the abundance of such metrics and the absence of standardized criteria impede their usage in novel contexts. In this work, we introduce a unified human-likeness metric built from a hierarchically weighted sum of individual metrics. The proposed metric is derived from a thorough analysis of eleven existing human-likeness criteria and is applicable across various tasks and robot models. We evaluate its performance in the context of motion retargeting of bimanual tasks with three different humanoid robots."
MiBOT: A Head-Worn Robot That Modulates Cardiovascular Responses through Human-Like Soft Massage,"Alice Mylaeus, Stephanie Vogt, Berken Utku Demirel, Marcel Gort, Mirko Meboldt, Manuel Meier, Christian Holz","ETH Zurich,ETH Zürich",Modeling and Simulating Humans,"Massage therapy is helpful for the rehabilitation of various diseases, such as headaches caused by migraines and stress. Existing robotic systems have focused on massage therapy on the torso and limbs, but performing massage motions through suitable actuation on a personâ€™s head has been a challenge. In this paper, we present MiBOT, a head-worn massage robot that actuates two soft tactors to produce touch motions mimicking human massage. A key design principle behind MiBOT is its silent actuation, which we achieve through pneumatic artificial muscles in conjunction with a controller loop to respond to contact pressure. We evaluated the effectiveness of MiBOT in a controlled study and assessed subjectsâ€™ blood pressure and heart rate levels while applying MiBOT. We found that our mechanical system generated positive and conclusive quantitative outcomes that are similar to the human-administered massage, decreasing participantsâ€™ mean systolic and diastolic blood pressure by 2.8 mmHg and 1.7 mmHg, respectively, as well as calming their heart rate by 8â€“10% on average."
ESP: Extro-Spective Prediction for Long-Term Behavior Reasoning in Emergency Scenarios,"Dingrui Wang, Lai Zheyuan, Yuda Li, Yi Wu, Yuexin Ma, Johannes Betz, Ruigang Yang, Wei Li","Technical University of Munich,Inceptio,Inceptio Technology,Nanjing University of Posts and Telecommunications,ShanghaiTech University,University of Kentucky",Modeling and Simulating Humans,"Emergent-scene safety is the key milestone for fully autonomous driving, and reliable on-time prediction is essential to maintain safety in emergency scenarios. However, these emergency scenarios are long-tailed and hard to collect, which restricts the system from getting reliable predictions. In this paper, we build a new dataset, which aims at the long-term prediction with the inconspicuous state variation in history for the emergency event, named the Extro-Spective Prediction (ESP) problem. Based on the proposed dataset, a flexible feature encoder for ESP is introduced to various prediction methods as a seamless plug-in, and its consistent performance improvement underscores its efficacy. Furthermore, a new metric named clamped temporal error (CTE) is proposed to give a more comprehensive evaluation of prediction performance, especially in time-sensitive emergency events of subseconds. Interestingly, as our ESP features can be described in human-readable language naturally, the application of integrating into ChatGPT also shows huge potential. The ESP-dataset and all benchmarks are released at url{https://dingrui-wang.github.io/ESP-Dataset/"
SynthAct: Towards Generalizable Human Action Recognition Based on Synthetic Data,"David Schneider, Marco Keller, Zeyun Zhong, Kunyu Peng, Alina Roitberg, Jürgen Beyerer, Rainer Stiefelhagen","Karlsruhe Institute of Technology,Karlsruhe Institute of Technology (KIT),University of Stuttgart,Fraunhofer Gesellschaft",Modeling and Simulating Humans,"Synthetic data generation is a proven method for augmenting training sets without the need for extensive setups, yet its application in human activity recognition is underexplored. This is particularly crucial for human-robot collaboration in household settings, where data collection is often privacy-sensitive. In this paper, we introduce SynthAct, a synthetic data generation pipeline designed to significantly minimize the reliance on real-world data. Leveraging modern 3D pose estimation techniques, SynthAct can be applied to arbitrary 2D or 3D video action recordings, making it applicable for uncontrolled in-the-field recordings by robotic agents or smarthome monitoring systems. We present two SynthAct datasets: AMARV, a large synthetic collection with over 800k multi-view action clips, and Synthetic Smarthome, mirroring the Toyota Smarthome dataset. SynthAct generates a rich set of data, including RGB videos and depth maps from four synchronized views, 3D body poses, normal maps, segmentation masks and bounding boxes. We validate the efficacy of our datasets through extensive synthetic-to-real experiments on NTU RGB+D and Toyota Smarthome."
Visual-Tactile Robot Grasping Based on Human Skill Learning from Demonstrations Using a Wearable Parallel Hand Exoskeleton,"Zhenyu Lu, Lu Chen, Hengtai Dai, Haoran Li, Zhou Zhao, Bofang Zheng, Nathan Lepora, Chenguang Yang","Bristol Robotics Laboratory,Shanxi University,University of Bristol,Central China Normal University,University of Liverpool",Modeling and Simulating Humans,"The soft fingers and strategic grasping skills enable the human hands to grasp objects stably. This paper aims to model human grasping skills and transfer the learned skills to robots to improve grasping quality and success rate. First, we designed a wearable tool-like parallel hand exoskeleton equipped with optical tactile sensors to acquire multimodal information, including hand positions and postures, the relative distance of the exoskeleton claws, and tactile images. Using the demonstration data, we summarized three characteristics observed from human demonstrations: varying-speed actions, grasping effect read from tactile images and grasping strategies for different positions. The characteristics were then utilized in the robot skill modelling to achieve a more human-like grasp. Since no force sensors are fixed to the claws, we introduced a new variable, called ""grasp depth"", to represent the grasping effect on the object. The robot grasping strategy diagram is constructed as follows: First, grasp quality is predicted using a linear array network (LAN) and global visual images as inputs. The conditions such as grasp width, depth, position, and angle are also predicted. Second, with the grasp width and depth of the object determined, dynamic movement primitives (DMPs) are employed to mimic human grasp actions with varying velocities. A final action adjustment based on tactile detection is performed during the near-grasp time to enhance grasp quality further."
Workstation Suitability Maps: Generating Ergonomic Behaviors on a Population of Virtual Humans with Multi-Task Optimization,"Jacques Zhong, Vincent Weistroffer, Jean-Baptiste Mouret, Francis Colas, Pauline Maurice","CEA-List,CEA LIST,Inria,Inria Nancy Grand Est,CNRS - LORIA",Modeling and Simulating Humans,"In industrial workstations, the morphology of the worker is a key factor for the feasibility and the ergonomics of an activity. Existing digital human modeling tools can simulate different morphologies at work, but hardly scale to a large population of workers because of limited consideration of morphology-specific behaviors and computational cost. This paper presents a framework to efficiently evaluate the suitability of a workstation over a large population of workers in a physics-based simulation. Activities are simulated through a two-step optimization process, involving a quadratic-programming based whole-body controller and a multi-task optimizer for behavioral adaptation. On a screwdriving scenario, we demonstrate how our framework can help ergonomists improve workstation designs thanks to the resulting suitability maps where generated behaviors are optimized for each morphology w.r.t. ergonomics and performance."
Self Model for Embodied Intelligence: Modeling Full-Body Human Musculoskeletal System and Locomotion Control with Hierarchical Low-Dimensional Representation,"Kaibo He, Chenhui Zuo, Jing Shao, Yanan Sui",Tsinghua University,Modeling and Simulating Humans,"Modeling and control of the human musculoskeletal system is important for understanding human motor functions, developing embodied intelligence, and optimizing human-robot interaction systems. However, current open-source models are restricted to a limited range of body parts and often with a reduced number of muscles. There is also a lack of algorithms capable of controlling over 600 muscles to generate reasonable human movements. To fill this gap, we build a musculoskeletal model with 90 body segments, 206 joints, and 700 muscle-tendon units, allowing simulation of full-body dynamics and interaction with various devices. We develop a new algorithm using low-dimensional representation and hierarchical deep reinforcement learning to achieve state-of-the-art full-body control. We validate the effectiveness of our model and algorithm in simulations with real human locomotion data. The musculoskeletal model, along with its control algorithm, will be made available to the research community to promote a deeper understanding of human motion control and better design of interactive robots."
Keypoints-Guided Lightweight Network for Single-View 3D Human Reconstruction,"Yuhang Chen, Chenxing Wang",Southeast University,Modeling and Simulating Humans,"Single-view 3D human reconstruction has been a hot topic due to the potential of wide applications. To achieve high accuracy, existing works usually take computationally intensive models as backbone for exhaustive underlying features and then directly estimate human mesh vertices. These factors lead to redundant parameters, large calculations and low efficiency, while lightweight solutions to address these challenges are relatively scarce. In this work, based on the problems studied above, we propose a keypoints-guided lightweight network with an encoding-decoding framework. As the input is an image, a lightweight backbone named multi-stage and global feature enhanced network is designed for 2D encoding, where some operations of multi-scale fusion and frequency domain filtering are performed to extract more informative but low-resolution features. As the output is mesh of human body, we construct a keypoints-based 3D human template, with which the 2D low-resolution features can be mapped to 3D space to guide the 3D decoding with high efficiency and high accuracy. Extensive experiments on popular benchmarks 3DPW and Human3.6M illustrate the favorable trade-off between the accuracy and complexity of our method. Our code is publicly available at https://github.com/ChrisChenYh/EfficientHuman.git."
Moving Horizon Estimation of Human Kinematics and Muscle Forces,"Amedeo Ceglia, François Bailly, Mickael Begon","University of Montreal,INRIA, Université de Montpellier",Modeling and Simulating Humans,"Human-robot interaction based on real-time kinematics or electromyography (EMG) feedback improves rehabilitation using assist-as-needed strategies. Muscle forces are expected to provide even more comprehensive information than EMG to control these assistive rehabilitation devices. Measuring in vivo muscle force is challenging, leading to the development of numerical methods to estimate them. Due to their high computational cost, forward dynamics-based optimization algorithms were not viable for real-time estimation until recently. To achieve muscle forces estimation in real time, a moving horizon estimator (MHE) algorithm was used to track experimental biosignals. Two participants were equipped with EMG sensors and skin markers that were streamed in real time and used as targets for the MHE. The upper-limb musculoskeletal (MSK) model was composed of 10 degrees-of-freedom actuated by 31 muscles. The MHE relies on a series of overlapping trajectory optimization subproblems of which the following parameters have been adjusted: the fixed duration and the frame to export. We based this adjustment on the estimation delay, the muscle saturation, the joint kinematic mean power frequency, and errors to experimental data. Our algorithm provided consistent estimates of muscle forces and kinematics with visual feedback at 30 Hz with a 110 ms delay. This method is promising to guide rehabilitation and enrich assistive device control laws with personalized force estimations."
Reinforcement Learning with Energy-Exchange Dynamics for Spring-Loaded Biped Robot Walking,"Cheng-yu Kuo, Hirofumi Shin, Takamitsu Matsubara","Nara Institute of Science and Technology,Honda R&D Co.,Ltd.",Humanoid and Bipedal Locomotion,"This paper presents a probabilistic Model-based Reinforcement Learning (MBRL) approach for learning the Energy-exchange Dynamics (EED) of a spring-loaded biped robot. Our approach enables on-site walking acquisition with high sample efficiency, real-time planning capability and generalizability across skill conditions. Specifically, we learn the data-driven state transition dynamics of the robot in the formulation of energy-states, with their interaction characterized as energy-exchange to reduce dimensionality. To improve planning reliability with the learned EED, we design a control space based on a walking trajectory that follows the law of conservation of energy and formulated by energy-states. We evaluated our approach using a four-degree-of-freedom spring-loaded biped robot in simulation and hardware, and generalizability is validated by using same learning framework for different walking speeds and terrains in simulation and walking acquisition with hardware. All results showed successful on-site walking acquisition with a compact nine-dimension dynamics model, 40Hz real-time planning, and on-site learning within a few minutes.}"
Foot Shape-Dependent Resistive Force Model for Bipedal Walkers on Granular Terrains,"Xunjie Chen, Anikode Aditya, Jingang Yi, Tao Liu","Rutgers University,Zhejiang University",Humanoid and Bipedal Locomotion,"Legged robots have demonstrated high efficiency and effectiveness in unstructured and dynamic environments. However, it is still challenging for legged robots to achieve rapid and efficient locomotion on deformable, yielding substrates, such as granular terrains. We present an enhanced resistive force model for bipedal walkers on soft granular terrains by introducing effective intrusion depth correction. The enhanced force model captures fundamental kinetic results considering the robot foot shape, walking gait speed variation, and energy expense. The model is validated by extensive foot intrusion experiments with a bipedal robot. The results confirm the model accuracy on the given type of granular terrains. The model can be further integrated with the motion control of bipedal robotic walkers."
Adaptive Passive Biped Dynamic Walking on Unknown Uneven Terrain,"lishen pu, Yixuan Liu, Zheng Aiqun, Bofeng Qi, Chunquan Xu","Tongji University,Shanghai New Tobacco Product Research Institute Co., Ltd.,Department of Control Science & Engineering, Tongji University",Humanoid and Bipedal Locomotion,"In this paper, we propose an adaptive controller for virtual passive biped dynamic walking on unknown uneven terrain. The adaptive controller consists of a trajectory tracking control law developed via backstepping method to mimic reference passive gait, and a slope estimator for the inclination angle of the terrain. In addition, a re-planning approach is introduced to correct the robot state off-track from the reference gait due to the terrain changes. The controller is validated through the simulations on the mixed uneven terrain consisting of varying slopes and steps. The results suggest that the controller shows comparable cost of transport and greater adaptability to terrain changes compared with certain existing methods."
HumanMimic: Learning Natural Locomotion and Transitions for Humanoid Robot Via Wasserstein Adversarial Imitation,"Annan Tang, Takuma Hiraoka, Naoki Hiraoka, Fan Shi, Kento Kawaharazuka, Kunio Kojima, Kei Okada, Masayuki Inaba","The University of Tokyo,ETH Zürich",Humanoid and Bipedal Locomotion,"Transferring human motion skills to humanoid robots remains a significant challenge. In this study, we introduce a Wasserstein adversarial imitation learning system, allowing humanoid robots to replicate natural whole-body locomotion patterns and execute seamless transitions by mimicking human motions. First, we present a unified primitive-skeleton motion retargeting to mitigate morphological differences between arbitrary human demonstrators and humanoid robots. An adversarial critic component is integrated with Reinforcement Learning (RL) to guide the control policy to produce behaviors aligned with the data distribution of mixed reference motions. Additionally, we employ a specific Integral Probabilistic Metric (IPM), namely the Wasserstein-1 distance with a novel soft boundary constraint to stabilize the training process and prevent model collapse. Our system is evaluated on a full-sized humanoid JAXON in the simulator. The resulting control policy demonstrates a wide range of locomotion patterns, including standing, push-recovery, squat walking, human-like straight-leg walking, and dynamic running. Notably, even in the absence of transition motions in the demonstration dataset, the robot showcases an emerging ability to transit naturally between distinct locomotion patterns as desired speed changes."
Online Adaptive Motion Generation for Humanoid Locomotion on Non-Flat Terrain Via Template Behavior Extension,"Xiang Meng, Zhangguo Yu, Xuechao Chen, Zelin Huang, Fei Meng, Qiang Huang","Beijing Institute of Technology,Beijing Insititute of Technology",Humanoid and Bipedal Locomotion,"For humanoid robots, online motion generation on non-flat terrain remains an ongoing research challenge. Computational complexity is one of the primary restrictions that preclude motion planners from generating adaptive behaviors online. In this paper, we investigate this problem and decompose it into two sequential components: an Efficient Behavior Generator (EBG) and a Nonlinear Centroidal Model Predictive Controller (NC-MPC). The EBG is responsible for optimizing the physically feasible whole-body template behaviors, which can provide reliable warm-starts for NC-MPC, thereby greatly reducing the computational effort of online planning. With tailored objective function and feet complementary constraints, the EBG can search for a near-optimal solution after several iterations within seconds for different behaviors including walking, running, and jumping, even with intuitive initial guesses. To make the template behaviors extensible when the robot encounters possible different scenarios, the NC-MPC is proposed to regenerate the reactive motion online to adapt it to the real local environment. Finally, we validate the effectiveness of synthesizing EBG and NC-MPC for humanoid locomotion on non-flat terrain in simulation and on the real humanoid robot BHR7P."
A Bio-Plausible Approach to Realizing Heat-Evoked Nociceptive Withdrawal Reflex on the Upper Limb of a Humanoid Robot,"Fengyi Wang, Julio Rogelio Guadarrama-Olvera, Nitish V. Thakor, Gordon Cheng","Technical University of Munich,Johns Hopkins University, Baltimore, USA",Humanoid and Bipedal Locomotion,"In this letter, we present a method for realizing the heat-evoked nociceptive withdrawal reflex (NWR) in the upper limb of a humanoid robot so that it can avoid the potential damage caused by noxious heat. We use a spiking neuron network whose structure, encoding scheme, and form of information transmission mimic the reflex arc in humans to improve bio-plausibility. The proper synaptic strengths between the sensory neurons and the interneurons in the first two layers are learned using the bio-plausible reward modulated spike timing-dependent plasticity learning algorithm. By monitoring the spikes from the motor neuron in the third layer, a reflex matching the intensity of the stimulation can be evoked. Experimental evaluations show that noxious heat stimulation can be detected online and evoke the NWR. The experiments on a full-size humanoid robot show that the method enables robots to avoid potential damage robustly with proper NWR, depending on the site and intensity of the stimulation. We also verify that the method takes advantage of the intrinsic characteristics of its neuromorphic encoding scheme to reproduce essential features of the NWR e.g., spatial summation effect and temporal summation effect in humans. The improved bio-plausibility and the capability to reproduce the human-like features make the proposed method suitable for devices that provide perceptual feedback to human users and allow local processing with low energy consumption, such as cognitive prosth"
Fall Prediction for Bipedal Robots: The Standing Phase,"Margaret Eva Mungai, J.W Grizzle, Gokul Prabhakaran","University of Michigan,Univeristy of Michigan",Humanoid and Bipedal Locomotion,"This paper presents a novel approach to fall prediction for bipedal robots, specifically targeting the detection of potential falls while standing caused by abrupt, incipient, and intermittent faults. Leveraging a 1D convolutional neural network (CNN), our method aims to maximize lead time for fall prediction while minimizing false positive rates. The proposed algorithm uniquely integrates the detection of various fault types and estimates the lead time for potential falls. Our contributions include the development of an algorithm capable of detecting abrupt, incipient, and intermittent faults in full-sized robots, its implementation using both simulation and hardware data for a humanoid robot, and a method for estimating lead time. Evaluation metrics, including false positive rate, lead time, and response time, demonstrate the efficacy of our approach. Particularly, our model achieves impressive lead times and response times across different fault scenarios with a false positive rate of 0. The findings of this study hold significant implications for enhancing the safety and reliability of bipedal robotic systems."
Shape-Changing Robotic Mannequin Shoulder with Bio-Inspired Layered Structure,"Juncai Long, Jituo Li, Yiwen Lu, Chengdi Zhou, Guodong Lu, Yixiong Feng","ZheJiang University,Zhejiang University",Humanoid and Bipedal Locomotion,"Shape-changing robotic mannequin is a humanoid robot for imitating shapes of human bodies. The diversity of human bodies makes it difficult to imitate various body shapes, especially the shoulders. This paper proposes a rigid-flexible-soft coupling three-layered robotic mannequin shoulder inspired by human body anatomy. The robotic mannequin can adjust the anisotropic deformation of its human-like skin to imitate body dimensions, shape details and surface curvatures of target bodies. Structurally, the inner skeleton layer is composed of rigid framework and linear actuators for changing the global body dimensions. The middle muscle layer consists of flexible patches and layer-jamming bars with tunable stiffness for controlling the surface curvatures. The outer soft skin layer envelops the patches, forming a human-like surface of the robotic mannequin. To imitate a human body, the linear actuators drive the patches forward, which deforms the elastic skin layer. The tensioned skin layer inversely drives the bending deformation of patches, which can be controlled by layer-jamming bars. We design the three-layered structure by analyzing the shape differences of hundreds of scanned human models. An energy-based method is proposed to predict and control the coupling deformation of the layered structure. A physical robotic shoulder prototype has been built to verify the effectiveness of our method."
UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots,"Ines Sorrentino, Giulio Romualdi, Daniele Pucci","Istituto Italiano di Tecnologia,Italian Institute of Technology",Humanoid and Bipedal Locomotion,"This paper proposes a novel sensor fusion based on Unscented Kalman Filtering for the online estimation of joint-torques of humanoid robots without joint-torque sensors. At the feature level, the proposed approach considers multi-modal measurements (e.g. currents, accelerations, etc.) and non-directly measurable effects, such as external contacts, thus leading to joint torques readily usable in control architectures for human-robot interaction. The proposed sensor fusion can also integrate distributed, non-collocated force/torque sensors, thus being a flexible framework with respect to the underlying robot sensor suit. To validate the approach, we show how the proposed sensor fusion can be integrated into a two-level torque control architecture aiming at task-space torque control. The performances of the proposed approach are shown through extensive tests on the new humanoid robot ergoCub, currently being developed at Istituto Italiano di Tecnologia. We also compare our strategy with the existing state-of-the-art approach based on the recursive Newton-Euler algorithm. Results demonstrate that our method achieves low root mean square errors in torque tracking, ranging from 0.05 Nm to 2.5 Nm, even in the presence of external contacts."
"Waverider: Leveraging Hierarchical, Multi-Resolution Maps for Efficient and Reactive Obstacle Avoidance","Victor Reijgwart, Michael Pantic, Roland Siegwart, Lionel Ott","ETH Zurich,ETH Zürich",Reactive and Sensor-Based Planning,"Fast and reliable obstacle avoidance is an important task for mobile robots. In this work, we propose an efficient reactive system that provides high-quality obstacle avoidance while running at hundreds of hertz with minimal resource usage. Our approach combines wavemap, a hierarchical volumetric map representation, with a novel hierarchical and parallelizable obstacle avoidance algorithm formulated through Riemannian Motion Policies (RMP). Leveraging multi-resolution obstacle avoidance policies, the proposed navigation system facilitates precise, low-latency (36ms), and extremely efficient obstacle avoidance with a very large perceptive radius (30m). We perform extensive statistical evaluations on indoor and outdoor maps, verifying that the proposed system compares favorably to fixed-resolution RMP variants and CHOMP. Finally, the RMP formulation allows the seamless fusion of obstacle avoidance with additional objectives, such as goal-seeking, to obtain a fully-fledged navigation system that is versatile and robust. We deploy the system on a Micro Aerial Vehicle and show how it navigates through an indoor obstacle course. Our complete implementation, called waverider, is made available as open source."
Whisker-Based Tactile Navigation Algorithm for Underground Robots,"Tanel Kossas, Walid Remmas, Roza Gkliva, Asko Ristolainen, Maarja Kruusmaa","Tallinn University of Technology,Tallinn University of Technology / Université de Montpellier,Tallinn University of Technology (TalTech)",Reactive and Sensor-Based Planning,"This work explores the use of artificial whiskers as tactile sensors for enhancing the perception and navigation capabilities of mobile robots in challenging settings such as caves and underground mines. These environments exhibit inconsistent lighting conditions, locally self-similar textures, and general poor visibility conditions, that can cause the performance of state-of-the-art vision-based methods to decline. In order to evaluate the efficacy of tactile sensing in this context, three algorithms were developed and tested with simulated and physical experiments: a wall-follower, a navigation algorithm based on Theta*, and a hybrid approach that combines the two. The obtained results highlight the efficacy of tactile sensing for wall-following in intricate environments. When paired with an external method for pose estimation, it further aids in navigating unknown environments. Moreover, by integrating navigation with wall-following, the third, hybrid algorithm enhanced the map traversal speed by roughly 26âˆ’43% compared to standard navigation methods without wall-following."
Spline-Interpolated Model Predictive Path Integral Control with Stein Variational Inference for Reactive Navigation,"Takato Miura, Naoki Akai, Kohei Honda, Susumu Hara",Nagoya University,Reactive and Sensor-Based Planning,"This paper presents a reactive navigation method based on model predictive path integral (MPPI) control with spline interpolation of control input sequence and Stein variational gradient descent (SVGD). MPPI formulates a non-linear optimization problem that determines an optimal control input sequence and solves it using a sampling-based method. Results by MPPI significantly depend on sampling noises. To quickly find paths to avoid large and/or newly detected obstacles, the sampling noises should be set to large. However, the large noises yield non-smooth control input sequences, resulting in non-smooth paths. To prevent this problem, we first introduce spline interpolation of control input sequence in the MPPI process. Owing to the interpolation, smooth control input sequences can be obtained even though the large sampling noises are used. However, a vanilla MPPI algorithm still does not work in a case where there are optimal and near optimal solutions in one scene, e.g., there are several paths to avoid obstacles, because MPPI assumes that a distribution over an optimal control input sequence can be approximated by a Gaussian distribution. To overcome this problem, we further apply SVGD to MPPI with the spline interpolation. SVGD is based on the optimal transportation algorithm and has a property to put samples into around one optimal sample. As a result, we can achieve robust reactive navigation that quickly finds a path to avoid obstacles while keeping smoothness of control input sequence. We validate our proposals in a simulator of a quadrotor. Results show that our proposal involving both the spline interpolation and SVGD outperforms other base line methods."
RETOM: Leveraging Maneuverability for Reactive Tool Manipulation Using Wrench-Fields,"Felix Eberle, Riddhiman Laha, Haowen Yao, Abdeldjallil Naceri, Luis Felipe Cruz Figueredo, Sami Haddadin","Technical University of Munich,Technical Univerity of Munich,Technical University of Munich (TUM)",Reactive and Sensor-Based Planning,"This paper investigates the problem of effective tool manipulation for motion planning in complex human-like scenarios. Vector-field-based real-time strategies, although widely used, usually do not account for unwieldy tools or incorporate systematic methods to handle these extra maneuvers needed. Instead, we formalize the problem and propose a novel field- based reactive planner that explicitly accounts for rotational forces for seamless maneuvers based on the toolâ€™s geometry and featured points. Furthermore, we capture and encode robot performance through capability metrics and improve the same using an additional quality distribution method. This enables seamless integration of the robotâ€™s embodiment with the reactive force-torque (wrench) field giving rise to flexible tool usage in non-stationary environments. Extensive simulation analysis on a 7 DoF collaborative robot manipulating a common tool in an unorganized table-top layout reinforces our claim of robustness in stationary and non-stationary scenarios."
Optimal Prescribed-Time Control Based Reactive Planning System for Quadruped Robot Navigation,"Shaohang Xu, Wentao Zhang, Chin Pang Ho, Lijun Zhu","Huazhong University of Science and Technology,City University of Hong Kong",Reactive and Sensor-Based Planning,"In this paper, we propose a reactive planning system for quadruped robots based on prescribed-time control. The navigation of the quadruped robot is fundamentally depicted as omnidirectional movements, while a feedback control law is formulated to address any deviations the robot may encounter. In particular, our proposed feedback control system is theoretically proven to achieve convergence within a predefined finite time that is specified by the user. To further compute the optimal convergent time and the local goal state, we present a high-level planning node encompassing terrain-aware kinodynamic search and spatiotemporal trajectory optimization, which can generate collision-free, smooth, and efficient trajectories. The effectiveness of our proposed framework is validated through both numerical simulation and real-robot experiments in indoor and outdoor environments, including scenarios with cluttered obstacles, slopes, and external disturbances."
On the Fly Robotic-Assisted Medical Instrument Planning and Execution Using Mixed Reality,"Letian Ai, Yihao Liu, Mehran Armand, Amir Kheradmand, Alejandro Martin-gomez",Johns Hopkins University,Reactive and Sensor-Based Planning,"Robotic-assisted medical systems (RAMS) have gained significant attention for their advantages in alleviating surgeons' fatigue and improving patients' outcomes. These systems comprise a range of human-computer interactions, including medical scene monitoring, anatomical target planning, and robot manipulation. However, despite its versatility and effectiveness, RAMS demands expertise in robotics, leading to a high learning cost for the operator. In this work, we introduce a novel framework using mixed reality technologies to ease the use of RAMS. The proposed framework achieves real-time planning and execution of medical instruments by providing 3D anatomical image overlay, human-robot collision detection, and robot programming interface. These features, integrated with an easy-to-use calibration method for head-mounted display, improve the effectiveness of human-robot interactions. To assess the feasibility of the framework, two medical applications are presented in this work: 1) coil placement during transcranial magnetic stimulation and 2) drill and injector device positioning during femoroplasty. Results from these use cases demonstrate its potential to extend to a wider range of medical scenarios."
K-VIL: Keypoints-Based Visual Imitation Learning,"Jianfeng Gao, Zhi Tao, Noémie Jaquier, Tamim Asfour","Karlsruhe Institute of Technology (KIT),Karlsruhe Institute of Technology",Reactive and Sensor-Based Planning,"Visual imitation learning provides efficient and intuitive solutions for robotic systems to acquire novel manipulation skills. However, simultaneously learning geometric task constraints and control policies from visual inputs alone Visual imitation learning provides efficient and intuitive solutions for robotic systems to acquire novel manipulation skills. However, simultaneously learning geometric task constraints and control policies from visual inputs alone remains a challenging problem. In this paper, we propose the keypoint-based visual imitation learning (K-VIL) approach that automatically extracts sparse, object-centric, and embodiment-independent task representations from a small number of human demonstration videos. The task representation is composed of keypoint-based geometric constraints on principal manifolds, their associated local frames, and the movement primitives that are then needed for the task execution. Our approach is capable of extracting such task representations from a single demonstration video, and of incrementally updating them when new demonstrations are available. To reproduce manipulation skills using the learned set of prioritized geometric constraints in novel scenes, we introduce a novel keypoint-based admittance controller. We evaluate our approach in several real-world applications, showcasing its ability to deal with cluttered scenes, viewpoint mismatch, new instances of categorical objects, and large object pose and shape variations."
Circular Field Motion Planning for Highly-Dynamic Multi-Robot Systems with Application to Robot Soccer,"Fabrice Zeug, Marvin Becker, Matthias A. Muller",Gottfried Wilhelm Leibniz Universität Hannover,Reactive and Sensor-Based Planning,"The rise of autonomous driving in everyday life makes efficient and collision-free motion planning more important than ever. However, multi robot applications in highly dynamic environments still pose hard challenges for state-of-the-art motion planners. In this paper, we present a new iteration of a reactive circular fields motion planner with the focus on simultaneous control of multiple robots in robotic soccer games, which is able to operate omnidirectional robots safely and efficiently despite high measurement delays and inaccuracies. Our extension enables the definition and effective execution of complex tasks in soccer specific problems. We extensively evaluated our planner in several complex simulation environments and experimentally verified the approach in realistic scenarios on real soccer robots. Furthermore, we demonstrated the capabilities of our motion planner during the successful participation in the RoboCup 2022 and 2023."
Autonomous Mapless Navigation on Uneven Terrains,"Hassan Jardali, Mahmoud Ali, Lantao Liu",Indiana University,Reactive and Sensor-Based Planning,"We propose a new method for autonomous navigation in uneven terrains by utilizing a sparse Gaussian Process (SGP) based local perception model. The SGP local perception model is trained on local ranging observation (pointcloud) to learn the terrain elevation profile and extract the feasible navigation subgoals around the robot. Subsequently, a cost function, which prioritizes the safety of the robot in terms of keeping the robot's roll and pitch angles bounded within a specified range, is used to select a safety-aware subgoal that leads the robot to its final destination. The algorithm is designed to run in real-time and is intensively evaluated in simulation and real-world experiments. The results compellingly demonstrate that our proposed algorithm consistently navigates uneven terrains with high efficiency and surpasses the performance of other planners. The implementation of our method, including the supplementary video showing the experimental and real-world results, is available at https://rb.gy/3ov2r8."
Optimal Control of Granular Material,"Yuichiro Aoyama, Amin Haeri, Evangelos Theodorou","Georgia Institute of Technology,Concordia University",Optimization and Optimal Control III,"The control of granular materials, which are found in many industrial applications, is a challenging open research problem. Granular material systems are complex-behavior (as they could have solid-, fluid-, and gas-like behaviors) and high-dimensional (as they could have many grains/particles with at least 3 DOF in 3D) systems. Recently, a machine learning-based Graph Neural Network (GNN) simulator has been proposed to learn the underlying dynamics. In this paper, we perform optimal control of a rigid body-driven granular material system whose dynamics is learned by a GNN model trained by reduced data generated via a physics-based simulator and Principal Component Analysis (PCA). We use Differential Dynamic Programming (DDP) to obtain optimal control commands that can form granular particles into a target shape. The model and results are shown to be relatively fast and accurate. The control commands are also applied to the ground truth model, i.e., physics-based simulator, to further validate the approach."
CACTO: Continuous Actor-Critic with Trajectory Optimization---Towards Global Optimality,"Gianluigi Grandesso, Elisa Alboni, Gastone Pietro Rosati Papini, Patrick Wensing, Andrea Del Prete","University of Trento,University of Notre Dame",Optimization and Optimal Control III,"This paper presents a novel algorithm for the continuous control of dynamical systems that combines Trajectory Optimization (TO) and Reinforcement Learning (RL) in a single framework. The motivations behind this algorithm are the two main limitations of TO and RL when applied to continuous nonlinear systems to minimize a non-convex cost function. Specifically, TO can get stuck in poor local minima when the search is not initialized close to a â€œgoodâ€ minimum. On the other hand, when dealing with continuous state and control spaces, the RL training process may be excessively long and strongly dependent on the exploration strategy. Thus, our algorithm learns a â€œgoodâ€ control policy via TO-guided RL policy search that, when used as initial guess provider for TO, makes the trajectory optimization process less prone to converge to poor local optima. Our method is validated on several reaching problems featuring non-convex obstacle avoidance with different dynamical systems, including a car model with 6D state, and a 3-joint planar manipulator. Our results show the great capabilities of CACTO in escaping local minima, while being more computationally efficient than the Deep Deterministic Policy Gradient (DDPG) and Proximal Policy Optimization (PPO) RL algorithms."
Learning Model Predictive Control with Error Dynamics Regression for Autonomous Racing,"Haoru Xue, Edward Zhu, John Dolan, Francesco Borrelli","Carnegie Mellon University,University of California, Berkeley",Optimization and Optimal Control III,"This work presents a novel Learning Model Predictive Control (LMPC) strategy for autonomous racing at the handling limit that can iteratively explore and learn unknown dynamics in high-speed operational domains. We start from existing LMPC formulations and modify the system dynamics learning method. In particular, our approach uses a nominal, global, nonlinear, physics-based model with a local, linear, data-driven learning of the error dynamics. We conduct experiments in simulation, 1/10th scale hardware, and deployed the proposed LMPC on a full-scale autonomous race car used in the Indy Autonomous Challenge (IAC) with closed loop experiments at the Putnam Park Road Course in Indiana, USA. The results show that the proposed control policy exhibits improved robustness to parameter tuning and data scarcity. Incremental and safety-aware exploration toward the limit of handling and iterative learning of the vehicle dynamics in high-speed domains is observed both in simulations and experiments."
Safe Non-Stochastic Control of Control-Affine Systems: An Online Convex Optimization Approach,"Hongyu Zhou, Yichen Song, Vasileios Tzoumas","University of Michigan,Boston University,University of Michigan, Ann Arbor",Optimization and Optimal Control III,"We study how to safely control nonlinear control-affine systems that are corrupted with bounded non-stochastic noise, i.e., noise that is unknown a priori and that is not necessarily governed by a stochastic model.We focus on safety constraints that take the form of time-varying convex constraints such as collision-avoidance and control-effort constraints. We provide an algorithm with bounded dynamic regret, i.e., bounded suboptimality against an optimal clairvoyant controller that knows the realization of the noise a priori. We are motivated by the future of autonomy where robots will autonomously perform complex tasks despite real-world unpredictable disturbances such as wind gusts. To develop the algorithm, we capture our problem as a sequential game between a controller and an adversary, where the controller plays first, choosing the control input, whereas the adversary plays second, choosing the noise's realization. The controller aims to minimize its cumulative tracking error despite being unable to know the noise's realization a priori. We validate our algorithm in simulated scenarios of (i) an inverted pendulum aiming to stay upright, and (ii) a quadrotor aiming to fly to a goal location through an unknown cluttered environment."
Robust Balancing Control of Biped Robots for External Forces,"Hae Yeon Park, Jung Hoon Kim","POSTECH,Pohang University of Science and Technology",Optimization and Optimal Control III,"This paper develops a controller synthesis method for ensuring an admissible bound of external forces on biped robots in a desired level. We first introduce the authors` preceding results on the norm-based stability criterion for a biped walking constructed on its linear inverted pendulum model (LIPM). More precisely, an induced norm can be taken to formulate the fact that the balance for a biped robot is achieved if its zero moment point (ZMP) always stays in the supporting region at each step. Based on this norm-based criterion, we aim at making the maximum energy of external forces admissible for balancing the biped robot be a pregiven desired bound gamma(> 0). To achieve this objective, a robust controller is designed through the linear matrix inequality (LMI)-based approach. More importantly, a necessary and sufficient condition for the existence of a robust controller leading to the desired bound is characterized by some LMI conditions. The effectiveness of the overall arguments is validated through some comparative simulation results of a biped walking robot with external forces."
Robust Policy Iteration of Uncertain Interconnected Systems with Imperfect Data,"Omar Qasem, Weinan Gao","American International University,Florida Institute of Technology",Optimization and Optimal Control III,"This paper investigates the robust optimal control problem of a class of continuous-time, partially linear, interconnected systems. In addition to the dynamic uncertainties resulted from the interconnected dynamic system, unknown bounded disturbances and computational errors are taken into account throughout the learning process, wherein the systemâ€™s dynamics are also assumed unknown. These challenges lead the collected online data to be imperfect. In this scenario, traditional data-driven control techniques, such as adaptive dynamic programming (ADP) and robust ADP, encounter a challenge in learning the optimal control policy precisely due to imperfect data. In this paper, a novel data-driven robust policy iteration method is proposed to solve the robust optimal control problems. Without relying on the knowledge of the systemâ€™s dynamics, the external disturbances or the complete state, the implementation of the proposed method only needs to access the input and partial state information. Based on the small-gain theorem, the notions of strong unboundedness observability and input-to-output stability, it is guaranteed that the learned robust optimal control gain is stabilizing and that the solution of the closed-loop system is uniformly ultimately bounded despite the existence of dynamic uncertainties and unknown external disturbances. The simulation results reveal the efficiency and practicality of the proposed data-driven control method."
Robust Co-Design of Canonical Underactuated Systems for Increased Certifiable Stability,"Federico Girlanda, Shivesh Kumar, Lasse Maywald, Frank Kirchner","University of Padua,DFKI GmbH,Deutsches Forschungszentrum für Künstliche Intelligenz,University of Bremen",Optimization and Optimal Control III,"Optimal behaviours of a system to perform a specific task can be achieved by leveraging the coupling between trajectory optimization, stabilization and design optimization. This approach proves particularly advantageous for underactuated systems, which are systems that have fewer actuators than degrees of freedom and thus require for more elaborate control systems. This paper proposes a novel co-design algorithm, namely Robust Trajectory Control with Design optimization (RTC-D). An inner optimization layer (RTC) simultaneously performs direct transcription (DIRTRAN) to find a nominal trajectory while computing optimal hyperparameters for a stabilizing time-varying linear quadratic regulator (TVLQR). RTC-D augments RTC with a design optimization layer, maximizing the systemâ€™s robustness through a time-varying Lyapunov-based region of attraction (ROA) analysis. This analysis provides a formal guarantee of stability for a set of off-nominal states. The proposed algorithm has been tested on two different underactuated systems: the torque-limited simple pendulum and the cart-pole. Extensive simulations of off-nominal initial conditions demonstrate improved robustness, while real-system experiments show increased insensitivity to torque disturbances."
Whole-Body Ergodic Exploration with a Manipulator Using Diffusion,"Cem Bilaloglu, Tobias Löw, Sylvain Calinon","Idiap Research Institute, École Polytechnique Fédérale de Lausan,Idiap Research Institute, EPFL,Idiap Research Institute",Optimization and Optimal Control III,"This paper presents a whole-body robot control method for exploring and probing a given region of interest. The ergodic control formalism behind such an exploration behavior consists of matching the time-averaged statistics of a robot trajectory with the spatial statistics of the target distribution. Most existing ergodic control approaches assume the robots/sensors as individual point agents moving in space. We introduce an approach that decomposes the whole-body of a robotic manipulator into multiple kinematically constrained agents. Then, we generate control actions by calculating a consensus among the agents. To do so, we use an ergodic control formulation called heat equation-driven area coverage (HEDAC) and slow the diffusion using the non-stationary heat equation. Our approach extends HEDAC to applications where robots have multiple sensors on the whole-body (such as tactile skin) and use all sensors to optimally explore the given region. We show that our approach increases the exploration performance in terms of ergodicity and scales well to real-world problems. We compare our method in kinematic simulations with the state-of-the-art and demonstrate the applicability of an online exploration task with a 7-axis Franka Emika robot."
ReLU-QP: A GPU-Accelerated Quadratic Programming Solver for Model-Predictive Control,"Arun Bishop, John Zhang, Swaminathan Gurumurthy, Kevin Tracy, Zachary Manchester",Carnegie Mellon University,Optimization and Optimal Control III,"We present ReLU-QP, a GPU-accelerated solver for quadratic programs (QPs) that is capable of solving high-dimensional control problems at real-time rates. ReLU-QP is derived by exactly reformulating the Alternating Direction Method of Multipliers (ADMM) algorithm for solving QPs as a deep, weight-tied neural network with rectified linear unit (ReLU) activations. This reformulation enables the deployment of ReLU-QP on GPUs using standard machine-learning toolboxes. We evaluate the performance of ReLU-QP across three model-predictive control (MPC) benchmarks: stabilizing random linear dynamical systems with control limits, balancing an Atlas humanoid robot on a single foot, and performing a whole-body pick-up motion on a quadruped equipped with a six-degree-of-freedom arm. These benchmarks indicate that ReLU-QP is competitive with state-of-the-art CPU-based solvers for small-to-medium-scale problems and offers order-of-magnitude speed improvements for larger-scale problems."
Markerless Ultrasound Probe Pose Estimation in Mini-Invasive Surgery,"Mohammad Mahdi Kalantari, Erol Ozgur, Mohammad Alkhatib, Emmanuel Buc, Bertrand Le Roy, Richard Modrzejewski, Youcef Mezouar, Adrien Bartoli","Clermont Auvergne INP, CNRS, SIGMA Clermont, Institut Pascal,SIGMA-Clermont / Institut Pascal,Université Clermont Auvergne,University Hospital of Clermont-Ferrand, France,University Hospital of Saint-Etienne, France,SurgAR,Clermont Auvergne INP - SIGMA Clermont,UCA",Surgical Robotics I,"In mini-invasive surgery, the laparoscopic ultrasound probe is visible in the laparoscopic image. We address the problem of estimating the probe pose with respect to the laparoscope without using markers and additional sensors. We propose the first method using a single standard laparoscopic monocular RGB image. It is robust, initialization-free and runs at 10 fps, thus forming a promising tool to improve robotic and augmented reality-based surgery."
Occlusion-Robust Autonomous Robotic Manipulation of Human Soft Tissues with 3D Surface Feedback,"Junlei Hu, Dominic Jones, Mehmet Dogar, Pietro Valdastri",University of Leeds,Surgical Robotics I,"Robotic manipulation of 3D soft objects remains challenging in both the industrial and medical fields. Various methods based on mechanical modelling, data-driven approaches or explicit feature tracking have been proposed. A unifying disadvantage of these methods is the high computational cost of simultaneous imaging processing, identification of mechanical properties, and motion planning, leading to a need for less computationally intensive methods. We propose a method for autonomous robotic manipulation with 3D surface feedback to solve these issues. First, we produce a deformation model of the manipulated object, which estimates the robotsâ€™ movements by monitoring the displacement of surface points surrounding the manipulators. Then we develop a 6-degree-of-freedom velocity controller to manipulate the grasped object to achieve a desired shape. To validate our approach, we conduct comparative simulations with existing methods and perform experiments using phantom and cadaveric soft tissues with the da Vinci Research Kit. The results demonstrate the robustness of the method to occlusions and various materials. Compared to state-of-the-art linear and data-driven methods, our approach is more precise by 46.5% and 15.9%, and saves 55.2% and 25.7% manipulation time, respectively."
Sensorless Transparency Optimized Haptic Teleoperation on the Da Vinci Research Kit,"Nural Yilmaz, Brendan Burkhart, Anton Deguet, Peter Kazanzides, Ugur Tumerdem","Marmara University,Johns Hopkins University",Surgical Robotics I,"The da Vinci surgical robot introduced remote control of instruments, providing surgeons with increased dexterity and precision. A major drawback, however, is the loss of sense of touch due to a lack of kinesthetic coupling between the surgical field and the surgeon. This paper presents a framework for sensorless transparency optimized four channel teleoperation. It is sensorless because forces are estimated from existing actuator feedback, with a deep network for dynamics identification. Performance is further optimized by introducing robust acceleration control, with disturbance observers. Experiments performed on the da Vinci Research Kit (dVRK), an open research platform based on the clinically deployed robotic hardware, show improvements in control, force estimation and reflection. The significance is that we demonstrate that high-performance bilateral teleoperation is feasible in clinical systems, without hardware changes, and is available to the dVRK community through a software update."
Multimodal Transformers for Real-Time Surgical Activity Prediction,"Keshara Weerasinghe, Seyed Hamidreza Roodabeh, Kay Hutchinson, Homa Alemzadeh",University of Virginia,Surgical Robotics I,Real-time recognition and prediction of surgical activities are fundamental to advancing safety and autonomy in robot-assisted surgery. This paper presents a multimodal transformer architecture for real-time recognition and prediction of surgical gestures and trajectories based on short segments of kinematic and video data. We conduct an ablation study to evaluate the impact of fusing different input modalities and their representations on gesture recognition and prediction performance. We perform an end-to-end assessment of the proposed architecture using the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) dataset. Our model outperforms the state-of-the-art (SOTA) with 89.5% accuracy for gesture prediction through effective fusion of kinematic features with spatial and contextual video features. It achieves the real-time performance of 1.1-1.3ms for processing a 1-second input window by relying on a computationally efficient model.
Learning Needle Pick-And-Place without Expert Demonstrations,"Rokas Bendikas, Valerio Modugno, Dimitrios Kanoulas, Francisco Vasconcelos, Danail Stoyanov","UCL,University College London",Surgical Robotics I,"We introduce a novel approach for learning a complex multi-stage needle pick-and-place manipulation task for surgical applications using Reinforcement Learning without expert demonstrations or explicit curriculum. The proposed method is based on a recursive decomposition of the original task into a sequence of sub-tasks with increasing complexity and utilizes an actor-critic algorithm with deterministic policy output. In this work, exploratory bottlenecks have been used by a human expert as convenient boundary points for partitioning complex tasks into simpler subunits. Our method has successfully learnt a policy for the needle pick-and-place task, whereas the state-of-the-art TD3+HER method is unable to achieve success without the help of expert demonstrations. Comparison results show that our method achieves the highest performance with a 91% average success rate."
Learning Nonprehensile Dynamic Manipulation: Sim2real Vision-Based Policy with a Surgical Robot,"Radian Gondokaryono, Mustafa Haiderbhai, Sai Aneesh Suryadevara, Lueder Alexander Kahrs","University of Toronto,Indian Institute of Technology Bombay,University of Toronto Mississauga",Surgical Robotics I,"Surgical tasks such as tissue retraction, tissue exposure, and needle suturing remain challenging in autonomous surgical robotics. One challenge in these tasks is nonprehensile manipulation such as pushing tissue, pressing cloth, and needle threading. In this work, we isolate the problem of nonprehensile manipulation by implementing a vision-based reinforcement learning agent for rolling a block, a task that has complex dynamics interactions, small scale objects, and a narrow field of view. We train agents in simulation with a reward formulation that encourages efficient and safe learning, domain randomization that allows for robust sim2real transfer, and a recurrent memory layer that enables reasoning about randomized dynamics parameters. We successfully transfer our agents from simulation to real and show robust execution of our vision-based policy with a 96.3% success rate. We analyze and discuss the success rate, trajectories, and recovery behaviours for various models that are either using the recurrent memory layer or are trained with a difficult physics environment. Further project information is available at href{https://medcvr.utm.utoronto.ca/ral2023-rollblock.html}{https://medcvr.utm.utoronto.ca/ral2023-rollblock.html}."
Realistic Data Generation for 6D Pose Estimation of Surgical Instruments,"Juan Antonio Barragan, Jintan Zhang, Haoying Zhou, Adnan Munawar, Peter Kazanzides","Johns Hopkins University,Worcester Polytechnic Institute",Surgical Robotics I,"Automation in surgical robotics has the potential to improve patient safety and surgical efficiency, but it is difficult to achieve due to the need for robust perception algorithms. In particular, 6D pose estimation of surgical instruments is critical to enable the automatic execution of surgical maneuvers based on visual feedback. In recent years, supervised deep learning algorithms have shown increasingly better performance at 6D pose estimation tasks; yet, their success depends on the availability of large amounts of annotated data. In household and industrial settings, synthetic data, generated with 3D computer graphics software, has been shown as an alternative to minimize annotation costs of 6D pose datasets. However, this strategy does not translate well to surgical domains as commercial graphics software have limited tools to generate images depicting realistic instrument-tissue interactions. To address these limitations, we propose an improved simulation environment for surgical robotics that enables the automatic generation of large and diverse datasets for 6D pose estimation of surgical instruments. Among the improvements, we developed an automated data generation pipeline and an improved surgical scene. To show the applicability of our system, we generated a dataset of 7.5k images with pose annotations of a surgical needle that was used to evaluate a state-of-the-art pose estimation network. The trained model obtained a mean translational error of 2.59mm on a challenging dataset that presented varying levels of occlusion. These results highlight our pipeline's success in training and evaluating novel vision algorithms for surgical robotics applications."
Surgical Gym: A High-Performance GPU-Based Platform for Reinforcement Learning with Surgical Robots,"Samuel Schmidgall, Axel Krieger, Jason Eshraghian","Johns Hopkins University,University of California, Santa Cruz",Surgical Robotics I,"Recent advances in robot-assisted surgery have resulted in progressively more precise, efficient, and minimally invasive procedures, sparking a new era of robotic surgical intervention. This enables doctors, in collaborative interaction with robots, to perform traditional or minimally invasive surgeries with improved outcomes through smaller incisions. Recent efforts are working toward making robotic surgery more autonomous which has the potential to reduce variability of surgical outcomes and reduce complication rates. Deep reinforcement learning methodologies offer scalable solutions for surgical automation, but their effectiveness relies on extensive data acquisition due to the absence of prior knowledge in successfully accomplishing tasks. Due to the intensive nature of simulated data collection, previous works have focused on making existing algorithms more efficient. In this work, we focus on making the simulator more efficient, making training data much more accessible than previously possible. We introduce Surgical Gym, an open-source high performance platform for surgical robot learning where both the physics simulation and reinforcement learning occur directly on the GPU. We demonstrate between 100-5000x faster training times compared with previous surgical learning platforms. The code is available at: https://github.com/SamuelSchmidgall/SurgicalGym."
Multi-Objective Cross-Task Learning Via Goal-Conditioned GPT-Based Decision Transformers for Surgical Robot Task Automation,"Jiawei Fu, Yonghao Long, Kai Chen, Wang Wei, Qi Dou","Institute of Artificial Intelligence and Robotics,The Chinese University of Hong Kong",Surgical Robotics I,"Surgical robot task automation has been a promising research topic for improving surgical efficiency and quality. Learning-based methods have been recognized as an interesting paradigm and been increasingly investigated. However, existing approaches encounter difficulties in long-horizon goal-conditioned tasks due to the intricate compositional structure, which requires decision-making for a sequence of sub-steps and understanding of inherent dynamics of goal-reaching tasks. In this paper, we propose a new learning-based framework by leveraging the strong reasoning capability of the GPT-based architecture to automate surgical robotic tasks. The key to our approach is developing a goal-conditioned decision transformer to achieve sequential representations with goal-aware future indicators in order to enhance temporal reasoning. Moreover, considering to exploit a general understanding of dynamics inherent in manipulations, thus making the model's reasoning ability to be task-agnostic, we also design a cross-task pretraining paradigm that uses multiple training objectives associated with data from diverse tasks. We have conducted extensive experiments on 10 tasks using the surgical robot learning simulator SurRoL. The results show that our new approach achieves promising performance and task versatility compared to existing methods. The learned trajectories can be deployed on the da Vinci Research Kit (dVRK) for validating its practicality in real surgical robot settings. Our project website is at: https://med-air.github.io/SurRoL."
Safe-By-Design Digital Twins for Human-Robot Interaction: A Use Case for Humanoid Service Robots,"Jon Škerlj, Mazin Hamad, Jean Elsner, Abdeldjallil Naceri, Sami Haddadin","Technical University of Munich,Technical University of Munich (TUM)",Safety in HRI,"Integrating humanoid service mobile robots into human environments presents numerous challenges, primarily concerning the safety of interactions between robots and humans. To address these safety concerns, we propose a novel approach that leverages the capabilities of digital twin technology by tailoring it to incorporate comprehensive and robust safety concepts. This paper introduces a ``safe-by-design'' digital twin that operates alongside the real twin robot in the loop, engaging real-time safety framework during physical interactions with the surrounding environment, including humans. To validate the effectiveness of our proposed safe-by-design digital twin framework, we conducted experiments using a humanoid service mobile robot alongside simulated human counterparts. Our results demonstrate the capability of the integrated impact safety module within the proposed digital twin approach to limit the velocities of both the robot's base and arms, adhering to injury biomechanics-based safety thresholds. These findings emphasize the promise of our proposed approach for ensuring the physical safety of humanoid service mobile robots operating in dynamic human environments. It enables the digital twin to preemptively identify potential safety hazards and formulate safe intervention actions to ensure the robot's compliance with safety regulations, paving the way for safer and more widespread adoption of robotic systems in various service domains."
Safe Execution of Learned Orientation Skills with Conic Control Barrier Functions,"Zheng Shen, Matteo Saveriano, Fares Abu-Dakka, Sami Haddadin","TU Munich,University of Trento,Mondragon University,Technical University of Munich",Safety in HRI,"In the field of Learning from Demonstration (LfD), Dynamical Systems (DSs) have gained significant attention due to their ability to generate real-time motions and reach predefined targets. However, the conventional convergence-centric behavior exhibited by DSs may fall short in safety-critical tasks, specifically, those requiring precise replication of demonstrated trajectories or strict adherence to constrained regions even in the presence of perturbations or human intervention. Moreover, existing DS research often assumes demonstrations solely in Euclidean space, overlooking the crucial aspect of orientation in various applications. To alleviate these shortcomings, we present an innovative approach geared toward ensuring the safe execution of learned orientation skills within constrained regions surrounding a reference trajectory. This involves learning a stable DS on SO(3), extracting time-varying conic constraints from the variability observed in expert demonstrations, and bounding the evolution of the DS with Conic Control Barrier Function (CCBF) to fulfill the constraints. We validated our approach through extensive evaluation in simulation and showcased its effectiveness for a cutting skill in the context of assisted teleoperation."
Real-Time Batched Distance Computation for Time-Optimal Safe Path Tracking,"Shohei Fujii, Quang-Cuong Pham","Nanyang Technological University, DENSO Corporation,NTU Singapore",Safety in HRI,"In human-robot collaboration, there has been a trade-off relationship between the speed of collaborative robots and the safety of human workers. In our previous paper, we introduced a time-optimal path tracking algorithm designed to maximize speed while ensuring safety for human workers. This algorithm runs in real-time and provides the safe and fastest control input for every cycle with respect to ISO standards. However, true optimality has not been achieved due to inaccurate distance computation resulting from conservative model simplification. To attain true optimality, we require a method that can compute distances 1. at many robot configurations to examine along a trajectory 2. in real-time for online robot control 3. as precisely as possible for optimal control. In this paper, we propose a batched, fast and precise distance checking method based on precomputed link-local SDFs. Our method can check distances for 500 waypoints along a trajectory within less than 1 millisecond using a GPU at runtime, making it suited for time-critical robotic control. Additionally, a neural approximation has been proposed to accelerate preprocessing by a factor of 2. Finally, we experimentally demonstrate that our method can navigate a 6-DoF robot earlier than a geometric-primitives-based distance checker in a dynamic and collaborative environment."
Overcoming Hand and Arm Occlusion in Human-To-Robot Handovers: Predicting Safe Poses with a Multimodal DNN Regression Model,"Catherine Lollett, Advaith Sriram, Mitsuhiro Kamezaki, Shigeki Sugano","Waseda University,The University of Tokyo",Safety in HRI,"Handovers play a key role in human-robot interactions. However, current research focuses on visible-hand handovers, thereby heavily relying on hand detection. Large objects in human-robot interactions present a unique challenge: they inherently block the person's hands and arms from the robot's view. This occlusion raises the robot's risk of unintended physical contact with the person, leading to discomfort and safety concerns. This study aims to develop a model that can determine a pose for the robot that ensures a handover that avoids physical contact with the person, especially in scenarios when hands and arms are occluded. Toward this goal, a three-branch multimodal Deep Neural Network (DNN) regression model was implemented. First, a robust human-pose keypoints detection to calculate shoulder-elbow angles is applied. Secondly, we extract the refined object's segmented mask. Thirdly, we compute two intrinsic object properties. The concatenated outputs from these branches pass through extra dense layers, resulting in the prediction of the robot's 14 arms-joint angles. Compared to an only keypoint data processed-based model, our multimodal approach made a 17.7% accuracy improvement. The experiments highlight each pipeline step's significance, showing important results even when hands and arms were heavily occluded, adjusting to different variations."
Legible and Proactive Robot Planning for Prosocial Human-Robot Interactions,"Jasper Geldenbott, Karen Yan Ming Leung",University of Washington,Safety in HRI,"Humans have a remarkable ability to fluently engage in joint collision avoidance in crowded navigation tasks despite the complexities and uncertainties inherent in human behavior. Underlying these interactions is a mutual understanding that (i) individuals are prosocial, that is, there is equitable responsibility in avoiding collisions, and (ii) individuals should behave legibly, that is, move in a way that clearly conveys their intent to reduce ambiguity in how they intend to avoid others. Toward building robots that can safely and seamlessly interact with humans, we propose a general robot trajectory planning framework for synthesizing legible and proactive behaviors and demonstrate that our robot planner naturally leads to prosocial interactions. Specifically, we introduce the notion of a markup factor to incentivize legible and proactive behaviors and an inconvenience budget constraint to ensure equitable collision avoidance responsibility. We evaluate our approach against well-established multi-agent planning algorithms and show that using our approach produces safe, fluent, and prosocial interactions. We demonstrate the real-time feasibility of our approach with human-in-the-loop simulations. Project page can be found at https://uw-ctrl.github.io/phri/."
Integrated Data-Driven Inference and Planning-Based Human Motion Prediction for Safe Human-Robot Interaction,"Youngim Nam, Cheolhyeon Kwon",Ulsan National Institute of Science and Technology,Safety in HRI,"This paper presents a unified prediction and planning algorithm for an autonomous vehicle to interact with an uncertain human-driven vehicle. Predicting human motion is challenging due to inherent uncertainties in diverse human internal states, i.e., driving styles and rationality. To address these complexities, we propose a hierarchical prediction strategy that combines data-driven internal state inference and planning-based human motion prediction. First, we employ Long Short Term Memory Networks (LSTM) based inference modules to capture both driving styles and rationality from the observed motion of human driver. With these inferred internal states, we predict the future trajectories of human-driven vehicle by formulating a human planning model as an optimization problem. Lastly, we present a Stochastic Model Predictive Control (SMPC) for the autonomous vehicle to safely interact with the human-driven vehicle while actively inferring human internal states. The simulation results, demonstrating the lane change scenarios, indicate the proposed method outperforms the existing work in both predicting the human motion and achieving the robot's goal."
How Does Perception Affect Safety: New Metrics and Strategy,"Xiaotong Zhang, Jinger Chong, Kamal Youcef-Toumi",Massachusetts Institute of Technology,Safety in HRI,"Perception plays a pivotal role in enhancing the functionality of autonomous agents. However, the intricate relationship between robotic perception metrics and actuation metrics remains unclear, leading to ambiguity in the development and fine-tuning of perception algorithms. In this paper, we introduce a methodology for quantifying this relationship, taking into account factors such as detection rate, detection quality, and latency. Furthermore, we introduce two novel perception metrics for Human-Robot Collaboration safety predicated upon basic perception metrics: Critical Collision Probability (CCP) and Average Collision Probability (ACP). To validate the utility of these metrics in facilitating algorithm development and tuning, we develop an attentive processing strategy that focuses exclusively on key input features. This approach significantly reduces computational time while preserving a similar level of accuracy. Experimental findings demonstrate that integrating this strategy into an object detector results in a notable maximum reduction of 30.09% in inference time and 26.53% in total time per frame. Additionally, the strategy lowers the CCP and ACP in a baseline model by 11.25% and 13.50%, respectively."
Constrained Passive Interaction Control: Leveraging Passivity and Safety for Robot Manipulators,"Zhiquan Zhang, Tianyu Li, Nadia Figueroa",University of Pennsylvania,Safety in HRI,"Passivity is necessary for robots to fluidly collaborate and interact with humans physically. Nevertheless, due to the unconstrained nature of passivity-based impedance control laws, the robot is vulnerable to infeasible and unsafe configurations upon physical perturbations. In this paper, we propose a novel control architecture that allows a torque-controlled robot to guarantee safety constraints such as kinematic limits, self-collisions, external collisions and singularities and is passive only when feasible. This is achieved by constraining a dynamical system based impedance control law with a relaxed hierarchical control barrier function quadratic program subject to multiple concurrent, possibly contradicting, constraints. Joint space constraints are formulated from efficient data-driven self- and external C^2 collision boundary functions. We theoretically prove constraint satisfaction and show that the robot is passive when feasible. Our approach is validated in simulation and real robot experiments on a 7DoF Franka Research 3 manipulator."
Boosting Adversarial Training in Safety-Critical Systems through Boundary Data Selection,"Yifan Jia, Christopher M. Poskitt, Peixin Zhang, Jingyi Wang, Jun Sun, Sudipta Chattopadhyay","Zhejiang University,Singapore Management University,Singapore University of Technology and Design",Safety in HRI,"AI-enabled collaboration robots are designed to be used in close collaborations with humans, thus demanding stringent safety standards and quick response times. However, adversarial attacks pose a significant threat to deep learning models of AI-enabled industrial systems, making it crucial to develop methods to improve the models' robustness against them. Adversarial training is one approach to achieve this, but its effectiveness heavily relies on the quality of the training data, which can be expensive to acquire. In this work, we try to balance the need for quality data with the goal to minimize cost by selecting the most `important' of it for adversarial training. In particular, we propose a task-based robust fast (RAST) learning method that selects training data near to the boundary by considering adversarial samples. Our method improves the speed of model training on CIFAR-10 by 68.67%, and compared to other data selection methods, has 10% higher accuracy with 10% training data selected, and 7% higher robustness with 4% training data selected. Our method also significantly improves efficiency by at least 25% on adversarial training with the same performance. Finally, we evaluate our method on a physical robotic arm system with object detection, generating adversarial patches as our attack, and adopting our method as the defense. We find that RAST can defend against 60% of untargeted attacks and 20% of targeted attacks. Therefore, our work highlights the benefits of"
Automated Assembly by Two-Fingered Microhand for Fabrication of Soft Magnetic Microrobots,"Yue Zhao, Xiaoming Liu, Ruixi Wang, Dan Liu, Masaru Kojima, Qiang Huang, Tatsuo Arai","Beijing Institute of Technology,Tsinghua University,Osaka University,University of Electro-Communications",Micro/Nano Robots I,"Micro-assembly is an emerging method to fabricate microrobots with multiple modules or particles. However, there is always a lack of a flexible and efficient method to freely create the desired magnetic soft microrobots. In this paper, an automated assembly system based on a two-fingered microhand is presented for fabricating magnetic soft microrobots. Our proposed system can automatically pick and place components to assemble microrobots with a two-fingered micromanipulator, and orient these components through an external magnetic field. The automated assembly has the advantages of high accuracy, high speed, and high success rate. It can endow magnetic microrobots with flexible material selection, arbitrary geometry design, and programable magnetization profile. We can make full use of this system to fabricate multiple magnetic soft microrobots. The experiment results demonstrate that this system can efficiently fabricate microrobots with excellent mechanical properties, which have application potential in robotics, biomedical engineering, and environmental governance."
Thin-Film NiTi Microactuator with a Magnetic Spring for a Tiny Launcher Mechanism,"Sukjun Kim, Sarah Bergbreiter","University of California, San Diego,Carnegie Mellon University",Micro/Nano Robots I,"In this work, we present a thin-film shape memory alloy (NiTi) microactuator with a magnetic spring. This novel actuator design utilizes two permanent magnets and 3D-printed magnet holders to effectively apply a tensile strain on the NiTi thin-film. This actuator is expected to generate 8.7 mN of blocking force, and a free displacement of 30 Î¼m is experimentally characterized. The actuator leverages bare NiTi film (âˆ¼ 1 Î¼m thick) for actuation, enabling a high actuator bandwidth up to 50 Hz. A comprehensive analytical model is also studied, which was then validated by comparing to the experimental results. A launcher mechanism was designed and integrated with the NiTi actuator, and this mechanism was used to launch a microscale projectile (a salt grain) thereby demonstrating the relative high power actuation achievable with thin-film NiTi."
"Nature-Inspired Bubble Magnetic Microrobots for Multimode Locomotion, Cargo Delivery, Imaging, and Biosensing","Zichen Xu, Qingsong Xu, Hon Ho Yu","University of Macau,Kiang Wu Hospital",Micro/Nano Robots I,"Wirelessly actuated magnetic microrobots are promising tools in medical applications due to their tiny sizes and attractive robotic properties. However, it remains a huge challenge to integrate sufficient functionalities in a limited volume. Microscopic natural phenomenon is a great reference for current microrobot design, where the underlying intelligence and subtlety spurs related modern artificial systems. Inspired by air bubbles in nature, herein, we report a kind of novel magnetic air bubble microrobots. The air bubble-based structure enables multiple functionalities including cargo delivery, multimode locomotion, micromanipulation, medical imaging, and biosensing. The proposed microrobot is essentially Pickering bubbles composed of magnetic particles and air bubbles. Their hollow structures help produce lighter microrobots with density less than 1 g/cm$^3$, enabling buoyancy-based self-propulsion. Buoyancy and magnetic forces actuation enables flexible 3D locomotion in fluidic environments. Experimental results show that the microrobots can be controlled properly for designated assignments. Furthermore, the introduction of air bubble enhances ultrasound imaging, facilitating further in vivo applications. These findings offer a significant microrobot design paradigm by exploiting natural physical intelligence at the small scale."
Magnetic Mobile Micro-Gripping MicroRobots (MMÎ¼GRs) with Two Independent Magnetic Actuation Modes,"Aaron C. Davis, Emmett Freeman, Dave Cappelleri",Purdue University,Micro/Nano Robots I,"In this paper, we introduce magnetic mobile micro-gripping microrobots with two independent actuation modes. By aligning two magnets with slight variations in magnetic moment orientations, we create a net magnetic moment for precise position and orientation control through external fields, while harnessing opposing torques on the magnets to induce internal stresses needed for gripping. Our microrobot design features a compliant spring-like structure for significant deflection, enabling a gripping motion under specific magnetic field conditions. Magnet rotation allows precise control over gripper actions, returning to a default state (normally open or closed) when the magnetic field diminishes. This work advances magnetic field-controlled microrobotics, bridging the millimeter-to-micrometer gap. It holds promise for applications in microsurgery, micro-assembly, and microscale exploration."
Electroosmotic Self-Propelled Microswimmer with Magnetic Steering,"Toshiro Yamanaka, Fumihito Arai",The University of Tokyo,Micro/Nano Robots I,"Microswimmers have significant potential for medical applications such as long-term drug administration, precise surgery, and so on. The outstanding challenge is to realize power supply, propulsion, and steering mechanisms suitable for operations within the human body and microscale fluids. We propose the microswimmer composed of a self-propulsive disk-shaped module with multiple channels using biofuel cell (BFC) and electroosmotic propulsion (EOP) and a magnetic rod using magnetic steering (MS). The BFC produces an open-circuit potential (OCP) between a bioanode and a biocathode by redox reactions. The EOP generates a self-propulsive velocity due to counteracting forces of electroosmotic flows produced by the OCP in the channels arranged between the electrodes. The MS works by aligning the magnetic rod in a controlled magnetic field direction. The prototype was designed and fabricated using an insulating polymer layer, two conductive layers incorporating silver nanoparticles with anodic/cathodic enzymes, and a magnetic layer containing magnetic nanoparticles. The fast self-propulsion of continuously rotating 30 Âµm prototypes by the steering in a glucose solution was demonstrated as expected theoretically. This concept has the potential to be used as microrobots for future medical applications such as a pulling mechanism to assist in guidewire insertion or agents delivering drugs."
A Flat Tendon-Driven Continuum Microrobot for Brain Interventions,"Lorenzo Noseda, Addison Liu, Lucio Pancaldi, Mahmut Selman Sakar","EPFL,Harvard University",Micro/Nano Robots I,"Navigating biomedical instruments inside the brain remains challenging and high-risk. The delicate nature of the tissues involved requires the development of cutting- edge robotic technologies to enhance precision and safety. In response to these demands, this paper presents a novel ribbon-shaped, tendon-driven continuum microrobot designed explicitly to navigate through brain tissues. The microrobot has a cross-sectional area of 1 mm2, and its design is readily compatible with conventional microfabrication techniques for further miniaturization. The flat geometry aims to provide superior maneuverability and opens up new challenges for modeling and control. We detail the design methodology and fabrication, followed by in vitro characterization and testing within brain tissue phantoms."
A Selectively Controllable Triple-Helical Micromotor,"Hongyu Zhao, Min Ye, Bradley Nelson, Xiaopu Wang","Shenzhen Institute of Artificial Intelligence and Robotics for S,Shenzhen Institute of Artificial inteligence and Robotics for So,ETH Zurich",Micro/Nano Robots I,"Selective control mechanisms of microrobots have attracted significant attention from researchers. So far, selective control within multiple/swarm magnetic microrobots has been achieved with many strategies, such as utilizing locally specified magnetic fields, applying electrostatic anchoring, taking the advantages of geometry/wettability heterogeneity of the microrobots, etc. Using the step-out behavior of helical microrobots driven by a rotating magnetic field, researchers have proposed a mathematical model for multihelical motor that can be selectively controlled. Based on this model, we developed a micromotor that consists of three geometrically heterogeneous helices that can be selectively driven within a specific narrow frequency range. This type of micromotor shows bi-direction motion capability and has the potential to be used as an actuation unit for multiple types of functional micromechanisms."
System Identification of Space Manipulator Systems and Its Implications on Robust Control Performance,"Georgios Rekleitis, Evangelos Papadopoulos",National Technical University of Athens,Space Robotics II,"Space manipulator system (SMS) maneuvers can excite flexible appendages, while fuel sloshing effects impact its dynamics and performance. To predict this behavior and control such systems, sloshing and flexible appendages are modeled. A novel system identification scheme is developed, which identifies all parameters required for the reconstruction of system dynamics despite unmeasurable sloshing and modal states. This is achieved by two identification experiments. In Exp.1 all unmeasurable states are eliminated, while in Exp.2 the unmeasurable sloshing states are eliminated, and a novel estimator is used for the unmeasurable modal states. The significance of accurate SYSID in controller design and performance is demonstrated by simulating a 3D SMS controlled by model-based and robust controllers. In both cases, using the identified parameters results in significant robust control performance enhancement."
Model Design and Concept of Operations of Standard Interface for On-Orbit Construction,"Jingdong Zhao, Zirui Wang, Ziyi Liu, Liangliang Zhao, Qifan Duan, Hong Liu",Harbin Institute of Technology,Space Robotics II,"The construction of large-scale space facilities requires the use of on-orbit construction technology. However, several of its key components, such as standard interface design, compliant control methods, and path planning for multi-branch robots, still need improvement before practical application. This paper presents a comprehensive solution for on-orbit construction tasks, encompassing a novel standard interface, docking control method, and path planning method for space multi-branch robots. Firstly, a novel standard interface is introduced, which features multiple mating modes and a lightweight design. Additionally, a compliant docking method is provided to generate lower contact forces along the Z-direction. Furthermore, for four-armed space robots, a hierarchical planning method is proposed, which innovates in environment map construction and locomotion planning. Specifically, the closedform Minkowski sum method is employed to solve the robotâ€™s free space, and a concise locomotion method is elucidated based on transition support points. Finally, simulations and experiments are conducted."
Enabling Faster Locomotion of Planetary Rovers with a Mechanically-Hybrid Suspension,"David Rodríguez-Martínez, Kentaro Uno, Kenta Sawa, Masahiro Uda, Gen Kudo, Gustavo Hernan Diaz Huenupan, Ayumi Umemura, Shreya Santra, Kazuya Yoshida","École polytechnique fédérale de Lausanne (EPFL),Tohoku University",Space Robotics II,
RoboBall: An All-Terrain Spherical Robot with a Pressurized Shell,"Micah Oevermann, Derek Pravecek, Joseph Garrett Jibrail, Rishi Jangale, Robert Owen Ambrose",Texas A&M University,Space Robotics II,"Spherical robots are a different type of mobility platform. A spherical robot is self-contained within its shell rather than relying on a chassis with wheels to navigate. In this shell, it is completely shielded from dust and the environment. This benefit of geometric simplicity has led to the spherical robot becoming an advantageous option for all-terrain exploration and surveying. This paper focuses on a novel iteration of such a robot with a pressurized pneumatic shell design. A soft robot of this type brings benefits of a passive, compliant contact surface that can affect its performance. However, the added softness of its shell adds new unmodeled dynamics into the system that impair commonly used control schemes. This paper outlines the design and manufacture of a soft, inflatable, spherical shell designed for a robot driven by an internal 2-DOF pendulum. In addition, it presents models for controlling the pendulum and understanding the shell dynamics. The paper concludes with experimental validations of these models and field tests of the system on slopes, gravel, rough grass, and on water."
Enhanced Multifunctional Interface for Reconfigurability of Robotic Teams in Planetary Applications,"Mehmed Yüksel, Wiebke Brinkmann, Marko Jankovic, Hilmi Dogu Kücüker, Frank Kirchner","DFKI GmbH,DFKI Robotics Innovation Center Bremen,German Research Center for Artificial Intelligence GmbH (DFKI),DFKI-RIC,University of Bremen",Space Robotics II,"Exploration missions on extra terrestrial celestial bodies are to date performed by complex and heavy robotic systems. The trend is towards lighter modular systems that can be (re)configured in situ according to mission specific requirements. To facilitate flexible configurability, a multifunctional interconnect is used to mechanically couple the involved systems while providing electrical power and data transmission. The paper presents the further development of the reliable electro-mechanical interface (EMI) from the TransTerrA project, which has been proven in several field tests and reached TRL 4. Docking under loads of up to 550 N has been successfully tested with the new design. The experiments presented include undocking at various inclinations with different loads expected for the application scenario.The maximum determined static load that can be carried by the further developed EMI is 2000 N. In further experiments, new contact blocks responsible for the transfer of electrical power and data were tested for water resistance and resilience to environmental factors, as well as power and data transfer.The obtained results will be helpful in the development of a multi-functional interface suitable for lunar applications and missions having similar challenging environmental conditions"
Autonomous Perching on Flat Surfaces for Free-Flying Robots with Gecko Adhesive Gripper,"Daichi Hirano, Nobutaka Tanishima, Tony G. Chen","Japan Aerospace Exploration Agency,JAXA,Stanford University",Space Robotics II,"Gecko-inspired adhesives have the advantage of being able to grasp and release flat surfaces in a vacuum using their microwedge structures. This makes them an especially attractive solution for perching on and grasping flat objects in space for free-flying robots. To grasp and anchor onto these flat surfaces, the gripper must ensure contact between the gecko adhesives and the surface before applying the appropriate forces to activate their adhesion. However, in the case of a free-flying robot in microgravity, physical contact with the surface induces reaction forces, causing the robot to quickly bounce away from the surface. To solve this issue, we propose a simple passive mechanism and a control method of a robotic arm on a free-flying robot with a gecko adhesive gripper. The gripper utilizes a single-motor controlled tendon-driven mechanism mounted at the end of a robotic arm equipped with controllable stiffness joints and a linear spring-damper system. A free-flying robot on an air-bearing platform can successfully perch on a flat surface with a velocity of up to 72.5 mm/s and with an approach angle misalignment of up to 33.0 degrees."
VWDER: A Variable Wheel-Diameter Ellipsoidal Robot,"Ziao Qin, Jingzhou Song, Xinglong Gong, Changrui Liu","Beijing University of Posts and Telecommunications,Beijing university of Posts and Telecommunications",Space Robotics II,"In recent years, many researchers have conducted extensive research on spherical robots due to their high flexibility and anti-overturning capabilities. Nevertheless, compared with legged and traditional wheeled robots, spherical robots face certain limitations. The spherical robot is composed of a closed spherical shell structure, which makes the capacity of carrying workloads weak. At the same time, the single point contact with the ground cause the contact friction force with the ground is small, so it is hard to climb obstacles such as steps and doorsill. Therefore, we propose a new solution: the variable wheel diameter ellipsoidal robot (VWDER), which combines the characteristics of two-wheel differential driven robot and spherical robot driven by equivalent pendulum.VWDER is equipped with six retractable shell-shaped legs on each side and this innovative design allows both wheels to independently change diameter while rolling. The main frame of the VWDER can keep the top of the frame facing up under the action of equivalent pendulum during the locomotion, which makes it possible to carry workloads such as manipulator arms, cameras, IMU etc. The VWDER robot can climb steps or doorsill using its two adjacent shell-shaped legs. This paper introduces the design of the VWDER and analyzes the kinematics and dynamics of the VWDER. The experimental results verified the performance of the VWDER, including its autonomous opening and closing, obstacle crossing, automatic reorientation and slope climbing etc."
On Robust Control Laws Trade-Off Analysis for Space Manipulators with Uncertain Parameters and Flexible Appendages,"Kostas Nanos, Efstathios Chachamis, Evangelos Papadopoulos",National Technical University of Athens,Space Robotics II,"To accurately accomplish on-orbit tasks using Space Manipulator Systems (SMS), advanced model-based controllers, dependent on the knowledge of SMS parameters, can be employed. However, these parameters may change on orbit for several reasons. Also, during an SMS task, excitation of flexible appendages, such as solar panels, or fuel sloshing may introduce significant end-effector errors. Therefore, controllers robust to parametric uncertainty and disturbances are needed. A robust controller attractive due to its small computational effort is the Linear Parameter Varying (LPV) gain-scheduled controller. However, its design for spatial SMS is not trivial and has not been studied yet. Therefore, the aim of this work is to study and compare robust controllers and examine their applicability to SMS. An LPV plus Hinfcontroller is compared with a Model-Based PD, and a Model-Based PD plus Hinf controller, in the presence of parametric uncertainty, noisy measurements and disturbances, using a planar example. The criteria considered include: (i) Design Complexity, (ii) Trajectory Errors, (iii) Required Torques, and (iv) Computational Effort."
N-MPC for Deep Neural Network-Based Collision Avoidance Exploiting Depth Images,"Martin Jacquet, Kostas Alexis","NTNU,NTNU - Norwegian University of Science and Technology",Perception and Autonomy I,"This paper introduces a Nonlinear Model Predictive Control (N-MPC) framework exploiting a Deep Neural Network for processing onboard-captured depth images for collision avoidance in trajectory-tracking tasks with UAVs. The network is trained on simulated depth images to output a collision score for queried 3D points within the sensor field of view. Then, this network is translated into an algebraic symbolic equation and included in the N-MPC, explicitly constraining predicted positions to be collision-free throughout the receding horizon. The N-MPC achieves real time control of a UAV with a control frequency of 100Hz. The proposed framework is validated through statistical analysis of the collision classifier network, as well as Gazebo simulations and real experiments to assess the resulting capabilities of the N-MPC to effectively avoid collisions in cluttered environments. The associated code is released open-source along with the training images."
Aerial Transportation of Cable-Suspended Loads with an Event Camera,"Fotis Panetsos, George Karras, Kostas Kyriakopoulos","National Technical University of Athens,University of Thessaly,New York University - Abu Dhabi",Perception and Autonomy I,"In this work, we investigate the integration of a Dynamic Vision Sensor (DVS) into an Unmanned Aerial Vehicle (UAV) with a cable-suspended load in order to achieve a robust and fast estimation of the cable's state during the transportation of the load. Based on the advantageous properties of event cameras, our ultimate goal is to design a computationally lightweight event processing method that persistently identifies the cable and estimates its complete state â€“ required for any controller with feedback of the cable's state â€“ within a much shorter time period compared to frame-based algorithms. Using a point cloud representation for the incoming event streams, the proposed method achieves the fast detection of the cable while the respective measurements are afterward fitted to a BÃ©zier curve in order to approximate both the cable angle and angular velocity. Our method is initially validated in an indoor environment, where ground truth is available from a motion capture system, and is subsequently deployed in an outdoor one in order to evaluate its robustness against noise. Throughout the outdoor experiment, the feedback provided by the DVS is incorporated into a Nonlinear Model Predictive Control (NMPC) scheme which drives an octorotor towards reference setpoint positions while minimizing the cable angular motion."
Autonomous Overhead Powerline Recharging for Uninterrupted Drone Operations,"Viet Duong Hoang, Frederik Falk Nyboe, Nicolaj Malle, Emad Ebeid",University of Southern Denmark,Perception and Autonomy I,"We present a fully autonomous self-recharging drone system capable of long-duration sustained operations near powerlines. The drone is equipped with a robust onboard perception and navigation system that enables it to locate powerlines and approach them for landing. A passively actuated gripping mechanism grasps the powerline cable during landing after which a control circuit regulates the magnetic field inside a split-core current transformer to provide sufficient holding force as well as battery recharging. The system is evaluated in an active outdoor three-phase powerline environment. We demonstrate multiple contiguous hours of fully autonomous uninterrupted drone operations composed of several cycles of flying, landing, recharging, and takeoff, validating the capability of extended, essentially unlimited, operational endurance."
LPNet: A Reaction-Based Local Planner for Autonomous Collision Avoidance Using Imitation Learning,"Lu Junjie, Bailing Tian, Hongming Shen, Xuewei Zhang, Yulin Hui","Tianjin University,Nanyang Technological University",Perception and Autonomy I,"In this work, we propose a reaction-based local planner for autonomous collision avoidance of quadrotor in obstacle-cluttered environment without relying on an explicit map. Our approach searches for feasible trajectory using a set of motion primitives in state lattice and represents the optimal one as a polynomial by solving an optimal control problem. A modified Q-network, termed LPNet, is presented to predict the action-values of motion primitives from the current depth image and the state estimation of the quadrotor directly. To train the proposed LPNet, a primitive-based expert policy with privileged information about the surroundings and unconstrained computational budget is developed to provide demonstrations for imitation learning. Finally, a series of experiments are conducted to demonstrate the effectiveness and time-efficiency of the proposed method in both simulation and real-world."
Autonomous Exploration of Unknown 3D Environments Using a Frontier-Based Collector Strategy,"Ivan David Changoluisa Caiza, Ana Milas, Marco Antonio Montes Grova, Francisco Javier Perez Grau, Tamara Petrovic","University of Zagreb,University of Zagreb, Faculty of Electrical Engineering and Comp,Center for Advanced Aerospace Technologies (CATEC),Center for Advanced Aerospace Technologies,Univ. of Zagreb",Perception and Autonomy I,"Autonomous exploration using unmanned aerial vehicles (UAVs) is essential for various tasks such as building inspections, rescue operations, deliveries, and warehousing. However, there are two main limitations to previous approaches: they may not be able to provide a complete map of the environment and assume that the map built during exploration is accurate enough for safe navigation, which is usually not the case. To address these limitations, a novel exploration method is proposed that combines frontier-based exploration with a collector strategy that achieves global exploration and complete map creation. In each iteration, the collector strategy stores and validates frontiers detected during exploration and selects the next best frontier to navigate to. The collector strategy ensures global exploration by balancing the exploitation of a known map with the exploration of unknown areas. In addition, the online path replanning ensures safe navigation through the map created during motion. The performance of the proposed method is verified by exploring 3D simulation environments in comparison with the state-of-the-art methods. Finally, the proposed approach is validated in a real-world experiment."
Learning Multi-Scale Context Mask-RCNN Network for Slant Angled Aerial Imagery in Instance Segmentation Using Sim2Real,"QIRANUL SAADIYEAN, Samprithi S P, Suresh Sundaram","Indian Institute of Science, Banglore,PES University,Indian Institute of Science",Perception and Autonomy I,"While instance segmentation models excel at object detection in satellite imagery, their performance drops when applied to slant-angled aerial images due to occlusion and scale variation. This is mainly caused by a lack of training data for such diverse viewpoints and scales. To address this limitation, we propose the Sim2Real-based Multi-Scale Context Mask-RCNN (MSC-RCNN) network, specifically designed for slant-angled aerial imagery. Sim2Real-based transfer learning is adapted to compensate for the limited availability of real-world slant-angle training data. A synthetic dataset is generated using Unreal Engine, detailing the methodology of replicating the real-world scene, for producing diverse slant-angle drone datasets with various weather conditions and backgrounds. The model leverages two distinct feature pyramid backbones, with one incorporating dilated convolutions to address large-scale objects and the other optimized for regular convolutions. Their outputs are fused to effectively detect objects across various scales and angles. Through experiments, it was demonstrated that incorporating this synthetic data significantly reduces reliance on real data while maintaining high mean Average Precision (mAP) scores. Compared to the baseline Mask R-CNN, the proposed approach with Sim2Real adaptation and the MSC-RCNN architecture achieves a remarkable 7.6% performance improvement in instance segmentation accuracy with only a 6% increase in model size."
Real-Time Multi-Modal Active Vision for Object Detection on UAVs Equipped with Limited Field of View LiDAR and Camera,"Chuanbeibei Shi, Ganghua Lai, Yushu Yu, Mauro Bellone, Vincenzo Lippiello","Univeristy of Bristol,Beijing Institute of Technology,Tallinn University of Technology,University of Naples FEDERICO II",Perception and Autonomy I,"This paper aims to solve the challenging problems in multi-modal active vision for object detection on unmanned aerial vehicles (UAVs) with a monocular camera and a limited Field of View (FoV) LiDAR. The point cloud acquired from the low-cost LiDAR is firstly converted into a 3-channel tensor via motion compensation, accumulation, projection, and upsampling processes. The generated 3-channel point cloud tensor and RGB image are fused into a 6-channel tensor using an early fusion strategy for object detection based on a Gaussian YOLO network structure. To solve the low computational resource problem and improve the real-time performance, the velocity information of the UAV is further fused with the detection results based on an extended Kalman Filter (EKF). A perception-aware model predictive control (MPC) is designed to achieve active vision on our UAV. According to our performance evaluation, our pre-processing step improves other literature methods running time by a factor of 10 while maintaining acceptable detection performance. Furthermore, our fusion architecture reaches 94.6 mAP on the test set, outperforming the individual sensor networks by roughly 5%. We also described an implementation of the overall algorithm on a UAV platform and validated it in real-world experiments."
High-Speed Stereo Visual SLAM for Low-Powered Computing Devices,"Ashish Kumar, Jaesik Park, Laxmidhar Behera","Indian Institute of Technology, Kanpur,Seoul National University,IIT Kanpur",Perception and Autonomy I,"We present an accurate and GPU-accelerated Stereo Visual SLAM design called Jetson-SLAM. It exhibits frame- processing rates above 60FPS on NVIDIAâ€™s low-powered 10W Jetson-NX embedded computer and above 200FPS on desktop- grade 200W GPUs, even in stereo configuration and in the multiscale setting. Our contributions are threefold: (i) a Bounded Rectification technique to prevent tagging many non-corner points as a corner in FAST detection, improving SLAM accuracy. (ii) A novel Pyramidal Culling and Aggregation (PyCA) technique that yields robust features while suppressing redundant ones at high speeds by harnessing a GPU device. PyCA uses our new Multi-Location Per Thread culling strategy (MLPT) and Thread-Efficient Warp-Allocation (TEWA) scheme for GPU to enable Jetson-SLAM achieving high accuracy and speed on embedded devices. (iii) Jetson-SLAM library achieves resource efficiency by having a data-sharing mechanism. Our experiments on three challenging datasets: KITTI, EuRoC, and KAIST-VIO, and two highly accurate SLAM backends: Full- BA and ICE-BA show that Jetson-SLAM is the fastest available accurate and GPU-accelerated SLAM system (Fig. 1)."
Robotic Assessment of a Crop's Need for Watering,"Amel Dechemi, Dimitrios Chatziparaschis, Joshua Chen, Merrick Campbell, Azin Shamshirgaran, Caio Cesar Rodrigues Mucchiani, Amit Roy-chowdhury, Stefano Carpin, Konstantinos Karydis","University of California, Riverside,UC Riverside,University of California, Merced,University of California Riverside",Robotics and Automation in Agriculture and Forestry II,"This paper focuses on developing a robot-assisted system for stem water potential (SWP) measurement in orchards. SWP is a metric frequently used by agronomists and growers to optimize irrigation schedules for crops. However, such measurements are currently being made via a time- and labor-intensive procedure that faces the challenges of sparse sampling and human variability in determining SWP. In response to these challenges, our proposed robotic system aims to automate time-consuming and difficult to perform tasks, by collecting multiple leaves and automating some parts of the overall SWP analysis process. To achieve so, this work considers three core components: 1) informed planning, to determine where to collect leaves to get the most informative readings; 2) system design and integration for autonomous leaf retrieval with a mobile manipulator and a custom-made end-effector; and 3) learning-based machine vision for automated visual identification of leaf xylem wetness during SWP analysis. Taken together, these constitute the core building-blocks toward enabling complete robot autonomy in physical specimen sampling and transport in the field."
Enhancement on Target-Gripper Alignment: A Tomato Harvesting Robot with Dual-Camera Image-Based Visual Servoing,"Feng-li Lian, Lu-ching Wang, Yen-cheng Chu",National Taiwan University,Robotics and Automation in Agriculture and Forestry II,"Automation application in crop harvesting has increased in the past decades. Various types of harvesting robots are emerging in both commercial and research areas. One of the main challenges is the precision alignment of the gripper and the target crop. An undesired dislocation can harm both the gripper and the crop, which is mainly caused by uncertainties from the sensors and the manipulator. To solve the problem, the dual-camera setup is designed and implemented on a self-built robot. The perception of the tomato is done by a fixed depth camera and a camera without depth on the gripper. The proposed dual-camera image-based visual servoing (IBVS) controller is designed to deal with the image feedback from both cameras and the proof of asymptotically convergence is provided. Furthermore, the cumulative error compensation reduces the time for the harvesting process. The experiments were conducted in the greenhouse and tested under various conditions. The time cost is formulated as a function and the success picking rate of tomatoes is 68.4%."
Few-Shot Fruit Segmentation Via Transfer Learning,"Jordan James, Heather K. Manching, Amanda M. Hulse-kemp, William Beksi","University of Texas at Arlington,North Carolina State University,The University of Texas at Arlington",Robotics and Automation in Agriculture and Forestry II,"Advancements in machine learning, computer vision, and robotics have paved the way for transformative solutions in various domains, particularly in agriculture. For example, accurate identification and segmentation of fruits from field images plays a crucial role in automating jobs such as harvesting, disease detection, and yield estimation. However, achieving robust and precise infield fruit segmentation remains a challenging task since large amounts of labeled data are required to handle variations in fruit size, shape, color, and occlusion. In this paper, we develop a few-shot semantic segmentation framework for infield fruits using transfer learning. Concretely, our work is aimed at addressing agricultural domains that lack publicly available labeled data. Motivated by similar success in urban scene parsing, we propose specialized pre-training using a public benchmark dataset for fruit transfer learning. By leveraging pre-trained neural networks, accurate semantic segmentation of fruit in the field is achieved with only a few labeled images. Furthermore, we show that models with pre-training learn to distinguish between fruit still on the trees and fruit that have fallen on the ground, and they can effectively transfer the knowledge to the target fruit dataset."
Spatio-Temporal Correspondence Estimation of Growing Plants by Hausdorff Distance Based Skeletonization for Organ Tracking,"Sharmistha B Pandey, David Colliaux, Ayan Chaudhury","ISRO,Sony CSL Paris,Indian Institute of Technology Kharagpur",Robotics and Automation in Agriculture and Forestry II,"Tracking of plant organs over spatio-temporal sequence of point cloud data is one of the demanding tasks of agricultural robotics for automated plant monitoring and growth analysis. Due to the complex geometry of plants, it is extremely difficult to identify and track the individual organs in different growth stages of plants. In this paper, we present an approach to perform correspondence estimation of different plant organs over a series of spatio-temporal data. The approach is based on two stages. In the first stage we develop a robust skeleton extraction method from unstructured plant point cloud data by adopting Hausdorff distance metric and modified breadth first search algorithm. The proposed skeletonization method is shown to be performing better than state-of-the-art, especially in handling very thin and delicate branches. We also address an overlooked problem of connecting skeleton points in the form of a graph, and demonstrate that different types of plant phenotype parameters can be obtained in a fully automatic manner from the skeleton graph. In the second stage, we exploit the skeleton graphs in developing an algorithm to perform correspondence estimation among the skeleton nodes using a cosine similarity based approach. We demonstrate the effectiveness of the proposed skeletonization technique in tracking different organs of the plant by finding good quality correspondences. Experiments are performed on three datasets on real and synthetic sequence of spatio-temporal plant point cloud data to demonstrate the effectiveness of the proposed method."
Field Robot for High-Throughput and High-Resolution 3D Plant Phenotyping,"Felix Esser, Radu Alexandru Rosu, Andre Cornelissen, Lasse Klingbeil, Heiner Kuhlmann, Sven Behnke",University of Bonn,Robotics and Automation in Agriculture and Forestry II,"With the need to feed a growing world population, the efficiency of crop production is of paramount importance. To support breeding and field management, various characteristics of the plant phenotype need to be measured a time-consuming process when performed manually. We present a robotic platform equipped with multiple laser and camera sensors for high-throughput, high-resolution in-field plant scanning. We create digital twins of the plants through 3D reconstruction. This allows the estimation of phenotypic traits such as leaf area, leaf angle, and plant height. We validate our system on a real field, where we reconstruct accurate point clouds and meshes of sugar beet, soybean, and maize."
A Hybrid Controller Enhancing Transient Performance for an Aerial Manipulator Extracting a Wedged Object,"Jeonghyun Byun, Inkyu Jang, Dongjae Lee, H. Jin Kim",Seoul National University,Robotics and Automation in Agriculture and Forestry II,"Autonomous aerial manipulation requires the capability to handle inevitable dynamic changes during physical interaction. Previously, very few studies have addressed the stability and transient performance of the scenarios involving abrupt changes in dynamics. This paper proposes a hybrid controller enhancing transient performance for an aerial manipulator extracting an object wedged in a static structure. This task incurs a significant jump in the interaction force on the end-effector so that the analysis using the concept of hybrid dynamical systems is required. To demonstrate the dynamic characteristics of the object-extracting aerial manipulator, we derive the dynamic equations for two flight modes, i.e., free-flight and object-extracting, and the rule of state jumps. Also, we design control strategies which enhance the transient performance during flight mode transition. Then, the stability of the proposed control law is proven, and the overshoot reduction after the object extraction is analyzed. To show the improved performance, we conduct plug-pulling experiments with a quadrotor-based aerial manipulator using the proposed controller and two different existing controllers. The comparative results confirm that our controller enables the aerial manipulator to maintain its stability after the flight mode transition and shows the best transient performance in overshoot minimization among three controllers."
VACNA: Visibility-Aware Cooperative Navigation with Application in Inventory Management,"Houman Masnavi, Jatan Shrestha, Karl Kruusamäe, Arun Singh","Institute of Technology, University of Tartu,University of Tartu",Robotics and Automation in Agriculture and Forestry II,"This paper presents an online trajectory planning algorithm for an Unmanned Aerial Vehicle (UAV) to autonomously scan warehouse racks for inventory management. Our main motivation is to make small-sized UAVs with limited computing and sensing hardware capable of reliably performing the scanning task in cluttered environments. To this end, we propose a cooperative system where an Unmanned Ground Vehicle (UGV) guides the UAV using the novel template of visibility-aware cooperative navigation (VACNA). We propose a Cross-Entropy Method (CEM) based approach for solving the trajectory optimization underpinning VACNA. In particular, our CEM projects sampled vehicle trajectories onto the constraint sets before evaluating the cost functions. We further learn a deep generative model in the form of a Conditional Variational Autoencoder (CVAE) from expert demonstrations to warm-start our optimizer. We improve the state-of-the-art in the following respects. First, we present a detailed analysis of the role of our proposed cost and constraint functions for cooperative occlusion-free navigation. Second, we compare our custom CEM optimizer with conventional variants and show significantly reduced collision and occlusion rates. Finally, our CVAE initialization allows our optimizer to operate with smaller batch sizes and achieve real-time performance even on embedded hardware devices like NVIDIA Jetson Xavier."
A CBF-Adaptive Control Architecture for Visual Navigation for UAV in the Presence of Uncertainties,"Viswa Narayanan Sankaranarayanan, Akshit Saradagi, Sumeet Satpute, George Nikolakopoulos","Lulea University of Techonology,Luleå University of Technology, Luleå, Sweden,Luleå University of Technology",Robotics and Automation in Agriculture and Forestry II,"In this article, we propose a control solution for the safe transfer of a quadrotor UAV between two surface robots positioning itself only using the visual features on the surface robots, which enforces safety constraints for precise landing and visual locking, in the presence of modeling uncertainties and external disturbances. The controller handles the ascending and descending phases of the navigation using a visual locking control barrier function (VCBF) and a parametrizable switching descending CBF (DCBF) respectively, eliminating the need for an external planner. The control scheme has a backstepping approach for the position controller with the CBF filter acting on the position kinematics to produce a filtered virtual velocity control input, which is tracked by an adaptive controller to overcome modeling uncertainties and external disturbances. The experimental validation is carried out with a UAV that navigates from the base to the target using an RGB camera."
Multimodal Indoor Localization Using Crowdsourced Radio Maps,"Zhaoguang Yi, Xiangyu Wen, Qiyue Xia, Peize Li, Francisco Zampella, Firas Alsehly, Chris Xiaoxuan Lu","University of Edinburgh,The University of Edinburgh,Edinburgh Research Center, Huawei Technologies Co., Ltd.,Huawei Technologies R&D UK",Localization VII,"Indoor Positioning Systems (IPS) traditionally rely on odometry and building infrastructures like WiFi, often supplemented by building floor plans for increased accuracy. However, the limitation of floor plans in terms of availability and timeliness of updates challenges their wide applicability. In contrast, the proliferation of smartphones and WiFi-enabled robots has made crowdsourced radio maps â€“ databases pairing locations with their corresponding Received Signal Strengths (RSS) â€“ increasingly accessible. These radio maps not only provide WiFi fingerprint-location pairs but encode movement regularities akin to the constraints imposed by floor plans. This work investigates the possibility of leveraging these radio maps as a substitute for floor plans in multimodal IPS. We introduce a new framework to address the challenges of radio map inaccuracies and sparse coverage. Our proposed system integrates an uncertainty-aware neural network model for WiFi localization and a bespoken Bayesian fusion technique for optimal fusion. Extensive evaluations on multiple real-world sites indicate a significant performance enhancement, with results showing $sim25%$ improvement over the best baseline."
CLIP-Loc: Multi-Modal Landmark Association for Global Localization in Object-Based Maps,"Shigemichi Matsuzaki, Takuma Sugino, Kazuhito Tanaka, Zijun Sha, Shintaro Nakaoka, Shintaro Yoshizawa, Kazuhiro Shintani","Toyota Motor Corporation,Keio University,TOYOTA MOTOR CORPORATION",Localization VII,"This paper describes a multi-modal data association method for global localization using object-based maps and camera images. In global localization, or relocalization, using object-based maps, existing methods typically resort to matching all possible combinations of detected objects and landmarks with the same object category, followed by inlier extraction using RANSAC or brute-force search. This approach becomes infeasible as the number of landmarks increases due to the exponential growth of correspondence candidates. In this paper, we propose labeling landmarks with natural language descriptions and extracting correspondences based on conceptual similarity with image observations using a Vision Language Model (VLM). By leveraging detailed text information, our approach efficiently extracts correspondences compared to methods using only object categories. Through experiments, we demonstrate that the proposed method enables more accurate global localization with fewer iterations compared to baseline methods, exhibiting its efficiency."
Multiple Update Particle Filter: Position Estimation by Combining GNSS Pseudorange and Carrier Phase Observations,Taro Suzuki,Chiba Institute of Technology,Localization VII,"This paper presents an efficient method for updating particles in a particle filter (PF) to address the position estimation problem when dealing with sharp-peaked likelihood functions derived from multiple observations. Sharp-peaked likelihood functions commonly arise from millimeter-accurate distance observations of carrier phases in the global navigation satellite system (GNSS). However, when such likelihood functions are used for particle weight updates, the absence of particles within the peaks leads to all particle weights becoming zero. To overcome this problem, in this study, a straightforward and effective approach is introduced for updating particles when dealing with sharp-peaked likelihood functions obtained from multiple observations. The proposed method, termed as the multiple update PF, leverages prior knowledge regarding the spread of distribution for each likelihood function and conducts weight updates and resampling iteratively in the particle update process, prioritizing the likelihood function spreads. Experimental results demonstrate the efficacy of our proposed method, particularly when applied to position estimation utilizing GNSS pseudorange and carrier phase observations. The multiple update PF exhibits faster convergence with fewer particles when compared to the conventional PF. Moreover, vehicle position estimation experiments conducted in urban environments reveal that the proposed method outperforms conventional GNSS positioning techniques, yielding more accurate position estimates."
Robust Lifelong Indoor LiDAR Localization Using the Area Graph,"Fujing Xie, Soeren Schwertfeger","Shanghaitech University,ShanghaiTech University",Localization VII,"Lifelong indoor localization in a given map is the basis for navigation of autonomous mobile robots. In this letter, we address the problem of robust localization in cluttered indoor environments like office spaces and corridors using 3D LiDAR point clouds in a given Area Graph, which is a hierarchical, topometric semantic map representation that uses polygons to demark areas such as rooms, corridors or buildings. This representation is very compact, can represent different floors of buildings through its hierarchy and provides semantic information that helps with localization, like poses of doors and glass. In contrast to this, commonly used map representations, such as occupancy grid maps or point clouds, lack these features and require frequent updates in response to environmental changes (e.g. moved furniture), unlike our approach, AGLoc, which matches against lifelong architectural features such as walls and doors. For that we apply filtering to remove clutter from the 3D input point cloud and then employ further scoring and weight functions for localization. Given a broad initial guess from WiFi and barometer localization, our experiments show that our global localization and the weighted point to line ICP pose tracking perform very well, even when compared to localization and SLAM algorithms that use the current, feature-rich cluttered map for localization."
Multi-Camera Asynchronous Ball Localization and Trajectory Prediction with Factor Graphs and Human Poses,"Qingyu Xiao, Zulfiqar Zaidi, Matthew Gombolay",Georgia Institute of Technology,Localization VII,"The rapid and precise localization and prediction of a ball are critical for developing agile robots in ball sports, particularly in sports like tennis characterized by high-speed ball movements and powerful spins. The Magnus effect induced by spin adds complexity to trajectory prediction during flight and bounce dynamics upon contact with the ground. In this study, we introduce an innovative approach that combines a multi-camera system with factor graphs for real-time and asynchronous 3D tennis ball localization. Additionally, we estimate hidden states like velocity and spin for trajectory prediction. Furthermore, to enhance spin inference early in the ball's flight, where limited observations are available, we integrate human pose data using a temporal convolutional network (TCN) to compute spin priors within the factor graph. This refinement provides more accurate spin priors at the beginning of the factor graph, leading to improved early-stage hidden state inference for prediction. Our results show the trained TCN can predict the spin priors with RMSE of 5.27 Hz. Integrating TCN into the factor graph reduces the prediction error of landing positions by over 63.6% compared to a baseline method that utilized an adaptive extended Kalman filter."
Doppler-Only Single-Scan 3D Vehicle Odometry,"Andres Galeote-luque, Vladimir Kubelka, Martin Magnusson, J.r. Ruiz-sarmiento, Javier Gonzalez-jimenez","Universidad de Málaga,Örebro University,University of Malaga",Localization VII,"We present a novel 3D odometry method that recovers the full motion of a vehicle only from a Doppler-capable range sensor. It leverages the radial velocities measured from the scene, estimating the sensor's velocity from a single scan. The vehicle's 3D motion, defined by its linear and angular velocities, is calculated taking into consideration its kinematic model which provides a constraint between the velocity measured at the sensor frame and the vehicle frame. Experiments carried out prove the viability of our single-sensor method compared to mounting an additional IMU. Our method provides a more reliable translation of the sensor, compared to the errors linked to IMUs due to noise and biases. Its short-term accuracy and fast operation (~5ms) make it a proper candidate to supply the initialization to more complex localization algorithms or mapping pipelines. Not only does it reduce the error of the mapper, but it does so at a comparable level of accuracy as an IMU would. All without the need to mount and calibrate an extra sensor on the vehicle."
Do We Need Scan-Matching in Radar Odometry?,"Vladimir Kubelka, Emil Fritz, Martin Magnusson",Örebro University,Localization VII,"There is a current increase in the development of â€œ4Dâ€ Doppler-capable radar and lidar range sensors that produce 3D point clouds where all points also have information about the radial velocity relative to the sensor. 4D radars in particular are interesting for object perception and navigation in low-visibility conditions (dust, smoke) where lidars and cameras typically fail. With the advent of high-resolution Doppler-capable radars comes the possibility of estimating odometry from single point clouds, foregoing the need for scan registration which is error-prone in feature-sparse field environments. We compare several odometry estimation methods, from direct integration of Doppler/IMU data and Kalman filter sensor fusion to 3D scan-to-scan and scan-to-map registration, on three datasets with data from two recent 4D radars and two IMUs. Surprisingly, our results show that the odometry from Doppler and IMU data alone give similar or better results than 3D point cloud registration. In our experiments, the position drift can be as low as 0.9% over 1.8 and 4.5 km trajectories. That allows accurate estimation of 6-DOF ego-motion over long distances also in feature-sparse mine environments. These results are useful not least for applications of navigation with resource-constrained robot platforms in feature-sparse and low-visibility conditions such as mining, construction, and search & rescue operations."
Outram: One-Shot Global Localization Via Triangulated Scene Graph and Global Outlier Pruning,"Pengyu Yin, Cao Haozhi, Thien-Minh Nguyen, Shenghai Yuan, Shuyang Zhang, Kangcheng Liu, Lihua Xie","Nanyang Technological University,NANYANG TECHNOLOGICAL UNIVERSITY,The Hong Kong University of Science and Technology,ETH Zurich,NanyangTechnological University",Localization VII,"One-shot LiDAR localization refers to the ability to estimate the robot pose from one single point cloud, which yields significant advantages in initialization and relocalization processes. In the point cloud domain, the topic has been extensively studied as a global descriptor retrieval (i.e., loop closure detection) and pose refinement (i.e., point cloud registration) problem both in isolation or combined. However, few have explicitly considered the relationship between candidate retrieval and correspondence generation in pose estimation, leaving them brittle to substructure ambiguities. To this end, we propose a hierarchical one-shot localization algorithm called Outram that leverages substructures of 3D scene graphs for locally consistent correspondence searching and global substructure-wise outlier pruning. Such a hierarchical process couples the feature retrieval and the correspondence extraction to resolve the substructure ambiguities by conducting a local-to-global consistency refinement. We demonstrate the capability of Outram in a variety of scenarios in multiple large-scale outdoor datasets. Our implementation is open-sourced: https://github.com/Pamphlett/Outram."
Fast and Consistent Covariance Recovery for Sliding-Window Optimization-Based VINS,"Chuchu Chen, Yuxiang Peng, Guoquan Huang",University of Delaware,Localization VII,"In this paper, we introduce a novel and efficient technique for consistent covariance recovery in nonlinear optimization-based Visual-Inertial Navigation Systems (VINS). Estimating uncertainty in real-time is crucial for evaluating system performance and enhancing downstream operations such as data association. However accessing the marginal covariance of the state variables of interest in optimization-based VINS resents a significant challenge â€“ a computational bottleneck due to the need to invert the high-dimensional information (Hessian) matrix. In our recent work [1], the First-Estimates Jacobian (FEJ) methodology was used to properly fix state linearization points in the optimization-based VINS, which seems counter-intuitive but improves the estimation performance in both consistency and accuracy. Capitalizing on this unique aspect of the FEJ strategy, in this work we carefully design the covariance recovery algorithm to improve efficiency by avoiding redundant computation. Remarkably, our approach achieves a computational speed that is 4-10 times faster than the existing methods. Through comprehensive numerical evaluations across four state-of-the-art marginalization archetypes, we not only affirm the consistency of our covariance estimates but underscore its superior computational efficiency."
LONER: LiDAR Only Neural Representations for Real-Time SLAM,"Seth Isaacson, Pou-Chun Kung, Mani Ramanagopal, Ram Vasudevan, Katherine A. Skinner","University of Michigan,University of Michigan, Ann Arbor,Carnegie Mellon University",SLAM IV,"This paper proposes LONER, the first real-time LiDAR SLAM algorithm that uses a neural implicit scene representation. Existing implicit mapping methods for LiDAR show promising results in large-scale reconstruction, but either require groundtruth poses or run slower than real-time. In contrast, LONER uses LiDAR data to train an MLP to estimate a dense map in real-time, while simultaneously estimating the trajectory of the sensor. To achieve real-time performance, this paper proposes a novel information-theoretic loss function that accounts for the fact that different regions of the map may be learned to varying degrees throughout online training. The proposed method is evaluated qualitatively and quantitatively on two open-source datasets. This evaluation illustrates that the proposed loss function converges faster and leads to more accurate geometry reconstruction than other loss functions used in depth-supervised neural implicit frameworks. Finally, this paper shows that LONER estimates trajectories competitively with state-of-the-art LiDAR SLAM methods, while also producing dense maps competitive with existing real-time implicit mapping methods that use groundtruth poses."
LIO-EKF: High Frequency LiDAR-Inertial Odometry Using Extended Kalman Filters,"Yibin Wu, Tiziano Guadagnino, Louis Wiesmann, Lasse Klingbeil, Cyrill Stachniss, Heiner Kuhlmann",University of Bonn,SLAM IV,"Odometry estimation is crucial for every autonomous system requiring navigation in an unknown environment. In modern mobile robots, 3D LiDAR-inertial systems are often used for this task. By fusing LiDAR scans and IMU measurements, these systems can reduce the accumulated drift caused by sequentially registering individual LiDAR scans and provide a robust pose estimate. Although effective, LiDAR-inertial odometry systems require proper parameter tuning to be deployed. In this paper, we propose LIO-EKF, a tightly-coupled LiDAR-inertial odometry system based on point-to-point registration and the classical extended Kalman filter scheme. We propose an adaptive data association that considers the relative pose uncertainty, the map discretization errors, and the LiDAR noise. In this way, we can substantially reduce the parameters to tune for a given type of environment. The experimental evaluation suggests that the proposed system performs on par with the state-of-the-art LiDAR-inertial odometry pipelines but is significantly faster in computing the odometry. The source code of our implementation is publicly available (https://github.com/YibinWu/LIO-EKF)."
Multi-LIO: A Lightweight Multiple LiDAR-Inertial Odometry System,"Qi Chen, Guanghao Li, Xiangyang Xue, Jian Pu",Fudan University,SLAM IV,"The integration of multiple LiDAR sensors has the potential to significantly enhance odometry systems by providing comprehensive environmental measurements. However, current multiple LiDAR-inertial odometry frameworks face challenges in real-time processing due to the voluminous data generated. This paper introduces a real-time, computationally efficient multiple LiDAR-inertial odometry system (Multi-LIO) that outperforms existing state-of-the-art solutions in accuracy and scalability. Utilizing a novel parallel strategy for state updates and a voxelized map format, Multi-LIO optimizes computational efficiency. Furthermore, we introduce a point-wise uncertainty estimation method to augment the accuracy of scan-to-map registration, particularly in large-scale and complex scenarios. We validate our system's performance through extensive experiments on various challenging sequences. Multi-LIO emerges as a robust, scalable, and extensible solution, adaptable to various LiDAR configurations."
The Importance of Coordinate Frames in Dynamic SLAM,"Jesse Morris, Yiduo Wang, Viorela Ila","University of sydney,University of Sydney,The University of Sydney",SLAM IV,"Most Simultaneous localisation and mapping (SLAM) systems have traditionally assumed a static world, which does not align with real-world scenarios. To enable robots to safely navigate and plan in dynamic environments, it is essential to employ representations capable of handling moving objects. Dynamic SLAM is an emerging field in SLAM research as it improves the overall system accuracy while providing additional estimation of object motions. State-of-the-art literature informs two main formulations for Dynamic SLAM, representing dynamic object points in either the world or object coordinate frame. While expressing object points in their local reference frame may seem intuitive, it does not necessarily lead to the most accurate and robust solutions. This paper conducts and presents a thorough analysis of various Dynamic SLAM formulations, identifying the best approach to address the problem. To this end, we introduce a front-end agnostic framework using GTSAM that can be used to evaluate various Dynamic SLAM formulations."
VoxelMap++: Mergeable Voxel Mapping Method for Online LiDAR(-Inertial) Odometry,"Chang Wu, Yuan You, Yifei Yuan, Xiaotong Kong, Ying Zhang, Qiyan Li, Kaiyong Zhao","University of Electronic Science and Technology of China (UESTC),University of Electronic Science and Technology of China,Hong Hong Baptist University",SLAM IV,"This paper presents VoxelMap++: a voxel mapping method with plane merging which can effectively improve the accuracy and efficiency of LiDAR(-inertial) based simultaneous localization and mapping (SLAM). This map is a collection of voxels that contains one plane feature with 3DOF representation and corresponding covariance estimation. Considering total map will contain a large number of coplanar features (kid planes), these kid planes' 3DOF estimation can be regarded as the measurements with covariance of a larger plane (father plane). Thus, we design a plane merging module based on union-find which can save resources and further improve the accuracy of plane fitting. This module can distinguish the kid planes in different voxels and merge these kid planes to estimate the father plane. After merging, the father plane 3DOF representation will be more accurate than the kids plane and the uncertainty will decrease significantly which can further improve the performance of LiDAR(-inertial) odometry. Experiments on challenging environments such as corridors and forests demonstrate the high accuracy and efficiency of our method compared to other state-of-the-art methods (see our attached video). By the way, our implementation VoxelMap++ is open-sourced on GitHub which is applicable for both non-repetitive scanning LiDARs and traditional scanning LiDAR."
Efficient and Consistent Bundle Adjustment on Lidar Point Clouds,"Zheng Liu, Xiyuan Liu, Fu Zhang","University of Hong Kong,The University of Hong Kong",SLAM IV,"Simultaneous determination of sensor poses and scene geometry is a fundamental problem for robot vision that is often achieved by Bundle Adjustment (BA). This paper presents an efficient and consistent bundle adjustment method for lidar sensors. The method employs edge and plane features to represent the scene geometry, and directly minimizes the natural Euclidean distance from each raw point to the respective geometry feature. A nice property of this formulation is that the geometry features can be analytically solved, drastically reducing the dimension of the numerical optimization. To represent and solve the resultant optimization problem more efficiently, this paper then adopts and formalises the concept of point cluster, which encodes all raw points associated to the same feature by a compact set of parameters, the point cluster coordinates. We derive the closed-form derivatives, up to the second order, of the BA optimization based on the point cluster coordinates and show their theoretical properties such as the null spaces and sparsity. Based on these theoretical results, this paper develops an efficient second-order BA solver. Besides estimating the lidar"
DORF: A Dynamic Object Removal Framework for Robust Static LiDAR Mapping in Urban Environments,"Zhiming Chen, Kun Zhang, Hua Chen, Michael Yu Wang, Wei Zhang, Hongyu Yu","Hong Kong University of Science and Technology,Southern University of Science and Technology,[email protected],The Hong Kong University of Science and Technology",SLAM IV,"3D point cloud maps are widely used in robotic tasks like localization and planning. However, dynamic objects, such as cars and pedestrians, can introduce ghost artifacts during the map generation process,leading to reduced map quality and hindering normal robot navigation. Online dynamic object removal methods are restricted to utilize only local scope information and have limited performance. To address this challenge, we propose DORF (Dynamic Object Removal Framework), a novel coarse-to-fine offline framework that exploits global 4D spatial-temporal LiDAR information to achieve clean static point cloud map generation, which reaches the state of the art performance among existing offline methods. DORF first conservatively preserves the definite static points leveraging the Receding Horizon Sampling (RHS) mechanism proposed by us. Then DORF gradually recovers more ambiguous static points, guided by the inherent characteristic of dynamic objects in urban environments which necessitates their interaction with the ground. We validate the effectiveness and robustness of DORF across various types of highly dynamic datasets."
ImMesh: An Immediate LiDAR Localization and Meshing Framework,"Jiarong Lin, Yuan Chongjian, Yixi Cai, Haotian Li, Yunfan Ren, Yuying Zou, Xiaoping Hong, Fu Zhang","The University of Hong Kong,University of Hong Kong,THE UNIVERSITY OF HONG KONG,Southern University of Science and Technology",SLAM IV,"In this paper, we propose a novel LiDAR(-inertial) odometry and mapping framework to achieve the goal of simultaneous localization and meshing in real-time. This proposed framework termed ImMesh comprises four tightly-coupled modules: receiver, localization, meshing, and broadcaster. The localization module first utilizes the preprocessed sensor data from the receiver, estimates the sensor pose online by registering LiDAR scans to maps, and dynamically grows the map. Then, our meshing module takes the registered LiDAR scan for incrementally reconstructing the triangle mesh on the fly. Finally, the real-time odometry, map, and mesh are published via our broadcaster. The primary contribution of this work is the meshing module, which represents a scene by an efficient voxel structure, performs fast finding of voxels observed by new scans, and incrementally reconstructs triangle facets in each voxel. To the best of our knowledge, this is the first work in literature that can reconstruct online the triangle mesh of large-scale scenes, just relying on a standard CPU without GPU acceleration."
OASIS: Optimal Arrangements for Sensing in SLAM,"Pushyami Kaveti, Matthew Giamou, Hanumant Singh, David Rosen","Northeastern University,McMaster University,Northeatern University",SLAM IV,"The number and arrangement of sensors on mobile robot dramatically influence its perception capabilities. Ensuring that sensors are mounted in a manner that enables accurate detection, localization, and mapping is essential for the success of downstream control tasks. However, when designing a new robotic platform, researchers and practitioners alike usually mimic standard configurations or maximize simple heuristics like field-of-view (FOV) coverage to decide where to place exteroceptive sensors. In this work, we conduct an information-theoretic investigation of this overlooked element of robotic perception in the context of simultaneous localization and mapping (SLAM). We show how to formalize the sensor arrangement problem as a form of subset selection under the E-optimality performance criterion. While this formulation is NP-hard in general, we show that a combination of greedy sensor selection and fast convex relaxation-based post-hoc verification enables the efficient recovery of emph{certifiably optimal} sensor designs in practice. Results from synthetic experiments reveal that sensors placed with OASIS outperform benchmarks in terms of mean squared error of visual SLAM estimates."
See to Touch: Learning Tactile Dexterity through Visual Incentives,"Irmak Guzey, Yinlong Dai, Ben Evans, Soumith Chintala, Lerrel Pinto","New York University,NYU,Facebook AI Research",Dexterous Manipulation I,"Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise, contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing fails to provide adequate cues for reasoning about objects' spatial configurations, limiting the ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn visual representations. Next, we construct a reward function using these visual representations through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement learning on our robot to optimize tactile-based policies that maximize the visual reward. In six challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects, TAVI achieves a success rate of 73% using our four-fingered Allegro robot hand. The increase in performance is 108% higher than policies using tactile and vision-based rewards and 135% higher than policies without tactile observational input. Robot videos are best viewed on our project website: https://see-to-touch.github.io/."
Real-Time Contact State Estimation in Shape Control of Deformable Linear Objects under Small Environmental Constraints,"Kejia Chen, Zhenshan Bing, Yansong Wu, Fan Wu, Liding Zhang, Sami Haddadin, Zhenshan Bing","Technical University of Munich,Technische Universität München,Tech. Univ. Muenchen TUM",Dexterous Manipulation I,"Controlling the shape of deformable linear objects using robots and constraints provided by environmental fixtures has diverse industrial applications. In order to establish robust contacts with these fixtures, accurate estimation of the contact state is essential for preventing and rectifying potential anomalies. However, this task is challenging due to the small sizes of fixtures, the requirement for real-time performances, and the infinite degrees of freedom of the deformable linear objects. In this paper, we propose a real-time approach for estimating both contact establishment and subsequent changes by leveraging the dependency between the applied and detected contact force on the deformable linear objects. We seamlessly integrate this method into the robot control loop and achieve an adaptive shape control framework which avoids, detects and corrects anomalies automatically. Real-world experiments validate the robustness and effectiveness of our contact estimation approach across various scenarios, significantly increasing the success rate of shape control processes."
Self-Supervised Learning for Joint Pushing and Grasping Policies in Highly Cluttered Environments,"Yongliang Wang, Kamal Mokhtar, Cock Heemskerk, Hamidreza Kasaei","University of Groningen,Heemkerk Innovative Technology",Dexterous Manipulation I,"Robotic systems often face challenges when attempting to grasp a target object due to interference from surrounding items. We propose a Deep Reinforcement Learning (DRL) method that develops joint policies for grasping and pushing, enabling effective manipulation of target objects within untrained, densely cluttered environments. In particular, a dual RL model is introduced, which presents high resilience in handling complicated scenes, reaching an average of 98% task completion in simulation and real-world scenes. To evaluate the proposed method, we conduct comprehensive simulation experiments in three distinct environments: densely packed building blocks, randomly positioned building blocks, and common household objects. Further, real-world tests are conducted using actual robots to confirm the robustness of our approach in various untrained and highly cluttered environments. The results from experiments underscore the superior efficacy of our method in both simulated and real-world scenarios, outperforming recent state-of-the-art methods. To ensure reproducibility and further the academic discourse, we make available a demonstration video, the trained models, and the source code for public access."
A Robust Model Predictive Controller for Tactile Servoing,"Shuai Wang, Yihao Huang, Wang Wei Lee, Tianliang Liu, Xiao Teng, Yu Zheng, Qiang Li","Tencent,The University of Manchester,Harbin Institute of Technology,Keppel-NUS Corporate Lab, National University of Singapore,Shenzhen Technology University",Dexterous Manipulation I,"Tactile servoing is an effective approach to enabling robots to safely interact with unknown environments. One of the core problems in tactile servoing is to robustly converge the contact features to the desired ones via a dedicated controller. This paper proposes a Data-Driven Model Predictive Controller (DDMPC) to compute the motion command given the previous interaction experience and feature deviations in tactile space. Compared with the manually designed PID-based controller, the proposed controller depends on the sound control theory and its convergence is guaranteed from a computational perspective. It is applied to the balancing control of a rolling bottle on a robotic forearm covered by a custom tactile sensor array. The real experiment demonstrates the superior robustness of the proposed approach and shows its great potential for other tactile servoing scenarios with measurement noise, which is inevitable for current tactile sensors."
"Harnessing the Synergy between Pushing, Grasping, and Throwing to Enhance Object Manipulation in Cluttered Scenarios","Hamidreza Kasaei, Mohammadreza Kasaei","University of Groningen,University of Edinburgh",Dexterous Manipulation I,"In this work, we delve into the intricate synergy among non-prehensile actions like pushing, and prehensile actions such as grasping and throwing, within the domain of robotic manipulation. We introduce an innovative approach to learning these synergies by leveraging model-free deep reinforcement learning. The robot's workflow involves detecting the pose of the target object and the basket at each time step, predicting the optimal push configuration to isolate the target object, determining the appropriate grasp configuration, and inferring the necessary parameters for an accurate throw into the basket. This empowers robots to skillfully reconfigure cluttered scenarios through pushing, creating space for collision-free grasping actions. Simultaneously, we integrate throwing behavior, showcasing how this action significantly extends the robot's operational reach. Ensuring safety, we developed a simulation environment in Gazebo for robot training, applying the learned policy directly to our real robot. Notably, this work represents a pioneering effort to learn the synergy between pushing, grasping, and throwing actions. Extensive experimentation in both simulated and real-robot scenarios substantiates the effectiveness of our approach across diverse settings. Our approach achieves a success rate exceeding 80% in both simulated and real-world scenarios. A video showcasing our experiments is available online at: https://youtu.be/q1l4BJVDbRw"
Direct Self-Identification of Inverse Jacobians for Dexterous Manipulation through Particle Filtering,"Joshua Grace, Podshara Chanrungmaneekul, Kaiyu Hang, Aaron Dollar","Yale University,Rice University",Dexterous Manipulation I,"The ability to plan and control robotic in-hand manipulation is challenged by several issues, including the required amount of prior knowledge of the system and the sophisticated physics that varies across different robot hands or even grasp instances. One of the most direct models of in-hand manipulation is the inverse Jacobian, which can directly map from the desired in-hand object motions to the required hand actuator controls. However, acquiring such inverse Jacobians without complex hand-object system models is typically infeasible. We present a method for controlling in-hand manipulation using inverse Jacobians that are self-identified by a particle filter-based estimation scheme that leverages the ability of underactuated hands to maintain a passively stable grasp during self-identification movements. This method requires no a priori knowledge of the specific hand-object system and learns the system's inverse Jacobian through small exploratory motions. Our system approximates the underlying inverse Jacobian closely, which can be used to perform manipulation tasks across a range of objects successfully. With extensive experiments on a Yale Model O hand, we show that the proposed system can provide accurate in-hand manipulation of sub-millimeter precision and that the inverse Jacobian-based controller can support real-time manipulation control of up to 900Hz."
Masked Visual-Tactile Pre-Training for Robot Manipulation,"Qingtao Liu, Qi Ye, Zhengnan Sun, Yu Cui, Gaofeng Li, Jiming Chen",Zhejiang University,Dexterous Manipulation I,"Recent works on the pretraining for robot manipulation have demonstrated that representations learning from large human manipulation data can generalize well to new manipulation tasks and environments. However, these approaches mainly focus on human vision or natural language, neglecting tactile feedback. In this article, we make an attempt to explore how to pre-train a representation model for robotic manipulation using both human manipulation visual and tactile data. We develop a system for collecting visual and tactile data, featuring a cost-effective tactile glove to capture human tactile data and Hololens2 for capturing visual data. With this system, we collect a dataset of turning bottle caps. Furthermore, we introduce a novel visual-tactile fusion network and learning strategy, with one key module to tokenize 20 sparse binary tactile signals sensing touch states for the learning of tactile context and the other key module applying the attention and mask mechanism to the interaction of visual and tactile tokens for visual-tactile representation learning. We utilize our dataset to pre-train the fusion model and embed the pre-trained model into a reinforcement learning framework for downstream tasks. Experimental results demonstrate that our pre-trained model significantly aids in learning manipulation skills. Compared to methods without pre-training, our approach achieves a success rate increase of over 60%. Additionally, when compared to current visual pre-training methods, our success rate exceeds them by more than 50%."
Tactile Estimation of Extrinsic Contact Patch for Stable Placement,"Kei Ota, Devesh Jha, Krishna Murthy, Asako Kanezaki, Joshua Tenenbaum","Tokyo Institute of Technology,Mitsubishi Electric Research Laboratories,MIT,Massachusetts Institute of Technology",Dexterous Manipulation I,"Precise perception of contact interactions is essential for fine-grained manipulation skills for robots. In this paper, we present the design of feedback skills for robots that must learn to stack complex-shaped objects on top of each other (see Fig.1). To design such a system, a robot should be able to reason about the stability of placement from very gentle contact interactions. Our results demonstrate that it is possible to infer the stability of object placement based on tactile readings during contact formation between the object and its environment. In particular, we estimate the contact patch between a grasped object and its environment using force and tactile observations to estimate the stability of the object during a contact formation. The contact patch could be used to estimate the stability of the object upon release of the grasp. The proposed method is demonstrated in various pairs of objects that are used in a very popular board game."
Robotic Manipulation of Hand Tools: The Case of Screwdriving,"Ling Tang, Yan-Bin Jia, Yuechuan Xue","Iowa State University,Amazon.com",Dexterous Manipulation I,"Despite decades of steady research progress, the robotic hand is still far behind the human hand in terms of dexterity and versatility. A milestone in this quest for human-level performance will be possessing the skills of manipulating hand tools, for their non-trivial geometries and for the intricacies of controlling their contact-based interactions with objects, which are the final targets of manipulation. This paper investigates screwdriving by a robotic arm/hand pair, dealing with the chain of contacts connecting the substrate, screw, screwdriver, and fingertips. Considering rolling contacts and finger gaits, our force control scheme is derived through backward chaining to leverage the dynamics of the screwdriver and arm/hand. To maintain the fastening effort, estimations are carried out sequentially for the screwdriver's pose via optimization under visual and kinematic constraints, and for its applied wrench on the screw via solution drawing upon dynamics. This wrench, adjusted based on position/force feedback, is mapped by the grasp matrix to the desired fingertip forces, which are then used for computing torques to be exerted by the arm and hand to close the loop. Simulation and experiments with a Shadow Hand have been conducted for validations."
You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects,"Lei Zhou, Haozhe Wang, Zhengshen Zhang, Zhiyang Liu, Eng Hock Francis Tay, Marcelo Ang","National University of Singapore,NUS",Perception for Grasping and Manipulation I,"In the realm of robotic grasping, achieving accurate and reliable interactions with the environment is a pivotal challenge. Traditional methods of grasp planning methods utilizing partial point clouds derived from depth image often suffer from reduced scene understanding due to occlusion, ultimately impeding their grasping accuracy. Furthermore, scene reconstruction methods have primarily relied upon static techniques, which are susceptible to environment change during manipulation process limits their efficacy in real-time grasping tasks. To address these limitations, this paper introduces a novel two-stage pipeline for dynamic scene reconstruction. In the first stage, our approach takes scene scanning as input to register each target object with mesh reconstruction and novel object pose tracking. In the second stage, pose tracking is still performed to provide object poses in real-time, enabling our approach to transform the reconstructed object point clouds back into the scene. Unlike conventional methodologies, which rely on static scene snapshots, our method continuously captures the evolving scene geometry, resulting in a comprehensive and up-to-date point cloud representation. By circumventing the constraints posed by occlusion, our method enhances the overall grasp planning process and empowers state-of-the-art 6-DoF robotic grasping algorithms to exhibit markedly improved accuracy."
Thermoformed Electronic Skins for Conformal Tactile Sensor Arrays,"Peng Lu, Jiaming Liang, Bidan Huang, Sicheng Yang, Wang Wei Lee",Tencent,Perception for Grasping and Manipulation I,"Robots and prostheses are increasingly designed with curvilinear surfaces for functional, aesthetic, aerodynamic, and safety reasons. Electronic skins (e-skins) capable of sensing contact location and pressure across complex, non-developable surfaces are essential for empowering next-generation robots with tactile awareness. This will facilitate safe and natural human-machine interactions while enhancing object manipulation capabilities. Despite the evident advantages of conformal e-skins, current fabrication methods face significant challenges in realizing their full potential. In this paper, we introduce thermoforming as a technique to efficiently fabricate tactile sensitive e-skins that conform to curvilinear surfaces. The performance, repeatability and uniformity of the sensors are characterized in detail. We also present a custom calibration pipeline where accurate digital replicas of conformal e-skins are generated for use in simulations. Finally, we demonstrate the benefits of 3D e-skins in a tool manipulation task."
The GEM-C Controller for Load Compensation in Object Manipulation,"Emmanouil Papadakis, Markos Sigalas, Michail Vangos, Panos Trahanias","Foundation for Research and Technology - Hellas,Foundation for Research and Technology-Hellas,Foundation for Research and Technology – Hellas (FORTH)",Perception for Grasping and Manipulation I,"Nowadays, robotic arms are ubiquitously employed for object manipulation across a spectrum of applications, spanning from production lines to warehouses, and encompassing both stationary and mobile robotic systems. Among the most prevalent end-effectors, used for the majority of these applications, are the suction cups. The rudimentary act of grasping an object and relocating it, devoid of a cognizant awareness of the forces stemming from the object's motion and grip, can result in suboptimal and inefficient robot movements. In more dire circumstances, such negligent handling may precipitate detachment of the object from the end-effector, potentially incurring damage to either the object or the arm. In this paper, we build upon the advanced sensing and attaching capabilities of our suction cup MIGHTY, and introduce GEM-C, a novel Gravity, External forces and Motion Compensation controller, that constantly adapts the orientation of the suction cup so as to enhance the quality of attachment. Throughout all examined scenarios and experiments, our approach remarkably improved the robot's performance. by providing with the optimal end-effector pose while also reducing the stress on the motors and the overall power consumption. The derived results, clearly demonstrate the MIGHTY and GEM-C schema's potential for a wide range of long-term and complex robotic manipulation applications."
Sim-To-Real Grasp Detection with Global-To-Local RGB-D Adaptation,"Haoxiang Ma, Ran Qin, Modi Shi, Boyang Gao, Di Huang","Beihang University,Geometry Robotics Ltd. Harbin Institute of Technology",Perception for Grasping and Manipulation I,"This paper focuses on the sim-to-real issue of RGB-D grasp detection and formulates it as a domain adaptation problem. In this case, we present a global-to-local method to address hybrid domain gaps in RGB and depth data and insufficient multi-modal feature alignment. First, a self-supervised rotation pre-training strategy is adopted to deliver robust initialization for RGB and depth networks. We then propose a global-to-local alignment pipeline with individual global domain classifiers for scene features of RGB and depth images as well as a local one specifically working for grasp features in the two modalities. In particular, we propose a grasp prototype adaptation module, which aims to facilitate fine-grained local feature alignment by dynamically updating and matching the grasp prototypes from the simulation and real-world scenarios throughout the training process. Due to such designs, the proposed method substantially reduces the domain shift and thus leads to consistent performance improvements. Extensive experiments are conducted on the GraspNet-Planar benchmark and physical environment, and superior results are achieved which demonstrate the effectiveness of our method."
Residual-NeRF : Learning Residual NeRFs for Transparent Object Manipulation,"Bart Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski","Carnegie Mellon University,DSTA",Perception for Grasping and Manipulation I,"Transparent objects are ubiquitous in industry, pharmaceuticals, and households. Grasping and manipulating these objects is a significant challenge for robots. Existing methods have difficulty reconstructing complete depth maps for challenging transparent objects, leaving holes in the depth reconstruction. Recent work has shown neural radiance fields (NeRFs) work well for depth perception in scenes with transparent objects, and these depth maps can be used to grasp transparent objects with high accuracy. NeRF-based depth reconstruction can still struggle with especially challenging transparent objects and lighting conditions. In this work, we propose Residual-NeRF, a method to improve depth perception and training speed for transparent objects. Robots often operate in the same area, such as a kitchen. By first learning a background NeRF of the scene without transparent objects to be manipulated, we reduce the ambiguity faced by learning the changes with the new object. We propose training two additional networks: a residual NeRF learns to infer residual RGB values and densities, and a Mixnet learns how to combine background and residual NeRFs. We contribute synthetic and real experiments that suggest Residual-NeRF improves depth perception of transparent objects. The results on synthetic data suggest Residual-NeRF outperforms the baselines with a 46.1% lower RMSE and a 29.5% lower MAE. Real-world qualitative experiments suggest Residual-NeRF leads to more robust depth maps with less noise and fewer holes. Website: https://residual-nerf.github.io"
Online 3D Edge Reconstruction of Wiry Structures from Monocular Image Sequences,"Hyelim Choi, Minji Lee, Jiseock Kang, Dongjun Lee",Seoul National University,Perception for Grasping and Manipulation I,"Three-dimensional (3D) reconstruction of wiry structures from vision suffers from thin geometry, lack of texture, and severe self-occlusions. We propose an online 3D edge reconstruction framework that uses monocular image sequences to reconstruct the wiry structures whose skeletons are mainly straight as commonly found in the real world. To reconstruct such structures in an efficient manner, we employ straight edges constructed from points as underlying primitives of the representation. This is to address the harsh geometric nature of wiry objects (e.g., severe self-occlusion) and also to avoid a typically expensive line matching process. Specifically, we first construct sparse 3D points by tracking feature points, while simultaneously refining the camera poses via a robust maximum a posteriori (MAP) inference. These sparse points are then used to generate edge candidates and the belief of each candidate is updated in a Bayesian fashion using a likelihood evaluated on the image observation. Finally, we take the set of 3D edges with beliefs greater than a threshold and apply a post-processing step to reject false edges. We experimentally validate our framework using real-world wiry objects and demonstrate a manipulation task using the reconstruction. The proposed framework exhibits superior performance over state-of-the-art algorithms for the class of wiry structures and the potential to be easily used for subsequent robotic tasks."
Kitchen Artist: Precise Control of Liquid Dispensing for Gourmet Plating,"Hung-Jui Huang, Jingyi Xiang, Wenzhen Yuan","Carnegie Mellon University,University of Illinois at Urbana-Champaign,University of Illinois",Perception for Grasping and Manipulation I,"Manipulating liquid is widely required for many tasks, especially in cooking. A common way to address this is extruding viscous liquid from a squeeze bottle. In this work, our goal is to create a sauce plating robot, which requires precise control of the thickness of squeezed liquids on a surface. Different liquids demand different manipulation policies.We command the robot to rotate the container and monitor the liquid response using a force sensor to identify liquid properties. Based on the liquid properties, we predict the liquid behavior with fixed squeezing motions in a data-driven way and calculate the required drawing speed for the desired stroke size. This open-loop system works effectively even without sensor feedback. Our experiments demonstrate accurate stroke size control across different liquids and fill levels. We show that understanding liquid properties can facilitate effective liquid manipulation. More importantly, our dish garnishing robot has a wide range of applications and holds significant commercialization potential."
HandNeRF: Learning to Reconstruct Hand-Object Interaction Scene from a Single RGB Image,"Hongsuk Choi, Nikhil Chavan-dafle, Jiacheng Yuan, Volkan Isler, Hyun Soo Park","Samsung AI Center, New York,Samsung Research America,University of Minnesota",Perception for Grasping and Manipulation I,"This paper presents a method to learn hand-object interaction prior for reconstructing a 3D hand-object scene from a single RGB image. The inference as well as training-data generation for 3D hand-object scene reconstruction is challenging due to the depth ambiguity of a single image and occlusions by the hand and object. We turn this challenge into an opportunity by utilizing the hand shape to constrain the possible relative configuration of the hand and object geometry. We design a generalizable implicit function, HandNeRF, that explicitly encodes the correlation of the 3D hand shape features and 2D object features to predict the hand and object scene geometry. With experiments on real-world datasets, we show that HandNeRF is able to reconstruct hand-object scenes of novel grasp configurations more accurately than comparable methods. Moreover, we demonstrate that object reconstruction from HandNeRF ensures more accurate execution of downstream tasks, such as grasping for robotic hand-over."
Robotic Grasping of Harvested Tomato Trusses Using Vision and Online Learning,"Luuk Van Den Bent, Tomás Coleman, Robert Babuska","Technical University Delft,TU Delft,Delft University of Technology",Perception for Grasping and Manipulation I,"Currently, truss tomato weighing and packaging require significant manual work. The main obstacle to automation lies in the difficulty of developing a reliable robotic grasping system for already harvested trusses. We propose a method to grasp trusses that are stacked in a crate with considerable clutter, which is how they are commonly stored and transported after harvest. The method consists of a deep learning-based vision system to first identify the individual trusses in the crate and then determine a suitable grasping location on the stem. To this end, we have introduced a grasp pose ranking algorithm with online learning capabilities. After selecting the most promising grasp pose, the robot executes a pinch grasp without needing touch sensors or geometric models. Lab experiments with a robotic manipulator equipped with an eye-in-hand RGB-D camera showed a 100% clearance rate when tasked to pick all trusses from a pile. 93% of the trusses were successfully grasped on the first try, while the remaining 7% required more attempts."
RISeg: Robot Interactive Object Segmentation Via Body Frame-Invariant Features,"Howard H. Qian, Yangxiao Lu, Kejia Ren, Gaotian Wang, Ninad Khargonkar, Yu Xiang, Kaiyu Hang","Rice University,the University of Texas at Dallas,University of Texas at Dallas",Object Detection III,"In order to successfully perform manipulation tasks in new environments, such as grasping, robots must be proficient in segmenting unseen objects from the background and/or other objects. Previous works perform unseen object instance segmentation (UOIS) by training deep neural networks on large-scale data to learn RGB/RGB-D feature embeddings, where cluttered environments often result in inaccurate segmentations. We build upon these methods and introduce a novel approach to correct inaccurate segmentation, such as under-segmentation, of static image-based UOIS masks by using robot interaction and a designed body frame-invariant feature. We demonstrate that the relative linear and rotational velocities of frames randomly attached to rigid bodies due to robot interactions can be used to identify objects and accumulate corrected object-level segmentation masks. By introducing motion to regions of segmentation uncertainty, we are able to drastically improve segmentation accuracy in an uncertainty-driven manner with minimal, non-disruptive interactions (ca. 2-3 per scene). We demonstrate the effectiveness of our proposed interactive perception pipeline in accurately segmenting cluttered scenes by achieving an average object segmentation accuracy rate of 80.7%, an increase of 28.2% when compared with other state-of-the-art UOIS methods."
PUMA: Fully Decentralized Uncertainty-Aware Multiagent Trajectory Planner with Real-Time Image Segmentation-Based Frame Alignment,"Kota Kondo, Claudius Taroon Tewari, Mason B. Peterson, Annika Thomas, Jouko Kinnari, Andrea Tagliabue, Jonathan Patrick How","Massachusetts Institute of Technology,Saab Finland Oy",Object Detection III,"Fully decentralized, multiagent trajectory planners enable complex tasks like search and rescue or package delivery by ensuring safe navigation in unknown environments. However, deconflicting trajectories with other agents and en- suring collision-free paths in a fully decentralized setting is complicated by dynamic elements and localization uncertainty. To this end, this paper presents (1) an uncertainty-aware multiagent trajectory planner and (2) an image segmentation-based frame alignment pipeline. The uncertainty-aware planner propagates uncertainty associated with the future motion of detected obstacles, and by incorporating this propagated uncertainty into optimization constraints, the planner effectively navigates around obstacles. Unlike conventional methods that emphasize explicit obstacle tracking, our approach integrates implicit tracking. Sharing trajectories between agents can cause potential collisions due to frame misalignment. Addressing this, we introduce a novel frame alignment pipeline that rectifies inter-agent frame misalignment. This method leverages a zero-shot image segmentation model for detecting objects in the environment and a data association framework based on geometric consistency for map alignment. Our approach accurately aligns frames with only 0.18 m and 2.7 degrees of mean frame alignment error in our most challenging simulation scenario. In addition, we conducted hardware experiments and successfully achieved 0.29 m and 2.59 degrees of frame alignment error. Together with the alignment framework, our planner ensures safe navigation in unknown environments and collision avoidance in decentralized settings."
Open-Vocabulary Affordance Detection Using Knowledge Distillation and Text-Point Correlation,"Tuan Van Vo, Minh Nhat Vu, Baoru Huang, Toan Nguyen, Ngan Le, Thieu Vo, Anh Nguyen","FPT Software,TU Wien, Austria,Imperial College London,University of Arkansas,Ton Duc Thang University,University of Liverpool",Object Detection III,"Affordance detection presents intricate challenges and has a wide range of robotic applications. Previous works have faced limitations such as the complexities of 3D object shapes, the wide range of potential affordances on real-world objects, and the lack of open-vocabulary support for affordance understanding. In this paper, we introduce a new open-vocabulary affordance detection method in 3D point clouds, leveraging knowledge distillation and text-point correlation. Our approach employs pre-trained 3D models through knowledge distillation to enhance feature extraction and semantic understanding in 3D point clouds. We further introduce a new text-point correlation method to learn the semantic links between point cloud features and open-vocabulary labels. The intensive experiments show that our approach outperforms previous works and adapts to new affordance labels and unseen objects. Notably, our method achieves the improvement of 7.96% mIOU score compared to the baselines. Furthermore, it offers real-time inference which is well-suitable for robotic manipulation applications."
MEDL-U: Uncertainty-Aware 3D Automatic Annotation Based on Evidential Deep Learning,"Helbert Paat, Qing Lian, Weilong Yao, Tong Zhang","HKUST,Autowise.AI,Hong Kong University of Science and Technology",Object Detection III,"Advancements in deep learning-based 3D object detection necessitate the availability of large-scale datasets. However, this requirement introduces the challenge of manual annotation, which is often both burdensome and time-consuming. To tackle this issue, the literature has seen the emergence of several weakly supervised frameworks for 3D object detection which can automatically generate pseudo labels for unlabeled data. Nevertheless, these generated pseudo labels contain noise and are not as accurate as those labeled by humans. In this paper, we present the first approach that addresses the inherent ambiguities present in pseudo labels by introducing an Evidential Deep Learning (EDL) based uncertainty estimation framework. Specifically, we propose MEDL-U, an EDL framework based on MTrans, which not only generates pseudo labels but also quantifies the associated uncertainties. However, applying EDL to 3D object detection presents three key challenges: (1) lower pseudo label quality in comparison to other autolabelers; (2) high evidential uncertainty estimates; and (3) lack of clear interpretability and effective utilization of uncertainties for downstream tasks. We tackle these issues through the introduction of an uncertainty-aware IoU-based loss, an evidence-aware multi-task loss, and the implementation of a post-processing stage for uncertainty refinement. Our experimental results demonstrate that probabilistic detectors trained using the outputs of MEDL-U surpass deterministic detectors trained using outputs from previous 3D annotators on the KITTI val set for all difficulty levels. Moreover, MEDL-U achieves state-of-the-art results on the KITTI official test set compared to existing 3D automatic annotators. Code is publicly available at https://github.com/paathelb/MEDL-U."
HandyPriors: Physically Consistent Perception of Hand-Object Interactions with Differentiable Priors,"Shutong Zhang, Yi-ling Qiao, Guanglei Zhu, Eric Heiden, Dylan Turpin, Jingzhou Liu, Ming C. Lin, Miles Macklin, Animesh Garg","University of Toronto,University of Maryland, College Park,University of Toronto, Carnegie Mellon University,NVIDIA,University of Toronto, NVIDIA,University of Maryland at College Park,University of Copenhagen, NVIDIA,Georgia Institute of Technology",Object Detection III,"Various heuristic objectives for modeling hand-object interaction have been proposed in past work. However, due to the lack of a cohesive framework, these objectives often possess a narrow scope of applicability and are limited by their efficiency or accuracy. In this paper, we propose HandyPriors, a unified and general pipeline for human-object interaction scenes by leveraging recent advances in differentiable physics and rendering. Our approach employs rendering priors to align with input images and segmentation masks along with physics priors to mitigate penetration and relative-sliding across frames. Furthermore, we present two alternatives for hand and object pose estimation. The optimization-based pose estimation achieves higher accuracy, while the filtering-based tracking, which utilizes the differentiable priors as dynamics and observation models, executes faster. We demonstrate that HandyPriors attains comparable or superior results in the pose estimation task, and that the differentiable physics module can predict contact information for pose refinement. We also show that our approach generalizes to perception tasks, including robotic hand manipulation and human-object pose estimation in the wild."
Dynamic Occupancy Grids for Object Detection: A Radar-Centric Approach,"Max Peter Ronecker, Markus Schratter, Lukas Kuschnig, Daniel Watzenig","SETLabs Research GmbH,Virtual Vehicle Research GmbH,TU Graz",Object Detection III,"Dynamic Occupancy Grid Mapping is a technique used to generate a local map of the environment, containing both static and dynamic information. Typically, these maps are primarily generated using lidar measurements. However, with improvements in radar sensing, resulting in better accuracy and higher resolution, radar is emerging as a viable alternative to lidar as the primary sensor for mapping. In this paper, we propose a radar-centric dynamic occupancy grid mapping algorithm with adaptations to the state computation, inverse sensor model, and field-of-view computation tailored to the specifics of radar measurements. We extensively evaluate our approach with real data to demonstrate its effectiveness and establish the first benchmark for radar-based dynamic occupancy grid mapping using the publicly available Radarscenes dataset."
Dynamic Object Classification of Low-Resolution Point Clouds: An LSTM-Based Ensemble Learning Approach,"Shaoming Zhang, Tangjun Yao, Jianmei Wang, Tiantian Feng, Zhong Wang",Tongji University,Object Detection III,"In unmanned vehicle perception, dynamic object classification is applied to classify objects accurately and timely, providing decision-making for obstacle avoidance and planning. Low-resolution LiDAR is one of the most important sensors for this task. Unfortunately, the existing approaches perform unsatisfactorily due to the huge domain gap between low-resolution and high-resolution point cloud classification. Some schemes try to reduce the gap by fusing multi-scan information through SLAM or completing single-scan point clouds. However, these methods rely on high positioning accuracy or the wholeness of object data. To this end, differently, we propose a dynamic object classification method of low-resolution data from the perspective of time-series fusion. By modeling time series of sparse data, we indicate change rules of separate classification models for object representation. Subsequently, based on ensemble learning, our method performs feature-level fusion on multiple networks to exploit their different expression capabilities. Finally, we utilize long short-term memory to gradually classify dynamic objects. Besides, we also propose a dataset of the low-resolution point clouds and manually annotate the ground truth, which contains abundant samples of cars, pedestrians, and motorcycles. Through testing actual low-resolution data, the accuracy of our method is verified to improve a lot than the state-of-the-art approaches."
Dynablox: Real-Time Detection of Diverse Dynamic Objects in Complex Environments,"Lukas Maximilian Schmid, Olov Andersson, Aurelio Sulser, Patrick Pfreundschuh, Roland Siegwart","Massachusetts Institute of Technology (MIT),KTH Royal Institute,ETH Zurich",Object Detection III,"Real-time detection of moving objects is an essential capability for robots acting autonomously in dynamic environments. We thus propose Dynablox, a novel online mapping-based approach for robust moving object detection in complex unstructured environments. The central idea of our approach is to incrementally estimate high confidence free-space areas by modeling and accounting for sensing, state estimation, and mapping limitations during online robot operation. The spatio-temporally conservative free space estimate enables robust detection of moving objects without making any assumptions on the appearance of objects or environments. This allows deployment in complex scenes such as multi-storied buildings or staircases, and for diverse moving objects such as people carrying various items, doors swinging or even balls rolling around. We thoroughly evaluate our approach on real-world data sets, achieving 86% IoU at 17 FPS in typical robotic settings. The method outperforms a recent appearance-based classifier and approaches the performance of offline methods. We demonstrate its generality on a novel data set with rare moving objects in complex environments. We make our efficient implementation and the novel data set available as open-source."
3D Object Detection with VI-SLAM Point Clouds: The Impact of Object and Environment Characteristics on Model Performance,"Lin Duan, Tim Scargill, Ying Chen, Maria Gorlatova",Duke University,Object Detection III,"3D object detection (OD) is a crucial element in scene understanding. However, most existing 3D OD models have been tailored to work with light detection and ranging (LiDAR) and RGB-D point cloud data, leaving their performance on commonly available visual-inertial simultaneous localization and mapping (VI-SLAM) point clouds unexamined. In this paper, we create and release two datasets: VIP500, 4772 VI-SLAM point clouds covering 500 different object and environment configurations, and VIP500-D, an accompanying set of 20 RGB-D point clouds for the object classes and shapes in VIP500. We then use these datasets to quantify the differences between VI-SLAM point clouds and dense RGB-D point clouds, as well as the discrepancies between VI-SLAM point clouds generated with different object and environment characteristics. Finally, we evaluate the performance of three leading OD models on the diverse data in our VIP500 dataset, revealing the promise of OD models trained on VI-SLAM data; we examine the extent to which both object and environment characteristics impact performance, along with the underlying causes."
ERRA: An Embodied Representation and Reasoning Architecture for Long-Horizon Language-Conditioned Manipulation Tasks,"Chao Zhao, Shuai Yuan, Chunli Jiang, Junhao Cai, Hongyu Yu, Michael Yu Wang, Qifeng Chen","Hong Kong University of Science and Technology,The Hong Kong University of Science and Technology,[email protected],HKUST",Robotics with Large Language Models,"This letter introduces ERRA, an embodied learning architecture that enables robots to jointly obtain three fundamental capabilities (reasoning, planning, and interaction) for solving long-horizon language-conditioned manipulation tasks. ERRA is based on tightly-coupled probabilistic inferences at two granularity levels. A coarse-resolution inference is formulated as sequence generation through a large language model, which infers action language from natural language instruction and environment state. The robot then zooms to the fine-resolution inference part to perform the concrete action corresponding to the action language. Fine-resolution inference is constructed as a Markov decision process, which takes action language and environmental sensing as observations and outputs the action. The results of action execution in environments provide feedback for subsequent coarse-resolution reasoning. Such coarse-to-fine inference allows the robot to decompose and achieve long-horizon tasks interactively. In extensive experiments, we show that ERRA can complete various long-horizon manipulation tasks specified by abstract language instructions. We also demonstrate successful generalization to the novel but similar natural language instructions."
Grasp-Anything: Large-Scale Grasp Dataset from Foundation Models,"An Dinh Vuong, Minh Nhat Vu, Le Trung Hieu, Baoru Huang, Huynh Thi Thanh Binh, Thieu Vo, Andreas Kugi, Anh Nguyen","FPT Software,TU Wien, Austria,Imperial College London,School of Information and Communication Technology (Hanoi Univer,Ton Duc Thang University,TU Wien,University of Liverpool",Robotics with Large Language Models,"Foundation models such as ChatGPT have made significant strides in robotic tasks due to their universal representation of real-world domains. In this paper, we leverage foundation models to tackle grasp detection, a persistent challenge in robotics with broad industrial applications. Despite numerous grasp datasets, their object diversity remains limited compared to real-world figures. Fortunately, foundation models possess an extensive repository of real-world knowledge, including objects we encounter in our daily lives. As a consequence, a promising solution to the limited representation in previous grasp datasets is to harness the universal knowledge embedded in these foundation models. We present Grasp-Anything, a new large-scale grasp dataset synthesized from foundation models to implement this solution. Grasp-Anything excels in diversity and magnitude, boasting 1M samples with text descriptions and more than 3M objects, surpassing prior datasets. Empirically, we show that Grasp-Anything successfully facilitates zero-shot grasp detection on vision-based tasks and real-world robotic experiments. Our dataset and code are available at https://grasp-anything-2023.github.io."
Anticipate & Act : Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments,"Raghav Arora, Shivam Singh, Karthik Swaminathan, Ahana Datta, Snehasis Banerjee, Brojeshwar Bhowmick, Krishna Murthy, Mohan Sridharan, Madhava Krishna","IIIT Hyderabad,International Institute of Information Technology Hyderabad,International Institutue of Information Technology - Hyderabad (,International Institute of Information Technology, Hyderabad,IIIT-H / TCS,Tata Consultancy Services,MIT,University of Edinburgh",Robotics with Large Language Models,"Assistive agents performing household tasks such as making the bed or cooking breakfast often compute and execute actions that accomplish one task at a time. However, efficiency can be improved by anticipating upcoming tasks and computing an action sequence that jointly achieves these tasks. State-of-the-art methods for task anticipation use data-driven deep networks and Large Language Models (LLMs), but they do so at the level of high-level tasks and/or require many training examples. Our framework leverages the generic knowledge of LLMs through a small number of prompts to perform high-level task anticipation, using the anticipated tasks as goals in a classical planning system to compute a sequence of finer-granularity actions that jointly achieve these goals. We ground and evaluate our framework's abilities in realistic scenarios in the VirtualHome environment and demonstrate a 31% reduction in execution time compared with a system that does not consider upcoming tasks."
Conditionally Combining Robot Skills Using Large Language Models,"K.r. Zentner, Ryan Julian, Brian Ichter, Gaurav Sukhatme","University of Southern California,Google,Google Brain",Robotics with Large Language Models,"This paper combines two contributions. First, we introduce an extension of the Meta-World benchmark, which we call ``Language-World,'' which allows a large language model to operate in a simulated robotic environment using semi-structured natural language queries and scripted skills described using natural language. By using the same set of tasks as Meta-World, Language-World results can be easily compared to Meta-World results, allowing for a point of comparison between recent methods using Large Language Models (LLMs) and those using Deep Reinforcement Learning. Second, we introduce a method we call Plan Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of high-level plans using end-to-end demonstrations. Using Language-World, we show that PCBC is able to achieve strong performance in a variety of few-shot regimes, often achieving task generalization with as little as a single demonstration. We have made Language-World available as open-source software at https://github.com/krzentner/language-world/"
Interactive Planning Using Large Language Models for Partially Observable Robotic Tasks,"Lingfeng Sun, Devesh Jha, Chiori Hori, Siddarth Jain, Radu Ioan Corcodel, Xinghao Zhu, Masayoshi Tomizuka, Diego Romeres","University of California, Berkeley,Mitsubishi Electric Research Laboratories,Mitsubishi Electric Research Laboratories (MERL),University of California,Mitsubishi Electric research laboratories",Robotics with Large Language Models,"Designing robotic agents to perform open vocabulary tasks has been the long-standing goal in robotics and AI. Recently, Large Language Models (LLMs) have achieved impressive results in creating robotic agents for performing open vocabulary tasks. However, planning for these tasks in the presence of uncertainties is challenging as it requires enquote{chain-of-thought} reasoning, aggregating information from the environment, updating state estimates, and generating actions based on the updated state estimates. In this paper, we present an interactive planning technique for partially observable tasks using LLMs. In the proposed method, an LLM is used to collect missing information from the environment using a robot, and infer the state of the underlying problem from collected observations while guiding the robot to perform the required actions. We also use a fine-tuned Llama 2 model via self-instruct and compare its performance against a pre-trained LLM like GPT-4. Results are demonstrated on several tasks in simulation as well as real-world environments."
Optimal Scene Graph Planning with Large Language Model Guidance,"Zhirui Dai, Arash Asgharivaskasi, Thai Duong, Shusen Lin, Mariliza Tzes, George J. Pappas, Nikolay A. Atanasov","UC San Diego,University of California, San Diego,University of Pennsylvania",Robotics with Large Language Models,"Recent advances in metric, semantic, and topological mapping have equipped autonomous robots with concept grounding capabilities to interpret natural language tasks. Leveraging these capabilities, this work develops an efficient task planning algorithm for hierarchical metric-semantic models. We consider a scene graph model of the environment and utilize a large language model (LLM) to convert a natural language task into a linear temporal logic (LTL) automaton. Our main contribution is to enable optimal hierarchical LTL planning with LLM guidance over scene graphs. To achieve efficiency, we construct a hierarchical planning domain that captures the attributes and connectivity of the scene graph and the task automaton, and provide semantic guidance via an LLM heuristic function. To guarantee optimality, we design an LTL heuristic function that is provably consistent and supplements the potentially inadmissible LLM guidance in multi-heuristic planning. We demonstrate efficient planning of complex natural language tasks in scene graphs of virtualized real environments."
CAPE: Corrective Actions from Precondition Errors Using Large Language Models,"Shreyas Sundara Raman, Vanya Cohen, Ifrah Idrees, Eric Rosen, Raymond Mooney, Stefanie Tellex, David Paulius","Brown University,University of Texas at Austin,Brown",Robotics with Large Language Models,"Extracting knowledge and reasoning from large language models (LLMs) offers a path to designing intelligent robots. Common approaches that leverage LLMs for planning are unable to recover when actions fail and resort to retrying failed actions without resolving the underlying cause. We propose a novel approach (CAPE) that generates corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans through few-shot reasoning on action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while maintaining semantic correctness and minimizing re-prompting. In VirtualHome, CAPE improves a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan, whilst achieving competitive executability. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves correctness by 76.49% with higher executability compared to SayCan. Our approach enables embodied agents to follow natural language commands and robustly recover from failures."
GraspGPT: Leveraging Semantic Knowledge from a Large Language Model for Task-Oriented Grasping,"Chao Tang, Dehao Huang, Wenqi Ge, Weiyu Liu, Hong Zhang","Southern University of Science and Technology,Stanford University,SUSTech",Robotics with Large Language Models,"Task-oriented grasping (TOG) refers to the problem of predicting grasps on an object that enable subsequent manipulation tasks. To model the complex relationships between objects, tasks, and grasps, existing methods incorporate semantic knowledge as priors into TOG pipelines. However, the existing semantic knowledge is typically constructed based on closed-world concept sets, restraining the generalization to novel concepts out of the pre-defined sets. To address this issue, we propose GraspGPT, a large language model (LLM) based TOG framework that leverages the open-end semantic knowledge from an LLM to achieve zero-shot generalization to novel concepts. We conduct experiments on Language Augmented TaskGrasp (LA-TaskGrasp) dataset and demonstrate that GraspGPT outperforms existing TOG methods on different held-out settings when generalizing to novel concepts out of the training set. The effectiveness of GraspGPT is further validated in real-robot experiments. Our code, data, appendix, and video are publicly available at https://sites.google.com/view/graspgpt."
DOS: A Deployment Operating System for RoboOps,"Guo Ye, Qinjie Lin, Zening Luo, Han Liu",Northwestern University,Robotics with Large Language Models,"We propose a new system named DOS(D eployment O perating System for RoboOps) for reliably deploying any data driven robots in both production and simulation environments. Compared to existing systems, DOS features a unique CI/CD (continuous i ntegration and continuous deployment) architecture which allows us to seamlessly integrate agile development and reliable operation in a fully automated fashion. With this CI/CD architecture, this paper mainly introduces three essential components that uniquely differentiate DOS from existing robotic systems: (i) A cloud orchestrator that build and schedule pipeline components; (ii) A bridge tool named DOS Connect that make cloud-edge bidirectional communication feasible; (iii) An analytical profiler that collects any set of user-defined performance metrics for system optimization. DOS significantly increases the reliability and maintainability of the deployed robotic systems. To illustrate this point, we demonstrate performance of DOS by training and deploying deep reinforcement learning based methods in a challenging real-world environment with a swarm of robots."
Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving,"Long Chen, Oleg Sinavski, Jan Hünermann, Alice Karnsund, Andrew Willmott, Danny Birch, Daniel Maund, Jamie Shotton","Wayve,Wayve Technologies Ltd",Autonomous Vehicle Navigation I,"Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available for further exploration."
HHGNN: Heterogeneous Hypergraph Neural Network for Traffic Agents Trajectory Prediction in Grouping Scenarios,"Hetian Guo, Yingzhi Peng, Zipei Fan, He Zhu, Xuan Song","Southern University of Science and Technology,University of Tokyo",Autonomous Vehicle Navigation I,"In many intelligent transportation systems, predicting the future motion of heterogeneous traffic participants is a fundamental but challenging task due to various factors encompassing the agents' dynamic states, interactions with neighboring agents and surrounding traffic infrastructures, and their stochastic and multi-modal natural behavior tendencies. However, existing approaches have limitations as they either focus solely on static, pairwise interactions, ignoring interactions of varied granularity, or fail to tackle agents' heterogeneity. In this paper, instead of focusing solely on pairwise interactions, we propose a Heterogenous Hypergraph Graph Neural Network (HHGNN) based motion prediction model that leverages the nature of hypergraph to encode the groupwise interactions among traffic participants. Moreover, we propose the type-aware two-level hypergraph message passing module (TTHMS) with learnable hyperedge-type embeddings to model the intra-group and inter-group level interactions among heterogeneous traffic agents (e.g., vehicles, pedestrians, and cyclists). Besides, We integrate a scene context fusion layer in TTHMS to incorporate the scene context. Comparison and ablation experiments on the Waymo Open Motion Dataset (WOMD) demonstrate HHGNN's effectiveness within the motion prediction task."
Odometry Estimation by Fusing Multiple Radar Sensors and an Inertial Measurement Unit,"Tim Brühl, Tim Dieter Eberhardt, Robin Schwager, Lukas Ewecker, Tin Stribor Sohn, Soeren Hohmann","Dr. Ing. h.c. F. Porsche AG,HTW Berlin,Porsche AG,Dr. Ing h.c. F. Porsche AG,Institute of Control Systems, Karlsruhe Institute of Technology",Autonomous Vehicle Navigation I,"This paper presents a framework for odometry estimation in automotive application using six asynchronously operating millimeter wave radar sensors and a combination of gyroscope and accelerometer. Two different motion models are combined to estimate motion with three degrees of freedom. For this purpose, we propose a novel three-part radar filtering method for outlier detection: By analyzing uncertainties and system limits, sensor-specific outliers are detected and removed in the first filter. We introduce knowledge about the previous motion state by a status-quo-ante filter and hereby identify further false positive raw targets in the current measure which are not accessible from the previous state. Moreover, we suggest employing a downstream, resampling-based algorithm for additional outlier detection. Based on the filtered data, radar motion state estimation is performed by use of curve fitting methods. To fuse the radar odometry estimation with the acceleration and yaw rate measurements handling non-linearities, an Unscented Kalman Filter is used. The developed framework is evaluated with reference data in various scenarios. The results demonstrate that it accurately and robustly determines motion and position states even in radar-challenging scenes, such as environments with few radar targets or with heavy metal structures. Our method keeps up with common approaches such as wheel speed sensor odometry while outperforming it in terms of drift-impairment."
Thermal Voyager: A Comparative Study of RGB and Thermal Cameras for Night-Time Autonomous Navigation,"Aditya N G, Dhruval Pb, Jehan Shalabi, Shubhankar Jape, Xueji Wang, Zubin Jacob","PES University,Purdue University",Autonomous Vehicle Navigation I,"Achieving reliable autonomous navigation during nighttime remains a substantial obstacle in the field of robotics. Although systems utilizing Light Detection and Ranging (LiDAR) and Radio Detection and Ranging (RADAR) enable environmental perception regardless of lighting conditions, they face significant challenges in environments with a high density of agents due to their dependence on active emissions. Cameras operating in the visible spectrum represent a quasi-passive alternative, yet they see a substantial drop in efficiency in low-light conditions, consequently hindering both scene perception and path planning. Here, we introduce a novel end-to-end navigation system, the ""Thermal Voyager"", which leverages infrared thermal vision to achieve true passive perception in autonomous entities. The system utilizes our architecture, TrajNet to interpret thermal visual inputs to produce desired trajectories and employs a model predictive control strategy to determine the optimal steering angles needed to actualize those trajectories. We train our TrajNet on a comprehensive video dataset incorporating visible and thermal footage alongside Controller Area Network (CAN) frames. We demonstrate that nighttime navigation facilitated by Long-Wave Infrared (LWIR) thermal cameras can rival the performance of daytime navigation systems using RGB cameras. Our work paves the way for scene perception and trajectory prediction empowered entirely by passive thermal sensing technology, heralding a new era where autonomous navigation is both feasible and reliable irrespective of the time of day. We make our code and thermal trajectory dataset public."
Rethinking Imitation-Based Planners for Autonomous Driving,"Jie Cheng, Yingbing Chen, Xiaodong Mei, Bowen Yang, Bo Li, Ming Liu","Hong Kong University of Science and Technology,The Hongkokng University of Science and Technology,HKUST,The Hong Kong University of Science and Technology, Robotics Ins,Lotus Technology Ltd.,Hong Kong University of Science and Technology (Guangzhou)",Autonomous Vehicle Navigation I,"In recent years, imitation-based driving planners have reported considerable success. However, due to the absence of a standardized benchmark, the effectiveness of various designs remains unclear. The newly released nuPlan addresses this issue by offering a large-scale real-world dataset and a standardized closed-loop benchmark for equitable comparisons. Utilizing this platform, we conduct a comprehensive study on two fundamental yet underexplored aspects of imitation-based planners: the essential features for ego planning and the effective data augmentation techniques to reduce compounding errors. Furthermore, we highlight an imitation gap that has been overlooked by current learning systems. Finally, integrating our findings, we propose a strong baseline modelâ€”PlanTF. Our results demonstrate that a well-designed, purely imitation-based planner can achieve highly competitive performance compared to state-of-the-art methods involving hand-crafted rules and exhibit superior generalization capabilities in long-tail cases. Our models and benchmarks are publicly available. Project website https://jchengai.github.io/planTF."
Traffic Flow-Based Crowdsourced Mapping in Complex Urban Scenario,"Tong Qin, Haihui Huang, Ziqiang Wang, Tongqing Chen, Wenchao Ding","Shanghai Jiao Tong University,Zhejiang University,Autonomous Driving Solution, IAS BU, Huawei,Huawei Technology,Fudan University",Autonomous Vehicle Navigation I,"An accurate road topological structure is of great importance for autonomous driving in complex urban environments. Currently, most autonomous vehicles highly rely on the High-Definition map (HD map) to cruise across the city. Without the prior map, it's hard for vehicles to find right-turning and left-turning ways in large intersections. However, due to the complexity of intersections, producing such a map by human resources is time-consuming and error-prone. In this paper, we proposed a framework to automatically produce the topological map of complicated intersections. This framework adopts the crowdsourcing way to collect semantic information about the environment and traffic flows. The topological structure is inferred from traffic flows correctly and automatically. We highlight that this framework is highly automatic and scalable, which can greatly speed up HD map production and decrease the cost. The proposed system is validated by real-world crowdsourcing data and the result is comparable to the traditional HD maps."
Scene Informer: Anchor-Based Occlusion Inference and Trajectory Prediction in Partially Observable Environments,"Bernard Lange, Jiachen Li, Mykel Kochenderfer","Stanford University,University of California, Riverside",Autonomous Vehicle Navigation I,"Navigating complex and dynamic environments requires autonomous vehicles (AVs) to reason about both visible and occluded regions. This involves predicting the future motion of observed agents, inferring occluded ones, and modeling their interactions based on vectorized scene representations of the partially observable environment. However, prior work on occlusion inference and trajectory prediction have developed in isolation, with the former based on simplified rasterized methods and the latter assuming full environment observability. We introduce the Scene Informer, a unified approach for predicting both observed agent trajectories and inferring occlusions in a partially observable setting. It uses a transformer to aggregate various input modalities and facilitate selective queries on occlusions that might intersect with the AV's planned path. The framework estimates occupancy probabilities and likely trajectories for occlusions, as well as forecast motion for observed agents. We explore common observability assumptions in both domains and their performance impact. Our approach outperforms existing methods in both occupancy prediction and trajectory prediction in partially observable setting on the Waymo Open Motion Dataset."
Monocular Localization with Semantics Map for Autonomous Vehicles,"Jixiang Wan, Xudong Zhang, Shuzhou Dong, Yuwei Zhang, Yuchen Yang, Wu Ruoxi, Ye Jiang, Jijunnan Li, Jinquan Lin, Ming Yang","OPPO Research Institute,OPPO,OPPO Research Institute, Shanghai, China,Shanghai Jiao Tong University",Autonomous Vehicle Navigation I,"Accurate and robust localization remains a significant challenge for autonomous vehicles. The cost of sensors and limitations in local computational efficiency make it difficult to scale to large commercial applications. Traditional vision-based approaches focus on texture features that are susceptible to changes in lighting, season, perspective, and appearance. Additionally, the large storage size of maps with descriptors and complex optimization processes hinder system performance. To balance efficiency and accuracy, we propose a novel lightweight visual semantic localization algorithm that employs stable semantic features instead of low-level texture features. First, semantic maps are constructed offline by detecting semantic objects, such as ground markers, lane lines, and poles, using cameras or LiDAR sensors. Then, online visual localization is performed through data association of semantic features and map objects. We evaluated our proposed localization framework in the publicly available KAIST Urban dataset and in scenarios recorded by ourselves. The experimental results demonstrate that our method is a reliable and practical localization solution in various autonomous driving localization tasks."
DiPA: Probabilistic Multi-Modal Interactive Prediction for Autonomous Driving,"Anthony Knittel, Majd Hawasly, Stefano V. Albrecht, John Redford, Subramanian Ramamoorthy","Five AI,FiveAI Ltd,University of Edinburgh,Five AI Ltd,The University of Edinburgh",Autonomous Vehicle Navigation I,"Accurate prediction is important for operating an autonomous vehicle in interactive scenarios. Prediction must be fast, to support multiple requests from a planner exploring a range of possible futures. The generated predictions must accurately represent the probabilities of predicted trajectories, while also capturing different modes of behaviour (such as turning left vs continuing straight at a junction). To this end, we present DiPA, an interactive predictor that addresses these challenging requirements. Previous interactive prediction methods use an encoding of k-mode-samples, which under-represents the full distribution. Other methods optimise closest-mode evaluations, which test whether one of the predictions is similar to the ground-truth, but allow additional unlikely predictions to occur, over-representing unlikely predictions. DiPA addresses these limitations by using a Gaussian-Mixture-Model to encode the full distribution, and optimising predictions using both probabilistic and closest-mode measures. These objectives respectively optimise probabilistic accuracy and the ability to capture distinct behaviours, and there is a challenging trade-off between them. We are able to solve both together using a novel training regime. DiPA achieves new state-of-the-art performance on the INTERACTION and NGSIM datasets, and improves over the baseline (MFP) when both closest-mode and probabilistic evaluations are used. This demonstrates effective prediction for supporting a pla"
MacFormer: Map-Agent Coupled Transformer for Real-Time and Robust Trajectory Prediction,"Chen Feng, Hangning Zhou, Huadong Lin, Zhigang Zhang, Ziyao Xu, Chi Zhang, Boyu Zhou, Shaojie Shen","Hong Kong University of Science and Technology,Megvii,Beihang University,Megvii.Inc,Mach,Sun Yat-sen University",Intelligent Transportation Systems IV,"Predicting the future behavior of agents is a fundamental task in autonomous vehicle domains. Accurate prediction relies on comprehending the surrounding map, which significantly regularizes agent behaviors. However, existing methods have limitations in exploiting the map and exhibit a strong dependence on historical trajectories, which yield unsatisfactory prediction performance and robustness. Additionally, their heavy network architectures impede real-time applications. To tackle these problems, we propose Map-Agent Coupled Transformer (MacFormer) for real-time and robust trajectory prediction. Our framework explicitly incorporates map constraints into the network via two carefully designed modules named coupled map and reference extractor. A novel multi-task optimization strategy (MTOS) is presented to enhance learning of topology and rule constraints. We also devise bilateral query scheme in context fusion for a more efficient and lightweight network. We evaluated our approach on Argoverse 1, Argoverse 2, and nuScenes real-world benchmarks, where it all achieved state-of-the-art performance with the lowest inference latency and smallest model size. Experiments also demonstrate that our framework is resilient to imperfect tracklet inputs. Furthermore, we show that by combining with our proposed strategies, classical models outperform their baselines, further validating the versatility of our framework."
Real-Time Capable Decision Making for Autonomous Driving Using Reachable Sets,"Niklas Kochdumper, Stanley Bak",Stony Brook University,Intelligent Transportation Systems IV,"Despite large advances in recent years, real-time capable motion planning for autonomous road vehicles re- mains a huge challenge. In this work, we present a decision module that is based on set-based reachability analysis: First, we identify all possible driving corridors by computing the reachable set for the longitudinal position of the vehicle along the lanelets of the road network, where lane changes are modeled as discrete events. Next, we select the best driving corridor based on a cost function that penalizes lane changes and deviations from a desired velocity profile. Finally, we generate a reference trajectory inside the selected driving corridor, which can be used to guide or warm start low-level trajectory planners. For the numerical evaluation we combine our decision module with a motion-primitive-based and an optimization-based planner and evaluate the performance on 2000 challenging CommonRoad traffic scenarios as well in the realistic CARLA simulator. The results demonstrate that our decision module is real-time capable and yields significant speed-ups compared to executing a motion planner standalone without a decision module."
Vehicle Behavior Prediction by Episodic-Memory Implanted NDT,"Peining Shen, Jianwu Fang, Hongkai Yu, Jianru Xue","Chang'an University,Xian Jiaotong University,Cleveland State University,Xi'an Jiaotong University",Intelligent Transportation Systems IV,"In autonomous driving, predicting the behavior (turning left, stopping, etc.) of target vehicles is crucial for the self-driving vehicle to make safe decisions and avoid accidents. Existing deep learning-based methods have shown excellent and accurate performance, but the black-box nature makes it untrustworthy to apply them in practical use. In this work, we explore the interpretability of behavior prediction of target vehicles by an Episodic Memory implanted Neural Decision Tree (abbrev. eMem-NDT). The structure of eMem-NDT is constructed by hierarchically clustering the text embedding of vehicle behavior descriptions. eMem-NDT is a neural-backed part of a pre-trained deep learning model by changing the soft-max layer of the deep model to eMem-NDT, for grouping and aligning the memory prototypes of the historical vehicle behavior features in training data on a neural decision tree. Each leaf node of eMem-NDT is modeled by a neural network for aligning the behavior memory prototypes. By eMem-NDT, we infer each instance in behavior prediction of vehicles by bottom-up Memory Prototype Matching (MPM) (searching the appropriate leaf node and the links to the root node) and top-down Leaf Link Aggregation (LLA) (obtaining the probability of future behaviors of vehicles for certain instances). We validate eMem-NDT on BLVD and LOKI datasets, and the results show that our model can obtain a superior performance to other methods with clear explainability. The code is available in https://github.com/JWFangit/eMem-NDT."
Optimal Driver Warning Generation in Dynamic Driving Environment,"Chenran Li, Aolin Xu, Enna Sachdeva, Teruhisa Misu, Behzad Dariush","University of California, Berkeley,HRI,Honda Research Institute,Honda Research Institute USA, Inc.,Honda Research Institute USA",Intelligent Transportation Systems IV,"The driver warning system that alerts the human driver about potential risks during driving is a key feature of an advanced driver assistance system. Existing driver warning technologies, mainly the forward collision warning and unsafe lane change warning, can reduce the risk of collision caused by human errors. However, the current design methods have several major limitations. Firstly, the warnings are mainly generated in a one-shot manner without modeling the ego driver's reactions and surrounding objects, which reduces the flexibility and generality of the system over different scenarios. Additionally, the triggering conditions of warning are mostly rule-based threshold-checking given the current state, which lacks the prediction of the potential risk in a sufficiently long future horizon. In this work, we study the problem of optimally generating driver warnings by considering the interactions among the generated warning, the driver behavior, and the states of ego and surrounding vehicles on a long horizon. The warning generation problem is formulated as a partially observed Markov decision process (POMDP). An optimal warning generation framework is proposed as a solution to the proposed POMDP. The simulation experiments demonstrate the superiority of the proposed solution to the existing warning generation methods."
Active Learning with Dual Model Predictive Path-Integral Control for Interaction-Aware Autonomous Highway On-Ramp Merging,"Jacob Knaup, Jovin D'sa, Behdad Chalaki, Tyler Naes, Hossein Nourkhiz Mahjoub, Ehsan Moradi-Pari, Panagiotis Tsiotras","Georgia Institute of Technology,Honda Research Institute, USA,Honda Research Institute USA, Inc.,Honda Research Institute US,Honda Research Institute,Georgia Tech",Intelligent Transportation Systems IV,"Merging into dense highway traffic for an autonomous vehicle is a complex decision-making task, wherein the vehicle must identify a potential gap and coordinate with surrounding human drivers, each of whom may exhibit diverse driving behaviors. Many existing methods consider other drivers to be dynamic obstacles and, as a result, they are incapable of capturing the full intent of the human drivers through this passive planning. In this paper, we propose a novel dual control framework based on Model Predictive Path-Integral control to generate interactive trajectories. This framework incorporates a Bayesian inference approach to actively learn the agentsâ€™ parameters, i.e., other driversâ€™ model parameters. The proposed framework employs a sampling-based approach that is suitable for real-time implementation through the utilization of GPUs. We illustrate the effectiveness of our proposed methodology through comprehensive numerical simulations conducted in both high and low-fidelity simulation scenarios focusing on autonomous on-ramp merging."
Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions,"Daniel Bogdoll, Jing Qin, Moritz Nekolla, Ahmed Abouelazm, Tim Joseph, Johann Marius Zöllner","FZI Research Center for Information Technoloy,Karlsruhe Institute of Technology,FZI Forschungszentrum Informatik",Intelligent Transportation Systems IV,"Reinforcement Learning is a highly active re- search field with promising advancements. In the field of autonomous driving, however, often very simple scenarios are being examined. Common approaches use non-interpretable control commands as the action space and unstructured reward designs, which are unsuitable for complex scenarios. In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source. We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward that allows the agent to learn situations that require controlled traffic rule exceptions. Our method is applicable to arbitrary RL models. We successfully demonstrate high completion rates of complex scenarios with recent model-based agents."
Chance-Aware Lane Change with High-Level Model Predictive Control through Curriculum Reinforcement Learning,"Yubin Wang, Yulin Li, Zengqi Peng, Hakim Ghazzai, Jun Ma","The Hong Kong University of Science and Technology (Guangzhou),Hong Kong university of Science and Technology(HKUST),King Abdullah University of Science and Technology,The Hong Kong University of Science and Technology",Intelligent Transportation Systems IV,"Lane change in dense traffic typically requires the recognition of an appropriate opportunity for maneuvers, which remains a challenging problem in self-driving. In this work, we propose a chance-aware lane-change strategy with high-level model predictive control (MPC) through curriculum reinforcement learning (CRL). In our proposed framework, full-state references and regulatory factors concerning the relative importance of each cost term in the embodied MPC are generated by a neural policy. Furthermore, effective curricula are designed and integrated into an episodic reinforcement learning (RL) framework with policy transfer and enhancement, to improve the convergence speed and ensure a high-quality policy. The proposed framework is deployed and evaluated in numerical simulations of dense and dynamic traffic. It is noteworthy that, given a narrow chance, the proposed approach generates high-quality lane-change maneuvers such that the vehicle merges into the traffic flow with a high success rate of 96%. Finally, our framework is validated in the high-fidelity simulator under dense traffic, demonstrating satisfactory practicality and generalizability."
Human Observation-Inspired Trajectory Prediction for Autonomous Driving in Mixed-Autonomy Traffic Environments,"Liao Haicheng, Shangqian Liu, Yong Kang Li, Zhenning Li, Chengyue Wang, Li Yunjian, Shengbo Li, Cheng-zhong Xu","University of Macau,University of Electronic Science and Technology of China,Macau University of Science and Technology,Tsinghua University",Intelligent Transportation Systems IV,"In the burgeoning field of autonomous vehicles (AVs), trajectory prediction remains a formidable challenge, especially in mixed autonomy environments. Traditional approaches often rely on computational methods such as time-series analysis. Our research diverges significantly by adopting an interdisciplinary approach that integrates principles of human cognition and observational behavior into trajectory prediction models for AVs. We introduce a novel ''adaptive visual sector'' mechanism that mimics the dynamic allocation of attention human drivers exhibit based on factors like spatial orientation, proximity, and driving speed. Additionally, we develop a ''dynamic traffic graph'' using Convolutional Neural Networks (CNN) and Graph Attention Networks (GAT) to capture spatio-temporal dependencies among agents. Benchmark tests on the NGSIM, HighD, and MoCAD datasets reveal that our model (GAVA) outperforms state-of-the-art baselines by at least 15.2%, 19.4%, and 12.0%, respectively. Our findings underscore the potential of leveraging human cognition principles to enhance the proficiency and adaptability of trajectory prediction algorithms in AVs."
Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction,"Pei Xu, Jean-Bernard Hayet, Ioannis Karamouzas","Stanford University,CIMAT,Clemson University",Intelligent Transportation Systems IV,"Real-time, accurate prediction of human steering behaviors has wide applications, from developing intelligent traffic systems to deploying autonomous driving systems in both real and simulated worlds. In this paper, we present ContextVAE, a context-aware approach for multi-modal vehicle trajectory prediction. Built upon the backbone architecture of a timewise variational autoencoder, ContextVAE employs a dual attention mechanism for observation encoding that accounts for the environmental context information and the dynamic agents' states in a unified way. By utilizing features extracted from semantic maps during agent state encoding, our approach takes into account both the social features exhibited by agents on the scene and the physical environment constraints to generate map-compliant and socially-aware trajectories. We perform extensive testing on the nuScenes prediction challenge, Lyft Level 5 dataset and Waymo Open Motion Dataset to show the effectiveness of our approach and its state-of-the-art performance. In all tested datasets, ContextVAE models are fast to train and provide high-quality multi-modal predictions in real-time."
Probably Approximately Correct Nonlinear Model Predictive Control (PAC-NMPC),"Adam Polevoy, Marin Kobilarov, Joseph Moore","Johns Hopkins University Applied Physics Lab,Johns Hopkins University",Integrated Planning and Control,"Approaches for stochastic nonlinear model predictive control (SNMPC) typically make restrictive assumptions about the system dynamics and rely on approximations to characterize the evolution of the underlying uncertainty distributions. For this reason, they are often unable to capture more complex distributions (e.g., non-Gaussian or multi-modal) and cannot provide accurate guarantees of performance. In this paper, we present a sampling-based SNMPC approach that leverages recently derived sample complexity bounds to certify the performance of a feedback policy without making assumptions about the system dynamics or underlying uncertainty distributions. By parallelizing our approach, we are able to demonstrate real-time receding-horizon SNMPC with statistical safety guarantees in simulation and on hardware using a 1/10th scale rally car and a 24-inch wingspan fixed-wing UAV."
QuAD: Query-Based Interpretable Neural Motion Planning for Autonomous Driving,"Sourav Biswas, Sergio Casas Romero, Quin Sykora, Ben Agro, Abbas Sadat, Raquel Urtasun","Waabi, University of Toronto,University of Toronto,UofT, Waabi,Waabi",Integrated Planning and Control,"A self-driving vehicle must understand its environment to determine the appropriate action. Traditional autonomy systems rely on object detection to find the agents in the scene. However, object detection assumes a discrete set of objects and loses information about uncertainty, so any errors compound when predicting the future behavior of those agents. Alternatively, dense occupancy grid maps have been utilized to understand free-space. However, predicting a grid for the entire scene is wasteful since only certain spatio-temporal regions are reachable and relevant to the self-driving vehicle. We present a unified, interpretable, and efficient autonomy framework that moves away from cascading modules that first perceive, then predict, and finally plan. Instead, we shift the paradigm to have the planner query occupancy at relevant spatio-temporal points, restricting the computation to those regions of interest. Exploiting this representation, we evaluate a candidate trajectory around key factors such as collision avoidance, comfort, and progress for safety and interpretability. Our approach achieves better highway driving quality than the state-of-the-art on high-fidelity closed-loop simulations."
Safe Receding Horizon Motion Planning with Infinitesimal Update Interval,"Inkyu Jang, Sunwoo Hwang, Jeonghyun Byun, H. Jin Kim",Seoul National University,Integrated Planning and Control,"Safety verification in motion planning is known to be computationally burdensome, despite its importance in robotics. In this paper, we investigate the behavior of safe receding horizon motion planners when the update interval becomes infinitesimal. By requiring the trajectory parameters to evolve continuously in time, the trajectory optimization problem is reformulated into a time-derivative form, whose decision variables are their rate of change. This results in a quadratic programming problem which directly provides safe input, and can be regarded as a real-time safety filter. The input expressivity is also enhanced by leveraging the differentiable structure of the parameter space. The proposed safety filter is experimentally validated using a wheeled ground robot in obstacle-cluttered environments. The result shows that the safety filter is capable of generating safe inputs in real-time, while addressing hundreds of constraints simultaneously."
NPC: Neural Predictive Control for Fuel-Efficient Autonomous Trucks,"Jiaping Ren, Jiahao Xiang, Hongfei Gao, Jinchuan Zhang, Yiming Ren, Yuexin Ma, Yi Wu, Ruigang Yang, Wei Li","Inceptio Technology,Tongji University, Inceptio Technology,ShanghaiTech University,Nanjing University of Posts and Telecommunications,University of Kentucky,Inceptio",Integrated Planning and Control,"Fuel efficiency is a crucial aspect of long-distance cargo transportation by oil-powered trucks that economize on costs and decrease carbon emissions. Current predictive control methods depend on an accurate model of vehicle dynamics and engine, including weight, drag coefficient, and the Brake-specific Fuel Consumption (BSFC) map of the engine. We propose a pure data-driven method, Neural Predictive Control (NPC), which does not use any physical model for the vehicle. After training with over 20,000 km of historical data, the novel proposed NVFormer implicitly models the relationship between vehicle dynamics, road slope, fuel consumption, and control commands using the attention mechanism. Based on the online sampled primitives from the past of the current freight trip and anchor-based future data synthesis, the NVFormer can infer optimal control command for reasonable fuel consumption. The physical model free NPC outperforms the base PCC method with 2.41% and 3.45% more fuel saving in simulation and the open-road highway testing, respectively."
Robustified Time-Optimal Collision-Free Motion Planning for Autonomous Mobile Robots under Disturbance Conditions,"Shuhao Zhang, Mathias Bos, Bastiaan Vandewal, Wilm Decré, Joris Gillis, Jan Swevers","KU Leuven,Katholieke Universiteit Leuven",Integrated Planning and Control,"This paper presents a robustified time-optimal motion planning approach for navigating an Autonomous Mobile Robot (AMR) from an initial state to a terminal state without colliding with obstacles, even when subjected to disturbances, which are modeled as random process noise and measurement noise. The approach iteratively solves the robustified problem by incorporating updated state-dependent safety margins for collision avoidance, the evolution of which is derived separately from the robustified problem. Additionally, a strategy for selecting an alternative terminal state to reach is introduced, which comes into play when the desired terminal state becomes infeasible considering the disturbances. Both of these contributions are integrated into a robustified motion planning and control pipeline, the efficacy of which is validated through simulation experiments."
Learning-Aided Warmstart of Model Predictive Control in Uncertain Fast-Changing Traffic,"Mohamed-khalil Bouzidi, Yue Yao, Joerg Reichardt, Daniel Goehring","Continental, FU Berlin,Freie Universität Berlin & Continental AG,Continental AG,Freie Universität Berlin",Integrated Planning and Control,"Model Predictive Control lacks the ability to escape local minima in nonconvex problems. Furthermore, in fastchanging, uncertain environments, the conventional warmstart, using the optimal trajectory from the last timestep, often falls short of providing an adequately close initial guess for the current optimal trajectory. This can potentially result in convergence failures and safety issues. Therefore, this paper proposes a framework for learning-aided warmstarts of Model Predictive Control algorithms. Our method leverages a neural network based multimodal predictor to generate multiple trajectory proposals for the autonomous vehicle, which are further refined by a sampling-based technique. This combined approach enables us to identify multiple distinct local minima and provide an improved initial guess. We validate our approach with Monte Carlo simulations of traffic scenarios."
Co-Learning Planning and Control Policies Constrained by Differentiable Logic Specifications,"Zikang Xiong, Daniel Lawson, Joe Kurian Eappen, Ahmed H. Qureshi, Suresh Jagannathan",Purdue University,Integrated Planning and Control,"Synthesizing planning and control policies in robotics is a fundamental task, further complicated by factors such as complex logic specifications and high-dimensional robot dynamics. This paper presents a novel reinforcement learning approach to solving high-dimensional robot navigation tasks with complex logic specifications by co-learning planning and control policies. Notably, this approach significantly reduces the sample complexity in training, allowing us to train high-quality policies with much fewer samples compared to existing reinforcement learning algorithms. In addition, our methodology streamlines complex specification extraction from map images and enables the efficient generation of long-horizon robot motion paths across different map layouts. Moreover, our approach also demonstrates capabilities for high-dimensional control and avoiding suboptimal policies via policy alignment. The efficacy of our approach is demonstrated through experiments involving simulated high-dimensional quadruped robot dynamics and a real-world differential drive robot (TurtleBot3) under different types of task specifications."
Output-Sampled Model Predictive Path Integral Control (o-MPPI) for Increased Efficiency,"Leon Yan, Santosh Devasia",University of Washington,Integrated Planning and Control,"The success of the model predictive path integral control (MPPI) approach depends on the appropriate selection of the input distribution used for sampling. However, it can be challenging to select inputs that satisfy output constraints in dynamic environments. The main contribution of this paper is to propose an output-sampling-based MPPI (o-MPPI), which improves the ability of samples to satisfy output constraints and thereby, increases MPPI efficiency. Comparative simulations and experiments of dynamic autonomous driving of bots around a track are provided to show that the proposed o-MPPI is more efficient and requires substantially (20-times) less number of rollouts and (4-times) smaller prediction horizon when compared with the standard MPPI for similar success rates."
The Virtues of Laziness: Multi-Query Kinodynamic Motion Planning with Lazy Methods,"Anuj Pasricha, Alessandro Roncone",University of Colorado Boulder,Integrated Planning and Control,"In this work, we introduce LazyBoE, a multi-query method for kinodynamic motion planning with forward propagation. This algorithm allows for the simultaneous exploration of a robot's state and control spaces, thereby enabling a wider suite of dynamic tasks in real-world applications. Our contributions are three-fold: i) a method for discretizing the state and control spaces to amortize planning times across multiple queries; ii) lazy approaches to collision checking and propagation of control sequences that decrease the cost of physics-based simulation; and iii) LazyBoE, a robust kinodynamic planner that leverages these two contributions to produce dynamically-feasible trajectories. The proposed framework not only reduces planning time but also increases success rate in comparison to previous approaches."
Characterizing Physical Adversarial Attacks on Robot Motion Planners,"Wenxi Wu, Fabio Pierazzi, Yali Du, Martim Brandao",King's College London,Motion Planning II,"As the adoption of robots across society increases, so does the importance of considering cybersecurity issues such as vulnerability to adversarial attacks. In this paper we investigate the vulnerability of an important component of autonomous robots to adversarial attacks - robot motion planning algorithms. We particularly focus on attacks on the physical environment, and propose the first such attacks to motion planners: ""planner failure"" and ""blindspot"" attacks. Planner failure attacks make changes to the physical environment so as to make planners fail to find a solution. Blindspot attacks exploit occlusions and sensor field-of-view to make planners return a trajectory which is thought to be collision-free, but is actually in collision with unperceived parts of the environment. Our experimental results show that successful attacks need only to make subtle changes to the real world, in order to obtain a drastic increase in failure rates and collision rates - leading the planner to fail 95% of the time and collide 90% of the time in problems generated with an existing planner benchmark tool. We also analyze the transferability of attacks to different planners, and discuss underlying assumptions and future research directions. Overall, the paper shows that physical adversarial attacks on motion planning algorithms pose a serious threat to robotics, which should be taken into account in future research and development."
Eclares: Energy-Aware Clarity-Driven Ergodic Search,"Kaleb Ben Naveed, Devansh Agrawal, Christopher Vermillion, Dimitra Panagou","University of Michigan, Ann Arbor,University of Michigan",Motion Planning II,"Planning informative trajectories while considering the spatial distribution of the information over the environment, as well as constraints such as the robot's limited battery capacity, makes the long-time horizon persistent coverage problem complex. Ergodic search methods consider the spatial distribution of environmental information while optimizing robot trajectories; however, current methods lack the ability to construct the target information spatial distribution for environments that vary stochastically across space and time. Moreover, current coverage methods dealing with battery capacity constraints either assume simple robot and battery models or are computationally expensive. To address these problems, we propose a framework called Eclares, in which our contribution is two-fold. 1) First, we propose a method to construct the target information spatial distribution for ergodic trajectory optimization using clarity, an information measure bounded between [0,1]. The clarity dynamics allow us to capture information decay due to a lack of measurements and to quantify the maximum attainable information in stochastic spatiotemporal environments. 2) Second, instead of directly tracking the ergodic trajectory, we introduce the energy-aware (eware) filter, which iteratively validates the ergodic trajectory to ensure that the robot has enough energy to return to the charging station when needed. The proposed eware filter is applicable to nonlinear robot models and is computationally lightweight. We demonstrate the working of the framework through a simulation case study."
Tight Motion Planning by Riemannian Optimization for Sliding and Rolling with Finite Number of Contact Points,"Dror Livnat, Michael Moshe Bilevich, Dan Halperin",Tel Aviv University,Motion Planning II,"We address a challenging problem in motion planning where robots must navigate through narrow passages in their configuration space. Our novel approach leverages optimization techniques to facilitate sliding and rolling movements across critical regions, which represent semi-free configurations, where the robot and the obstacles are in contact. Our algorithm seamlessly traverses widely free regions, follows semi-free paths in narrow passages, and smoothly transitions between the two types. We specifically focus on scenarios resembling 3D puzzles, intentionally designed to be complex for humans by requiring intricate simultaneous translations and rotations. Remarkably, these complexities also present computational challenges. Our contributions are threefold: First, we solve previously unsolved problems; second, we outperform state-of-the-art algorithms on certain problem types; and third, we present a rigorous analysis supporting the consistency of the algorithm. In the Supplementary Material we provide theoretical foundations for our approach. The Supplementary Material and our open source software are available at https://github.com/TAU-CGL/tr-rrt-public. This research sheds light on effective approaches to address motion planning difficulties in intricate 3D puzzle-like scenarios."
Online Trajectory Deformation and Tracking for Self-Entanglement-Free Differential-Driven Robots,"Jiangpin Liu, Tong Yang, Wangtao Lu, Yue Wang, Rong Xiong",Zhejiang University,Motion Planning II,"This paper introduces an optimisation-based trajectory deformation and tracking algorithm for tethered differential-driven mobile robots. The motivation of this work is to generate self-entanglement-free (SEF) commands for a tethered differential-driven robot to track a path. Whilst existing path planner has been capable of generating SEF paths for tethered differential-driven robots lacking an omni-directional tether retracting mechanism, no trajectory planner can handle the unavoidable movement errors that cause robot pose deviate from the pre-defined path. The trajectory deformation and tracking is challenging because the admissible heading direction of the robot is highly constrained by the SEF constraint. As a result, even with an SEF path, the robot still encounters self-entanglement issues during execution. This paper fills this gap by formulating the trajectory deforming and tracking (TDT) problem of a tethered robot into a multi-objective optimisation framework. Explicit consideration of the constraint of the relative angle between the tether stretching direction and the robotâ€™s heading direction to be admissible during its movement is provided in this framework. The proposed algorithm repeatedly deforms the pre-defined path for easier tracking, whilst generating a suitable velocity profile for robot execution. Compared to directly applying the commonly used untethered trajectory deformation and tracking algorithm into tethered cases, the proposed algorithm demonstrates improved performance in terms of minimising the risk of self-entanglement and maximising robot safety. These are validated in both simulated and real scenarios. An open-sourcesourcing implementation has also been provided for the benefit of the robotics community"
Efficient Motion Planning for Manipulators with Control Barrier Function-Induced Neural Controller,"Mingxin Yu, Chenning Yu, Mohammad Mahdi Naddaf Shargh, Devesh Upadhyay, Sicun Gao, Chuchu Fan","Massachusetts Institute of Technology,University of California San Diego,Ford Motor Company,Saab,UCSD",Motion Planning II,"Sampling-based motion planning methods for manipulators in crowded environments often suffer from expensive collision checking and high sampling complexity, which make them difficult to use in real time. To address this issue, we propose a new generalizable control barrier function (CBF)- based steering controller to reduce the number of samples needed in a sampling-based motion planner RRT. Our method combines the strength of CBF for real-time collision-avoidance control and RRT for long-horizon motion planning, by using CBF-induced neural controller (CBF-INC) to generate control signals that steer the system towards sampled configurations by RRT. CBF-INC is learned as Neural Networks and has two variants handling different inputs, respectively: state (signed distance) input and point-cloud input from LiDAR. In the latter case, we also study two different settings: fully and partially observed environmental information. Compared to manually crafted CBF which suffers from over-approximating robot geometry, CBF-INC can balance safety and goal-reaching better without being over-conservative. Given state-based input, our neural CBF-induced neural controller-enhanced RRT (CBF-INC-RRT) can increase the success rate by 14% while reducing the number of nodes explored by 30%, compared with vanilla RRT on hard test cases. Given LiDAR input where vanilla RRT is not directly applicable, we demonstrate that our CBF-INC-RRT can improve the success rate by 10%, compared with planning with other steering controllers. Our project page with supplementary material is at https://mit-realm.github.io/CBF-INC-RRT-website/."
Planning Optimal Trajectories for Mobile Manipulators under End-Effector Trajectory Continuity Constraint,"Quang-Nam Nguyen, Quang-Cuong Pham","Nanyang Technological University,NTU Singapore",Motion Planning II,"Mobile manipulators have been employed in many applications that are traditionally performed by either multiple fixed-base robots or a large robotic system. This capability is enabled by the mobility of the mobile base. However, the mobile base also brings redundancy to the system, which makes mobile manipulator motion planning more challenging. In this paper, we tackle the mobile manipulator motion planning problem under the end-effector trajectory continuity constraint in which the end-effector is required to traverse a continuous task-space trajectory (time-parametrized path), such as in mobile printing or spraying applications. Our method decouples the problem into: (1) planning an optimal base trajectory subject to geometric task constraints, end-effector trajectory continuity constraint, collision avoidance, and base velocity constraint; which ensures that (2) a manipulator trajectory is computed subsequently based on the obtained base trajectory. To validate our method, we propose a discrete optimal base trajectory planning algorithm to solve several mobile printing tasks in hardware experiment and simulations."
Zero-Shot Constrained Motion Planning Transformers Using Learned Sampling Dictionaries,"Jacob Johnson, Ahmed H. Qureshi, Michael Yip","UCSD,Purdue University,University of California, San Diego",Motion Planning II,"In this work, we explore using a transformer-based model for motion planning with task space constraints for manipulation systems. Vector Quantized-Motion Planning Transformer (VQ-MPT) is a recent learning-based model that reduces the search space for unconstraint planning for sampling-based motion planners. We propose to adapt a pre-trained VQ-MPT model to reduce the search space for constraint planning without retraining or finetuning the model. We also propose to update the neural network output to move sampling regions closer to the constraint manifold. Our experiments show how VQ-MPT improves planning times and accuracy compared to traditional planners in simulated and real-world environments. Unlike previous learning methods, which require task-related data, our method uses pre-trained neural network models and requires no additional data for training and finetuning the model. We also tested our method on a physical Franka Panda robot with real-world sensor data, demonstrating the generalizability of our algorithm."
LSTP: Long Short-Term Motion Planning for Legged and Legged-Wheeled Systems,"Edo Jelavic, Kaixian Qu, Farbod Farshidian, Marco Hutter","Swiss Federal Institute of Technology Zurich,ETH Zürich,ETH Zurich",Motion Planning II,"This article presents a hybrid motion planning and control approach applicable to various ground robot types and morphologies. Our two-step approach uses a sampling-based planner to compute an approximate motion which is then fed to numerical optimization for refinement. The sampling-based stage finds a long-term global plan consisting of a contact schedule and a sequence of stable whole-body configurations. Subsequently, the optimization refines the solution with a short-term planning horizon to satisfy all dynamics constraints. The proposed planner can compute plans for scenarios that would be difficult for trajectory optimization or sampling planner alone. We present tasks of traversing challenging terrain that requires discovering a contact schedule, navigating non-convex obstacles, and coordinating many degrees of freedom. Our hybrid planner has been applied to three different robots: a quadruped, a wheeled quadruped, and a legged excavator. We validate our hybrid planner in the real world and simulation, generating behaviors we could not achieve with previous methods. The results show that computing and executing hybrid locomotion plans is possible on hardwar"
Risk-Inspired Aerial Active Exploration for Enhancing Autonomous Driving of UGV in Unknown Off-Road Environments,"Rongchuan Wang, Mengyin Fu, Jing Yu, Yi Yang, Wenjie Song",Beijing Institute of Technology,Motion Planning II,"Unknown area exploration is a crucial but challeng- ing task for autonomous driving of unmanned ground vehicles (UGV) in unknown off-road environments. However, the exploration efficiency of single UGV is low due to its limited sensing range. To solve this problem, this paper proposes a risk-inspired aerial active exploration system, which utilizes the flexibility and view field advantages of Unmanned Aerial Vehicles (UAV) to guide the UGV in unknown off-road environments. Firstly, a fast terrain risk mapping method that can be used for both UAV and UGV is developed. This method efficiently combines quadtree and hash table data structure to enable UAV to analyze large scale terrain point cloud in real time. Based on the risk mapping result, a risk inspired active exploration method is proposed to actively search a safe reference path for the UGV, which introduces terrain risk information into the process of travel point selection. Finally, the reference path is gradually generated and optimized, so that the UGV can safely and smoothly follow the path to the target location. Compared with single UGV exploration system, our approach reduces the overall path risk by 26.8% in simulated experiments, showing that the proposed system can enhance autonomous driving of the UGV and help it effectively avoid high-risk areas in unknown off-road environments."
Sim-On-Wheels: Physical World in the Loop Simulation for Self-Driving,"Yuan Shen, Bhargav Chandaka, Zhi-hao Lin, Albert Zhai, Hang Cui, David Forsyth, Shenlong Wang","UIUC,University of Illinois at Urbana-Champaign,University of Illinois at Urbana-Ch",Autonomous Agents I,"We present Sim-on-Wheels, a safe, realistic, and vehicle-in-loop framework to test autonomous vehiclesâ€™ performance in the real world under safety-critical scenarios. Sim-on-wheels runs on a self-driving vehicle operating in the physical world. It creates virtual traffic participants with risky behaviors and seamlessly inserts the virtual events into images perceived from the physical world in real-time. The manipulated images are fed into autonomy, allowing the self-driving vehicle to react to such virtual events. The full pipeline runs on the actual vehicle and interacts with the physical world, but the safety-critical events it sees are virtual. Sim-on-Wheels is safe, interactive, realistic, and easy to use. The experiments demonstrate the potential of Sim-on-Wheels to facilitate the process of testing autonomous driving in challenging real-world scenes with high fidelity and low risk."
Safety-Critical Scenario Generation Via Reinforcement Learning Based Editing,"Haolan Liu, Liangjun Zhang, Siva Kumar Sastry Hari, Jishen Zhao","University of California San Diego,Baidu,NVIDIA,UC San Diego",Autonomous Agents I,"Generating safety-critical scenarios is essential for testing and verifying the safety of autonomous vehicles. Traditional optimization techniques suffer from the curse of dimensionality and limit the search space to fixed parameter spaces. To address these challenges, we propose a deep reinforcement learning approach that generates scenarios by sequential editing, such as adding new agents or modifying the trajectories of the existing agents. Our framework employs a reward function consisting of both risk and plausibility objectives. The plausibility objective leverages generative models, such as a variational autoencoder, to learn the likelihood of the generated parameters from the training datasets; It penalizes the generation of unlikely scenarios. Our approach overcomes the dimensionality challenge and explores a wide range of safety-critical scenarios. Our evaluation demonstrates that the proposed method generates safety-critical scenarios of higher quality compared with previous approaches."
Robust Autonomous Vehicle Pursuit without Expert Steering Labels,"Jiaxin Pan, Changyao Zhou, Mariia Gladkova, Qadeer Khan, Daniel Cremers",Technical University of Munich,Autonomous Agents I,"In this work, we present a learning method for lateral and longitudinal motion control of an ego-vehicle for vehicle pursuit. The car being controlled does not have a pre-defined route; rather, it adapts to follow a target vehicle while maintaining a safety distance. To train our model, we do not rely on steering labels recorded by an expert driver but effectively leverage a classical controller as an offline label generation tool. In addition, we account for the errors in the predicted control values, which can lead to a loss of tracking and catastrophic crashes of the controlled vehicle. To this end, we propose an effective data augmentation approach, which allows the training of a network capable of handling different views of the target vehicle. During the pursuit, the target vehicle is firstly localized using a Convolutional Neural Network. The network takes a single RGB image along with cars' velocities and estimates the target vehicle's pose with respect to the ego-vehicle. This information is then fed to a Multi-Layer Perceptron, which regresses the control commands for the ego-vehicle, namely throttle and steering angle. We extensively validate our approach using the CARLA simulator on various terrains. Our method demonstrates real-time performance robustness to different scenarios, including unseen trajectories and high route completion. Our project page can be found at https://changyaozhou.github.io/Autonomous-Vehicle-Pursuit/."
Risk-Aware Trajectory Prediction by Incorporating Spatio-Temporal Traffic Interaction Analysis,"Divya Thuremella, Lewis Ince, Lars Kunze","University of Oxford, Robotics Institute,University of Oxford",Autonomous Agents I,"To operate in open-ended environments where humans interact in complex, diverse ways, autonomous robots must learn to predict their behaviour, especially when that behavior is potentially dangerous to other agents or to the robot. However, reducing the risk of accidents requires prior knowledge of where potential collisions may occur and how. Therefore, we propose to gain this information by analyzing locations and speeds that commonly correspond to high-risk interactions within the dataset, and use it within training to generate better predictions in high risk situations. Through these location-based and speed-based re-weighting techniques, we achieve improved overall performance, as measured by most-likely FDE and KDE, as well as improved performance on high-speed vehicles, and vehicles within high-risk locations."
Reinforcement Learning with Human Feedback for Realistic Traffic Simulation,"Yulong Cao, Boris Ivanovic, Chaowei Xiao, Marco Pavone","NVIDIA,University of Michigan,Stanford University",Autonomous Agents I,"In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. Towards this end, in this work we develop a framework that employs reinforcement learning from human feedback (RLHF) to enhance the realism of existing traffic models. This work also identifies two main challenges: capturing the nuances of human preferences on realism and unifying diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generating realistic traffic scenarios that are well-aligned with human preferences through comprehensive evaluations on the nuScenes dataset."
Plug in the Safety Chip: Enforcing Constraints for LLM-Driven Robot Agents,"Ziyi Yang, Shreyas Sundara Raman, Ankit Jayesh Shah, Stefanie Tellex","Brown University,Massachusetts Institute of Technology,Brown",Autonomous Agents I,"Recent advancements in large language models (LLMs) have enabled a new research domain, LLM agents, for solving robotics and planning tasks by leveraging the world knowledge and general reasoning abilities of LLMs obtained during pretraining. However, while considerable effort has been made to teach the robot the ""dos"", the ""don'ts"" received relatively less attention. We argue that, for any practical usage, it is as crucial to teach the robot the ""don'ts"": conveying explicit instructions about prohibited actions, assessing the robot's comprehension of these restrictions, and, most importantly, ensuring compliance. Moreover, verifiable safe operation is essential for deployments that satisfy worldwide standards such as ISO 61508, which defines standards for safely deploying robots in industrial factory environments worldwide. Aiming at deploying the LLM agents in a collaborative environment, we propose a queryable safety constraint module based on linear temporal logic (LTL) that simultaneously enables natural language (NL) to temporal constraints encoding, safety violation reasoning and explaining, and unsafe action pruning. To demonstrate the effectiveness of our system, we conducted experiments in VirtualHome environment and on a real robot. The experimental results show that our system strictly adheres to the safety constraints and scales well with complex temporal constraints, highlighting its potential for practical utility."
InterCoop: Spatio-Temporal Interaction Aware Cooperative Perception for Networked Vehicles,"Wentao Wang, Haoran Xu, Guang Tan","Sun Yat-sen University,SUN YAT-SEN UNIVERSITY",Autonomous Agents I,"In autonomous driving, leveraging cooperative perception through vehicle-to-vehicle (V2V) communication is considered crucial for enhancing traffic safety and efficiency. However, existing methods often simplify the handling of perception data from multiple vehicles. In these approaches, the ego-vehicle aggregates observations from all neighboring connected cooperative vehicles (CCV), without considering the interactions between agents or making differentiated use of the acquired sensing data. This approach can result in suboptimal system performance due to the amplification of noise and the large transmission delay. In this paper, we introduce a novel approach to cooperative perception. By fusing both the road topology and trajectory histories of neighboring CCVs, our model learns an interaction score for each CCV. These scores prioritize vehicles that are most relevant to the current driving scenario, offering valuable guidance for selective fusion of sensor data, thereby enhancing driving decision-making. The proposed method is validated through extensive experiments conducted on the CARLA simulator. Results demonstrate that our approach surpasses existing methods in terms of performance and robustness."
Improving Autonomous Driving Safety with POP: A Framework for Accurate Partially Observed Trajectory Predictions,"Sheng Wang, Yingbing Chen, Jie Cheng, Xiaodong Mei, Ren Xin, Yongkang Song, Ming Liu","Hong Kong University of Science and Technology,The Hongkokng University of Science and Technology,HKUST,the Hong Kong University of Science and Technology,Ningbo Lotus Robotics Co., Ltd,Hong Kong University of Science and Technology (Guangzhou)",Autonomous Agents I,"Accurate trajectory prediction is crucial for safe and efficient autonomous driving, but handling partial observations presents significant challenges. To address this, we propose a novel trajectory prediction framework called Partial Observations Prediction (POP) for congested urban road scenarios. The framework consists of two key stages: self-supervised learning (SSL) and feature distillation. POP first employs SLL to help the model learn to reconstruct history representations, and then utilizes feature distillation as the fine-tuning task to transfer knowledge from the teacher model, which has been pre-trained with complete observations, to the student model, which has only few observations. POP achieves comparable results to top-performing methods in open-loop experiments and outperforms the baseline method in closed-loop simulations, including safety metrics. Qualitative results illustrate the superiority of POP in providing reasonable and safe trajectory predictions."
FIMP: Future Interaction Modeling for Multi-Agent Motion Prediction,"Sungmin Woo, Minjung Kim, Donghyeong Kim, Sungjun Jang, Sangyoun Lee",Yonsei University,Autonomous Agents I,"Multi-agent motion prediction is a crucial concern in autonomous driving, yet it remains a challenge owing to the ambiguous intentions of dynamic agents and their intricate interactions. Existing studies have attempted to capture interactions between road entities by using the definite data in history timesteps, as future information is not available and involves high uncertainty. However, without sufficient guidance for capturing future states of interacting agents, they frequently produce unrealistic trajectory overlaps. In this work, we propose Future Interaction modeling for Motion Prediction (FIMP), which captures potential future interactions in an end-to-end manner. FIMP adopts a future decoder that implicitly extracts the potential future information in an intermediate feature-level, and identifies the interacting entity pairs through future affinity learning and top-k filtering strategy. Experiments show that our future interaction modeling improves the performance remarkably, leading to superior performance on the Argoverse motion forecasting benchmark."
EasyHeC: Accurate and Automatic Hand-Eye Calibration Via Differentiable Rendering and Space Exploration,"Linghao Chen, Yuzhe Qin, Xiaowei Zhou, Hao Su","Zhejiang University,UC San Diego,UCSD",Calibration and Identification I,"Hand-eye calibration is a critical task in robotics, as it directly affects the efficacy of critical operations such as manipulation and grasping. Traditional methods for achieving this objective necessitate the careful design of joint poses and the use of specialized calibration markers, while most recent learning-based approaches using solely pose regression are limited in their abilities to diagnose inaccuracies. In this work, we introduce a new approach to hand-eye calibration called EasyHeC, which is markerless, white-box, and delivers superior accuracy and robustness. We propose to use two key technologies: differentiable rendering-based camera pose optimization and consistency-based joint space exploration, which enables accurate end-to-end optimization of the calibration process and eliminates the need for the laborious manual design of robot joint poses. Our evaluation demonstrates superior performance in synthetic and real-world datasets, enhancing downstream manipulation tasks by providing precise camera poses for locating and interacting with objects. The code is available at the project page: https://ootts.github.io/easyhec."
Zero-Training LiDAR-Camera Extrinsic Calibration Method Using Segment Anything Model,"Zhaotong Luo, Guohang Yan, Xinyu Cai, Botian Shi","Tsinghua University,Shanghai AI Laboratory",Calibration and Identification I,"Extrinsic calibration for LiDAR and camera is an essential prerequisite for sensor fusion. Recently, automatic and target-less extrinsic calibration has become the mainstream of academic research. However, geometric feature-based methods still have requirements on the scene. Deep learning methods, while achieving high accuracy and good adaptability, rely on large annotated dataset and need additional training. We propose a novel LiDAR-camera calibration method by using the Segment Anything Model(SAM) without additional training. With the automatically generated masks, we optimize the extrinsic parameters by maximizing the consistency score of the point attributes that fall on each mask. The point cloud attributes include intensity, normal vector and segmentation class. Experiments on different real-world dataset demonstrate the accuracy and robustness of our proposed method. The code is available at https://github.com/OpenCalib/CalibAnything."
Dive Deeper into Rectifying Homography for Stereo Camera Online Self-Calibration,"Hongbo Zhao, Yikang Zhang, Qijun Chen, Rui Fan",Tongji University,Calibration and Identification I,"Accurate estimation of stereo camera extrinsic parameters is crucial to guarantee the performance of stereo matching algorithms. In prior arts, the online self-calibration of stereo cameras has commonly been formulated as a specialized visual odometry problem, without taking into account the principles of stereo rectification. In this paper, we first delve deeply into the concept of rectifying homography, which serves as the cornerstone for the development of our novel stereo camera online self-calibration algorithm, for cases where only a single pair of images is available. Furthermore, we introduce a simple yet effective solution for global optimum extrinsic parameter estimation in the presence of stereo video sequences. Additionally, we emphasize the impracticality of using three Euler angles and three components in the translation vectors for performance quantification. Instead, we introduce four new evaluation metrics to quantify the robustness and accuracy of extrinsic parameter estimation, applicable to both single-pair and multi-pair cases. Extensive experiments conducted across indoor and outdoor environments using various experimental setups validate the effectiveness of our proposed algorithm. The comprehensive evaluation results demonstrate its superior performance in comparison to the baseline algorithm. Our source code, demo video, and supplement are publicly available at mias.group/StereoCalibrator."
Online Camera-LiDAR Calibration Monitoring and Rotational Drift Tracking,"Jaroslav Moravec, Radim Šára","Faculty of Electrical Engineering, Czech Technical University in,Faculty of Electrical Enginnering, Czech Technical University in",Calibration and Identification I,"The relative poses of visual perception sensors distributed over a vehicleâ€™s body may vary due to dynamic forces, thermal dilations, or minor accidents. This paper proposes two methods, OCAMO and LTO, that monitor and track the LiDAR-Camera extrinsic calibration parameters online. Calibration monitoring provides a certificate for reference-calibration parameters validity. Tracking follows the calibration parameters drift in time. OCAMO is based on an adaptive online stochastic optimization with a memory of past evolution. LTO uses a fixed-grid search for the optimal parameters per frame and without memory. Both methods use low-level point-like features, a robust kernel-based loss function, and work with a small memory footprint and computational overhead. Both include a preselection of informative data that limits their divergence. The statistical accuracy of both calibration monitoring methods is over 98%, whereas OCAMO monitoring can detect small decalibrations better, and LTO monitoring reacts faster on abrupt decalibrations. The tracking variants of both methods follow random calibration drift with an accuracy of about 0.03 degrees in the yaw angle."
PeLiCal: Targetless Extrinsic Calibration Via Penetrating Lines for RGB-D Cameras with Limited Co-Visibility,"Jaeho Shin, Seungsang Yun, Ayoung Kim","SNU,Seoul National University, SNU,Seoul National University",Calibration and Identification I,"RGB-D cameras are crucial in robotic perception, given their ability to produce images augmented with depth data. However, their limited field of view (FOV) often requires multiple cameras to cover a broader area. In multi-camera RGB-D setups, the goal is typically to reduce camera overlap, optimizing spatial coverage with as few cameras as possible. The extrinsic calibration of these systems introduces additional complexities. Existing methods for extrinsic calibration either necessitate specific tools or highly depend on the accuracy of camera motion estimation. To address these issues, we present PeLiCal, a novel line-based calibration approach for RGB-D camera systems exhibiting limited overlap. Our method leverages long line features from surroundings, and filters out outliers with a novel convergence voting algorithm, achieving targetless, real-time, and outlier-robust performance compared to existing methods. We open source our implementation on https://github.com/joomeok/PeLiCal.git."
"A Novel, Efficient and Accurate Method for Lidar Camera Calibration","Zhanhong Huang, Xiao Zhang, Antony Garcia, Xinming Huang",Worcester Polytechnic Institute,Calibration and Identification I,"As autonomous systems evolve, the precise calibration of lidar and camera sensors remains a pivotal concern. Among the myriad of available techniques, target-based calibration methods, which employ planar boards with distinct geometry and image patterns, have been a popular choice. These methods simplify the task of extracting corresponding features between the image and lidar point cloud. But many of these approaches also face a significant challenge, which is their sensitivity to lidar resolution and Field of View (FOV), which may degrade the reliability of the calibration results. Therefore, our research introduces a novel calibration method using a uniquely designed acrylic checkerboard which allows the lidar beam to pass through the white grids and reflect back from the black grids. This innovative technique sidesteps the common challenges associated with lidar feature extraction. Our method's distinct advantage lies in its ability to perform accurate calibrations at close distances, owing to the efficient feature extraction from both lidar and camera sensors. This novel, efficient, and accurate method can provide state-of-the-art results for camera lidar calibration in the field."
An Extrinsic Calibration Method between LiDAR and GNSS/INS for Autonomous Driving,"Jiahao Pi, Guohang Yan, Chengjie Wang, Xinyu Cai, Botian Shi","Shanghai AI Laboratory,fudan university",Calibration and Identification I,"Accurate and reliable sensor calibration is critical for fusing LiDAR and inertial measurements in autonomous driving. This paper proposes a novel three-stage extrinsic calibration method between LiDAR and GNSS/INS for autonomous driving. The first stage can quickly calibrate the extrinsic parameters between the sensors through point cloud surface features so that the extrinsic can be narrowed from a large initial error to a small error range in little time. The second stage can further calibrate the extrinsic parameters based on LiDAR-mapping space occupancy while removing motion distortion. In the final stage, the z-axis (the vertical direction relative to the ground plane) errors caused by the plane motion of the autonomous vehicle are corrected, and an accurate extrinsic parameter is finally obtained. Specifically, This method utilizes the planar features in the environment, making it possible to quickly carry out calibration. Experimental results on real-world datasets demonstrate the reliability and accuracy of our method. The codes are open-sourced on the Github website. The code link is https://github.com/OpenCalib/LiDAR2INS."
SGCalib: A Two-Stage Camera-LiDAR Calibration Method Using Semantic Information and Geometric Features,"Zhipeng Lin, Zhi Gao, Xinyi Liu, Jialiang Wang, Weiwei Song, Ben M. Chen, Chenyang Li, Yue Huang, Yuhan Zhu","The Chinese University of Hong Kong,Temasek Laboratories @ NUS,Wuhan University,Peng Cheng Laboratory,Chinese University of Hong Kong",Calibration and Identification I,"Extrinsic calibration is an essential prerequisite for the applications of camera-LiDAR fusion. Existing methods either suffer from the complex offline setting of man-made targets or tend to produce suboptimal and unrobust results. In this paper, we propose an online two-stage calibration method that estimates robust and accurate extrinsic parameters between camera and LiDAR. This is a novel work to use semantic information and geometric features jointly in calibration to promote accuracy and robustness. In the first stage, we detect objects in the image and point cloud and build graphs on the objects using Delaunay triangulation. Then, we design a novel graph matching algorithm to associate the objects in the two data domains and extract pairs of 2D-3D points. Using the PnP solver, we get robust initial extrinsic parameters. Then, in the second stage, we design a new optimization formulation with semantic information and geometric features to generate accurate extrinsic parameters with the initial value from the first stage. Extensive experiments on solid-state LiDAR, conventional spinning LiDAR and KITTI datasets have verified the robustness and accuracy of our method which outperforms existing works. We will share the code publicly to benefit the community (after review stages)."
EWand: An Extrinsic Calibration Framework for Wide Baseline Frame-Based and Event-Based Camera Systems,"Thomas Gossard, Andreas Ziegler, Levin Kolmar, Jonas Tebbe, Andreas Zell","University of Tübingen,University of Tuebingen,University Tuebingen",Calibration and Identification I,"Accurate calibration is crucial for using multiple cameras to triangulate the position of objects precisely. However, it is also a time-consuming process that needs to be repeated for every displacement of the cameras. The standard approach is to use a printed pattern with known geometry to estimate the intrinsic and extrinsic parameters of the cameras. The same idea can be applied to event-based cameras, though it requires extra work. By using frame reconstruction from events, a printed pattern can be detected. A blinking pattern can also be displayed on a screen. Then, the pattern can be directly detected from the events. Such calibration methods can provide accurate intrinsic calibration for both frame- and event-based cameras. However, using 2D patterns has several limitations for multi-camera extrinsic calibration, with cameras possessing highly different points of view and a wide baseline. The 2D pattern can only be detected from one direction and needs to be of significant size to compensate for its distance to the camera. This makes the extrinsic calibration time-consuming and cumbersome. To overcome these limitations, we propose eWand, a new method that uses blinking LEDs inside opaque spheres instead of a printed or displayed pattern. Our method provides a faster, easier-to-use extrinsic calibration approach that maintains high accuracy for both event- and frame-based cameras."
"Benchmarking Multi-Robot Coordination in Realistic, Unstructured Human-Shared Environments","Lukas Heuer, Luigi Palmieri, Anna Mannucci, Sven Koenig, Martin Magnusson","Örebro University, Robert Bosch GmbH,Robert Bosch GmbH,Robert Bosch GmbH Corporate Research,University of Southern California,Örebro University",Path Planning for Multiple Mobile Robots or Agents II,"Coordinating a fleet of robots in unstructured, human-shared environments is challenging. Human behavior is hard to predict, and its uncertainty impacts the performance of the robotic fleet. Various multi-robot planning and coordination algorithms have been proposed, including Multi-Agent Path Finding (MAPF) methods to precedence-based algorithms. However, it is still unclear how human presence impacts different coordination strategies in both simulated environments and the real world. With the goal of studying and further improving multi-robot planning capabilities in those settings, we propose a method to develop and benchmark different multi-robot coordination algorithms in realistic, unstructured and human-shared environments. To this end, we introduce a multi-robot benchmark framework that is based on state-of-the-art open-source navigation and simulation frameworks and can use different types of robots, environments and human motion models. We show a possible application of the benchmark framework with two different environments and three centralized coordination methods (two MAPF algorithms and a loosely-coupled coordination method based on precedence constraints). We evaluate each environment for different human densities to investigate its impact on each coordination method. We also present preliminary results that show how informing each coordination method about human presence can help the coordination method to find faster paths for the robots."
Conflict Area Prediction for Boosting Search-Based Multi-Agent Pathfinding Algorithms,"Jaesung Ryu, Youngjoon Kwon, Sangho Yoon, Kyungjae Lee",Chung-Ang University,Path Planning for Multiple Mobile Robots or Agents II,"We address the challenge of efficiently controlling multi-agent systems, crucial in fields like logistics and traffic management. We propose a novel approach that combines learning-based techniques with search-based methods, focusing on enhancing the conflict-based search (CBS). The CBS ensures optimality but suffers from increasing complexity as agents or maps grow. To tackle this, we leverage learning-based approaches to enhance computational efficiency. By training a conflict area prediction (CAP) network, we anticipate potential conflict zones, allowing for low-level path planners to explore conflict-free paths. Our experiments demonstrate the effectiveness of our method in reducing computational demands compared to existing approaches."
AMSwarmX: Safe Swarm Coordination in CompleX Environments Via Implicit Non-Convex Decomposition of the Obstacle-Free Space,"Vivek Kantilal Adajania, Siqi Zhou, Arun Singh, Angela P. Schoellig","University of Toronto,Technical University of Munich,University of Tartu,TU Munich",Path Planning for Multiple Mobile Robots or Agents II,"Quadrotor motion planning in complex environments leverage the concept of safe flight corridor (SFC) to facilitate static obstacle avoidance. Typically, SFCs are constructed through convex decomposition of the environment's free space into cuboids, convex polyhedra, or spheres. However, such SFCs can be overly conservative when dealing with a quadrotor swarm, substantially limiting the available free space for quadrotors to coordinate. This paper presents an Alternating Minimization-based approach that does not require building a conservative free-space approximation. Instead, both static and dynamic collision constraints are treated in a unified manner. Dynamic collisions are handled based on shared position trajectories of the quadrotors. Static obstacle avoidance is coupled with distance queries from the Octomap, providing an implicit non-convex decomposition of free space. As a result, our approach is scalable to arbitrary complex environments. Through extensive comparisons in simulation, we demonstrate a $60%$ improvement in success rate, an average $1.8times$ reduction in mission completion time, and an average $23times$ reduction in per-agent computation time compared to SFC-based approaches. We also experimentally validated our approach using a Crazyflie quadrotor swarm of up to 12 quadrotors in obstacle-rich environments. The code, supplementary materials, and videos are released for reference."
Conflict-Based Model Predictive Control for Scalable Multi-Robot Motion Planning,"Ardalan Tajbakhsh, Lorenz Biegler, Aaron Johnson",Carnegie Mellon University,Path Planning for Multiple Mobile Robots or Agents II,"This paper presents a scalable multi-robot motion planning algorithm called Conflict-Based Model Predictive Control (CB-MPC). Inspired by Conflict-Based Search (CBS), the planner leverages a modified high-level conflict tree to efficiently resolve robot-robot conflicts in the continuous space, while reasoning about each agent's kinematic and dynamic constraints and actuation limits using MPC as the low-level planner. We show that tracking high-level multi-robot plans with a vanilla MPC controller is insufficient, and results in unexpected collisions in tight navigation scenarios under realistic execution. Compared to other variations of multi-robot MPC like joint, prioritized, and distributed, we demonstrate that CB-MPC improves the executability and success rate, allows for closer robot-robot interactions, and scales better with higher numbers of robots without compromising the solution quality across a variety of environments."
Db-CBS: Discontinuity-Bounded Conflict-Based Search for Multi-Robot Kinodynamic Motion Planning,"Akmaral Moldagalieva, Joaquim Ortiz De Haro, Marc Toussaint, Wolfgang Hoenig","Technical University of Berlin,TU Berlin",Path Planning for Multiple Mobile Robots or Agents II,"This paper presents a multi-robot kinodynamic motion planner that enables a team of robots with different dynamics, actuation limits, and shapes to reach their goals in challenging environments. We solve this problem by combining Conflict-Based Search (CBS), a multi-agent path finding method, and discontinuity-bounded A*, a single-robot kinodynamic motion planner. Our method, db-CBS, operates in three levels. Initially, we compute trajectories for individual robots using a graph search that allows bounded discontinuities between precomputed motion primitives. The second level identifies inter-robot collisions and resolves them by imposing constraints on the first level. The third and final level uses the resulting solution with discontinuities as an initial guess for a joint space trajectory optimization. The procedure is repeated with a reduced discontinuity bound. Our approach is anytime, probabilistically complete, asymptotically optimal, and finds near-optimal solutions quickly. Experimental results with robot dynamics such as unicycle, double integrator, and car with trailer in different settings show that our method is capable of solving challenging tasks with a higher success rate and lower cost than the existing state-of-the-art."
ALPHA Attention-Based Long-Horizon Pathfinding in Highly-Structured Areas,"Chengyang He, Tianze Yang, Tanishq Harish Duhan, Yutong Wang, Guillaume Sartoretti","National University Singapore,National University of Singapore,National university of singapore,National University of Singapore (NUS)",Path Planning for Multiple Mobile Robots or Agents II,"The multi-agent pathfinding (MAPF) problem seeks collision-free paths for a team of agents from their current positions to their pre-set goals in a known environment, and is an essential problem found at the core of many logistics, transportation, and general robotics applications. Existing learning-based MAPF approaches typically only let each agent make decisions based on a limited field-of-view (FOV) around its position, as a natural means to fix the input dimensions of its policy network. However, this often makes policies short-sighted, since agents lack the ability to perceive and plan for obstacles/agents beyond their FOV. To address this challenge, we propose ALPHA, a new framework combining the use of ground truth proximal (local) information and fuzzy distal (global) information to let agents sequence local decisions based on the full current state of the system, and avoid such myopicity. We further allow agents to make short-term predictions about each others' paths, as a means to reason about each others' path intentions, thereby enhancing the level of cooperation among agents at the whole system level. Our neural structure relies on a Graph Transformer architecture to allow agents to selectively combine these different sources of information and reason about their inter-dependencies at different spatial scales. Our simulation experiments demonstrate that ALPHA outperforms both globally-guided MAPF solvers and communication-learning based ones, showcasing its potential towards scalability in realistic deployments."
Online On-Demand Multi-Robot Coverage Path Planning,"Ratijit Mitra, Indranil Saha",IIT Kanpur,Path Planning for Multiple Mobile Robots or Agents II,"We present an online centralized path planning algorithm to cover a large, complex, unknown workspace with multiple homogeneous mobile robots. Our algorithm is horizon-based, synchronous, and on-demand. The recently proposed horizon-based synchronous algorithms compute the paths for all the robots in each horizon, significantly increasing the computation burden in large workspaces with many robots. As a remedy, we propose an algorithm that computes the paths for a subset of robots that have traversed previously computed paths entirely (thus on-demand) and reuses the remaining paths for the other robots. We formally prove that the algorithm guarantees the complete coverage of the unknown workspace. Experimental results on several standard benchmark workspaces show that our algorithm scales to hundreds of robots in large complex workspaces and consistently outperforms a state-of-the-art online centralized multi-robot coverage path planning algorithm in terms of the time required to achieve complete coverage. For validation, we perform ROS+Gazebo simulations in five 2D grid benchmark workspaces with 10 Quadcopters and 10 TurtleBots, respectively. In addition, to establish its practical feasibility, we conduct one indoor experiment with two real TurtleBot2 robots and one outdoor experiment with three real Quadcopters."
Scalable Multi-Robot Motion Planning for Congested Environments with Topological Guidance,"Courtney McBeth, James Motes, Diane Uwacu, Marco Morales, Nancy Amato","University of Illinois Urbana-Champaign,Texas A&M University,University of Illinois at Urbana-Champaign & Instituto Tecnológ,University of Illinois",Path Planning for Multiple Mobile Robots or Agents II,"Multi-robot motion planning (MRMP) is the problem of finding collision-free paths for a set of robots in a continuous state space. The difficulty of MRMP increases with the number of robots and is exacerbated in environments with narrow passages that robots must pass through, like warehouse aisles where coordination between robots is required. In single-robot settings, topology-guided motion planning methods have shown improved performance in these constricted environments. In this work, we extend an existing topology-guided single-robot motion planning method to the multi-robot domain to leverage the improved efficiency provided by topological guidance. We demonstrate our methodâ€™s ability to efficiently plan paths in complex environments with many narrow passages, scaling to robot teams of size up to 25 times larger than existing methods in this class of problems. By leveraging knowledge of the topology of the environment, we also find higher-quality solutions than other methods."
Mixed Integer Programming for Time-Optimal Multi-Robot Coverage Path Planning with Efficient Heuristics,"Jingtao Tang, Hang Ma",Simon Fraser University,Path Planning for Multiple Mobile Robots or Agents II,"We investigate the time-optimal Multi-Robot Coverage Path Planning (MCPP) problem for both unweighted and weighted terrains, which aims to minimize the coverage time, defined as the maximum travel time of all robots. Specifically, we focus on a reduction from the MCPP problem to the Rooted Min-Max Tree Cover (RMMTC) problem. For the first time, we propose a Mixed Integer Programming (MIP) model to optimally solve the RMMTC problem, resulting in an MCPP solution with a coverage time that is provably at most four times the optimal. Moreover, we propose two suboptimal yet effective heuristics that reduce the number of variables in the MIP model, thus improving its efficiency for large-scale MCPP problems. We show that both heuristics result in reduced-size MIP models that remain complete for RMMTC problems. Additionally, we explore the use of model optimization warm-startup to further improve the efficiency of both the original MIP model and the reduced-size MIP models. We validate the effectiveness of our MIP-based MCPP planner through experiments that compare it with two state-of-the-art MCPP planners on various instances, demonstrating a reduction in the coverage time by an average of 42.42% and 39.16% over them, respectively."
MOTPose: Multi-Object 6D Pose Estimation for Dynamic Video Sequences Using Attention-Based Temporal Fusion,"Arul Selvam Periyasamy, Sven Behnke",University of Bonn,Visual Learning II,"Cluttered bin-picking environments are challenging for pose estimation models. Despite the impressive progress enabled by deep learning, single-view RGB pose estimation models perform poorly in cluttered dynamic environments. Imbuing the rich temporal information contained in the video of scenes has the potential to enhance models' ability to deal with the adverse effects of occlusion and the dynamic nature of the environments. Moreover, joint object detection and pose estimation models are better suited to leverage the co-dependent nature of the tasks for improving the accuracy of both tasks. To this end, we propose attention-based temporal fusion for multi-object 6D pose estimation that accumulates information across multiple frames of a video sequence. Our MOTPose method takes a sequence of images as input and performs joint object detection and pose estimation for all objects in one forward pass. It learns to aggregate both object embeddings and object parameters over multiple time steps using cross-attention-based fusion modules. We evaluate our method on the physically-realistic cluttered bin-picking dataset SynPick and the YCB-Video dataset and demonstrate improved pose estimation accuracy as well as better object detection accuracy."
Generalizable Thermal-Based Depth Estimation Via Pre-Trained Visual Foundation Model,"Ruoyu Fan, Wang Zhao, Matthieu Lin, Qi Wang, Yong-Jin Liu, Wenping Wang","Tsinghua University,Guizhou University,The University of Hong Kong",Visual Learning II,"Depth estimation is a crucial task in computer vision, applicable to various domains such as 3D reconstruction, robotics, and autonomous driving. In particular, thermal-based depth estimation has unique advantages, including night-time vision. However, the existing depth estimation method remains challenging in robust generalization due to limited data resources and spectral differences between thermal and RGB images. In this paper, we present a self-supervised approach to enhance thermal-based depth estimation by leveraging pre-trained visual models initially designed for RGB data. In detail, we design a novel two-stage training strategy, incorporating Low-rank Adapters and Convolutional Adapters, which not only significantly improves accuracy and robustness but also enables impressive zero-shot generalization capabilities. Our method outperforms existing thermal-based depth estimation models, opening new possibilities for cross-modal applications in computer vision and robotics research."
OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-Assisted Surgery,"Long Bai, Guankun Wang, Jie Wang, Xiaoxiao Yang, Huxin Gao, Xin Liang, An Wang, Mobarakol Islam, Hongliang Ren","The Chinese University of Hong Kong,Beijing Institute of Technology,Qilu Hospital of Shandong University,National University of Singapore,Tongji University,University College London,Chinese Univ Hong Kong (CUHK) & National Univ Singapore(NUS)",Visual Learning II,"In the realm of automated robotic surgery and computer-assisted interventions, understanding robotic surgical activities stands paramount. Existing algorithms dedicated to surgical activity recognition predominantly cater to pre-defined closed-set paradigms, ignoring the challenges of real-world open-set scenarios. Such algorithms often falter in the presence of test samples originating from classes unseen during training phases. To tackle this problem, we introduce an innovative Open-Set Surgical Activity Recognition (OSSAR) framework. Our solution leverages the hyperspherical reciprocal point strategy to enhance the distinction between known and unknown classes in the feature space. Additionally, we address the issue of over-confidence in the closed-set by refining model calibration, avoiding misclassification of unknown classes as known ones. To support our assertions, we establish an open-set surgical activity benchmark utilizing the public JIGSAWS dataset. Besides, we also collect a novel dataset on endoscopic submucosal dissection for surgical activity tasks. Extensive comparisons and ablation experiments on these datasets demonstrate the significant outperformance of our method over existing state-of-the-art approaches. Our proposed solution can effectively address the challenges of real-world surgical scenarios. Our code is publicly accessible at github.com/longbai1006/OSSAR."
FSD: Fast Self-Supervised Single RGB-D to Categorical 3D Objects,"Mayank Lunayach, Sergey Zakharov, Dian Chen, Rares Ambrus, Zsolt Kira, Zubair Irshad","Georgia Institute of Technology,Toyota Research Institute",Visual Learning II,"In this work, we address the challenging task of 3D object recognition without the reliance on real-world 3D labeled data. Our goal is to predict the 3D shape, size, and 6D pose of objects within a single RGB-D image, operating at the category level and eliminating the need for CAD models during inference. While existing self-supervised methods have made strides in this field, they often suffer from inefficiencies arising from non-end-to-end processing, reliance on separate models for different object categories, and slow surface extraction during the training of implicit reconstruction models; thus hindering both the speed and real-world applicability of the 3D recognition process. Our proposed method leverages a multi-stage training pipeline, designed to efficiently transfer synthetic performance to the real-world domain. This approach is achieved through a combination of 2D and 3D supervised losses during the synthetic domain training, followed by the incorporation of 2D supervised and 3D self-supervised losses on real-world data in two additional learning stages. By adopting this comprehensive strategy, our method successfully overcomes the aforementioned limitations and outperforms existing self-supervised 6D pose and size estimation baselines on the NOCS test-set with a 16.4% absolute improvement in mAP for 6D pose estimation while running in near real-time at 5 Hz. Project page: https://fsd6d.github.io/"
Keypoint Detection and Tracking in Low-Quality Image Frames with Events,"Xiangyuan Wang, Kuangyi Chen, Wen Yang, Lei Yu, Yannan Xing, Huai Yu","Wuhan University,SynSense Co. Ltd.",Visual Learning II,"Keypoint detection and tracking in traditional image frames are often compromised by image quality issues such as motion blur and extreme lighting conditions. Event cameras offer potential solutions to these challenges by virtue of their high temporal resolution and high dynamic range. However, they have limited performance in practical applications due to their inherent noise in event data. This paper advocates fusing the complementary information from image frames and event streams to achieve more robust keypoint detection and tracking. Specifically, we propose a novel keypoint detection network that fuses the textural and structural information from image frames with the high-temporal-resolution motion information from event streams, namely FE-DeTr. The network leverages a temporal response consistency for supervision, ensuring stable and efficient keypoint detection. Moreover, we use a spatio-temporal nearest-neighbor search strategy for robust keypoint tracking. Extensive experiments are conducted on a new dataset featuring both image frames and event data captured under extreme conditions. The experimental results confirm the superior performance of our method over both existing frame-based and event-based methods. Our code, pre-trained models, and dataset are available at url{https://github.com/yuyangpoi/FE-DeTr}."
TiV-ODE: A Neural ODE-Based Approach for Controllable Video Generation from Text-Image Pairs,"Yucheng Xu, Nanbo Li, Arushi Goel, Zonghai Yao, Zijian Guo, Mohammadreza Kasaei, Hamidreza Kasaei, Zhibin Li","University of Edinburgh,UMass Amherst,Boston University,University of Groningen,University College London",Visual Learning II,"Videos capture the evolution of continuous dynamical systems over time in the form of discrete image sequences. Recently, video generation models have been widely used in robotic research. However, generating controllable videos from image-text pairs is an important yet underexplored research topic in both robotic and computer vision communities. This paper introduces an innovative and elegant framework named TiV-ODE, formulating this task as modeling the dynamical system in a continuous space. Specifically, our framework leverages the ability of Neural Ordinary Differential Equations (Neural ODEs) to model the complex dynamical system depicted by videos as a nonlinear ordinary differential equation. The resulting framework offers control over the generated videos' dynamics, content, and frame rate, a feature not provided by previous methods. Experiments demonstrate the ability of the proposed method to generate highly controllable and visually consistent videos and its capability of modeling dynamical systems. Overall, this work is a significant step towards developing advanced controllable video generation models that can handle complex and dynamic scenes."
TVFusionGAN: Thermal-Visible Image Fusion Based on Multi-Level Adversarial Network Strategy,Guoyu Lu,University of Georgia,Visual Learning II,"Thermal images excel at distinguishing objects from the background in low-light or nighttime conditions due to thermal radiation differences. However, they lack texture compared to visible images. Conversely, visible images retain more texture information at higher resolutions, particularly during the daytime, but perform poorly at night. To address the limitations of both image modalities, recent methods have employed traditional fusion techniques or fusion networks to generate fused images that combine thermal and visible properties. This paper introduces an end-to-end fusion network that leverages generative adversarial networks (GANs) to fuse salient image components from the two modalities. Our network comprises a generator and two discriminators. The generator aims to produce fusion images with salient objects using a specially designed CIoU loss. The two adversarial networks ensure that the fused images are salient both in a holistic sense and at a local scale. One discriminator encourages the fused images to resemble visible images holistically, while the other ensures that the targeted objects in the fused images are as salient as in thermal images. Our method effectively preserves the thermal radiation of salient objects in infrared images while incorporating the textures of visible images."
You Only Label Once: 3D Box Adaptation from Point Cloud to Image with Semi-Supervised Learning,"Jieqi Shi, Peiliang Li, Xiaozhi Chen, Shaojie Shen","the Hong Kong University of Science and Technology,HKUST, Robotics Institute,DJI,Hong Kong University of Science and Technology",Visual Learning II,"The image-based 3D object detection task expects that the predicted 3D bounding box has a â€œtightnessâ€ projection (also referred to as cuboid) to facilitate 2D-based training, which fits the object contour well on the image while still remaining reasonable on the 3D space. These requirements bring significant challenges to the annotation. Simply projecting the Lidar-labeled 3D boxes to the image leads to non-trivial misalignment, while directly drawing a cuboid on the image cannot access the original 3D information. In this work, we propose a learning-based 3D box adaptation approach that automatically adjusts minimum parameters of the 360â—¦ Lidar 3D bounding box to perfectly fit the image appearance of panoramic cameras. With only a few 2D boxes annotation as guidance during the training phase, our network can produce accurate image-level cuboid annotations with 3D properties from Lidar boxes. We call our method â€œyou only label onceâ€, which means labeling on the point cloud once and automatically adapting to all surrounding cameras. Our refinement balances the accuracy and efficiency well and dramatically reduces the labeling effort for accurate cuboid annotation. Extensive experiments on the public Waymo and NuScenes datasets show that our method can produce human-level cuboid annotation on the image without needing manual adjustment, and can accelerate the monocular-3D training tasks."
Self-Supervised Pretraining and Finetuning for Monocular Depth and Visual Odometry,"Boris Chidlovskii, Leonid Antsfeld",Naver Labs Europe,Visual Learning II,"For the task of simultaneous monocular depth and visual odometry estimation, we propose two-step learning of self-supervised transformer-based models. We first benefit from the generic pretrained models oriented towards understanding 3D geometry, followed by self-supervised finetuning on non-annotated videos. We show that our self-supervised models can reach the state-of-the-art performance 'without bells and whistles' using the standard components such as visual transformers, dense prediction transformers and adapters. We demonstrate the effectiveness of our proposed method by running evaluations on six benchmark datasets, both static and dynamic, indoor and outdoor, with synthetic and real images. For all datasets, our method outperforms state-of-the-art methods, in particular for depth prediction task."
Amodal Optical Flow,"Maximilian Luz, Rohit Mohan, Ahmed Rida Sekkat, Oliver Sawade, Elmar Matthes, Thomas Brox, Abhinav Valada","University of Freiburg,IAV GmbH",Computer Vision for Transportation,"Optical flow estimation is very challenging in situations with transparent or occluded objects. In this work, we address these challenges at the task level by introducing Amodal Optical Flow, which integrates optical flow with amodal perception. Instead of only representing the visible regions, we define amodal optical flow as a multi-layered pixel-level motion field that encompasses both visible and occluded regions of the scene. To facilitate research on this new task, we extend the AmodalSynthDrive dataset to include pixel-level labels for amodal optical flow estimation. We present several strong baselines, along with the Amodal Flow Quality metric to quantify the performance in an interpretable manner. Furthermore, we propose the novel AmodalFlowNet as an initial step toward addressing this task. AmodalFlowNet consists of a transformer-based cost-volume encoder paired with a recurrent transformer decoder which facilitates recurrent hierarchical feature propagation and amodal semantic grounding. We demonstrate the tractability of amodal optical flow in extensive experiments and show its utility for downstream tasks such as panoptic tracking. We make the dataset, code, and trained models publicly available at http://amodal-flow.cs.uni-freiburg.de."
AutoGraph: Predicting Lane Graphs from Traffic Observations,"Jannik Zuern, Ingmar Posner, Wolfram Burgard","University of Freiburg,Oxford University,University of Technology Nuremberg",Computer Vision for Transportation,
3DSF-MixNet: Mixer-Based Symmetric Scene Flow Estimation from 3D Point Clouds,"Shuaijun Wang, Rui Gao, Ruihua Han, Qi Hao","Southern University of Science and Technology,University of Hong Kong,SOUTHERN UNIVERSITY OF SCIENCE AND TECHNOLOGY",Computer Vision for Transportation,"The scene flow estimation aims at accurately achieving the motion of 3D points, imposing challenges like mis-registration, object occlusions, and non-uniform upsampling. This paper introduces a scene flow estimation framework featuring a unified scene flow estimator, a symmetric cost volume approach, and a geometric/semantic feature based upsampling strategy. The novelty of this work is threefold: (1) developing a novel progressive framework which integrates the cost volume module and scene flow estimator, enhancing scene flow estimation. (2) developing a symmetric inter-frame correlation feature extraction method through CV estimation using MLP-Mixer operations; (3) developing an upsampling strategy based on both the semantic and geometric feature similarities between sparse and dense samples. Experiment results show that our method outperforms state-of-the-art baseline methods, especially in scenarios involving challenging conditions, the improvements of our method achieving at most 0.1094m/0.089m/0.091m in EPE3D, 54.23%/53.67%/74.1% in AS, 32.75%/21.87%/40.25% in AR, and 70.981%/58.06%/43.56% in outliers, when tested on FlyingThings3D (FT3D_S, FT3D_H) and KITTI_H datasets, respectively."
CVFormer: Learning Circum-View Representation and Consistency Constraints for Vision-Based Occupancy Prediction Via Transformers,"Zhengqi Bai, Wenjun Shi, Dongchen Zhu, Hanlong Kang, Guanghui Zhang, Gang Ye, Yang Xiao, Lei Wang, Xiaolin Zhang, Bo Li, Jiamao Li","Shanghai Institute of Microsystem and Information Technology,Shanghai Institute of Microsystem and Information Technology,Chi,Lotus Robotics,Shanghai Institute of Microsystem and Information Technology, Ch,Lotus Technology Ltd.,Shanghai Institute of Microsystem And Information Technology,Chi",Computer Vision for Transportation,"With the increasing demands for perception accuracy in autonomous driving, there is a growing focus on fine-grained 3D semantic occupancy prediction. Effectively representing detailed three-dimensional scenes has become a significant challenge in the development of this task. In this paper, we present a novel transformer-based framework named CVFormer, which leverages two-dimensional circum-views from the ego to excavate three-dimensional features of the surrounding environment. Circum-views provide a novel solution for effectively addressing the representation of dense and fine-grained scenes. Specifically, a multi-attention module CTMA is designed for fusing temporal features from circum-views to fully exploit the spatiotemporal correlations between frames and capture more comprehensive clues. Furthermore, a novel 2D projection constraint is established by observing objects from different perspective directions, and multiple 3D constraints based on object invariance and semantic consistency are also conducted for supervising the network, which enhances its performance of understanding the scene. Experimental results on nuScenes dataset demonstrate that the proposed CVFormer obviously outperforms existing methods for occupancy prediction."
Lightweight Event-Based Optical Flow Estimation Via Iterative Deblurring,"Yilun Wu, Federico Paredes-valles, Guido De Croon","TU Delft,Delft University of Technology",Computer Vision for Transportation,"Inspired by frame-based methods, state-of-the-art event-based optical flow networks rely on the explicit construction of correlation volumes, which are expensive to compute and store, rendering them unsuitable for robotic applications with limited compute and energy budget. Moreover, correlation volumes scale poorly with resolution, prohibiting them from estimating high-resolution flow. We observe that the spatiotemporally continuous traces of events provide a natural search direction for seeking pixel correspondences, obviating the need to rely on gradients of explicit correlation volumes as such search directions. We introduce IDNet (Iterative Deblurring Network), a lightweight yet high-performing event-based optical flow network directly estimating flow from event traces without using correlation volumes. We further propose two iterative update schemes: ""ID"" which iterates over the same batch of events, and ""TID"" which iterates over time with streaming events in an online fashion. Our top-performing model (ID) sets a new state of the art on DSEC benchmark. Meanwhile, the base model (TID) is competitive with prior arts while using 80% fewer parameters, consuming 20x less memory footprint and running 40% faster on the NVidia Jetson Xavier NX. Furthermore, the ""TID"" scheme is even more efficient offering an additional 5x faster inference speed and 8 ms ultra-low latency at the cost of only a 9% performance drop, making it the only model among current literature capable of real-time operation while maintaining decent performance."
ActFormer: Scalable Collaborative Perception Via Active Queries,"Suozhi Huang, Juexiao Zhang, Yiming Li, Chen Feng","Tsinghua University,New York University",Computer Vision for Transportation,"Collaborative perception leverages rich visual observations from multiple robots to extend a single robot's perception ability beyond its field of view. Many prior works receive messages broadcast from all collaborators, leading to a scalability challenge when dealing with a large number of robots and sensors. In this work, we aim to address scalable camera-based collaborative perception with a Transformer-based architecture. Our key idea is to enable a single robot to intelligently discern the relevance of the collaborators and their associated cameras according to a learned spatial prior. This proactive understanding of the visual features' relevance does not require the transmission of the features themselves, enhancing both communication and computation efficiency. Specifically, we present ActFormer, a Transformer that learns bird's eye view (BEV) representations by using predefined BEV queries to interact with multi-robot multi-camera inputs. Each BEV query can actively select relevant cameras for information aggregation based on pose information, instead of interacting with all cameras indiscriminately. Experiments on the V2X-Sim dataset demonstrate that ActFormer improves the detection performance from 29.89% to 45.15% in terms of [email protected] with about 50% fewer queries, showcasing the effectiveness of ActFormer in multi-agent collaborative 3D object detection."
LiDAR Data Synthesis with Denoising Diffusion Probabilistic Models,"Kazuto Nakashima, Ryo Kurazume",Kyushu University,Computer Vision for Transportation,"Generative modeling of 3D LiDAR data is an emerging task with promising applications for autonomous mobile robots, such as scalable simulation, scene manipulation, and sparse-to-dense completion of LiDAR point clouds. While existing approaches have demonstrated the feasibility of image-based LiDAR data generation using deep generative models, they still struggle with fidelity and training stability. In this work, we present R2DM, a novel generative model for LiDAR data that can generate diverse and high-fidelity 3D scene point clouds based on the image representation of range and reflectance intensity. Our method is built upon denoising diffusion probabilistic models (DDPMs), which have shown impressive results among generative model frameworks in recent years. To effectively train DDPMs in the LiDAR domain, we first conduct an in-depth analysis of data representation, loss functions, and spatial inductive biases. Leveraging our R2DM model, we also introduce a flexible LiDAR completion pipeline based on the powerful capabilities of DDPMs. We demonstrate that our method surpasses existing methods in generating tasks on the KITTI-360 and KITTI-Raw datasets, as well as in the completion task on the KITTI-360 dataset. Our project page can be found at url{https://kazuto1011.github.io/r2dm}."
Multi-Task Learning for Real-Time Autonomous Driving Leveraging Task-Adaptive Attention Generator,"Wonhyeok Choi, Mingyu Shin, Hyukzae Lee, Jaehoon Cho, Jaehyeon Park, Sunghoon Im","DGIST,daegu gyeongbuk institute of science and technology,Hyundai Motor Company,Hyundai Motor Company R&D Division,Hyundai Motor",Computer Vision for Transportation,"Real-time processing is crucial in autonomous driving systems due to the imperative of instantaneous decision-making and rapid response. In real-world scenarios, autonomous vehicles are continuously tasked with interpreting their surroundings, analyzing intricate sensor data, and making decisions within split seconds to ensure safety through numerous computer vision tasks. In this paper, we present a new real-time multi-task network adept at three vital autonomous driving tasks: monocular 3D object detection, semantic segmentation, and dense depth estimation. To counter the challenge of negative transfer â€” the prevalent issue in multi-task learning â€” we introduce a task-adaptive attention generator. This generator is designed to automatically discern interrelations across the three tasks and arrange the task-sharing pattern, all while leveraging the efficiency of the hard-parameter sharing approach. To the best of our knowledge, the proposed model is pioneering in its capability to concurrently handle multiple tasks, notably 3D object detection, while maintaining real-time processing speeds. Our rigorously optimized network, when tested on the Cityscapes-3D datasets, consistently outperforms various baseline models. Moreover, an in-depth ablation study substantiates the efficacy of the methodologies integrated into our framework."
LiDARFormer: A Unified Transformer-Based Multi-Task Network for LiDAR Perception,"Zixiang Zhou, Dongqiangzi Ye, Weijia Chen, Yufei Xie, Y W, Panqu Wang, Hassan Foroosh","University of Central Florida,Tusimple,N/A,ZERON",Computer Vision for Transportation,"There is a recent need in the LiDAR perception field for unifying multiple tasks in a single strong network with improved performance, as opposed to using separate networks for each task. In this paper, we introduce a new LiDAR multi-task learning paradigm based on the transformer. The proposed LiDARFormer utilizes cross-space global contextual feature information and exploits cross-task synergy to boost the performance of LiDAR perception tasks across multiple large-scale datasets and benchmarks. Our novel transformer-based framework includes a cross-space transformer module that learns attentive features between the 2D dense Bird's Eye View (BEV) and 3D sparse voxel feature maps. Additionally, we propose a transformer decoder for the segmentation task to dynamically adjust the learned features by leveraging the categorical feature representations. Furthermore, we combine the segmentation and detection features in a shared transformer decoder with cross-task attention layers to enhance and integrate the object-level and class-level features. LiDARFormer is evaluated on the large-scale nuScenes and the Waymo Open datasets for both 3D detection and semantic segmentation tasks, and it achieves state-of-the-art performance on both tasks."
Adaptive Robot-Human Handovers with Preference Learning,"Gojko Perovic, Francesco Iori, Angela Mazzeo, Marco Controzzi, Egidio Falotico",Scuola Superiore Sant'Anna,Machine Learning for Robot Control I,"This paper proposes an adaptive method for robot-to-human handovers under different scenarios. The method combines Dynamic Movement Primitives (DMP) with Preference Learning (PL) to generate online trajectories that are reactive to human motion, modulating the speed of the robot. The PL allows for tuning the coupling parameters of the DMP, tailoring the interaction to each participant personally, and allowing for qualitative analysis of user preferences. Simulation of an interaction-constrained learning task with different optimization techniques is performed to determine an appropriate learning approach for a handover task. The validity of the approach is demonstrated through experiments with participants on two handover tasks, with results indicating that the proposed method leads to seamless and pleasurable interactions."
NaviFormer: A Data-Driven Robot Navigation Approach Via Sequence Modeling and Path Planning with Safety Verification,"Xuyang Zhang, Ziyang Feng, Quecheng Qiu, Yu'an Chen, Bei Hua, Jianmin Ji","University of Science and Technology of China,School of Data Science, USTC, Hefei ,,,,,,, China,University of science and technology of China",Machine Learning for Robot Control I,"Reinforcement learning has shown great potential in improving the performance of robot navigation. In response to the increasing deployments of mobile robots within various scenarios, a data-driven paradigm of navigation approach with safety verification is preferred where one can train RL algorithms with large amounts of prior data, keep learning continuously, and ensure safe navigation in applications. Conventional end-to-end reinforcement learning navigation paradigms have encountered multiple challenges in meeting these demands. In this work, we introduce a novel robot navigation approach termed NaviFormer. This approach handles navigation tasks based on sequence modeling to obtain the data-driven ability. It also integrates rule-based verification for safety insurance. We conduct a series of experiments to validate the data-driven ability of our approach and to compare it with existing navigation methods. We also perform quantitative tests on a real-world robot platform, TurtleBot. The experimental results show our method's outstanding data-driven ability and highlight its superior arrival rate and generalization compared to other state-of-the-art methods like the PPO-based navigation method."
Real-Time Adaptive Safety-Critical Control with Gaussian Processes in High-Order Uncertain Models,"Yu Zhang, Long Wen, Xiangtong Yao, Zhenshan Bing, Linghuan Kong, Wei He, Zhenshan Bing","Technical University of Munich,University of Macau,University of Science and Technology Beijing,Tech. Univ. Muenchen TUM",Machine Learning for Robot Control I,"This paper presents an adaptive online learning framework for systems with uncertain parameters to ensure safety-critical control in non-stationary environments. Our approach consists of two phases. The initial phase is centered on a novel sparse Gaussian process (GP) framework. We first integrate a forgetting factor to refine a variational sparse GP algorithm, thus enhancing its adaptability. Subsequently, the hyperparameters of the Gaussian model are trained with a specially compound kernel, and the Gaussian modelâ€™s online inferential capability and computational efficiency are strengthened by updating a solitary inducing point derived from newly samples, in conjunction with the learned hyperparameters. In the second phase, we propose a safety filter based on high order control barrier functions (HOCBFs), synergized with the previously trained learning model. By leveraging the compound kernel from the first phase, we effectively address the inherent limitations of GPs in handling highdimensional problems for real-time applications. The derived controller ensures a rigorous lower bound on the probability of satisfying the safety specification. Finally, the efficacy of our proposed algorithm is demonstrated through real-time obstacle avoidance experiments executed using both simulation platform and a real-world 7-DOF robot."
DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable Object Manipulation,"Chenchang Li, Zihao Ai, Tong Wu, Xiaosa Li, Wenbo Ding, Huazhe Xu",Tsinghua University,Machine Learning for Robot Control I,"Manipulating deformable objects is a ubiquitous task in household environments, demanding adequate representation and accurate dynamics prediction due to the objects' infinite degrees of freedom. This work proposes DeformNet, which utilizes latent space modeling with a learned 3D representation model to tackle these challenges effectively. The proposed representation model combines a PointNet encoder and a conditional neural radiance field (NeRF), facilitating a thorough acquisition of object deformations and variations in lighting conditions. To model the complex dynamics, we employ a recurrent state-space model (RSSM) that accurately predicts the transformation of the latent representation over time. Extensive simulation experiments with diverse objectives demonstrate the generalization capabilities of DeformNet for various deformable object manipulation tasks, even in the presence of previously unseen goals. Finally, we deploy DeformNet on an actual UR5 robotic arm to demonstrate its capability in real-world scenarios."
Actor-Critic Model Predictive Control,"Angel Romero, Yunlong Song, Davide Scaramuzza",University of Zurich,Machine Learning for Robot Control I,"An open research question in robotics is how to combine the benefits of model-free reinforcement learning (RL) - known for its strong task performance and flexibility in optimizing general reward formulations - with the robustness and online replanning capabilities of model predictive control (MPC). This paper provides an answer by introducing a new framework called Actor-Critic Model Predictive Control. The key idea is to embed a differentiable MPC within an actor-critic RL framework. The proposed approach leverages the short-term predictive optimization capabilities of MPC with the exploratory and end-to-end training properties of RL. The resulting policy effectively manages both short-term decisions through the MPC-based actor and long-term prediction via the critic network, unifying the benefits of both model-based control and end-to-end learning. We validate our method in both simulation and the real world with a quadcopter platform across various high-level tasks. We show that the proposed architecture can achieve real-time control performance, learn complex behaviors via trial and error, and retain the predictive properties of the MPC to better handle out of distribution behaviour."
Tractable Joint Prediction and Planning Over Discrete Behavior Modes for Urban Driving,"Adam Villaflor, Brian Yang, Huangyuan Su, Aikaterini Fragkiadaki, John Dolan, Jeff Schneider","CMU,University of California, Berkeley,Harvard University,Carnegie Mellon University",Machine Learning for Robot Control I,"Significant progress has been made in training multimodal trajectory forecasting models for autonomous driving. However, effectively integrating these models with downstream planners and model-based control approaches is still an open problem. Although these models have conventionally been evaluated for open-loop prediction, we show that they can be used to parameterize autoregressive closed-loop models without retraining. We consider recent trajectory prediction approaches which leverage learned anchor embeddings to predict multiple trajectories, finding that these anchor embeddings can parameterize discrete and distinct modes representing high-level driving behaviors. We propose to perform fully reactive closed-loop planning over these discrete latent modes, allowing us to tractably model the causal interactions between agents at each step. We validate our approach on a suite of more dynamic merging scenarios, finding that our approach avoids the frozen robot problem which is pervasive in conventional planners. Our approach also outperforms the previous state-of-the-art in CARLA on challenging dense traffic scenarios when evaluated at realistic speeds."
GenDOM: Generalizable One-Shot Deformable Object Manipulation with Parameter-Aware Policy,"So Kuroki, Jiaxian Guo, Tatsuya Matsushima, Takuya Okubo, Masato Kobayashi, Yuya Ikeda, Ryosuke Takanami, Paul Yoo, Yutaka Matsuo, Yusuke Iwasawa","The university of Tokyo,The University of Tokyo,University of Tokyo,Osaka University,the University of Tokyo",Machine Learning for Robot Control I,"Due to the inherent uncertainty in their deformability during motion, previous methods in deformable object manipulation, such as rope and cloth, often required hundreds of real-world demonstrations to train a manipulation policy for each object, which hinders their applications in our ever-changing world. To address this issue, we introduce GenDOM, a framework that allows the manipulation policy to handle different deformable objects with only a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable object parameters and training it with a diverse range of simulated deformable objects so that the policy can adjust actions based on different object parameters. At the time of inference, given a new object, GenDOM can estimate the deformable object parameters with only a single real-world demonstration by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations in a differentiable physics simulator. Empirical validations on both simulated and real-world object manipulation setups clearly show that our method can manipulate different objects with a single demonstration and significantly outperforms the baseline in both environments (a 62% improvement for in-domain ropes and a 15% improvement for out-of-distribution ropes in simulation, as well as a 26% improvement for ropes and a 50% improvement for cloths in the real world), demonstrating the effectiveness of our approach in one-shot deformable object manipulation."
RiskBench: A Scenario-Based Benchmark for Risk Identification,"Chi-hsi Kung, Pang-yuan Pao, Pin-lun Chen, Hsin-cheng Lu, Chieh Chi Yang, Shu-wei Lu, Yi-ting Chen","National Tsing Hua University,National Yang Ming Chiao Tung University,National Taiwan University",Datasets for Robotic Vision,"Intelligent driving systems aim to achieve a zero-collision mobility experience, requiring interdisciplinary efforts to enhance safety performance. This work focuses on risk identification, the process of identifying and analyzing risks stemming from dynamic traffic participants and unexpected events. While significant advances have been made in the community, the current evaluation of different risk identification algorithms uses independent datasets, leading to difficulty in direct comparison and hindering collective progress toward safety performance enhancement. To address this limitation, we introduce RiskBench, a large-scale scenario-based benchmark for risk identification. Our benchmark is created using a scenario-based approach, which is widely accepted in the automotive industry. We design a scenario taxonomy and augmentation pipeline to enable a systematic collection of ground truth risks under different scenarios. We assess the ability of ten algorithms to (1) detect and locate risks, (2) anticipate risks, and (3) facilitate decision-making. We conduct extensive experiments and summarize future research on risk identification. Our aim is to encourage collaborative endeavors in achieving a society with zero collisions. To facilitate this, we have made our dataset and benchmark toolkit publicly at https://hcis-lab.github.io/RiskBench/"
Enhancing Inland Water Safety: The Lake Constance Obstacle Detection Benchmark,"Dennis Griesser, Matthias Franz, Georg Umlauf","University of Applied Sciences Konstanz, Institute for Optical S",Datasets for Robotic Vision,"Autonomous navigation on inland waters requires an accurate understanding of the environment in order to react to possible obstacles. Deep learning is a promising technique to detect obstacles robustly. However, supervised deep learning models require large data-sets to adjust their weights and to generalize to unseen data. Therefore, we equipped our research vessel with a laser scanner and a stereo camera to record a novel obstacle detection data-set for inland waters. We annotated 1974 stereo images and lidar point clouds with 3d bounding boxes. Furthermore, we provide an initial approach and a suitable metric to compare the results on the test data-set. The data-set is publicly available and seeks to make a contribution towards increasing the safety on inland waters."
IDD-X: A Multi-View Dataset for Ego-Relative Important Object Localization and Explanation in Dense and Unstructured Traffic,"Chirag Parikh, Rohit Saluja, C.V. Jawahar, Ravi Kiran Sarvadevabhatla","International Institute of Information Technology, Hyderabad,IIT Mandi,IIIT, Hyderabad,IIIT Hyderabad",Datasets for Robotic Vision,"Intelligent vehicle systems require a deep understanding of the interplay between road conditions, surrounding entities, and the ego vehicle's driving behavior for safe and efficient navigation. This is particularly critical in developing countries where traffic situations are often dense and unstructured with heterogeneous road occupants. Existing datasets, predominantly geared towards structured and sparse traffic scenarios, fall short of capturing the complexity of driving in such environments. To fill this gap, we present IDD-X, a large-scale dual-view driving video dataset. With 697K bounding boxes, 9K important object tracks, and 1-12 objects per video, IDD-X offers comprehensive ego-relative annotations for multiple important road objects covering 10 categories and 19 explanation label categories. The dataset also incorporates rearview information to provide a more complete representation of the driving environment. We also introduce custom-designed deep networks aimed at multiple important object localization and per-object explanation prediction. Overall, our dataset and introduced prediction models form the foundation for studying how road conditions and surrounding entities affect driving behavior in complex traffic situations."
LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,"Jin Fang, Dingfu Zhou, Jingjing Zhao, Chenming Wu, Chulin Tang, Cheng-zhong Xu, Liangjun Zhang","Baidu,tusimple,Baidu Research,University of California, Irvine,University of Macau",Datasets for Robotic Vision,"Over the past few years, there has been remarkable progress in research on 3D point clouds and their use in autonomous driving scenarios has become widespread. However, deep learning methods heavily rely on annotated data and often face domain generalization issues. Unlike 2D images whose domains usually pertain to the texture information present in them, the features derived from a 3D point cloud are affected by the distribution of the points. The lack of a 3D domain adaptation benchmark leads to the common practice of training a model on one benchmark (e.g. Waymo) and then assessing it on another dataset (e.g. KITTI). This setting results in two distinct domain gaps: scenarios and sensors, making it difficult to analyze and evaluate the method accurately. To tackle this problem, this paper presents LiDAR Dataset with Cross Sensors (LiDAR-CS Dataset), which contains large-scale annotated LiDAR point cloud under six groups of different sensors but with the same corresponding scenarios, captured from hybrid realistic LiDAR simulator. To our knowledge, LiDAR-CS Dataset is the first dataset that addresses the sensor-related gaps in the domain of 3D object detection in real traffic. Furthermore, we evaluate and analyze the performance using various baseline detectors and demonstrated its potential applications. Project page: https://opendriving.github.io/lidar-cs."
ROV6D: 6D Pose Estimation Benchmark Dataset for Underwater Remotely Operated Vehicles,"Jingyi Tang, Zeyu Chen, Bowen Fu, Wenjie Lu, Shengquan Li, Xiu Li, Xiangyang Ji","Tsinghua University,Harbin Institute of Technology (Shenzhen),Pengcheng Lab",Datasets for Robotic Vision,"Accurately localization between multi-robots is crucial for many underwater applications, such as tracking, convoying and subsea intervention tasks. 6D pose estimation is a fundamental task that enables precise object localization in 3D space with full six degrees of freedom. However, one critical challenge is the lack of available large-scale datasets due to the unbearable cost of labelled data collection. To overcome this difficulty, we propose a benchmark dataset, ROV6D, for 6D pose estimation of remotely operated vehicles (ROVs). The training subset consists of a large number of synthetic images with 6D pose ground truth for ROVs. These synthetic images are generated using BlenderProc and further rendered with the underwater neural rendering (UWNR) strategy to enhance their realism. The testing subsets cover different real-world scenarios, including the Pool subset and Maoming subset, focusing on challenging cases that involve partial occlusion and low visibility. Diverse recent methods are evaluated on the constructed dataset. The results show that methods based on dense coordinates currently perform best, outperforming both the keypoint-based method and the refinement-based method. Our dataset will be made publicly available soon."
The GOOSE Dataset for Perception in Unstructured Environments,"Peter Mortimer, Raphael Hagmanns, Miguel Granero, Thorsten Luettel, Janko Petereit, Hans J Wuensche","Universität der Bundeswehr München,Karlsruhe Institute of Technology,Fraunhofer IOSB",Datasets for Robotic Vision,"The potential for deploying autonomous systems can be significantly increased by improving the perception and interpretation of the environment. However, the development of deep learning-based techniques for autonomous systems in unstructured outdoor environments poses challenges due to limited data availability for training and testing. To address this gap, we present the German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset specifically designed for unstructured outdoor environments. The GOOSE dataset incorporates 10000 labeled pairs of images and point clouds, which are utilized to train a range of state-of-the-art segmentation models on both image and point cloud data. We open source the dataset, along with an ontology for unstructured terrain, as well as dataset standards and guidelines. This initiative aims to establish a common framework, enabling the seamless inclusion of existing datasets and a fast way to enhance the perception capabilities of various robots operating in unstructured environments. This framework also makes it possible to query data for specific weather conditions or sensor setups from a database in future. The dataset, pre-trained models for offroad perception, and additional documentation can be found at https://goose-dataset.de/."
CoAS-Net: Context-Aware Suction Network with a Large-Scale Domain Randomized Synthetic Dataset,"Yeong Gwang Son, Tat Hieu Bui, Juyong Hong, Yong Hyeon Kim, Seung Jae Moon, Chunsoo Kim, Issac Rhee, Hansol Kang, Hyouk Ryeol Choi","SungKyunKwan University,Sungkyunkwan University,Sungkyunkwan univ,Sungkyunkwan, mechanical engineering, Robottory,SKKU",Datasets for Robotic Vision,"Robotic grasping is one of the essential skills in robotics. From industrial to housework, robots are required to handle objects, enabling them to interact with their surroundings. Among the various tasks in robotic grasping, bin-picking is considered one of the most challenging because of the cluttered bin filled with objects. Also, for the next-level automation, they need to handle unseen objects and discriminate target objects and outliers. This paper proposes a novel dataset generation pipeline for suction-grasping in bin-picking tasks. This pipeline consists of a series of methods that progressively transit from a single object evaluation to an entire scene evaluation and lower the dimension of the labels to the image space. We trained a suction prediction FCN (Fully Convolution Network) with our dataset generated from the pipeline and conducted bin-picking experiments. Our large-scale collision-free annotation enables the network to understand the context of a bin-picking task, where collisions between the gripper and the bin or object are a concern, and distinguishing the background is crucial. The results show that our solution excels the existing methods, and the network demonstrates its context-aware grasp on objects with loosely defined RoI (Region of Interest). Our dataset and the grasp detection model are available at https://github.com/SonYeongGwang/CoAS-Net.git."
NYC-Indoor-VPR: A Long-Term Indoor Visual Place Recognition Dataset with Semi-Automatic Annotation,"Diwei Sheng, Anbang Yang, John-ross Rizzo, Chen Feng","New York University,NYU School of Medicine / NYU Tandon School of Engineering",Datasets for Robotic Vision,"Visual Place Recognition (VPR) in indoor environments is beneficial to humans and robots for better localization and navigation. It is challenging due to appearance changes at various frequencies, and difficulties of obtaining ground truth metric trajectories for training and evaluation. This paper introduces the NYC-Indoor-VPR dataset, a unique and rich collection of over 36,000 images compiled from 13 distinct crowded scenes in New York City taken under varying lighting conditions with appearance changes. Each scene has multiple revisits across a year. To establish the ground truth for VPR, we propose a semiautomatic annotation approach that computes the positional information of each image. Our method specifically takes pairs of videos as input and yields matched pairs of images along with their estimated relative locations. The accuracy of this matching is refined by human annotators, who utilize our annotation software to correlate the selected keyframes. Finally, we present a benchmark evaluation of several state-of-the-art VPR algorithms using our annotated dataset, revealing its challenge and thus value for VPR research."
TreeScope: An Agricultural Robotics Dataset for LiDAR-Based Mapping of Trees in Forests and Orchards,"Derek Cheng, Fernando Cladera, Ankit Prabhu, Xu Liu, Alan Zhu, Patrick Corey Green, Reza Ehsani, Pratik Chaudhari, Vijay Kumar","University of Pennsylvania,Virginia Polytechnic Institute and State University,UC Merced",Datasets for Robotic Vision,"Data collection for forestry, timber, and agriculture currently relies on manual techniques which are labor-intensive and time-consuming. We seek to demonstrate that robotics offers improvements over these techniques and accelerate agricultural research, beginning with semantic segmentation and diameter estimation of trees in forests and orchards. We present TreeScope v1.0, the first robotics dataset for precision agriculture and forestry addressing the counting and mapping of trees in forestry and orchards. TreeScope provides LiDAR data from agricultural environments collected with robotics platforms, such as UAV and mobile robot platforms carried by vehicles and human operators. In the first release of this dataset, we provide ground-truth data with over 1,800 manually annotated semantic labels for tree stems and field-measured tree diameters. We share benchmark scripts for these tasks that researchers may use to evaluate the accuracy of their algorithms. Finally, we run our open-source diameter estimation and off-the-shelf semantic segmentation algorithms and share our baseline results."
Long-HOT: A Modular Hierarchical Approach for Long-Horizon Object Transport,"Sriram N N, Dinesh Jayaraman, Manmohan Chandraker","NEC Labs America,University of Pennsylvania,University of California, San Diego",Task and Motion Planning I,"We aim to address key challenges in long-horizon embodied exploration and navigation by proposing a long-horizon object transport task called Long-HOT and a novel modular framework for temporally extended navigation. Agents in Long-HOT need to efficiently find and pick up target objects that are scattered in the environment, carry them to a goal location with load constraints, and optionally have access to a container. We propose a modular topological graph-based transport policy (HTP) that explores efficiently with the help of weighted frontiers. Our hierarchical approach uses a combination of motion planning algorithms to reach point goals within explored locations and object navigation policies for moving towards semantic targets at unknown locations. Experiments on both our proposed Habitat transport task and on MultiOn benchmarks show that our method outperforms baselines and prior works. Further, we analyze the agent's behavior for the usage of the container and demonstrate meaningful generalization to harder transport scenes with training only on simpler versions of the task."
COAST: Constraints and Streams for Task and Motion Planning,"Brandon Vu, Toki Migimatsu, Jeannette Bohg",Stanford University,Task and Motion Planning I,"Task and Motion Planning (TAMP) algorithms solve long-horizon robotics tasks by integrating task planning with motion planning; the task planner proposes a sequence of actions towards a goal state and the motion planner verifies whether this action sequence is geometrically feasible for the robot. However, state-of-the-art TAMP algorithms do not scale well with the difficulty of the task and require an impractical amount of time to solve relatively small problems. We propose Constraints and Streams for Task and Motion Planning (COAST), a probabilistically-complete, sampling-based TAMP algorithm that combines stream-based motion planning with an efficient, constrained task planning strategy. We validate COAST on three challenging TAMP domains and demonstrate that our method outperforms baselines in terms of cumulative task planning time by an order of magnitude. You can find more supplementary materials on our project website at url{https://branvu.github.io/coast.github.io}."
Extending the Cooperative Dual-Task Space in Conformal Geometric Algebra,"Tobias Löw, Sylvain Calinon","Idiap Research Institute, EPFL,Idiap Research Institute",Task and Motion Planning I,"In this work, we are presenting an extension of the cooperative dual-task space (CDTS) in conformal geometric algebra. The CDTS was first defined using dual quaternion algebra and is a well established framework for the simplified definition of tasks using two manipulators. By integrating conformal geometric algebra, we aim to further enhance the geometric expressiveness and thus simplify the modeling of various tasks. We show this formulation by first presenting the CDTS and then its extension that is based around a cooperative pointpair. This extension keeps all the benefits of the original formulation that is based on dual quaternions, but adds more tools for geometric modeling of the dual-arm tasks. We also present how this CGA-CDTS can be seamlessly integrated with an optimal control framework in geometric algebra that was derived in previous work. In the experiments, we demonstrate how to model different objectives and constraints using the CGA-CDTS. Using a setup of two Franka Emika robots we then show the effectiveness of our approach using model predictive control in real world experiments."
D-LGP: Dynamic Logic-Geometric Program for Reactive Task and Motion Planning,"Teng Xue, Amirreza Razmjoo, Sylvain Calinon","idiap/EPFL,Idiap Research Institute",Task and Motion Planning I,"Many real-world sequential manipulation tasks involve a combination of discrete symbolic search and continuous motion planning, collectively known as combined task and motion planning (TAMP). However, prevailing methods often struggle with the computational burden and intricate combinatorial challenges, limiting their applications for online replanning in the real world. To address this, we propose Dynamic Logic-Geometric Program (D-LGP), a novel approach integrating Dynamic Tree Search and global optimization for efficient hybrid planning. Through empirical evaluation on three benchmarks, we demonstrate the efficacy of our approach, showcasing superior performance in comparison to state-of-the-art techniques. We validate our approach through simulation and demonstrate its reactive capability to cope with online uncertainty and external disturbances in the real world."
Indoor Exploration and Simultaneous Trolley Collection through Task-Oriented Environment Partitioning,"Junjie Gao, Peijia Xie, Xuheng Gao, Zhirui Sun, Jiankun Wang, Max Qing Hu Meng","Harbin Institute of Technology,Southern University of Science and Technology,The Chinese University of Hong Kong",Task and Motion Planning I,"In this paper, we present a simultaneous exploration and object search framework for the application of autonomous trolley collection. For environment representation, a task-oriented environment partitioning algorithm is presented to extract diverse information for each sub-task. First, LiDAR data is classified as potential objects, walls, and obstacles after outlier removal. Segmented point clouds are then transformed into a hybrid map with the following functional components: object proposals to avoid missing trolleys during exploration; room layouts for semantic space segmentation; and polygonal obstacles containing geometry information for efficient motion planning. For exploration and simultaneous trolley collection, we propose an efficient exploration-based object search method. First, a traveling salesman problem with precedence constraints (TSP-PC) is formulated by grouping frontiers and object proposals. The next target is selected by prioritizing object search while avoiding excessive robot backtracking. Then, feasible trajectories with adequate obstacle clearance are generated by topological graph search. We validate the proposed framework through simulations and demonstrate the system with real-world autonomous trolley collection tasks."
Effort Level Search in Infinite Completion Trees with Application to Task-And-Motion Planning,"Marc Toussaint, Joaquim Ortiz De Haro, Valentin Hartmann, Erez Karpas, Wolfgang Hoenig","TU Berlin,ETH Zürich,Technion",Task and Motion Planning I,"Solving a Task-and-Motion Planning (TAMP) problem can be represented as a sequential (meta-) decision process, where early decisions concern the skeleton (sequence of logic actions) and later decisions concern what to compute for such skeletons (e.g., action parameters, bounds, RRT paths, or full optimal manipulation trajectories). We consider the general problem of how to schedule compute effort in such hierarchical solution processes. More specifically, we introduce infinite completion trees as a problem formalization, where before we can expand or evaluate a node, we have to solve a preemptible computational sub-problem of a priori unknown compute effort. Infinite branchings represent an infinite choice of random initializations of computational sub-problems. Decision making in such trees means to decide on where to invest compute or where to widen a branch. We propose a heuristic to balance branching width and compute depth using polynomial level sets. We show completeness of the resulting solver and that a round robin baseline strategy used previously for TAMP becomes a special case. Experiments confirm the robustness and efficiency of the method on problems including stochastic bandits and a suite of TAMP problems, and compare our approach to a round robin baseline. An appendix comparing the framework to bandit methods and proposing a corresponding tree policy version is found on the supplementary webpage."
Sense in Motion with Belief Clustering: Efficient Gas Source Localization with Mobile Robots,"Wanting Jin, Alcherio Martinoli",EPFL,Task and Motion Planning I,"Given the patchy nature of gas plumes and the slow response of conventional gas sensors, the use of mobile robots for Gas Source Localization (GSL) tasks presents significant challenges. These aspects increase the difficulties in obtaining gas measurements, encompassing both qualitative and quantitative aspects. Most existing model-based GSL algorithms rely on lengthy stops at each sampling point to ensure accurate gas measurements. However, this approach not only prolongs the time required for a single measurement but also hinders sampling during robot motion, thus exacerbating the scarcity of available gas measurements. In this work, our goal is to push the boundaries in terms of continuity in sampling to enhance system efficiency. Firstly, we decouple and comprehensively evaluate the impact of both plume dynamics and gas sensor properties on the GSL performance. Secondly, we demonstrate that adopting a continuous sampling strategy, which has been generally overlooked in prior research, markedly enhances the system efficiency by obviating the prolonged measurement pauses and leveraging all the data gathered during the robot motion. Thirdly, we further expand the capabilities of the continuous sampling by introducing a novel informative path-planning strategy, which takes into account all the information gathered along the robot's movement. The proposed method is evaluated in both simulation and reality under different scenarios emulating indoor environmental conditions."
R-LGP: A Reachability-Guided Logic-Geometric Programming Framework for Optimal Task and Motion Planning on Mobile Manipulators,"Kim Tien Ly, Valeriy Semenov, Mattia Risiglione, Wolfgang Merkt, Ioannis Havoutis","University of Oxford,Oxford University,Italian Institute of Technology",Task and Motion Planning I,"This paper presents an optimization-based solution to task and motion planning (TAMP) on mobile manipulators. Logic-geometric programming (LGP) has shown promising capabilities for optimally dealing with hybrid TAMP problems that involve abstract and geometric constraints. However, LGP does not scale well to high-dimensional systems (e.g. mobile manipulators) and can suffer from obstacle avoidance issues due to local minima. In this work, we extend LGP with a sampling-based reachability graph to enable solving optimal TAMP on high-DoF mobile manipulators. The proposed reachability graph can incorporate environmental information (obstacles) to provide the planner with sufficient geometric constraints. This reachability-aware heuristic efficiently prunes infeasible sequences of actions in the continuous domain, hence, it reduces replanning by securing feasibility at the final full path trajectory optimization. Our framework proves to be time-efficient in computing optimal and collision-free solutions, while outperforming the current state of the art on metrics of success rate, planning time, path length and number of steps. We validate our framework on the physical Toyota HSR robot and report comparisons on a series of mobile manipulation tasks of increasing difficulty."
Solving Sequential Manipulation Puzzles by Finding Easier Subproblems,"Svetlana Levit, Joaquim Ortiz De Haro, Marc Toussaint",TU Berlin,Task and Motion Planning I,"We consider a set of challenging sequential manipulation puzzles, where an agent has to interact with multiple movable objects and navigate narrow passages. Such settings are notoriously difficult for Task-and-Motion Planners, as they require interdependent regrasps and solving hard motion planning problems. In this paper, we propose to search over sequences of easier pick-and-place subproblems, which can lead to the solution of the manipulation puzzle. Our method combines a heuristic- driven forward subproblem search with an optimization based Task-and-Motion Planning solver. To guide the search, we introduce heuristics to generate and prioritize subgoals. We evaluate our approach on various manually designed and automatically generated scenes, demonstrating the benefits of auxiliary subproblems in sequential manipulation planning."
Control and Implementation of a Fluidic Elastomer Actuator for Active Suppression of Hand Tremor,"Yixin Wang, Xin-Jun Liu, Huichan Zhao",Tsinghua University,"Modeling, Control, and Learning for Soft Robots II","Active exoskeletons for tremor suppression show potential for treatment of pathological tremor thanks to their non-invasive nature. However, the active force was only used for the voluntary movement following. As a potential alternative, fluidic elastomer actuators (FEAs) possess compliance and flexibility that is important for wearable devices. In this study, we introduce the control implementation for a FEA to the application of active suppression of hand tremor, which allows a wearable FEA actively exerting force on the finger against tremor and meanwhile following the voluntary motion. The proposed pressure control algorithm could push the closed-loop pressure control to 19 Hz cutoff frequency. A combination neural network of GRU-MLP (Gated Recurrent Unit-Multilayer Perceptron) was proposed to identify and control a fiber-reinforced FEA following the voluntary movement of hand. The active tremor suppression effectiveness of the proposed method was tested on a bench-top tremor simulator, and such method could suppress the hand tremor from the original amplitude of more than 5Â° to less than 1Â°. The proposed method paves a new way for tremor suppression exoskeletons."
Adaptive State Estimation with Constant-Curvature Dynamics Using Force-Torque Sensors with Application to a Soft Pneumatic Actuator,"Phillip Maximilian Mehl, Max Bartholdt, Simon Ehlers, Thomas Seel, Moritz Schappler","Leibniz University Hannover,Institute of Mechatronic Systems, Leibniz Universität Hannover,Leibniz Universität Hannover,Institute of Mechatronic Systems, Leibniz Universitaet Hannover","Modeling, Control, and Learning for Soft Robots II","Abstractâ€” Using compliant materials leads to continuum robots undergoing large deformations. Their nonlinear behavior motivates the use of model-based controllers. They require state estimation as an essential step to be deployed. Available sensors are usually realized by introducing rigid bodies to the soft robot or inserting soft sensors made of materials different from the robot itself. Both approaches result in changes in the systemâ€™s dynamics. Optical measurements are problematic, especially in confined spaces. This can be avoided when the sensor is located at the robotâ€™s base. This paper studies the state estimation of a pneumatically actuated soft robot using the measured forces and torques at its base. For the first time, this is done using an unscented Kalman filter without restraining the dynamics to a planar or quasi-static motion while applying it to a real system. Real-time capability is achieved with our implementation. The state estimation is tested in a Cosserat rod simulation and on the physical system. The position is estimated with an accuracy of three to five millimeters for a 130 millimeter long pneumatic robot."
SofToss: Learning to Throw Objects with a Soft Robot,"Diego Bianchi, Michele Gabrio Ernesto Antonelli, Cecilia Laschi, Angelo Maria Sabatini, Egidio Falotico","Scuola Superiore Sant'Anna, Pisa, Italy,University of L’Aquila,National University of Singapore,Scuola Superiore Sant'Anna","Modeling, Control, and Learning for Soft Robots II","In this paper, we present, for the first time, a soft robot control system (SofToss) capable of throwing life-size objects toward target positions. SofToss is an open-loop controller based on deep reinforcement learning that generates, given the target position, an actuation pattern for the tossing task. To deal with the high non-linearity of the dynamics of soft robots, we deployed a neural network to learn the relationship between the actuation pattern and the target landing position, i.e., the direct model of the task. Then, a reinforcement learning method is used to predict the actuation pattern given the goal position. The proposed controller was tested on a modular soft robotic arm, I-Support, by tossing four objects of different shapes and weights in 140-mm squared target boxes. We registered a success rate of almost 65% of the throws in two actuation modalities (i.e., partial, keeping one module of the soft arm passive, and complete, with both modules active). This performance raises to 85% if one can choose the number of modules to actuate for each throwing direction. Furthermore, the results show that the proposed learning-based real-time controller achieves performance comparable to an optimization-based non-real-time controller. Our study contributes to the foundations for bringing soft robots into everyday life and industry, by performing more complex, dynamic tasks."
A Soft Robot Inverse Kinematics for Virtual Reality,"James Bern, William May, Austin Osborn, Francesco Stella, Sadra Zargarzadeh, Josie Hughes","Williams College,EPFL,University of Alberta","Modeling, Control, and Learning for Soft Robots II","We show how a variety of techniques from Computer Graphics can be leveraged to intuitively control the shape (configuration) of arbitrary 3D Soft Robots in VR. Our pipeline, Virtual Reality Soft Robot Inverse Kinematics (VR-Soft IK), overcomes fundamental limitations of general-purpose drag-and-drop soft robot control interfaces by leaving the 2D computer screen for 3D Virtual Reality (VR). VR-Soft IK uses a simulation based on the Finite Element Method (FEM) and a control method based on sensitivity analysis. Additionally, we show that our general control pipeline can be fused with techniques from 3D character animation to skin our simulation with a high-resolution surface mesh, pointing a way toward Mixed Reality Soft Robots. This full Skinned VR-Soft IK pipeline uses skeletal animation and GPU picking. We demonstrate the utility of our pipeline by doing real-time, open-loop control of the real-world 3D soft robotic arm Helix."
Toward the Use of Proxies for Efficient Learning Manipulation and Locomotion Strategies on Soft Robots,"Etienne Ménager, Quentin Peyron, Christian Duriez","Univ. Lille, Inria, CNRS, Centrale Lille, UMR ,,,, CRIStAL,Inria and CRIStAL UMR CNRS ,,,,, University of Lille,INRIA","Modeling, Control, and Learning for Soft Robots II","Soft robots are naturally designed to perform safe interactions with their environment, like locomotion and manipulation. In the literature, there are now many concepts, often bio-inspired, to propose new modes of locomotion or grasping. However, a methodology for implementing motion planning of these tasks, as exists for rigid robots, is still lacking. One of the difficulties comes from the modeling of these robots, which is very different, as it is based on the mechanics of deformable bodies. These models, whose dimension is often very large, make learning and optimization methods very costly. In this paper, we propose a proxy approach, as exists for humanoid robotics. This proxy is a simplified model of the robot that enables frugal learning of a motion strategy. This strategy is then transferred to the complete model to obtain the corresponding actuation inputs. Our methodology is illustrated and analyzed on two classical designs of soft robots doing manipulation and locomotion tasks."
Vision-Based Autonomous Steering of a Miniature Eversion Growing Robot,"Zicong Wu, S.m.hadi Sadati, Kawal Rhode, Christos Bergeles",King's College London,"Modeling, Control, and Learning for Soft Robots II","This paper presents vision-based autonomous navigation of a steerable soft growing robot. Our experimental platform is the previously presented MAMMOBOT, which is a small-diameter eversion growing robot with an embedded steerable catheter. The current manuscript first models the robot using kinematics (constant curvature) and mechanics (virtual work). Modelling considers the potential misalignment between the everting sheath and the embedded catheter. Second, a switching control architecture is proposed, wherein a model-based controller is employed for rapid convergence to a target position, followed by a closed-loop proportional controller that minimises the systemâ€™s steady-state error. Feedback is visually provided from a calibrated stereo vision system. Target-positioning and trajectory-tracking experiments are conducted to evaluate the performance of the control architecture. Experimental results demonstrate the superiority of the mechanics-based modelling and control approach, showing an average accuracy of 0.67 mm (0.66% arclength) in target positioning experiments, and an accuracy of 0.72 mm (1.11% arclength) and 0.72 mm (1.01% arclength) for tracking a square trajectory and a circular trajectory, respectively. The autonomous steering framework is showcased within a 3D-printed mammary duct phantom. This work sets the stage for endoscope-based autonomous navigation of MAMMOBOT and similar soft growing steerable robots."
POE: Acoustic Soft Robotic Proprioception for Omnidirectional End-Effectors,"Uksang Yoo, Ziven Lopez, Jeffrey Ichnowski, Jean Oh","Carnegie Mellon University,Northeastern University","Modeling, Control, and Learning for Soft Robots II","Shape estimation is crucial for precise control of soft robots. However, soft robot shape estimation and proprioception are challenging due to their complex deformation behaviors and infinite degrees of freedom. Their continuously deforming bodies complicate integrating rigid sensors and reliably estimating its shape. In this work, we present Proprioceptive Omnidirectional End-effector (POE), a tendon-driven soft robot with six embedded microphones. We first introduce novel applications of 3D reconstruction methods to acoustic signals from the microphones for soft robot shape proprioception. To improve the proprioception pipeline's training efficiency and model prediction consistency, we present POE-M. POE-M predicts key point positions from acoustic signal observations and uses an energy-minimization method to reconstruct a physically admissible high-resolution mesh of POE. We evaluate mesh reconstruction on simulated data and the POE-M pipeline with real-world experiments. Ablation studies suggest POE-M's guidance of the key points during the mesh reconstruction process provides robustness and stability to the pipeline. POE-M reduced the maximum Chamfer distance error by 23.1% compared to the state-of-the-art end-to-end soft robot proprioception models and achieved 4.91 mm average Chamfer distance error during evaluation. Supplemental materials, experiment data, and visualizations are available at sites.google.com/view/acoustic-poe."
A Data-Driven Approach to Geometric Modeling of Systems with Low-Bandwidth Actuator Dynamics,"Siming Deng, Junning Liu, Bibekananda Datta, Aishwarya Pantula, David H. Gracias, Thao Nguyen, Brian Bittner, Noah J. Cowan","Johns Hopkins University,Department of Mechanics and Engineering Sciences, College of Eng,The Johns Hopkins University, Baltimore,JHUAPL","Modeling, Control, and Learning for Soft Robots II","It is challenging to perform system identification on soft robots due to their underactuated, high-dimensional dynamics. In this work, we present a data-driven modeling framework, based on geometric mechanics (also known as gauge theory) that can be applied to systems with low-bandwidth control of the system's internal configuration. This method constructs a series of connected models comprising actuator and locomotor dynamics based on data points from stochastically perturbed, repeated behaviors. By deriving these connected models from general formulations of dissipative Lagrangian systems with symmetry, we offer a method that can be applied broadly to robots with first-order, low-pass actuator dynamics, including swelling-driven actuators used in hydrogel crawlers. These models accurately capture the dynamics of the system shape and body movements of a simplified swimming robot model. We further apply our approach to a stimulus-responsive hydrogel simulator that captures the complexity of chemo-mechanical interactions that drive shape changes in biomedically relevant micromachines. Finally, we propose an approach of numerically optimizing control signals by iteratively refining models, which is applied to optimize the input waveform for the hydrogel crawler. This transfer to realistic environments provides promise for applications in locomotor design and biomedical engineering."
Modeling and Control of Intrinsically Elasticity Coupled Soft-Rigid Robots,"Zach Patterson, Cosimo Della Santina, Daniela Rus","MIT,TU Delft","Modeling, Control, and Learning for Soft Robots II","While much work has been done recently in the realm of model-based control of soft robots and soft-rigid hybrids, most works examine robots that have an inherently serial structure. While these systems have been prevalent in the literature, there is an increasing trend toward designing soft-rigid hybrids with intrinsically coupled elasticity between various degrees of freedom. In this work, we seek to address the issues of modeling and controlling such structures, particularly when underactuated. We introduce several simple models for elastic coupling, typical of those seen in these systems. We then propose a controller that compensates for the elasticity, and we prove its stability with Lyapunov methods without relying on the elastic dominance assumption. This controller is applicable to the general class of underactuated soft robots. After evaluating the controller in simulated cases, we then develop a simple hardware platform to evaluate both the models and the controller. Finally, using the hardware, we demonstrate a novel use case for underactuated, elastically coupled systems in ""sensorless"" force control."
A Combination of a Controllable Clutch and an Oscillating Slider Crank Mechanism for Ease of Direct-Teaching with Various Payloads,"Muhammad Arifin, Yuta Kage, Yuchen Yang, Alexander Schmitz, Shigeki Sugano",Waseda University,Learning from Demonstration II,"Direct teaching is a straightforward way of teaching new motion to robots. Active methods with torque sensors, for example, can be used so that the robot can follow the movements of the human, but such methods introduce delays. Alternatively, series clutch actuators are easily backdrivable without delay. However, vertical joints are subject to gravity torques, which need to be compensated when disengaging the clutch. We implemented passive gravity compensation to counteract the robotâ€™s weight, but this mechanism cannot compensate for varying payloads, as adjustable passive gravity compensation is relatively slow and mechanically complex. The varying payload causes an unintended joint movement, i.e. the arm falls down on its own, which is unacceptable during direct teaching. Therefore, this paper demonstrates how the torque output controlled with series clutch actuators can be used to compensate for varying payloads while maintaining high backdrivability. The proposed method is evaluated on a collaborative robot with a clutch in series for each actuator. Real-world experiments with payloads from 0 to 3 kg are conducted. During the experiments, the operator force is measured to evaluate the proposed method."
WayEx: Waypoint Exploration Using a Single Demonstration,"Mara Levy, Nirat Saini, Abhinav Shrivastava","University of Maryland, College Park",Learning from Demonstration II,"We propose WayEx, a new method for learning complex goal-conditioned robotics tasks from a single demonstration. Our approach distinguishes itself from existing imitation learning methods by demanding fewer expert examples and eliminating the need for information about the actions taken during the demonstration. This is accomplished by introducing a new reward function and employing a knowledge expansion technique. We demonstrate the effectiveness of WayEx, our waypoint exploration strategy, across six diverse tasks, showcasing its applicability in various environments. Notably, our method significantly reduces training time by 50% as compared to traditional reinforcement learning methods. WayEx obtains a higher reward than existing imitation learning methods given only a single demonstration. Furthermore, we demonstrate its success in tackling complex environments where standard approaches fall short."
Generating Robotic Elliptical Excisions with Human-Like Tool-Tissue Interactions,"Artūras Straižys, Michael Burke, Subramanian Ramamoorthy","University of Edinburgh,Monash University,The University of Edinburgh",Learning from Demonstration II,"In surgery, the application of appropriate force levels is critical for the success and safety of a given procedure. While many studies are focused on measuring in situ forces, little attention has been devoted to relating these observed forces to surgical techniques. Answering questions like â€œCan certain changes to a surgical technique result in lower forces and increased safety margins?â€ could lead to improved surgical practice, and importantly, patient outcomes. However, such studies would require a large number of trials and professional surgeons, which is generally impractical to arrange. Instead, we show how robots can learn several variations of a surgical technique from a smaller number of surgical demonstrations and interpolate learnt behaviour via a parameterised skill model. This enables a large number of trials to be performed by a robotic system and the analysis of surgical techniques and their downstream effects on tissue. Here, we introduce a parameterised model of the elliptical excision skill and apply a Bayesian optimisation scheme to optimise the excision behaviour with respect to expert ratings, as well as individual characteristics of excision forces. Results show that the proposed framework can successfully align the generated robot behaviour with subjects across varying levels of proficiency in terms of excision forces."
Fitting Parameters of Linear Dynamical Systems to Regularize Forcing Terms in Dynamical Movement Primitives,"Freek Stulp, Adrià Colomé, Carme Torras","DLR - Deutsches Zentrum für Luft- und Raumfahrt e.V.,Institut de Robòtica i Informàtica Industrial (CSIC-UPC), Q,,,,,,CSIC - UPC",Learning from Demonstration II,"Due to their flexibility and ease of use, Dynamical Movement Primitives (DMPs) are widely used in robotics applications and research. DMPs combine linear dynamical systems to achieve robustness to perturbations and adaptation to moving targets with non-linear function approximators to fit a wide range of demonstrated trajectories. We propose a novel DMP formulation with a generalized logistic function as a delayed goal system. This formulation inherently has low initial jerk, and generates the bell-shaped velocity profiles that are typical of human movement. As the novel formulation is more expressive, it is able to fit a wide range of human demonstrations well, also without a non-linear forcing term. We exploit this increased expressiveness by automating the fitting of the dynamical system parameters through optimization. Our experimental evaluation demonstrates that this optimization regularizes the forcing term, and improves the interpolation accuracy of parametric DMPs."
AirExo: Low-Cost Exoskeletons for Learning Whole-Arm Manipulation in the Wild,"Hongjie Fang, Hao-shu Fang, Yiming Wang, Jieji Ren, Jingjing Chen, Ruo Zhang, Weiming Wang, Cewu Lu","Shanghai Jiao Tong University,University College London,ShangHai Jiao Tong University",Learning from Demonstration II,"While humans can use parts of their arms other than the hands for manipulations like gathering and supporting, whether robots can effectively learn and perform the same type of operations remains relatively unexplored. As these manipulations require joint-level control to regulate the complete poses of the robots, we develop AirExo, a low-cost, adaptable, and portable dual-arm exoskeleton, for teleoperation and demonstration collection. As collecting teleoperated data is expensive and time-consuming, we further leverage AirExo to collect cheap in-the-wild demonstrations at scale. Under our in-the-wild learning framework, we show that with only 3 minutes of the teleoperated demonstrations, augmented by diverse and extensive in-the-wild data collected by AirExo, robots can learn a policy that is comparable to or even better than one learned from teleoperated demonstrations lasting over 20 minutes. Experiments demonstrate that our approach enables the model to learn a more general and robust policy across the various stages of the task, enhancing the success rates in task completion even with the presence of disturbances. Project website: https://airexo.github.io/"
Bayesian Constraint Inference from User Demonstrations Based on Margin-Respecting Preference Models,"Dimitris Papadimitriou, Daniel Brown","UC Berkeley,University of Utah",Learning from Demonstration II,"It is crucial for robots to be aware of the presence of constraints in order to acquire safe policies. However, explicitly specifying all constraints in an environment can be a challenging task. State-of-the-art constraint inference algorithms learn constraints from demonstrations, but tend to be computationally expensive and prone to instability issues. In this paper, we propose a novel Bayesian method that infers constraints based on preferences over demonstrations. The main advantages of our proposed approach are that it 1) infers constraints without calculating a new policy at each iteration, 2) uses a simple and more realistic ranking of groups of demonstrations, without requiring pairwise comparisons over all demonstrations, and 3) adapts to cases where there are varying levels of constraint violation. Our empirical results demonstrate that our proposed Bayesian approach infers constraints of varying severity, more accurately than state-of-the-art constraint inference methods."
Instructing Robots by Sketching: Learning from Demonstration Via Probabilistic Diagrammatic Teaching,"Weiming Zhi, Tianyi Zhang, Matthew Johnson-Roberson",Carnegie Mellon University,Learning from Demonstration II,"Learning from Demonstration (LfD) enables robots to acquire new skills by imitating expert demonstrations, allowing users to communicate their instructions in an intuitive manner. Recent progress in LfD often relies on kinesthetic teaching or teleoperation as the medium for users to specify the demonstrations. Kinesthetic teaching requires physical handling of the robot, while teleoperation demands proficiency with additional hardware. This paper introduces an alternative paradigm for LfD called emph{Diagrammatic Teaching}. Diagrammatic Teaching aims to teach robots novel skills by prompting the user to sketch out demonstration trajectories on 2D images of the scene, these are then synthesised as a generative model of motion trajectories in 3D task space. Additionally, we present the Ray-tracing Probabilistic Trajectory Learning (RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying probability densities from the 2D sketches, applies ray-tracing to find corresponding regions in 3D Cartesian space and fits a probabilistic model of motion trajectories to these regions. New motion trajectories, which mimic those sketched by the user, can then be generated from the probabilistic model. We empirically validate our framework both in simulation and on real robots, which include a fixed-base manipulator and a quadruped-mounted manipulator."
Learning Distributional Demonstration Spaces for Task-Specific Cross-Pose Estimation,"Jenny Wang, Octavian Donca, David Held",Carnegie Mellon University,Learning from Demonstration II,"Relative placement tasks are an important category of tasks in which one object needs to be placed in a desired pose relative to another object. Previous work has shown success in learning relative placement tasks from just a small number of demonstrations, when using relational reasoning networks with geometric inductive biases. However, such methods fail to consider that demonstrations for the same task can be fundamentally multimodal, like a mug hanging on any of n racks. We propose a method that retains the provably translation-invariant and relational properties of prior work but incorporates additional properties that enable learning multimodal, distributional examples. We show that our method is able to learn precise relative placement tasks with a small number of multimodal demonstrations with no human annotations across a diverse set of objects within a category. Supplementary information can be found on the website: https://sites.google.com/view/tax-posed/home."
Globally Stable Neural Imitation Policies,"Amin Abyaneh, Mariana Sosa Guzmán, Hsiu-chin Lin","McGill University,Universidad Veracruzana",Learning from Demonstration II,"Imitation learning mitigates the resource-intensive nature of learning policies from scratch by mimicking expert behavior. While existing methods can accurately replicate expert demonstrations, they often exhibit unpredictability in unexplored regions of the state space, thereby raising major safety concerns when facing perturbations. We propose SNDS, an imitation learning approach aimed at efficient training of scalable neural policies while formally ensuring global stability. SNDS leverages a neural architecture that enables the joint training of the policy and its associated Lyapunov candidate to ensure global stability throughout the learning process. We validate our approach through extensive simulations and deploy the trained policies on a real-world manipulator arm. The results confirm SNDS's ability to address instability, accuracy, and computational intensity challenges highlighted in the literature, positioning it as a promising solution for scalable and stable policy learning in complex environments."
Pedestrian Trajectory Prediction Using Dynamics-Based Deep Learning,"Honghui Wang, Weiming Zhi, Gustavo Batista, Rohitash Chandra","University of New South Wales,Carnegie Mellon University,UNSW,UNSW Sydney",Deep Learning II,"Pedestrian trajectory prediction plays an important role in autonomous driving systems and robotics. Recent work utilizing prominent deep learning models for pedestrian motion prediction makes limited a priori assumptions about human movements, resulting in a lack of explainability and explicit constraints enforced on predicted trajectories. We present a dynamics-based deep learning framework with a novel asymptotically stable dynamical system integrated into a Transformer-based model. We use an asymptotically stable dynamical system to model human goal-targeted motion by enforcing the human walking trajectory, which converges to a predicted goal position, and to provide the Transformer model with prior knowledge and explainability. Our framework features the Transformer model that works with a goal estimator and dynamical system to learn features from pedestrian motion history. The results show that our framework outperforms prominent models using five benchmark human motion datasets."
POAQL: A Partially Observable Altruistic Q-Learning Method for Cooperative Multi-Agent Reinforcement Learning,"Lesong Tao, Miao Kang, Jinpeng Dong, Songyi Zhang, Ke Ye, Shitao Chen, Nan-Ning Zheng","Xi'an Jiaotong University,Xi’an Jiaotong University",Deep Learning II,"Multi-Agent Path Finding (MAPF) is an important issue in multi-agent cooperation. Many studies apply Multi-Agent Reinforcement Learning (MARL) to solve MAPF in partially observable settings. The objective of cooperative MARL is to maximize the cumulative team reward. Nevertheless, in partially observable settings, the team reward is misleading due to unpredictable factors from the behavior and state of unobserved agents. To address this issue, we propose a Partially Observable Altruistic Q-learning (POAQL) method. POAQL considers the cumulative reward of the observed subteam instead of the whole team, where Altruistic Q-learning plays an important role in learning the subteam action value. In addition, we design a new conflict resolution without additional guidance to emphasize the cooperative nature of MARL frameworks. Experimental results show that POAQL outperforms existing reinforcement learning methods in terms of efficiency and performance."
Statler: State-Maintaining Language Models for Embodied Reasoning,"Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie Lin, Benjamin Picker, David Yunis, Hongyuan Mei, Matthew Walter","Toyota Technological Institute at Chicago,Fudan University,The University of Chicago,University of Chicago,TTI-Chicago",Deep Learning II,"There has been a significant research interest in employing large language models to empower intelligent robots with complex reasoning. Existing work focuses on harnessing their abilities to reason about the histories of their actions and observations. In this paper, we explore a new dimension in which large language models may benefit robotics planning. In particular, we propose Statler, a framework in which large language models are prompted to maintain an estimate of the world state, which are often unobservable, and track its transition as new actions are taken. Our framework then conditions each action on the estimate of the current world state. Despite being conceptually simple, our Statler framework significantly outperforms strong competing methods (e.g., Code-as-Policies) on several robot planning tasks. Additionally, it has the potential advantage of scaling up to more challenging long-horizon planning tasks."
Multi-Granular Transformer for Motion Prediction with LiDAR,"Yiqian Gan, Hao Xiao, Yizhe Zhao, Ethan Zhang, Zhe Huang, Xin Ye, Lingting Ge","Tusimple, Inc,TuSimple Inc,TuSimple,University of Michigan,Arizona State University",Deep Learning II,"Motion prediction has been an essential component of autonomous driving systems since it handles highly uncertain and complex scenarios involving moving agents of different types. In this paper, we propose a Multi-Granular TRansformer (MGTR) framework, an encoder-decoder network that exploits context features in different granularities for different kinds of traffic agents. To further enhance MGTRâ€™s capabilities, we leverage LiDAR point cloud data by incorporating LiDAR semantic features from an off-the-shelf LiDAR feature extractor. We evaluate MGTR on Waymo Open Dataset motion prediction benchmark and show that the proposed method achieved state-of-the-art performance, ranking 1st on its leaderboard (https://waymo.com/open/challenges/2023/motion-prediction/) ."
What Matters for Active Texture Recognition with Vision-Based Tactile Sensors,"Alina Böhm, Tim Schneider, Boris Belousov, Alap Kshirsagar, Lisa Lin, Katja Doerschner, Knut Drewing, Constantin Rothkopf, Jan Peters","TU Darmstadt,Technical University Darmstadt,German Research Center for Artificial Intelligence - DFKI,Technische Universität Darmstadt,Justus-Liebig-Universität Gießen,Justus Liebig University Giessen,Giessen University,Frankfurt Institute for Advanced Studies",Deep Learning II,"This paper explores active sensing strategies that employ vision-based tactile sensors for robotic perception and classification of fabric textures. We formalize the active sampling problem in the context of tactile fabric recognition and provide an implementation of information-theoretic exploration strategies based on minimizing predictive entropy and variance of probabilistic models. Through ablation studies and human experiments, we investigate which components are crucial for quick and reliable texture recognition. Along with the active sampling strategies, we evaluate neural network architectures, representations of uncertainty, influence of data augmentation, and dataset variability. By evaluating our method on a previously published Active Clothing Perception Dataset and on the real system, we establish that the choice of the exploration strategy has only a minor influence on the recognition accuracy, whereas data augmentation and dropout rate play a significantly larger role. In a comparison study, while humans achieve 66.9% recognition accuracy, our best approach reaches 90.0% in under 5 touches, highlighting that vision-based tactile sensors are highly effective for fabric texture recognition."
Learning with Chemical versus Electrical Synapses - Does It Make a Difference?,"Monika Farsang, Mathias Lechner, David Lung, Ramin Hasani, Daniela Rus, Radu Grosu","TU Wien,Massachusetts Institute of Technology,Massachusetts Institute of Technology (MIT),MIT",Deep Learning II,"Bio-inspired neural networks have the potential to advance our understanding of neural computation and improve the state-of-the-art of AI systems. Bio-electrical synapses directly transmit neural signals, by enabling fast current flow between neurons. In contrast, bio-chemical synapses transmit neural signals indirectly, through neurotransmitters. Prior work showed that interpretable dynamics for complex robotic control, can be achieved by using chemical synapses, within a sparse, bio-inspired architecture, called Neural Circuit Policies (NCPs). However, a comparison of these two synaptic models, within the same architecture, remains an unexplored area. In this work we aim to determine the impact of using chemical synapses compared to electrical synapses, in both sparse and all-to-all connected networks. We conduct experiments with autonomous lane-keeping through a photorealistic autonomous driving simulator to evaluate their performance under diverse conditions and in the presence of noise. The experiments highlight the substantial influence of the architectural and synaptic-model choices, respectively. Our results show that employing chemical synapses yields noticeable improvements compared to electrical synapses, and that NCPs lead to better results in both synaptic models."
Distill-Then-Prune: An Efficient Compression Framework for Real-Time Stereo Matching Network on Edge Devices,"Baiyu Pan, Jichao Jiao, Jian Xin Pang, Jun Cheng","University of Macau,Beijing University of Posts and Telecommunications,UBtech Robotics Corp,Shenzhen Institutes of Advanced Technology",Deep Learning II,"In recent years, numerous real-time stereo matching methods have been introduced, but they often lack accuracy. These methods attempt to improve accuracy by introducing new modules or integrating traditional methods. However, the improvements are only modest. In this paper, we propose a novel strategy by incorporating knowledge distillation and model pruning to overcome the inherent trade-off between speed and accuracy. As a result, we obtained a model that maintains real-time performance while delivering high accuracy on edge devices. Our proposed method involves three key steps. Firstly, we review state-of-the-art methods and design our lightweight model by removing redundant modules from those efficient models through a comparison of their contributions. Next, we leverage the efficient model as the teacher to distill knowledge into the lightweight model. Finally, we systematically prune the lightweight model to obtain the final model. Through extensive experiments conducted on two widely-used benchmarks, Sceneflow and KITTI, we perform ablation studies to analyze the effectiveness of each module and present our state-of-the-art results."
Incremental 3D Reconstruction through a Hybrid Explicit-And-Implicit Representation,"Feifei Li, Panwen Hu, Qi Song, Rui Huang","The Chinese University of Hong Kong Shenzhen,The Chinese University of Hong Kong, Shenzhen",Deep Learning II,"3D reconstruction is an important task in computer vision and is widely used in robotics and autonomous driving. When building large-scale scenes, limitations in computing resources and the difficulty of accessing the entire dataset in a single task are inevitable. Therefore, an incremental reconstruction approach is desired. On the one hand, traditional explicit 3D reconstruction methods such as SLAM and SFM require global optimization, which means that time and space resources increase dramatically with the growth of training data. On the other hand, implicit methods like Neural Radiation Fields (NeRF) suffer from catastrophic forgetting if trained incrementally. In this paper, we incrementally reconstruct 3D models in a hybrid representation, where the density of the radiation field is formulated by a voxel grid, and the view-dependent color information of the points is inferred by a shallow MLP. The expansion of the voxel grid and the distillation of the shallow MLP are efficient in this case. Experimental results demonstrate that our incremental method achieves a level of accuracy on par with approaches employing global optimization techniques."
Sim2Real Bilevel Adaptation for Object Surface Classification Using Vision-Based Tactile Sensors,"Gabriele Mario Caddeo, Andrea Maracani, Paolo Didier Alfano, Nicola Agostino Piga, Lorenzo Rosasco, Lorenzo Natale","Istituto Italiano di Tecnologia,Istituto Italiano di Tecnologia and University of Genoa,Istituto Italiano di Tecnologia & MassachusettsInstitute ofTechn",Deep Learning II,"In this paper, we address the Sim2Real gap in the field of vision-based tactile sensors for classifying object surfaces. We train a Diffusion Model to bridge this gap using a relatively small dataset of real-world images randomly collected from unlabeled everyday objects via the DIGIT sensor. Subsequently, we employ a simulator to generate images by uniformly sampling the surface of objects from the YCB Model Set. These simulated images are then translated into the real domain using the Diffusion Model and automatically labeled to train a classifier. During this training, we further align features of the two domains using an adversarial procedure. Our evaluation is conducted on a dataset of tactile images obtained from a set of ten 3D printed YCB objects. The results reveal a total accuracy of 81.9%, a significant improvement compared to the 34.7% achieved by the classifier trained solely on simulated images. This demonstrates the effectiveness of our approach. We further validate our approach using the classifier on a 6D object pose estimation task from tactile data."
Towards Human-Robot Collaborative Surgery: Trajectory and Strategy Learning in Bimanual Peg Transfer,"Zhaoyang Jacopo Hu, Ziwei Wang, Yanpei Huang, Aran Sena, Ferdinando Rodriguez Y Baena, Etienne Burdet","Imperial College London,Lancaster University,Foster + Partners,Imperial College, London, UK,imperial college london",Human-Robot Collaboration V,"While the traditional control of surgical robots relies on fully manual teleoperations, human-robot collaborative systems promise to address issues such as workspace constrains and laborious tasks. In particular, shared control between human and robot can reduce the surgeon's workload and improve the overall surgical performance by supporting the surgeon effort during movements while keeping them in charge of complex control phases. In this letter, we propose a task segmentation of the bimanual peg transfer procedure that alternates manual and autonomous control correspondingly. The authority allocation in this shared control framework considers both the limitation of learning-based methods and the higher dexterity of humans during physical interaction. The human motion and strategies are transferred from an expert human to a da Vinci Research Kit (dVRK) using an epsilon-greedy on a maximum entropy inverse reinforcement learning algorithm. The model generated enables to train an intelligent agent that can skillfully collaborate with the human operator during the surgical task. The proposed shared control framework is verified both on a virtual platform and then on a real dVRK to assess its usability and robustness. The results show that, compared to traditional manual teleoperation, our method can achieve faster and more consistent peg transfers. An analysis of the participants' effort also reveals a significantly lower perception of the workload."
SIREN: Underwater Robot-To-Human Communication Using Audio,"Michael Fulton, Junaed Sattar, Rafa Absar","University of Minnesota,Metro State University",Human-Robot Collaboration V,"In this paper we present SIREN: a novel audio-based communication system for underwater human-robot interaction. SIREN utilizes a surface transducer to produce sound by vibrating the frame of an underwater robot, essentially turning the robot's outer surface into the vibrating membrane of a speaker. We employ this hardware in two forms of robot-to-human communication: synthesized text-to-speech (TTS-Sonemes) and synthesized musical indicators (Tone-Sonemes).To profile the system's capabilities with respect to underwater communication, we perform a substantial in-person human study with 12 participants. In this study, participants were trained on the use of one of the previously mentioned audio communication systems. Participants were then asked to identify the communication from their system in a pool at various distances. This study's results demonstrate that sound is a viable method of underwater communication. TTS-Sonemes outperform Tonal-Sonemes at close distances but fail at further distances, while Tonal-Sonemes remain recognizable as the distance to the robot increases."
Shared Autonomy Via Variable Impedance Control and Virtual Potential Fields for Encoding Human Demonstrations,"Shail Jadav, Johannes Heidersberger, Christian Ott, Dongheui Lee","IIT Gandhinagar,Technische Universität Wien,TU Wien,Technische Universität Wien (TU Wien)",Human-Robot Collaboration V,"This article introduces a framework for complex human-robot collaboration tasks, such as the co-manufacturing of furniture. For these tasks, it is essential to encode tasks from human demonstration and reproduce these skills in a compliant and safe manner. Therefore, two key components are addressed in this work: motion generation and shared autonomy. We propose a motion generator based on a time-invariant potential field, capable of encoding wrench profiles, complex and closed-loop trajectories, and additionally incorporates obstacle avoidance. Additionally, the paper addresses shared autonomy (SA) which enables synergetic collaboration between human operators and robots by dynamically allocating authority. Variable impedance control (VIC) and force control are employed, where impedance and wrench are adapted based on the human-robot autonomy factor derived from interaction forces. System passivity is ensured by an energy-tank based task passivation strategy. The framework's efficacy is validated through simulations and an experimental study employing a Franka Emika Research 3 robot."
Robustifying a Policy in Multi-Agent RL with Diverse Cooperative Behaviors and Adversarial Style Sampling for Assistive Tasks,"Takayuki Osa, Tatsuya Harada","University of Tokyo,The University of Tokyo",Human-Robot Collaboration V,"Autonomous assistance of people with motor impairments is one of the most promising applications of autonomous robotic systems. Recent studies have reported encouraging results using deep reinforcement learning (RL) in the healthcare domain. Previous studies showed that assistive tasks can be formulated as multi-agent RL, wherein there are two agents: a caregiver and a care-receiver. However, policies trained in multi-agent RL are often sensitive to the policies of other agents. In such a case, a trained caregiverâ€™s policy may not work for different care-receivers. To alleviate this issue, we propose a framework that learns a robust caregiverâ€™s policy by training it for diverse care-receiver responses. In our framework, diverse care-receiver responses are autonomously learned through trials and errors. In addition, to robustify the care-giverâ€™s policy, we propose a strategy for sampling a care-receiverâ€™s response in an adversarial manner during the training. We evaluated the proposed method using tasks in an Assistive Gym. We demonstrate that policies trained with a popular deep RL method are vulnerable to changes in policies of other agents and that the proposed framework improves the robustness against such changes."
Human Robot Shared Control in Surgery: A Performance Assessment,"Longrui Chen, Zhaoyang Jacopo Hu, Yanpei Huang, Etienne Burdet, Ferdinando Rodriguez Y Baena","Imperial college London,Imperial College London,imperial college london,Imperial College, London, UK",Human-Robot Collaboration V,"While surgical robots, such as the da Vinci Surgical System, have become prevalent in minimally invasive surgeries, they are predominantly used by the human operator to directly teleoperate the tools. This paper aims to analyse the different methods of human robot shared control in the surgical domain. We propose a reinforcement learning algorithm, transverse generative adversarial imitation learning (tGAIL), which is employed to train the robot from the expert's demonstration and show competitive generalization ability compared to inverse reinforcement learning and conventional GAIL. We then propose a priority-changing shared control method to effectively combine the surgeon and robot's strengths by dynamically adjusting control priority based on the deviation distance. We show that using this method in a supervision framework boosts the performance of the human operator when completing the peg transfer task. By learning from the expert and collaborating with the human during the task, the intelligent agent helps to reduce surgery time by 31.7% and the human input by 60.5% compared to direct teleoperation."
Distilling and Retrieving Generalizable Knowledge for Robot Manipulation Via Language Corrections,"Lihan Zha, Yuchen Cui, Li-heng Lin, Minae Kwon, Montse Gonzalez Arenas, Andy Zeng, Fei Xia, Dorsa Sadigh","Stanford University,Google Inc,Google DeepMind",Human-Robot Collaboration V,"Today's robot policies exhibit subpar performance when faced with the challenge of generalizing to novel environments. Therefore, adapting to and learning from online human corrections is essential but a non-trivial endeavor: not only do robots need to remember human feedback over time to retrieve the right information in new settings and reduce the intervention rate, but also they would need to be able to respond to feedback that can take arbitrary corrections about high-level human preferences to low-level adjustments to skill parameters. In this work, we present Distillation and Retrieval of Online Corrections (DROC), an LLM-based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving performance in novel settings. DROC is able to respond to a sequence of online language corrections that address failures in both high-level task plans and low-level skill primitives. We demonstrate DROC effectively distills the relevant information from the sequence of online corrections in a knowledge base and retrieves that knowledge in settings with new task or object instances. DROC outperforms baseline Code as Policies by using only half of the total number of corrections needed in the first round and requires little to no corrections after 2 iterations."
An Intuitive Manual Guidance Scheme to Operate Rotation and Translation Simultaneously,"Fan Shao, Fanny Ficuciello","University of Naples Federico II,Università di Napoli Federico II",Human-Robot Collaboration V,"During certain human-robot collaboration tasks, the operator interacts with the robot by hand guidance to adjust the end-effector pose for spatial operations. The rotational operation is less intuitive to humans than translation. In fact, imagining the path to the target orientation is more challenging. In the literature related to control strategies for robot manual guidance, it is usually proposed to control translation and rotation independently. Our research explored and quantified the factors that influence operational intuition. A Virtual Fixture spatial guidance framework with intuition maintenance is proposed. This novel guidance scheme enables operators to effortlessly and simultaneously control both orientation and position in an intuitive way. High operation precision and efficiency can be achieved without interfering with the main task by exploring the null space with constraint optimization."
An Ergo-Interactive Framework for Human-Robot Collaboration Via Learning from Demonstration,"Zhiwei Liao, Marta Lorenzini, Mattia Leonori, Fei Zhao, Gedong Jiang, Arash Ajoudani","Xi'an Jiaotong University,Istituto Italiano di Tecnologia,State Key Laboratory for Manufacturing Systems Engineering Xi'an",Human-Robot Collaboration V,"This work presents an ergonomic and interactive human-robot collaboration (HRC) framework, through which new collaborative skills are extracted from a one-shot human demonstration and learned through Riemannian dynamic movement primitives (DMP). The proposed framework responds to human-robot interaction forces to adapt to the task requirements, while generating virtual â€œergonomic forcesâ€ that guide the human toward more ergonomic postures, based on online monitoring of a kinematics-based index. The resulting motion is then integrated into the learned task trajectories. The framework is implemented on a mobile manipulator with a weighted whole-body Cartesian velocity controller, which meets the needs of large-scale HRC. To evaluate the proposed framework, a multi-subject experiment involving a human-robot co-carrying task is conducted. The performance of the ergo-interactive control in terms of task performance and ergonomics adaptation is verified under different experimental conditions. This is followed by a comparative statistical analysis. The experimental results show that the learned trajectory can be reproduced and generalized to several targets and adjusted online according to human preferences and ergonomics."
A User-Centered Shared Control Scheme with Learning from Demonstration for Robotic Surgery,"Haoyi Zheng, Zhaoyang Jacopo Hu, Yanpei Huang, Xiaoxiao Cheng, Ziwei Wang, Etienne Burdet","Imperial College London,Imperial College of Science, Technology and Medicine, London UK,Lancaster University,imperial college london",Human-Robot Collaboration V,"The utilization of shared control in the realm of surgical robotics augments precision and safety by amalgamating human expertise with autonomous assistance. This paper proposes a user-centered shared control framework enabling a robot to learn from expert demonstration, predict operators' intent and modulate control authority to provide natural assistance when needed. We employ deep inverse reinforcement learning (IRL) to enable the robot to learn path planning from expert demonstrations with fast convergence, subsequently enhancing the policy with a potential field method. The control authority is allocated seamlessly between the human operator and the autonomous agent based on the prediction of operatorsâ€™movement from an adaptive filter and fuzzy logic inference. The proposed method is executed using the da Vinci Research Kit (dVRK) robot in a simulation environment, and its effectiveness is assessed through user performance evaluation in a trajectory tracking task. Compared to direct control and simple shared control, the proposed shared control scheme exhibits superior tracking accuracy and trajectory smoothness under external disturbances. Subjective responses underscore users' perception of the method's efficacy in enhancing their performance."
Virtual Borders in 3D: Defining a Droneâ€™s Movement Space Using Augmented Reality,"Malte Riechmann, André Kirsch, Matthias König, Jan Rexilius","Bielefeld University of Applied Sciences,Bielefeld University of Applied Sciences and Arts",Human-Aware Motion Planning,"Robots are increasingly finding their way into home environments, where they can assist with household tasks like vacuuming or surveilling. While the robots can navigate on their own, users might not want them to go everywhere or not in a specific way. For example, users might not want a drone to fly over a table where important letters and the newspaper are stored, even though it is the shortest path to the goal. Therefore, an application is required, that is easy to learn and to apply even for inexperienced users. In this paper, we present a framework that uses a tablet as augmented reality (AR) device to modify a robotâ€™s movement space in 3D. A user can define virtual borders in the real world with the tablet and add them to a map, changing the navigational behavior of the robot. The framework is evaluated by a user study with inexperienced participants that verifies our approach. Further analyses show, that even complex scenarios can be covered with our framework."
Trajectory Prediction for Robot Navigation Using Flow-Guided Markov Neural Operator,"Rashmi Bhaskara, Hrishikesh Viswanath, Aniket Bera",Purdue University,Human-Aware Motion Planning,"Predicting pedestrian movements remains a complex and persistent challenge in robot navigation research. We must evaluate several factors to achieve accurate predictions, such as pedestrian interactions, the environment, crowd density, and social and cultural norms. Accurate prediction of pedestrian paths is vital for ensuring safe human-robot interaction, especially in robot navigation. Furthermore, this research has potential applications in autonomous vehicles, pedestrian tracking, and human-robot collaboration. Therefore, in this paper, we introduce FlowMNO, an Optical Flow-Integrated Markov Neural Operator designed to capture pedestrian behavior across diverse scenarios. Our paper models trajectory prediction as a Markovian process, where future pedestrian coordinates depend solely on the current state. This problem formulation eliminates the need to store previous states. We conducted experiments using standard benchmark datasets like ETH, HOTEL, ZARA1, ZARA2, UCY, and RGB-D pedestrian datasets. Our study demonstrates that FlowMNO outperforms some of the state-of-the-art deep learning methods like LSTM, GAN, and CNN-based approaches, by approximately 86.46% when predicting pedestrian trajectories. Thus, we show that FlowMNO can seamlessly integrate into robot navigation systems, enhancing their ability to navigate crowded areas smoothly."
Stranger Danger! Identifying and Avoiding Unpredictable Pedestrians in RL-Based Social Robot Navigation,"Sara Pohland, Alvin Tan, Prabal Dutta, Claire Tomlin","University of California, Berkeley,UC Berkeley",Human-Aware Motion Planning,"Reinforcement learning (RL) methods for social robot navigation show great success navigating robots through large crowds of people, but the performance of these learning-based methods tends to degrade in particularly challenging or unfamiliar situations due to the models' dependency on representative training data. To ensure human safety and comfort, it is critical that these algorithms handle uncommon cases appropriately, but the low frequency and high diversity of such situations present a significant challenge for these data-driven methods. To overcome this issue, we propose modifications to the learning process that encourage these RL policies to maintain additional caution in unfamiliar situations. Specifically, we improve the Socially Attentive Reinforcement Learning (SARL) policy by (1) modifying the training process to systematically introduce deviations into a pedestrian model, (2) updating the value network to estimate and utilize pedestrian-unpredictability features, and (3) implementing a reward function to learn an effective response to pedestrian unpredictability. Compared to the original SARL policy, our modified policy maintains similar navigation times and path lengths, while reducing the number of collisions by 82% and reducing the proportion of time spent in the pedestrians' personal space by up to 19 percentage points for the most difficult cases. We also describe how to apply these modifications to other RL policies and demonstrate that key high-level behaviors of our approach transfer to a physical robot."
"Robot Navigation in Risky, Crowded Environments: Understanding Human Preferences","Aamodh Suresh, Angelique Taylor, Laurel Riek, Sonia Martinez","US Army Research Laboratory,Cornell Tech,University of California San Diego,UC San Diego",Human-Aware Motion Planning,"The effective deployment of robots in risky and crowded environments (RCE) requires the specification of robot plans that are consistent with humans' behaviors. As is well known, humans perceive uncertainty and risk in a biased way, which can lead to a diversity of actions and expectations when interacting with others. To gain a better understanding of these behaviors, this work presents new data that aims to verify how these biases translate into a human navigational setting. More precisely, we conduct a novel study that recreates a COVID-19 pandemic grocery shopping scenario and asks participants to select among various paths with different levels of} time-risk tradeoffs. The data shows that participants exhibit a variety of path preferences: from risky and urgent to safe and relaxed. To model users' decision making, we evaluate three popular risk models and found that CPT captures people's decisions more accurately, corroborating previous theoretical results that CPT is more expressive and inclusive. We also find that people's self assessments of risk and time-urgency do not correlate with their path preferences in RCEs. Finally, we conduct thematic analysis of custom open-ended questions to gauge interest and preferences of navigational Explainable AI (XAI) in robots. A large majority also showed interest in understanding robot's intention (path plans and decisions) through various modalities like speech, touchscreen and gestures. We provide crucial XAI design insights."
Real-Time Human Presence Estimation for Indoor Robots,"Songyang Han, Apaar Sadhwani, Tushar Agarwal, Hamid Badiozamani, Tiago Etiene, Jing Zhu, Aarthi Raveendran, William Smart","Sony AI,Stanford University,Amazon Lab,,,,Amazon,Oregon State University",Human-Aware Motion Planning,"As robots become more versatile, fostering effective interactions with humans within shared spaces becomes paramount. This paper introduces the notion of real-time human presence estimation for indoor robots. Real-Time Presence (RTP) asks the question, ""Where in the home is the human now?"", which is an important primitive for home robots. Its answer is challenging and inherently probabilistic as the robot only observes humans sparsely through the day. The mobility of the robot adds further complexity. Conventional state estimation approaches can be adapted to handle sparsity and even mobility. However, they fail to leverage the diverse contextual cues of a home environment. We present a novel machine learning approach for RTP that is additionally scalable to diverse home signals. Trained using curriculum learning, the model incorporates both time-based and event-based signals. Experimental results demonstrate the model's proficiency in understanding floorplan topology and human behaviors."
MAC-ID: Multi-Agent Reinforcement Learning with Local Coordination for Individual Diversity,"Hojun Chung, Jeongwoo Oh, Jae Seok Heo, Gunmin Lee, Songhwai Oh",Seoul National University,Human-Aware Motion Planning,"With the increase of robots navigating through crowded environments in our daily lives, the demand for designing a socially-aware navigation method considering human-robot interaction has risen. When developing and assessing socially-aware navigation methods, pedestrian motion modeling plays a significant role. However, existing pedestrian models often struggle in complex environments and do not have the capacity to generate diverse pedestrian styles. In this paper, we propose multi-agent reinforcement learning with local coordination for individual diversity (MAC-ID), which can synthesize diverse pedestrian motions via local coordination factor (LCF). Our experiments have demonstrated that the manipulation of the LCF induces interpretable changes in pedestrian behaviors, along with a superior performance compared to existing pedestrian motion models. For evaluating socially-aware navigation methods using MAC-ID, we present a novel benchmark called BSON. It offers realistic and diverse social environments with pedestrians modeled via MAC-ID. We have trained and compared various navigation methods in BSON using a newly proposed metric called socially-aware navigation score. Through BSON, users can evaluate their socially-aware navigation methods and compare them to baselines."
Interactive Joint Planning for Autonomous Vehicles,"Yuxiao Chen, Sushant Veer, Peter Karkus, Marco Pavone","Nvidia research,NVIDIA,Stanford University",Human-Aware Motion Planning,"In highly interactive driving scenarios, the actions of one agent greatly influence those of its neighbors. Planning safe motions for autonomous vehicles in such interactive environments, therefore, requires reasoning about the impact of the egoâ€™s intended motion plan on nearby agentsâ€™ behavior. Deep-learning-based models have recently achieved considerable success in trajectory prediction and many models in the literature allow for ego-conditioned prediction. However, leveraging ego-conditioned prediction remains challenging in downstream planning due to the complex nature of neural networks, limiting the planner structure to simple ones, e.g., sampling-based planners. Despite the ability of gradient-based planning algorithms, such as model predictive control (MPC), to generate fine-grained high-quality motion plans, it is difficult for them to leverage ego-conditioned prediction due to their iterative nature and need for gradients. We present Interactive Joint Planning (IJP) that bridges MPC with learned prediction models in a computationally scalable manner to provide us with the best of both worlds. In particular, IJP jointly optimizes over the behavior of the ego and the surrounding agents and leverages deep-learned prediction models as prediction priors that the join trajectory optimization tries to stay close to. Furthermore, by leveraging free-end homotopy classesâ€”a novel concept we introduce in this paperâ€”IJP efficiently searches over diverse motion plans. Closed-"
Improve Computing Efficiency and Motion Safety by Analyzing Environment with Graphics,"Qianyi Zhang, Shichao Wu, Yuhang Jia, Yuang Xu, Jingtai Liu",Nankai University,Human-Aware Motion Planning,"Exploring topologically distinctive trajectories provides more options for robot motion planning. Since computing time grows greatly with environment complexity, improving exploration efficiency and picking the optimal trajectory in complex environments are critical issues. To this end, this paper proposes a Graphic- and Timed-Elastic-Band-based approach (GraphicTEB) with spatial completeness and high computing efficiency. The environment is analyzed utilizing computer graphics, where obstacles are extracted as nodes and their relationships are built as edges. Three contributions are presented. 1) By assembling directed detours formed by nodes and segmented paths formed by edges, a generalized path consisting of nodes and edges derives various normal paths efficiently. 2) By multiplying two vectors starting from the obstacle point closest to the waypoint and the boundary point farthest from the waypoint, an novel obstacle gradient is introduced to guide safer optimization. 3) By assigning edges with asymmetric Gaussian model, a trajectory evaluation strategy is designed to reflect motion tendency and motion uncertainty of dynamic obstacles. Qualitative and quantitative simulations demonstrate that the proposed GraphicTEB achieves spatial completeness, higher scene pass rate, and fastest computing efficiency. Experiments are implemented in long corridor and broad room scenarios, where the robot goes through gaps safely, finds trajectories quickly, passes pedestrians politely."
Generating Environment-Based Explanations of Motion Planner Failure: Evolutionary and Joint-Optimization Algorithms,"Qishuai Liu, Martim Brandao","University of Nebraska-Lincoln,King's College London",Human-Aware Motion Planning,"Motion planning algorithms are important components of autonomous robots, which are difficult to understand and debug when they fail to find a solution to a problem. In this paper we propose a solution to the failure-explanation problem, which are automatically-generated environment-based explanations. These explanations reveal the objects in the environment that are responsible for the failure, and how their location in the world should change so as to make the planning problem feasible. Concretely, we propose two methods - one based on evolutionary optimization and another on joint trajectory-and-environment continuous-optimization. We show that the evolutionary method is well-suited to explain sampling-based motion planners, or even optimization-based motion planners in situations where computation speed is not a concern (e.g. post-hoc debugging). However, the optimization-based method is 4000 times faster and thus more attractive for interactive applications, even though at the cost of a slightly lower success rate. We demonstrate the capabilities of the methods through concrete examples and quantitative evaluation."
Vision-Based Wearable Steering Assistance for People with Impaired Vision in Jogging,"liu xiaotong, Binglu Wang, Zhijun Li","University of Science and Technology of China,Northwestern Polytechnical University",Wearable Robotics I,"Outdoor sports pose a challenge for people with impaired vision. The demand for higher-speed mobility inspired us to develop a vision-based wearable steering assistance. To ensure broad applicability, we focused on a representative sports environment, the athletics track. Our efforts centered on improving the speed and accuracy of perception, enhancing planning adaptability for the real world, and providing swift and safe assistance for people with impaired vision. In perception, we engineered a lightweight multitask network capable of simultaneously detecting track lines and obstacles. Additionally, due to the limitations of existing datasets for supporting multi-task detection in athletics tracks, we diligently collected and annotated a new dataset (MAT) containing 1000 images. In planning, we integrated the methods of sampling and spline curves, addressing the planning challenges of curves. Meanwhile, we utilized the positions of the track lines and obstacles as constraints to guide people with impaired vision safely along the current track. Our system is deployed on an embedded device, Jetson Orin NX. Through outdoor experiments, it demonstrated adaptability in different sports scenarios, assisting users in achieving free movement of 400 meter at an average speed of 1.34 m/s, meeting the level of normal people in jogging. Our MAT dataset is publicly available from https://github.com/snoopy-l/MAT"
Variable Grounding Flexible Limb Tracking Center of Gravity for Sit-To-Stand Transfer Assistance,"Sojiro Sugiura, Jayant Unde, Yaonan Zhu, Yasuhisa Hasegawa",Nagoya University,Wearable Robotics I,"Wearable robotic limbs support sit-to-stand (STS) transfer with increased stability while maintaining a compact size, as well as providing body weight support. This paper proposes a new robotic limb, Variable Grounding Flexible Limb (VGFL). The VGFL achieved a novel strategy in which its grounding point tracks the forward shift of the wearer's center of gravity (CoG) during STS. Since the strategy keeps the distance between the grounding point and the CoG close, an upward force can efficiently work for the STS assistance. To implement the strategy, this paper utilized the High-Strength & Flexible Mechanism (HSFM). The HSFM can change its grounding point while being capable of lifting body weight with one motor. Owing to these unique characteristics, the VGFL can realize the strategy without multiple actuators and complex controllers. Furthermore, real-world experiments confirmed that the grounding point of the VGFL accurately tracked the forward shift of the CoG during the STS assistance. Moreover, experiments conducted with three healthy subjects showed that the VGFL reduced the surface myoelectricity of the lower limbs during STS transfer. The VGFL could demonstrate high support performance in STS with the CoG tracking strategy."
Towards Enhanced Stability of Human Stance with a Supernumerary Robotic Tail,"Sajeeva Abeywardena, Ildar Farkhatdinov","University of Surrey,Queen Mary University of London",Wearable Robotics I,"Neural control is paramount in maintaining upright stance of a human; however, the associated time delay affects stability. In the design and control of wearable robots to augment human stance, the neural delay dynamics are often overly simplified or ignored leading to over specified systems. In this letter, the neural delay dynamics of human stance are modelled and embedded in the control of a supernumerary robotic tail to augment human balance. The actuation, geometric and inertial parameters of the tail are examined. Through simulations it was shown that by incorporating the delay dynamics, the requirements of the tail can be greatly reduced. Further, it is shown that robustness of stance is significantly enhanced with a supernumerary tail and that there is positive impact on muscle fatigue."
"Active, Quasi-Passive, Pneumatic, and Portable Knee Exoskeleton with Bidirectional Energy Flow for Efficient Air Recovery in Sit-Stand Tasks","Luka Miskovic, Tilen Brecelj, Miha Dezman, Tadej Petric","JoÅ¾ef Stefan Institute,Jozef Stefan Institute,Karlsruhe Institute of Technology",Wearable Robotics I,"While existing literature encompasses exoskeleton-assisted sit-stand tasks, the integration of energy recovery mechanisms remains unexplored. To push the boundaries further, this study introduces a portable pneumatic knee exoskeleton that operates in both quasi-passive and active modes, where active mode is utilized for aiding in standing up (power generation), thus the energy flows from the exoskeleton to the user, and quasi-passive mode for aiding in sitting down (power absorption), where the device absorbs and can store energy in the form of compressed air, leading to energy savings in active mode. The absorbed energy can be stored and later reused without compromising exoskeleton transparency in the meantime. In active mode, an air pump inflates the pneumatic artificial muscle (PAM), which stores the compressed air, that can then be released into a pneumatic cylinder to generate torque. All electronic and pneumatic components are integrated into the system, and the exoskeleton weighs 3.9 kg with a maximum torque of 20 Nm at the knee joint. The paper describes the mechatronic design, mathematical model and includes a pilot study with an able-bodied subject performing sit-to-stand tasks. The results show that the exoskeleton can recover energy while assisting the subject and reducing mean muscle activity by âˆ¼31%. Further results highlight air regeneration's potential for energy saving in portable pneumatic exoskeletons, showing that the proposed device extends exoskeleton operation by âˆ¼27%."
Task-Based Human-Robot Collaboration Control of Supernumerary Robotic Limbs for Overhead Tasks,"Zhixin Tu, Yijun Fang, Yuquan Leng, Chenglong Fu","Southern University of Science and Technology,Southern University of Science and Technology (SUSTech)",Wearable Robotics I,
Safe and Individualized Motion Planning for Upper-Limb Exoskeleton Robots Using Human Demonstration and Interactive Learning,"Yu Chen, Gong Chen, Jing Ye, Xiangjun Qiu, Xiang Li","Tsinghua University,Shenzhen MileBot Robotics,Shenzhen MileBot Robotics Co. Ltd.",Wearable Robotics I,"A typical application of upper-limb exoskeleton robots is deployment in rehabilitation training, helping patients to regain manipulative abilities. However, as the patient is not always capable of following the robot, safety issues may arise during the training. Due to the bias in different patients, an individualized scheme is also important to ensure that the robot suits the specific conditions (e.g., movement habits) of a patient, hence guaranteeing effectiveness. To fulfill this requirement, this paper proposes a new motion planning scheme for upper-limb exoskeleton robots, which drives the robot to provide customized, safe, and individualized assistance using both human demonstration and interactive learning. Specifically, the robot first learns from a group of healthy subjects to generate a reference motion trajectory via probabilistic movement primitives (ProMP). It then learns from the patient during the training process to further shape the trajectory inside a moving safe region. The interactive data is fed back into the ProMP iteratively to enhance the individualized features for as long as the training process continues. The robot tracks the individualized trajectory under a variable impedance model to realize the assistance. Finally, the experimental results are presented in this paper to validate the proposed control scheme."
Pneumatic Back Exoskeleton for Lifting Posture Detection and Correction,"Yu Chen, Minda Wang, Yifan Wang",Nanyang Technological University,Wearable Robotics I,"Low back pain is a widespread issue that affects people worldwide and can lead to serious conditions such as herniated discs, spinal stenosis, or lumbar radiculopathy. Improper posture while lifting heavy weights is a common cause of back pain, especially among laborers. However, current back exoskeletons are often bulky and require electric motors, making them challenging to use and consuming significant power. Some passive exoskeletons donâ€™t require power, but their fixed stiffness constrains normal motion. This paper presents a novel solution: a pneumatic back exoskeleton made of structured fabrics that can adjust stiffness under various air pressures. Additionally, it includes IMU sensors to detect lifting posture and correct it in real time. The exoskeletonâ€™s effectiveness was tested through lifting experiments, demonstrating that it significantly corrects lifting posture, reduces stress on the lumbar spine, and mitigates back muscle stress. This pneumatic back exoskeleton offers a promising solution to prevent low back pain during weight-lifting tasks and provides guidance for future back exoskeleton designs."
Reactive Landing Controller for Quadruped Robots,"Francesco Roscia, Michele Focchi, Andrea Del Prete, Darwin G. Caldwell, Claudio Semini","Istituto Italiano di Tecnologia,Università di Trento,University of Trento",Whole-Body Motion Planning and Control,"Quadruped robots are machines intended for challenging and harsh environments. Despite the progress in locomotion strategy, safely recovering from unexpected falls or planned drops is still an open problem. It is further made more difficult when high horizontal velocities are involved. In this work, we propose an optimization-based reactive Landing Controller that uses only proprioceptive measures for torque-controlled quadruped robots that free-fall on a flat horizontal ground, knowing neither the distance to the landing surface nor the flight time. Based on an estimate of the Center of Mass horizontal velocity, the method uses the Variable Height Springy Inverted Pendulum model for continuously recomputing the feet position while the robot is falling. In this way, the quadruped is ready to attain a successful landing in all directions, even in the presence of significant horizontal velocities. The method is demonstrated to dramatically enlarge the region of horizontal velocities that can be dealt with by a naive approach that keeps the feet still during the airborne stage. To the best of our knowledge, this is the first time that a quadruped robot can successfully recover from falls with horizontal velocities up to 3 m/s in simulation. Experiments prove that the used platform, Go1, can successfully attain a stable standing configuration from falls with various horizontal velocities and different angular perturbations."
Hierarchical Optimization-Based Control for Whole-Body Loco-Manipulation of Heavy Objects,"Alberto Rigo, Muqun Hu, Satyandra K. Gupta, Quan Nguyen",University of Southern California,Whole-Body Motion Planning and Control,"In recent years, the field of legged robotics has seen growing interest in enhancing the capabilities of these robots through the integration of articulated robotic arms. However, achieving successful loco-manipulation, especially involving interaction with heavy objects, is far from straightforward, as object manipulation can introduce substantial disturbances that impact the robot's locomotion. This paper presents a novel framework for legged loco-manipulation that considers whole-body coordination through a hierarchical optimization-based control framework. First, an online manipulation planner computes the manipulation forces and manipulated object task-based reference trajectory. Then, pose optimization aligns the robot's trajectory with kinematic constraints. The resultant robot reference trajectory is executed via a linear MPC controller incorporating the desired manipulation forces into its prediction model. Our approach has been validated in simulation and hardware experiments, highlighting the necessity of whole-body optimization compared to the baseline locomotion MPC when interacting with heavy objects. Experimental results with Unitree Aliengo, equipped with a custom-made robotic arm, showcase its ability to successfully lift and carry an 8kg payload and manipulate doors."
Toward Self-Righting and Recovery in the Wild: Challenges and Benchmarks,"Rosario Scalise, Ege Ã‡Aäÿlar, Byron Boots, Chad C. Kessens","University of Washington,University of Washington - Seattle,United States Army Research Laboratory",Whole-Body Motion Planning and Control,"Self-recovery is a critical capability for robust, agile robots operating in the real world. Given truly challenging terrain, it is nearly inevitable that, at some point, the robot will fail and subsequently need to recover if it is to continue its task. One critical subset of recovery is standing back up after falling down (aka â€œself-rightingâ€), an essential early milestone for babies learning to walk, and an existential capability for animals. While some robots can be designed with multiple orientations for mobility, most seeking to affect the world would significantly benefit from planners/policies that facilitate self-righting whenever possible. In this work, we present a series of challenges that outline why recovery in the wild is difficult. We then present a set of benchmark policies trained in simulation using deep reinforcement learning (RL) and the Student-Teacher approach. Finally, we evaluate the performance of these policies on a set of benchmark contexts in simulation, and provide baseline validation on a physical robot."
Design of Morphable StateNet Based on Pseudo-Generalization of Standing up Motions for Humanoid with Variable Body Structure,"Tasuku Makabe, Kei Okada, Masayuki Inaba",The University of Tokyo,Whole-Body Motion Planning and Control,"In this paper, we explain the Morphable StateNet as the StateNet with pseudo-generalized behaviors for robots with various degree-of-freedom arrangements and link lengths. Pseudo-generalization is performed by analytically calculating joint angles that satisfy the desired support conditions, focusing on link lengths and antigravity joints that contribute to motion, with constraints placed on the contact conditions between the environment and the robot body. We apply Morphable StateNet to the standing-up motion of humanoids with variable body structures and conduct evaluation experiments. We have demonstrated the usefulness of the proposed method in environments with low friction coefficients with the environment by conducting evaluations using both a simulator and an actual humanoid."
Agile and Dynamic Standing-Up Control for Humanoids Using 3D Divergent Component of Motion in Multi-Contact Scenario,"Grazia Zambella, Robert Schuller, George Mesesan, Antonio Bicchi, Christian Ott, Jinoh Lee","TU Wien,German Aerospace Center (DLR),Fondazione Istituto Italiano di Tecnologia",Whole-Body Motion Planning and Control,"Standing-up is a task that humanoids need to be able to perform in order to be employed in real-world scenarios. This paper proposes a new robust strategy for a humanoid to stand up in challenging scenarios where no completely preplanned motion can accomplish the same task. This strategy exploits the concept of three-dimensional divergent component of motion and passivity-based whole-body control. The latter firstly maximizes the push forces applied to the robot's center of mass to make agile whole-body recovery motion. Then, during the rising phase, it reduces these forces to zero and stabilizes the robot in an upward position. Optimization of centroidal angular momentum is fully integrated into the proposed whole-body standing-up control to create the trajectories of the hip and the upper body joints online. The effectiveness of the proposed method is validated in simulations and experiments on the humanoid TORO."
Representing Robot Geometry As Distance Fields: Applications to Whole-Body Manipulation,"Yiming Li, Yan Zhang, Amirreza Razmjoo, Sylvain Calinon","Idiap Research Institute, École Polytechnique Fédérale de Lausan,EPFL,Idiap Research Institute",Whole-Body Motion Planning and Control,"In this work, we propose a novel approach to represent robot geometry as distance fields (RDF) that extends the principle of signed distance fields (SDFs) to articulated kinematic chains. Our method employs a combination of Bernstein polynomials to encode the signed distance for each robot link with high accuracy and efficiency while ensuring the mathematical continuity and differentiability of SDFs. We further leverage the kinematics chain of the robot to produce the SDF representation in joint space, allowing robust distance queries in arbitrary joint configurations. The proposed RDF representation is differentiable and smooth in both task and joint spaces, enabling its direct integration to optimization problems. Additionally, the 0-level set of the robot corresponds to the robot surface, which can be seamlessly integrated into whole-body manipulation tasks. We conduct various experiments in both simulations and with 7-axis Franka Emika robots, comparing against baseline methods, and demonstrating its effectiveness in collision avoidance and whole-body manipulation tasks. Project page: https://sites.google.com/view/lrdf/home"
Singularity-Robust Prioritized Whole-Body Tracking and Interaction Control with Smooth Task Transitions,"Xuwei Wu, Alin Albu-Schäffer, Alexander Dietrich","German Aerospace Center (DLR),DLR - German Aerospace Center",Whole-Body Motion Planning and Control,"In this work, we propose a singularity-robust whole-body control framework that ensures smooth task transitions while maintaining strict priorities. The weighted generalized inverse is adopted to derive a hierarchical control law compatible with singular and redundant tasks. Moreover, a smooth activation matrix is proposed to continuously shape both null-space projectors and task-level control actions. Validation has been conducted in MATLAB/Simulink and MuJoCo simulations with Rollinâ€™ Justin."
Learning Force Control for Legged Manipulation,"Tifanny Portela, Gabriel Margolis, Yandong Ji, Pulkit Agrawal","EPFL,Massachusetts Institute of Technology,UCSD,MIT",Whole-Body Motion Planning and Control,"Controlling the contact force during interactions is an inherent requirement for locomotion and manipulation tasks. Current reinforcement learning approaches to locomotion and manipulation rely implicitly on forceful interaction to accomplish tasks but do not explicitly regulate it. This paper proposes a reinforcement learning task specification that focuses on matching desired contact force levels. Integrating force control with the coordination of a robot's body and arm, we present an end-to-end policy for legged manipulator control. Force control enables us to realize compliant gripper and whole-body pulling movements that have not been previously demonstrated using a learned policy. It also facilitates a characterization of the force-tracking performance of learned policies in simulation and the real world, indicating their performance potential for force-critical tasks."
A Study of Shared-Control with Bilateral Feedback for Obstacle Avoidance in Whole-Body Telelocomotion of a Wheeled Humanoid,"Donghoon Baek, Yu-chen (johnny) Chang, Joao Ramos","University of Illinois Urbana-Champaign,University of Illinois, Urbana-Champaign,University of Illinois at Urbana-Champaign",Whole-Body Motion Planning and Control,"Teleoperation has emerged as an alternative solution to fully-autonomous systems for achieving human-level capabilities on humanoids. Specifically, teleoperation with wholebody control is a promising hands-free strategy to command humanoids but requires more physical and mental demand. To mitigate this limitation, researchers have proposed shared-control methods incorporating robot decision-making to aid humans on low-level tasks, further reducing operation effort. However, shared-control methods for wheeled humanoid telelocomotion on a whole-body level has yet to be explored. In this work, we explore how whole-body bilateral feedback with haptics affects the performance of different shared-control methods for obstacle avoidance in diverse environments. A time-derivative Sigmoid function (TDSF) is implemented to generate more intuitive haptic feedback from obstacles. Comprehensive human experiments were conducted and the results concluded that bilateral feedback enhances the whole-body telelocomotion performance in unfamiliar environments but could reduce performance in familiar environments. Conveying the robotâ€™s intention through haptics showed further improvements since the operator can utilize the feedback for reactive short-distance planning and visual feedback for long-distance planning."
MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Learning Via Interactive Perception,"Gyan Tatiya, Jonathan Francis, Ho-hsiang Wu, Yonatan Bisk, Jivko Sinapov","Tufts University,Bosch Center for Artificial Intelligence,Bosch Research,Carnegie Mellon University",Representation Learning I,"A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multimodal Object property learning with Self-Attention and Interactive Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from multimodal foundation models and aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC."
Neural Rearrangement Planning for Object Retrieval from Confined Spaces Perceivable by Robotâ€™s In-Hand RGB-D Sensor,"Hanwen Ren, Ahmed H. Qureshi",Purdue University,Representation Learning I,"Rearrangement planning for object retrieval tasks from confined spaces is a challenging problem, primarily due to the lack of open space for robot motion and limited perception. Several traditional methods exist to solve object retrieval tasks, but they require overhead cameras for perception and a time-consuming exhaustive search to find a solution and often make unrealistic assumptions, such as having identical, simple geometry objects in the environment. This paper presents a neural object retrieval framework that efficiently performs rearrangement planning of unknown, arbitrary objects in confined spaces to retrieve the desired object using a given robot grasp. Our method actively senses the environment with the robot's in-hand camera. It then selects and relocates the non-target objects such that they do not block the robot path homotopy to the target object, thus also aiding an underlying path planner in quickly finding robot motion sequences. Furthermore, we demonstrate our framework in challenging scenarios, including real-world cabinet-like environments with arbitrary household objects. The results show that our framework achieves the best performance among all presented methods and is, on average, two orders of magnitude computationally faster than the best-performing baselines."
MMPI: A Flexible Radiance Field Representation by Multiple Multi-Plane Images Blending,"Yuze He, Peng Wang, Yubin Hu, Wang Zhao, Ran Yi, Yong-Jin Liu, Wenping Wang","Tsinghua University,The University of Hong Kong,Shanghai Jiao Tong University",Representation Learning I,"This paper presents a flexible representation of neural radiance fields based on multi-plane images (MPI), for high-quality view synthesis of complex scenes. MPI with Normalized Device Coordinate (NDC) parameterization is widely used in NeRF learning for its simple definition, easy calculation, and powerful ability to represent unbounded scenes. However, existing NeRF works that adopt MPI representation for novel view synthesis can only handle simple forward-facing unbounded scenes (e.g., the scenes in the LLFF dataset), where the input cameras are all observing in similar directions with small relative translations. Hence, extending these MPI-based methods to more complex scenes like large-range or even 360-degree scenes is very challenging. In this paper, we explore the potential of MPI and show that MPI can synthesize high-quality novel views of complex scenes with diverse camera distributions and view directions, which are not only limited to simple forward-facing scenes. Our key idea is to encode the neural radiance field with multiple MPIs facing different directions and blend them with an adaptive blending operation. For each region of the scene, the blending operation gives larger blending weights to those advantaged MPIs with stronger local representation abilities while giving lower weights to those with weaker representation abilities. Such blending operation automatically modulates the multiple MPIs to appropriately represent the diverse local density and color information. Experiments on the KITTI dataset and ScanNet dataset demonstrate that our proposed MMPI synthesizes high-quality images from diverse camera pose distributions and is fast to train, outperforming the previous fast-training NeRF methods for novel view synthesis. Moreover, we show that MMPI can encode extremely long trajectories and produce novel view renderings, demonstrating its potential in applications like autonomous driving."
Neural Implicit Swept Volume Models for Fast Collision Detection,"Dominik Joho, Jonas Schwinn, Kirill Safronov","KUKA Deutschland GmbH,Kuka Deutschland GmbH",Representation Learning I,"Collision detection is one of the most time-consuming operations during motion planning. Thus, there is an increasing interest in exploring machine learning techniques to speed up collision detection and sampling-based motion planning. A recent line of research focuses on utilizing neural signed distance functions of either the robot geometry or the swept volume of the robot motion. Building on this, we present a novel neural implicit swept volume model to continuously represent arbitrary motions parameterized by their start and goal configurations. This allows to quickly compute signed distances for any point in the task space to the robot motion. Further, we present an algorithm combining the speed of the deep learning-based signed distance computations with the strong accuracy guarantees of geometric collision checkers. We validate our approach in simulated and real-world robotic experiments, and demonstrate that it is able to speed up a commercial bin picking application."
Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields,"Stephen Hausler, David Hall, Sutharsan Mahendren, Peyman Moghadam","CSIRO,Commonwealth Scientific and Industrial Research Organisation,Queensland University of Technology",Representation Learning I,"Neural fields, coordinate-based neural networks, have recently gained popularity for implicitly representing a scene. In contrast to classical methods that are based on explicit representations such as point clouds, neural fields provide a continuous scene representation able to represent 3D geometry and appearance in a way which is compact and ideal for robotics applications. However, limited prior methods have investigated registering multiple neural fields by directly utilising these continuous implicit representations. In this paper, we present Reg-NF, a neural fields-based registration that optimises for the relative 6-DoF transformation between two arbitrary neural fields, even if those two fields have different scale factors. Key components of Reg-NF include a bidirectional registration loss, multi-view surface sampling, and utilisation of volumetric signed distance functions (SDFs). We showcase our approach on a new neural field dataset for evaluating registration problems. We provide an exhaustive set of experiments and ablation studies to identify the performance of our approach, while also discussing limitations to provide future direction to the research community on open challenges in utilizing neural fields in unconstrained environments."
3D-OAE: Occlusion Auto-Encoders for Self-Supervised Learning on Point Clouds,"Junsheng Zhou, Xin Wen, Baorui Ma, Yu-shen Liu, Yue Gao, Yi Fang, Zhizhong Han","Tsinghua University,NVIDIA,Beijing Academy of Artificial Intelligence,New York University,Wayne State University",Representation Learning I,"The manual annotation for large-scale point clouds is still tedious and unavailable for many harsh real-world tasks. Self-supervised learning, which is used on raw and unlabeled data to pre-train deep neural networks, is a promising approach to address this issue. Existing works usually take the common aid from auto-encoders to establish the self-supervision by the self-reconstruction schema. However, the previous auto-encoders merely focus on the global shapes and do not distinguish the local and global geometric features apart. To address this problem, we present a novel and efficient self-supervised point cloud representation learning framework, named 3D Occlusion Auto-Encoder (3D-OAE), to facilitate the detailed supervision inherited in local regions and global shapes. We propose to randomly occlude some local patches of point clouds and establish the supervision via inpainting the occluded patches using the remaining ones. Specifically, we design an asymmetrical encoder-decoder architecture based on standard Transformer, where the encoder operates only on the visible subset of patches to learn local patterns, and a lightweight decoder is designed to leverage these visible patterns to infer the missing geometries via self-attention. We find that occluding a very high proportion of the input point cloud (e.g. 75%) will still yield a nontrivial self-supervisory performance, which enables us to achieve 3-4 times faster during training but also improve accuracy. Experimental results show that our approach outperforms the state-of-the-art on a diverse range of downstream discriminative and generative tasks. Code is available at https://github.com/junshengzhou/3D-OAE."
"Composing Pre-Trained Object-Centric Representations for Robotics from ""What"" and ""Where"" Foundation Models","Junyao Shi, Jianing Qian, Yecheng Jason Ma, Dinesh Jayaraman",University of Pennsylvania,Representation Learning I,"There have recently been large advances both in pre-training visual representations for robotic control and segmenting unknown category objects in general images. To leverage these for improved robot learning, we propose POCR, a new framework for building pre-trained object-centric representations for robotic control. Building on theories of ""what-where"" representations in psychology and computer vision, we use segmentations from a pre-trained model to stably locate across timesteps, various entities in the scene, capturing ""where"" information. To each such segmented entity, we apply other pre-trained models that build vector descriptions suitable for robotic control tasks, thus capturing ""what"" the entity is. Thus, our pre-trained object-centric representations for control are constructed by appropriately combining the outputs of off-the-shelf pre-trained models, with no new training. On various simulated and real robotic tasks, we show that imitation policies for robotic manipulators trained on POCR achieve better performance and systematic generalization than state of the art pre-trained representations for robotics, as well as prior object-centric representations that are typically trained from scratch."
SKT-Hang: Hanging Everyday Objects Via Object-Agnostic Semantic Keypoint Trajectory Generation,"Chia-liang Kuo, Yu-Wei Chao, Yi-ting Chen","National Yang Ming Chiao Tung University,NVIDIA",Representation Learning I,"We study the problem of hanging a wide range of grasped objects on diverse supporting items. Hanging objects is a ubiquitous task that is encountered in numerous aspects of our everyday lives. However, both the objects and supporting items can exhibit substantial variations in their shapes and structures, bringing two challenging issues: (1) determining the task-relevant geometric structures across different objects and supporting items, and (2) identifying a robust action sequence to accommodate the shape variations of supporting items. To this end, we propose Semantic Keypoint Trajectory (SKT), an object-agnostic representation that is highly versatile and applicable to various everyday objects. We also propose Shape-conditioned Trajectory Deformation Network (SCTDN), a model that learns to generate SKT by deforming a template trajectory based on the task-relevant geometric structure features of the supporting items. We conduct extensive experiments and demonstrate substantial improvements in our framework over existing robot hanging methods in the success rate and inference time. Finally, our simulation-trained framework shows promising hanging results in the real world. For videos and supplementary materials, please visit our project webpage: https://hcis-lab.github.io/SKTHang/."
A Novel Robotic Bronchoscope with a Spring-Based Extensible Segment for Improving Steering Ability,"Jie Wang, Chengquan Hu, Jingyi Kang, jiayuan Liu, Longfei Ma, Hongen Liao",Tsinghua University,Surgical Robotics II,"Bronchoscopy, as an essential minimally invasive diagnostic and therapeutic modality, assumes a pivotal role in the early detection of lung cancer. However, the complex anatomy of the airway and the fixed length of the bronchoscopeâ€™s bending segment, along with its external propulsion property, pose challenges, including the risk of bleeding. This paper introduces a 4 mm diameter robot-assisted bronchoscope with a spring-based extensible segment. By manipulating two driven rods, the segment can be lengthened or shortened. The advantages of the extensible segment are discussed in two main aspects through theoretical analysis and experimentation. Firstly, the extensible segment enables the bronchoscope to move in a follow-the-leader motion mode or fixed-angle motion mode, navigating through narrow corners that are inaccessible to fixed-length bronchoscopes. It can also be shortened to increase its stiffness when it reaches the target position, creating a stable surgical platform for procedures like biopsies. In addition, a tailored master device has been developed to control the extensible bronchoscope in an isotropic manner. Phantom experiments confirm the feasibility and effectiveness of the extensible bronchoscope."
Robust Surgical Tool Tracking with Pixel-Based Probabilities for Projected Geometric Primitives,"Christopher D'ambrosia, Florian Richter, Zih-Yun Chiu, Nikhil Shinde, Fei Liu, Henrik Christensen, Michael Yip","University of California, San Diego,University of California San Diego,UCSD",Surgical Robotics II,"Controlling robotic manipulators via visual feedback requires a known coordinate frame transformation between the robot and the camera. Uncertainties in mechanical systems as well as camera calibration create errors in this coordinate frame transformation. These errors result in poor localization of robotic manipulators and create a significant challenge for applications that rely on precise interactions between manipulators and the environment. In this work, we estimate the camera-to-base transform and joint angle measurement errors for surgical robotic tools using an image based insertion-shaft detection algorithm and probabilistic models. We apply our proposed approach in both a structured environment as well as an unstructured environment and measure to demonstrate the efficacy of our methods."
Ada-Tracker: Soft Tissue Tracking Via Inter-Frame and Adaptive-Template Matching,"Jiaxin Guo, Jiangliu Wang, Zhaoshuo Li, Tongyu Jia, Qi Dou, Yunhui Liu","The Chinese University of Hong Kong,Johns Hopkins University,Faculty of Urology, Third Medical Center, Chinese PLA General Ho,Chinese University of Hong Kong",Surgical Robotics II,"Soft tissue tracking is crucial for computer-assisted interventions. Existing approaches mainly rely on extracting discriminative features from the template and videos to recover corresponding matches. However, it is difficult to adopt these techniques in surgical scenes, where tissues are changing in shape and appearance throughout the surgery. To address this problem, we exploit optical flow to naturally capture the pixel-wise tissue deformations and adaptively correct the tracked template. Specifically, we first implement an inter-frame matching mechanism to extract a coarse region of interest based on optical flow from consecutive frames. To accommodate appearance change and alleviate drift, we then propose an adaptive-template matching method, which updates the tracked template based on the reliability of the estimates. Our approach, Ada-Tracker, enjoys both short-term dynamics modeling by capturing local deformations and long-term dynamics modeling by introducing global temporal compensation. We evaluate our approach on the public SurgT benchmark, which is generated from Hamlyn, SCARED, and Kidney boundary datasets. The experimental results show that Ada-Tracker achieves superior accuracy and performs more robustly against prior works. Code is available at https://github.com/wrld/Ada-Tracker."
Real-To-Sim Deformable Object Manipulation: Optimizing Physics Models with Residual Mappings for Robotic Surgery,"Xiao Liang, Fei Liu, Yutong Zhang, Yuelei Li, Shan Lin, Michael Yip","University of California San Diego,UCSD,University of California, San Diego",Surgical Robotics II,"Accurate deformable object manipulation (DOM) is essential for achieving autonomy in robotic surgery, where soft tissues are being displaced, stretched, and dissected. Many DOM methods can be powered by simulation, which ensures realistic deformation by adhering to the governing physical constraints and allowing for model prediction and control. However, real soft objects in robotic surgery, such as membranes and soft tissues, have complex, anisotropic physical parameters that a simulation with simple initialization from cameras may not fully capture. To use the simulation techniques in real surgical tasks, the real-to-sim gap needs to be properly compensated. In this work, we propose an online, adaptive parameter tuning approach for simulation optimization that (1) bridges the real-to-sim gap between a physics simulation and observations obtained 3D perceptions through estimating a residual mapping and (2) optimizes its stiffness parameters online. Our method ensures a small residual gap between the simulation and observation and improves the simulation's predictive capabilities. The effectiveness of the proposed mechanism is evaluated in the manipulation of both a thin-shell and volumetric tissue, representative of most tissue scenarios. This work contributes to the advancement of simulation-based deformable tissue manipulation and holds potential for improving surgical autonomy."
Efficient and Accurate Mapping of Subsurface Anatomy Via Online Trajectory Optimization for Robot Assisted Surgery,"Brian Y Cho, Alan Kuntz",University of Utah,Surgical Robotics II,"Robotic surgical subtask automation has the potential to reduce the per-patient workload of human surgeons. There are a variety of surgical subtasks that require geometric information of subsurface anatomy, such as the location of tumors, which necessitates accurate and efficient surgical sensing. In this work, we propose an automated sensing method that maps 3D subsurface anatomy to provide such geometric knowledge. We model the anatomy via a Bayesian Hilbert map-based probabilistic 3D occupancy map. Using the 3D occupancy map, we plan sensing paths on the surface of the anatomy via a graph search algorithm, A* search, with a cost function that enables the trajectories generated to balance between exploration of unsensed regions and refining the existing probabilistic understanding. We demonstrate the performance of our proposed method by comparing it against 3 different methods in several anatomical environments including a real-life CT scan dataset. The experimental results show that our method efficiently detects relevant subsurface anatomy with shorter trajectories than the comparison methods, and the resulting occupancy map achieves high accuracy."
A Cross-Entropy Motion Planning Framework for Hybrid Continuum Robots,"Jibiao Chen, Junyan Yan, Yufu Qiu, Haiyang Fang, Jianghua Chen, Shing Shin Cheng","The Chinese University of Hong Kong,The Chinese University of HongKong",Surgical Robotics II,
Evaluating the Task Generalization of Temporal Convolutional Networks for Surgical Gesture and Motion Recognition Using Kinematic Data,"Kay Hutchinson, Ian Reyes, Zongyu Li, Homa Alemzadeh","University of Virginia,IBM,The University of Virginia",Surgical Robotics II,"Fine-grained activity recognition enables explainable analysis of procedures for skill assessment, autonomy, and error detection in robot-assisted surgery. However, existing recognition models suffer from the limited availability of annotated datasets with both kinematic and video data and an inability to generalize to unseen subjects and tasks. Kinematic data from the surgical robot is particularly critical for safety monitoring and autonomy, as it is unaffected by common camera issues such as occlusions and lens contamination. We leverage an aggregated dataset of six dry-lab surgical tasks from a total of 28 subjects to train activity recognition models at the gesture and motion primitive (MP) levels and for separate robotic arms using only kinematic data. The models are evaluated using the LOUO (Leave-One-User-Out) and our proposed LOTO (Leave-One-Task-Out) cross validation methods to assess their ability to generalize to unseen users and tasks respectively. Gesture recognition models achieve higher accuracies and edit scores than MP recognition models. But, using MPs enables the training of models that can generalize better to unseen tasks. Also, higher MP recognition accuracy can be achieved by training separate models for the left and right robot arms. For task-generalization, MP recognition models perform best if trained on similar tasks and/or tasks from the same dataset."
Lens Capsule Tearing in Cataract Surgery Using Reinforcement Learning,"Rebekka Charlotte Peter, Steffen Peikert, Ludwig Haide, Doan Xuan Viet Pham, Tahar Chettaoui, Eleonora Tagliabue, Paul Maria Scheikl, Johannes Fauser, Matthias Hillenbrand, Gerhard Neumann, Franziska Mathis-Ullrich","Carl Zeiss AG,Friedrich-Alexander-University (FAU) Erlangen-Nuremberg,Carl Zeiss AG, Karlsruhe Institute of Technology - KIT (master t,Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU),Carl ZEISS AG,Karlsruhe Institute of Technology,Friedrich-Alexander-University Erlangen-Nurnberg (FAU)",Surgical Robotics II,"Cataract is the leading cause of blindness worldwide with an increasing number of patients due to changing demographics, making automation an important part in future surgical treatment. In this work, we focus on a substep of cataract surgery, the Continuous Curvilinear Capsulorhexis (CCC). With a high complexity, this task is an ideal candidate for Reinforcement Learning (RL) in simulation. First, we present an interactive and physically realistic simulation based on the Finite Element Method (FEM) that mimics the tearing behavior of soft tissue during CCC. Then, we train and evaluate RL models in simulation, demonstrating that the trained policies can complete the CCC in 85% of cases. We also show that applying domain randomization techniques make the policy more robust against changes in geometrical and biomechanical boundary conditions."
ORBIT-Surgical: An Open-Simulation Framework for Learning Surgical Augmented Dexterity,"Qinxi Yu, Masoud Moghani, Karthik Dharmarajan, Vincent Schorp, William Panitch, Jingzhou Liu, Kush Hari, Huang Huang, Mayank Mittal, Ken Goldberg, Animesh Garg","University of Toronto,UC Berkeley,UC Berkeley, AUTOLab,University of California, Berkeley,University of Toronto, NVIDIA,University of California at Berkeley,ETH Zurich,Georgia Institute of Technology",Surgical Robotics II,"Physics-based simulations have accelerated progress in robot learning for driving, manipulation, and locomotion. Yet, a fast, accurate, and robust surgical simulation environment remains a challenge. In this paper, we present ORBIT-Surgical, a physics-based surgical robot simulation framework with photorealistic rendering in NVIDIA Omniverse. We provide 14 benchmark surgical tasks for the da Vinci Research Kit (dVRK) and Smart Tissue Autonomous Robot (STAR) which represent common subtasks in surgical training. ORBIT-Surgical leverages GPU parallelization to train reinforcement learning and imitation learning algorithms to facilitate study of robot learning to augment human surgical skills. ORBIT-Surgical also facilitates realistic synthetic data generation for active perception tasks. We demonstrate ORBIT-Surgical sim-to-real transfer of learned policies onto a physical dVRK robot. Project website: orbit-surgical.github.io"
Relaxed Hover Solution Based Control for a Bi-Copter with Rotor and Servo Stuck Failure,"Haixin Zhao, Ruifeng Li, Quan Quan",Beihang University,Failure Detection and Recovery,"As the usage of bi-copters increases in military and civilian fields, the demand for reliable bi-copters is on the rise. This study focuses on controlling a bi-copter under rotor or servo stuck failure. A relaxed hover solution is derived for the bi-copter, by solving an optimization problem subject to rotor and servo stuck failures. The solution is used for designing a reduced attitude controller based on linear quadratic regulator (LQR). To ensure hover capability, we introduce a position controller based on a cascaded-PID. The numerical simulations are conducted to demonstrate that position control is possible, even with complete rotor or servo stuck failure, by driving the bi-copter into relaxed hover state through the abandonment of the yaw channel. Meanwhile, the FTC scheme is examined under constant wind disturbances and uncertainties in the rotational damping parameters."
Aim-Aware Collision Monitoring: Discriminating between Expected and Unexpected Post-Impact Behaviors,"Benn Proper, Alexander Andreas Kurdas, Saeed Abdolshah, Sami Haddadin, Alessandro Saccon","Eindhoven University of Technology,Technical University of Munich,KUKA Deutschland GmbH",Failure Detection and Recovery,"To speed up and reduce power consumption per cycle in robotic manipulation, one option is to exploit intentional collisions with the surrounding environment and objects, an approach referred to as impact-aware manipulation. Within this context, this paper focuses on developing an online collision monitoring framework for distinguishing between expected and unexpected post-impact behaviors. The classification is based on a desired post-impact motion created via an idealized rigid robot-object-environment model. To generate a classification error bound, it employs a causal envelop filter that is needed due to the unavoidable joint and environment flexibility. In this way, it becomes possible to compare a desired idealized rigid response, which is straightforward to obtain with existing tools, with a measured impact response, which is affected by difficult-to-model post-impact oscillations. The classifier can be used for single-contact as well as multi-contact impact scenarios, such as those occurring in surface-to-surface impacts, and allows for tuning of the sensitivity between expected and unexpected post-impact behaviors. The monitoring framework fuses a (bandpass) momentum observer with impact-aware control to extend the classical collision event pipeline. As a proof of concept, we show the effectiveness of the approach through numerical simulations as well as with preliminary experimental results."
The Voraus-AD Dataset for Anomaly Detection in Robot Applications,"Jan Thieß Brockmann, Marco Rudolph, Bodo Rosenhahn, Bastian Wandt","voraus robotik GmbH,Leibniz University Hannover,Institute of Information Processing, Leibniz Universität Hannove,Linköping University",Failure Detection and Recovery,"During the operation of industrial robots, unusual events may endanger the safety of humans and the quality of production. When collecting data to detect such cases, it is not ensured that data from all potentially occurring errors is included as unforeseeable events may happen over time. Therefore, anomaly detection (AD) delivers a practical solution, using only normal data to learn to detect unusual events. We introduce a dataset that allows training and benchmarking of anomaly detection methods for robotic applications based on machine data which will be made publicly available to the research community. As a typical robot task the dataset includes a pick-and-place application which involves movement, actions of the end effector and interactions with the objects of the environment. Since several of the contained anomalies are not task-specific but general, evaluations on our dataset are transferable to other robotics applications as well. Additionally, we present MVT-Flow as a new baseline method for anomaly detection: It relies on deep-learning-based density estimation with normalizing flows, tailored to the data domain by taking its structure into account."
Multimodal Detection and Classification of Robot Manipulation Failures,"Arda Inceoglu, Eren Erdal Aksoy, Sanem Sariel","istanbul technical university,Halmstad University,Istanbul Technical University",Failure Detection and Recovery,"An autonomous service robot should be able to interact with its environment safely and robustly without requiring human assistance. Unstructured environments are challenging for robots since the exact prediction of outcomes is not always possible. Even when the robot behaviors are well-designed, the unpredictable nature of physical robot-object interaction may prevent success in object manipulation. Therefore, execution of a manipulation action may result in an undesirable outcome involving accidents or damages to the objects or environment. Situation awareness becomes important in such cases to enable the robot to (i) maintain the integrity of both itself and the environment, (ii) recover from failed tasks in the short term, and (iii) learn to avoid failures in the long term. For this purpose, robot executions should be continuously monitored, and failures should be detected and classified appropriately. In this work, we focus on detecting and classifying both manipulation and post-manipulation phase failures using the same exteroception setup. We cover a diverse set of failure types for primary tabletop manipulation actions. In order to detect these failures, we propose FINO-Net [1], a deep multimodal sensor fusion based classifier network. Proposed network accurately detects and classifies failures from raw sensory data without any prior knowledge. In this work, we use our extended FAILURE dataset [1] with 99 new multimodal manipulation recordings and annotate them with"
FT-Net: Learning Failure Recovery and Fault-Tolerant Locomotion for Quadruped Robots,"Zeren Luo, Erdong Xiao, Peng Lu",The University of Hong Kong,Failure Detection and Recovery,"Quadruped robots, in recent years, have been increasingly used in extremely harsh and dangerous conditions. Consequently, diverse severe hardware failures may occur at any time during the working cycle of the robots. In this work, we propose a fault-tolerant (FT) control pipeline based on model-free reinforcement learning -- FT-Net, which is guided by an inverted pendulum model and support polygon. This pipeline allows the robot to dynamically and autonomously adapt to both partial and complete motor failures. Unlike conventional FT control methods that need to pinpoint the failed location, our controller identifies the fault implicitly with a neural network-based adaptor. Furthermore, we achieve a unified policy that is capable of switching from four-legged to three-legged walking mode when the complete motor failure occurs. It is shown by both extensive simulation and hardware experiments that FT-Net learns to effectively perform recovery behaviors. The fault-tolerant locomotion can even be executed in various dynamic tasks and terrains."
Utilizing a Malfunctioning 3D Printer by Modeling Its Dynamics with Machine Learning,"Renzo Caballero, Piotr PiÄ™kos, Eric Feron, Jürgen Schmidhuber","King Abdullah University of Science and Technology,Technische Universität München",Failure Detection and Recovery,"To create a self-repairing 3D printer, it must continue operating even after experiencing corruption. This work focuses on developing a method to effectively utilize a malfunctioning printer for reliable printing. This method can be applied by the printer itself for self-repair and enhance the reliability of commercial 3D printers. We achieve this by modeling the dynamics of the corrupted printer using a machine learning model that by observing one trajectory infers the corrupted printer dynamics to improve its accuracy. Our method is evaluated on a digital twin of the 3D printer, demonstrating its capability to enable the printer to operate reliably, even when encountering new corruptions not encountered during training."
A Novel Metric for Detecting Quadrotor Loss-Of-Control,"Jasper Van Beers, Prashant Solanki, Coen De Visser","Delft University of Technology,TU Delft",Failure Detection and Recovery,"Unmanned aerial vehicles (UAVs) are becoming an integral part of both industry and society. In particular, the quadrotor is now invaluable across a plethora of fields and recent developments, such as the inclusion of aerial manipulators, only extends their versatility. As UAVs become more widespread, preventing loss-of-control (LOC) is an ever growing concern. Unfortunately, LOC is not clearly defined for quadrotors, or indeed, many other autonomous systems. Moreover, any existing definitions are often incomplete and restrictive. A novel metric, based on actuator capabilities, is introduced to detect LOC in quadrotors. The potential of this metric for LOC detection is demonstrated through both simulated and real quadrotor flight data. It is able to detect LOC induced by actuator faults without explicit knowledge of the occurrence and nature of the failure. The proposed metric is also sensitive enough to detect LOC in more nuanced cases, where the quadrotor remains undamaged but nevertheless losses control through an aggressive yawing manoeuvre. As the metric depends only on system and actuator models, it is sufficiently general to be applied to other systems."
Specifying and Monitoring Safe Driving Properties with Scene Graphs,"Felipe Toledo, Trey Woodlief, Sebastian Elbaum, Matthew Dwyer",University of Virginia,Failure Detection and Recovery,"With the proliferation of autonomous vehicles (AVs) comes the need to ensure they abide by safe driving properties. Specifying and monitoring such properties, however, is challenging because of the mismatch between the semantic space over which typical driving properties are asserted (e.g., vehicles, pedestrians, intersections) and the sensed inputs of AVs. Existing efforts either assume for such semantic data to be available or develop bespoke methods for capturing it. Instead, this work introduces a framework that can extract scene graphs (SGs) from sensor inputs to capture the entities related to the AV, and a domain-specific language that enables building propositions over those graphs and composing them through temporal logic. We implemented the framework to monitor for specification violations of 3 top AVs from the CARLA Autonomous Driving Leaderboard, and found that the AVs violated 71% of properties during at least one test. Artifact available at https://github.com/less-lab-uva/SGSM"
Using Large Language Models to Generate and Apply Contingency Handling Procedures in Collaborative Assembly Applications,"Jeon Ho Kang, Neel Dhanaraj, Siddhant Ravindra Wadaskar, Satyandra K. Gupta",University of Southern California,Failure Detection and Recovery,"In manufacturing, minimizing operational delays is crucial for efficiency and resilience. Therefore efficiently handling contingencies is an important capability in the context of human-robot teams working on assembly (i.e., collaborative assembly) applications. This paper introduces a novel approach to generating contingency handling procedures by leveraging recent advances in Large Language Models (LMMs). Our approach uses LLMs to update the required tasks in hierarchical task networks (HTNs) to handle contingencies. The results demonstrate that our approach is able to handle a wide variety of contingencies in assembly applications and minimizes impact on the assembly completion time."
A Robotic Surgery Platform for Automated Tissue Micromanipulation in Zebrafish Embryos,"Ece Ozelci, Erfan Etesami, Laurel Rohde, Andrew Oates, Mahmut Selman Sakar",EPFL,Micro/Nano Robots II,"Microsurgical manipulations are key experimental techniques in life science research, particularly in embryology. These techniques are most often performed manually by highly skilled scientists, posing limitations on speed, precision, and re- producibility. Here we introduce a fully automated robotic mi- crosurgery platform that generates explants of specific tail tis- sue from growing zebrafish embryos, a popular model organism for vertebrate development. Our work leverages both classical and deep learning-based image-processing techniques to perform robotic micromanipulation on biological specimens. Using two ex- ample experimental cases as proof of concept, we show that our automated platform is more precise, accurate, and efficient than teleoperated and manual microsurgery conducted by experienced scientists. Moreover, we demonstrate the usefulness of our platform for inexperienced experimentalists, supporting an important role for robotic microsurgery in broadening the use of such techniques in experimental research."
Skill Learning in Robot-Assisted Micro-Manipulation through Human Demonstrations with Attention Guidance,"Yujian An, Jianxin Yang, Jinkai Li, Bingze He, Yao Guo, Guang-Zhong Yang",Shanghai Jiao Tong University,Micro/Nano Robots II,"For the development of robotic systems for micro-manipulation, it is challenging to design appropriate control strategies due to either the lack of sufficient information for feedback or the difficulty in extracting subtle yet critical visual features. With the same system under the teleoperated mode, however, human operators seem to be able to complete the task more successfully with an inherent motion and control strategy. The extraction of implicit human attention during the task and integration of this with robot control could provide crucial guidance in the design of feature extraction and motion control algorithms. In this paper, a micro-assembly task of miniature thin membrane sensors is considered. For human demonstrations, we collected data from repeated tests performed by ten operators following three motion strategies. The human attention during the task is explored according to the coordinates of the eye gaze, and then a neural network with gaze-guided attention is trained to segment the visual Region of Interest (ROI). After quantitative evaluation of operator results in terms of success rate, efficiency, reset time, and the Index of Pupillary Activity (IPA), an optimized motion strategy based on the ``palpation"" framework was derived. Consequently, we apply this strategy to automated tasks and achieve superior results than human operators, showing an average task completion time of 34.8Â±5.9s and a success rate of over 90%."
Automated Surgical Knot Tying on Mini-Incision with Micro-Suture Based on Dual-Arm Nanorobot under Stereo Microscope,"Yujie Jiang, Xiang Fu, Chengxi Zhong, Teng Li, Haojian Lu, Song Liu","ShanghaiTech University,Zhejiang University",Micro/Nano Robots II,"Knot tying is an essential task for robotic surgery, which is routinely realized by dual-arm robotic manipulation. Despite the well-established protocol and progress at macro scale so far, there remain challenges to further advance robotic knot tying technique, particularly in terms of decreasing space consumption with better dexterity, higher precision, and well biomechanical compatibility. In this paper, we propose a novel dual-arm nanorobotic system setup for automated knot tying performed on mini-incision under stereo microscope, featured by an additional rotation degree of freedom mounted on each arm. With this setup, an optimized motion trajectory planning under standard knot-tying protocol is also presented in order to support tying knots with shorter and thinner suture. Leveraging the natural advantage of nanorobotics and microscope, the proposed system is capable of tying consecutive throws with micro-suture on mini-incision, like in vascular anastomosis or microsurgery. We successfully evaluated the knot tying system on 2.0 mm wide bionic blood vessel with 30 mm long #8-0 micro-suture. We finally tested the mechanical strength of the knots for potential medical assessment."
Weakly-Supervised Depth Completion During Robotic Micromanipulation from a Monocular Microscopic Image,"Han Yang, Yufei Jin, Guanqiao Shan, Yibin Wang, Yongbin Zheng, Jiangfan Yu, Yu Sun, Zhuoran Zhang","The Chinese University of Hong Kong, Shenzhen,The Chinese Univiersity of Hong Kong(shenzhen),University of Toronto,The Chinese University of HongKong, Shenzhen,The Chinese University of Hong Kongï¼ŒShenzhen,Chinese University of Hong Kong, Shenzhen",Micro/Nano Robots II,"Obtaining three-dimensional information, especially the z-axis depth information, is crucial for robotic micromanipulation. Due to the unavailability of depth sensors such as lidars in micromanipulation setups, traditional depth acquisition methods such as depth from focus or depth from defocus directly infer depth from microscopic images and suffer from poor resolution. Alternatively, micromanipulation tasks obtain accurate depth information by detecting the contact between an end-effector and an object (e.g., a cell). Despite its high accuracy, only sparse depth data can be obtained due to its low efficiency. This paper aims to address the challenge of acquiring dense depth information during robotic cell micromanipulation. A weakly-supervised depth completion network is proposed to take cell images and sparse depth data obtained by contact detection as input to generate a dense depth map. A two-stage data augmentation method is proposed to augment the sparse depth data, and the depth map is optimized by a network refinement method. The experimental results show that the MAE value of the depth prediction error is less than 0.3 $mu$m, which proves the accuracy and effectiveness of the method. This deep learning network pipeline can be seamlessly integrated with the robotic micromanipulation tasks to provide accurate depth information."
Dynamic Adaptive Imaging System on Optoelectronic Tweezers Platform,"Ao Wang, Chunyuan Gan, Haocheng Han, Hongyi Xiong, Jiawei Zhao, Chutian Wang, Lin Feng","Beihang University,Beihang university,Beihang University,School of Mechanical Engineering and Automati",Micro/Nano Robots II,"Optoelectronic tweezers (OET) has shown great promise in various applications, especially in the precise manipulation of microparticles and microorganisms on a micron and nanometer scale. This technology significantly enhances the efficiency of single-cell sorting and the development of antibody-based drugs. However, conventional OET platforms are limited by issues such as low autofocusing accuracy, restricted imaging field of view, and uneven illumination. To overcome these limitations, we have innovatively developed a dynamic adaptive imaging system. By incorporating peak-finding and in situ Gaussian blur compensation algorithms, we achieved rapid automatic focusing and illumination shadow compensation across an expanded field of view. At the same time, the system can also dynamically adjust compensation parameters under different lighting conditions. Our system has successfully completed comprehensive scanning of the optoelectronic tweezers chip, achieving a 60% reduction in autofocus time and a 15.8% improvement in lighting uniformity. Moreover, this imaging system demonstrates robust versatility and can serve as a reference for other optical systems."
Automated Dissection of Intact Single Cell from Tissue Using Robotic Micromanipulation System,"Youchao Zhang, Xiangyu Guo, Qingyu Wang, Fanghao Wang, Chuanjie Liu, Mingchuan Zhou, Yibin Ying",Zhejiang University,Micro/Nano Robots II,"Obtaining single cell from tissues is important for intersection of information and bioscience research. In this article, a robotic framework based on micromanipulation system has been proposed, which can automatically and intelligently cut down single cells intact from tissue sections. The proposed method consists of several steps. An attention mechanism improved (AMI) tip localization neural network is proposed to detect and track the needle tip of micro-scale displacement end-effector within the limited field of view under microscopy. Then, the transformation matrix between the camera and coordinate system of the robot is calculated. And the cutting trajectory is generated and optimized. Finally, the end-effector is controlled to obtain intact single cells from tissues by model predictive control (MPC). The performance of the framework is verified in paraffin tissue sections dissection experiment which shows proposed framework is robust and precise enough to obtain an intact single cell. The error of autonomous single cell dissection is no more than 0.61 um."
Development of a 3-RRS Micromanipulator Based on Origami-Inspired Spherical Joint,"Haoqi Han, Xiaoming Liu, Yan Chen, Hao Pang, Xiaoqing Tang, Dan Liu, Qiang Huang, Tatsuo Arai","Shanghai Jiao Tong University,Beijing Institute of Technology,University of Electro-Communications",Micro/Nano Robots II,"In recent years, micromanipulation technology has achieved extensive applications in industry and life science. Improving the precision and bandwidth of the micromanipulator and simultaneously reducing size, weight, and cost pose significant challenges to the existing micromanipulator design and fabrication methods. Here, we propose a 3-RRS micromanipulator with an origami-inspired spherical joint based on the PC-MEMS process, aiming for miniaturization and cost-effectiveness. The spherical joint allows rotations of 140Â° around the x-axis approximately, 140Â° around the y-axis approximately, and 20Â° around the z-axis approximately. The micromanipulator has weights of 0.8 g, dimensions of 16 mm Ã— 16 mm Ã— 22 mm, and workspace of 0.7 mm3. The end platform of the micromanipulator can be equipped with various effectors to accomplish different kinds of tasks. Experimental results validated its high precision and bandwidth, exhibiting its potential to perform intricate micromanipulation tasks."
Singularity Analysis and Solutions for the Origami Transmission Mechanism of Fast-Moving Untethered Insect-Scale Robot,"Yide Liu, Bo Feng, Tianlun Cheng, Yanhong Chen, Xiyan Liu, Jiahang Zhang, Shaoxing Qu, Wei Yang","zhejiang university,Zhejiang University",Micro/Nano Robots II,"Designing insect-scale robots with high mobility is becoming an essential challenge in the field of robotics research. Among the methods for fabricating the transmission mechanism of the insect-scale robot, the smart composite microstructure method (SCM) is getting more and more attention. This method can construct compact and functional miniature origami mechanisms through planarized fabrication and folding assembly processes. Our previous work has proposed an untethered robot S2worm equipped with a novel 2-DoF origami transmission mechanism. The S2worm is fabricated through SCM and holds a top speed of 27.4 cm/s. In this work, we propose a novel strategy for designing the insect-scale robot with high mobility, that is, applying Grassmann-Cayley Algebra to avoid the singularity of the transmission mechanism. The experimental results prove that the singularity of the previous work has been solved. The new robot prototype S2worm-G weighs 4.71 g, scales 4.0 cm, achieves a top speed of 75.0 cm/s and a relative speed of 18.8 bodylength/s. To the best of our knowledge, the 2-DoF origami transmission mechanism is the first parallel mechanism designed for the insect-scale robot and the singularity of the mechanism is found and solved here. The experimental results prove that the refined S2worm-G robot is one of the best insect-scale robots for its size, mass, and mobility."
A Theoretical Investigation of the Ability of Magnetic Miniature Robots to Exert Forces and Torques for Biomedical Functionalities,"Yuxuan Xiang, Jiachen Zhang",City University of Hong Kong,Micro/Nano Robots II,"Magnetic miniature robots exert forces and torques onto the environment to conduct minimally invasive diagnostic and therapeutic tasks. The orders of magnitude of forces and torques determine what functionalities these robots can achieve. Although some studies have been dispersedly reported, the forces and torques have yet to be systematically investigated within biomedical context from underlying physical principles, leaving their theoretical limits elusive. This work constructs a theoretical framework from governing equations to calculate the forces and torques exerted by magnetic miniature robots in their respective targeted workspace to achieve functionalities. It reports that the existing miniature robots with a maximum characteristic length of 10^-2 m can exert a force and a torque up to the order of 10^-1 N and 10^-2 Nm, respectively, considering realistic actuation paradigms and constraints. The attainable force and torque magnitudes are on par with the requirements of surgeries at human head (e.g., brain, eyes, and ears surgeries) or within adjacent regions of human skin (e.g., surgeries in the bladder and some blood vessels), as well as the surgeries on small animals. But they are insufficient for operations in deep-buried regions of large animals and human (e.g., implant therapy, biopsy, and tissue removal). Hence, potential strategies to raise the ceiling of the ranges are examined to extend the functionality catalog and expand the operating scope of these robots."
Wearable Haptics for a Marionette-Inspired Teleoperation of Highly Redundant Robotic Systems,"Davide Torielli, Leonardo Franco, Maria Pozzi, Luca Muratore, Monica Malvezzi, Nikos Tsagarakis, Domenico Prattichizzo","Humanoids and Human Centered Mechatronics (HHCM), Istituto Itali,University of Siena,Istituto Italiano di Tecnologia",Telerobotics and Teleoperation I,"The teleoperation of complex, kinematically redundant robots with loco-manipulation capabilities represents a challenge for human operators, who have to learn how to operate the many degrees of freedom of the robot to accomplish a desired task. In this context, developing an easy-to-learn and easy-to-use human-robot interface is paramount. Recent works introduced a novel teleoperation concept, which relies on a virtual physical interaction interface between the human operator and the remote robot equivalent to a ""Marionette"" control, but whose feedback was limited to only visual feedback on the human side. In this paper, we propose extending the ""Marionette"" interface by adding a wearable haptic interface to cope with the limitations given by the previous work. Leveraging the additional haptic feedback modality, the human operator gains full sensorimotor control over the robot, and the awareness about the robot's response and interactions with the environment is greatly improved. We evaluated the proposed interface and the related teleoperation framework with naive users, assessing the teleoperation performance and the user experience with and without haptic feedback. The conducted experiments consisted in a loco-manipulation mission with the CENTAURO robot, a hybrid leg-wheel quadruped with a humanoid dual-arm upper body."
NetLfD: Network-Aware Learning from Demonstration for In-Contact Skills Via Teleoperation,"Basak Gulecyuz, Vincent Von Büren, Xiao Xu, Eckehard Steinbach",Technical University of Munich,Telerobotics and Teleoperation I,"When providing task demonstrations to a remote robot over the network via bilateral teleoperation, communication impairments are unavoidable, hindering the human operator from delivering high-quality demonstrations. Poor-quality demonstrations can negatively impact the robot's ability to learn and generalize. In this work, we propose to enhance learning performance by introducing a network-aware confidence weighting strategy for remote learning from demonstration. Our approach extends the Hidden Semi-Markov Model (HSMM) and its task-parameterized version (TP-HSMM) to their confidence-weighted versions, WHSMM and WTP-HSMM. We evaluated various weight metrics that serve as teleoperation transparency measures and demonstration quality indicators under varying communication delays. We validated the proposed approach in two different in-contact tasks using data collected from 18 participants. The results show that weighting improves task performance in reproduction by up to 42% in the force precision and 63% in the success rate, demonstrating the potential of the proposed approach to enhance the effectiveness of robot learning from remote demonstrations."
Lightweight and Compliant Bilateral Teleoperation System with Anthropomorphic Arms for Aerial and Ground Service Operations,"Alejandro Suarez, Antonio Gonzalez-morgado, Aníbal Ollero","University of Seville,Universidad de Sevilla,AICIA. G,,,,,,,,",Telerobotics and Teleoperation I,"This paper presents a bilateral teleoperation system based on smart servos for the realization of dexterous manipulation tasks with aerial robots or in ground service applications, facilitating the transferability of cognitive capabilities of human workers to robots operating remotely or in high altitude workspaces. The system consists of a pair of lightweight and compliant anthropomorphic dual arm manipulators (LiCAS) in leader-follower configuration. The leader dual arm (LDA) captures the movements of the operator's arms to obtain the desired joint references, sent to the follower dual arm (FDA) to reproduce in a natural and intuitive way the manipulation task. A model of the smart servos is derived, exploiting the feedback from the FDA actuators to provide the kinesthetic feedback to the LDA, using the pulse width modulation signal (PWM) along with the joint speed to estimate the interaction torque. The mechanical joint compliance of the FDA allows the passive accommodation of the arms to the physical interactions with the manipulated objects or the environment, whereas the very low weight of the arms (1.0 kg LDA, 2.5 kg FDA) and the human-size and human-like kinematics facilitate their use in a wide variety of applications. The performance of the system is evaluated using an industrial task board for benchmarking, and in two illustrative bimanual aerial manipulation tasks."
Intelligent Mode-Switching Framework for Teleoperation,"Burak Kizilkaya, Changyang She, Guodong Zhao, Muhammad Ali Imran","University of Glasgow,University of Sydney,University of Glasgow, UK",Telerobotics and Teleoperation I,"Teleoperation can be very difficult due to limited perception, high communication latency, and limited degrees of freedom (DoFs) at the operator side. Autonomous teleoperation is proposed to overcome this difficulty by predicting user intentions and performing some parts of the task autonomously to decrease the demand on the operator and increase the task completion rate. However, decision-making for mode-switching is generally assumed to be done by the operator, which brings an extra DoF to be controlled by the operator and introduces extra mental demand. On the other hand, the communication perspective is not investigated in the current literature, although communication imperfections and resource limitations are the main bottlenecks for teleoperation. In this study, we propose an intelligent mode-switching framework by jointly considering mode-switching and communication systems. User intention recognition is done at the operator side. Based on user intention recognition, a deep reinforcement learning (DRL) agent is trained and deployed at the operator side to seamlessly switch between autonomous and teleoperation modes. A real-world data set is collected from our teleoperation testbed to train both user intention recognition and DRL algorithms. Our results show that the proposed framework can achieve up to 50% communication load reduction with improved task completion probability."
Digital Twin-Driven Mixed Reality Framework for Immersive Teleoperation with Haptic Rendering,"Wen Fan, Xiaoqing Guo, Enyang Feng, Jialin Lin, Yuanyi Wang, Jiaming Liang, Martin Garrad, Jonathan Rossiter, Zhengyou Zhang, Nathan Lepora, Lei Wei, Dandan Zhang","University of Bristol,City University of Hong Kong,Tencent,Deakin University,Imperial College London",Telerobotics and Teleoperation I,"Teleoperation has widely contributed to many applications. Consequently, the design of intuitive and ergonomic control interfaces for teleoperation has become crucial. The rapid advancement of Mixed Reality (MR) has yielded tangible benefits in human-robot interaction. MR provides an immersive environment for interacting with robots, effectively reducing the mental and physical workload of operators during teleoperation. Additionally, the incorporation of haptic rendering, including kinaesthetic and tactile rendering, could further amplify the intuitiveness and efficiency of MR-based immersive teleoperation. In this study, we developed an immersive, bilateral teleoperation system, integrating Digital Twin-driven Mixed Reality (DTMR) manipulation with haptic rendering. This system comprises a commercial remote controller with a kinaesthetic rendering feature and a wearable cost-effective tactile rendering interface, called the Soft Pneumatic Tactile Array (SPTA). We carried out two user studies to assess the system's effectiveness, including a performance evaluation of key components within DTMR and a quantitative assessment of the newly developed SPTA. The results demonstrate an enhancement in both the human-robot interaction experience and teleoperation performance."
Design Octree-Based Method to Improve Model-Mediated Teleoperation in Tactile Internet,"Mads Mørch Antonsen, Francesco Chinello, Qi Zhang",Aarhus University,Telerobotics and Teleoperation I,"In this paper, we propose a model-mediated teleoperation (MMT) system using an octree-based model (OBM) to spatially map the environment impedance for the emerging use cases in Tactile Internet. Different from the existing just-noticeable-difference (JND) based MMT, our method avoids continuous transmission of environment impedance. Moreover, it allows the local model to generate accurate force feedback and reduces the number of model updates. Furthermore, the OBM can be deployed with or without previous knowledge of the environment. An online estimation of the OBM is proposed using a JND and a rate-of-change threshold. An offline estimation method is also proposed when the geometry and impedance parameters of the remote environment are known. In addition, a point cloud-based force rendering algorithm is tailored to use the OBM, thereby allowing the generating of force feedback for complex environments. An experiment without human-in-the-loop was conducted, showing that for an online estimated OBM, the accuracy of the force feedback was improved by up to 44 percent while using less than half the number of model updates when compared to JND-based MMT. Another experiment with a human operator interacting with a virtual environment showed that using an offline estimated OBM improves the accuracy of the force feedback and is reliable against packet loss and short temporal breakdown of the communication link."
Autonomous and Teleoperation Control of a Drawing Robot Avatar,"Lingyun Chen, Abdeldjallil Naceri, Abdalla Swikir, Sandra Hirche, Sami Haddadin","Technical University of Munich,Technische Universität München",Telerobotics and Teleoperation I,"A drawing robot avatar is a robotic system that allows for telepresence-based drawing, enabling users to remotely control a robotic arm and create drawings in real-time from a remote location. The proposed control framework aims to improve bimanual robot telepresence quality by reducing the user workload and required prior knowledge through the automation of secondary or auxiliary tasks. The introduced novel method calculates the near-optimal Cartesian end-effector pose in terms of visual feedback quality for the attached eye-to-hand camera with motion constraints in consideration. The effectiveness is demonstrated by conducting user studies of drawing reference shapes using the implemented robot avatar compared to stationary and teleoperated camera pose conditions. Our results demonstrate that the proposed control framework offers improved visual feedback quality and drawing performance."
Adaptive Haptic Control Interface for Safeguarding Robotic Teleoperation in Hazardous Steelmaking Environments,"Jaehyun Park, Il Seop Choi, Sang-woo Choi, Keehoon Kim","Pohang University of Science and Technology,POSCO,PoscoHoldings,POSTECH, Pohang University of Science and Technology",Telerobotics and Teleoperation I,"Steel mill is one of the most extreme and hazardous working environments due to molten iron erupted from blast furnace. Current manual labor to remove lump iron near the outlet, which is essential to prevent lump iron from scattering or blocking of molten iron, is performed by equipped human workers using a long stick tool. Thus, implementation of robotic teleoperation system is in demand to ensure safety of workers. However, the conventional command interface is not intuitive for tool manipulation (i.e. pivoting, sweeping). Besides, haptic interface, which is used to render interaction results efficiently, still limits performance due to narrow workspace and insufficient kinesthetic feedback output compared to requirements. This paper proposes a novel haptic command interface (POstick) specified to lump iron removal task with two types (KF and VF). Both POsticks have rod-shaped end tip which is identical to actual tool already used to accelerate training. POstick-KF has large workspace and high kinesthetic feedback output satisfying requirements. Further, POstick-VF has strength with unlimited workspace at the expense of the amount of haptic information from simple vibrotactile feedback. User study to compare the performance of POsticks and conventional interface reveals that POstick-KF and VF showed superior interaction and tracking ability, respectively. Moreover, these two properties are in trade-off relationship that cannot be compatible. Finally, we proposed a seamless and automatic conversion mechanism from POstick-VF to KF, and vice versa, to cover up inherent limits of haptic devices."
A Probabilistic Approach for Learning and Adapting Shared Control Skills with the Human in the Loop,"Gabriel Quere, Freek Stulp, David Filliat, João Silvério","DLR,DLR - Deutsches Zentrum für Luft- und Raumfahrt e.V.,ENSTA ParisTech,German Aerospace Center (DLR)",Telerobotics and Teleoperation I,"Assistive robots promise to be of great help to wheelchair users with motor impairments, for example for activities of daily living. Using shared control to provide task-specific assistance -- for instance with the Shared Control Templates (SCT) framework -- facilitates user control, even with low-dimensional input signals. However, designing SCTs is a laborious task requiring robotic expertise. To facilitate their design, we propose a method to learn one of their core components -- active constraints -- from demonstrated end-effector trajectories. We use a probabilistic model, Kernelized Movement Primitives, which additionally allows adaptation from user commands to improve the shared control skills, during both design and execution. We demonstrate that the SCTs so acquired can be successfully used to pick up an object, as well as adjusted for new environmental constraints, with our assistive robot EDAN."
Air Bumper: A Collision Detection and Reaction Framework for Autonomous MAV Navigation,"Ruoyu Wang, Zixuan Guo, Yizhou Chen, Xinyi Wang, Ben M. Chen","The Chinese University of Hong Kong,Chinese University of Hong Kong",Perception and Autonomy II,"Autonomous navigation in unknown environments with obstacles remains challenging for micro aerial vehicles (MAVs) due to their limited onboard computing and sensing resources. Although various collision avoidance methods have been developed, it is still possible for drones to collide with unobserved obstacles due to unpredictable disturbances, sensor limitations, and control uncertainty. Instead of completely avoiding collisions, this article proposes Air Bumper, a collision detection and reaction framework, for fully autonomous flight in 3D environments to improve flight safety. Our framework only utilizes the onboard inertial measurement unit (IMU) to detect and estimate collisions. We further design a collision recovery control for rapid recovery and collision-aware mapping to integrate collision information into general LiDAR-based sensing and planning frameworks. Our simulation and experimental results show that the drone can rapidly detect, estimate, and recover from collisions with obstacles in 3D space and continue the flight smoothly with the help of the collision-aware map. In addition, we will open-source the implementation of Air Bumper on GitHub."
Safety-Aware Perception for Autonomous Collision Avoidance in Dynamic Environments,"Ryan Bena, Chongbo Zhao, Quan Nguyen",University of Southern California,Perception and Autonomy II,"Autonomous collision avoidance requires accurate environmental perception; however, flight systems often possess limited sensing capabilities with field-of-view (FOV) restrictions. To navigate this challenge, we present a safety-aware approach for online determination of the optimal sensor-pointing direction, psi_d, which utilizes control barrier functions (CBFs). First, we generate a spatial density function, Phi, which leverages CBF constraints to map the collision risk of all local coordinates. Then, we convolve Phi with an attitude-dependent sensor FOV quality function to produce the objective function, Gamma, which quantifies the total observed risk for a given pointing direction. Finally, by finding the global optimizer for Gamma, we identify the value of psi_d which maximizes the perception of risk within the FOV. We incorporate psi_d into a safety-critical flight architecture and conduct a numerical analysis using multiple simulated mission profiles. Our algorithm achieves a success rate of 88-96%, constituting a 16-29% improvement compared to the best heuristic methods. We demonstrate the functionality of our approach via a flight demonstration using the Crazyflie 2.1 micro-quadrotor. Without a priori obstacle knowledge, the quadrotor follows a dynamic flight path while simultaneously calculating and tracking psi_d to perceive and avoid two static obstacles with an average computation time of 371 mus."
Incremental Multimodal Surface Mapping Via Self-Organizing Gaussian Mixture Models,"Kshitij Goel, Wennie Tabib",Carnegie Mellon University,Perception and Autonomy II,"This letter describes an incremental multimodal surface mapping methodology, which represents the environment as a continuous probabilistic model. This model enables high-resolution reconstruction while simultaneously compressing spatial and intensity point cloud data. The strategy employed in this work utilizes Gaussian mixture models (GMMs) to represent the environment. While prior GMM-based mapping works have developed methodologies to determine the number of mixture components using information-theoretic techniques, these approaches either operate on individual sensor observations, making them unsuitable for incremental mapping, or are not real-time viable, especially for applications where high-fidelity modeling is required. To bridge this gap, this letter introduces a spatial hash map for rapid GMM submap extraction combined with an approach to determine relevant and redundant data in a point cloud. These contributions increase computational speed by an order of magnitude compared to state-of-the-art incremental GMM-based mapping. In addition, the proposed approach yields a superior tradeoff in map accuracy and size when compared to state-of-the-art mapping methodologies (both GMM- and not GMM-based). Evaluations are conducted using both simulated and real-world data. The software is released open-source to benefit the robotics community."
Learning to Explore Indoor Environments Using Autonomous Micro Aerial Vehicles,"Yuezhan Tao, Eran Iceland, Beiming Li, Elchanan Zwecher, Uri Heinemann, Avraham Cohen, Amir Avni, Oren Gal, Ariel Barel, Vijay Kumar","University of Pennsylvania,Hebrew University Jerusalem Israel,Hebrew university,Hebrew University of Jerusalem,Technion,Technion - Israel Institute of Technology",Perception and Autonomy II,"In this paper, we address the challenge of exploring unknown indoor environments using autonomous aerial robots with Size Weight and Power (SWaP) constraints. The SWaP constraints induce limits on mission time requiring efficiency in exploration. We present a novel exploration framework that uses Deep Learning (DL) to predict the most likely indoor map given the previous observations, and Deep Reinforcement Learning (DRL) for exploration, designed to run on modern SWaP constraints neural processors. The DL-based map predictor provides a prediction of the occupancy of the unseen environment while the DRL-based planner determines the best navigation goals that can be safely reached to provide the most information. The two modules are tightly coupled and run onboard allowing the vehicle to safely map an unknown environment. Extensive experimental and simulation results show that our approach surpasses state-of-the-art methods by 50-60% in efficiency, which we measure by the fraction of the explored space as a function of the trajectory length."
FC-Planner: A Skeleton-Guided Planning Framework for Fast Aerial Coverage of Complex 3D Scenes,"Chen Feng, Haojia Li, Mingjie Zhang, Xinyi Chen, Boyu Zhou, Shaojie Shen","Hong Kong University of Science and Technology,The Hong Kong University of Science and Technology,Northwestern Polytechnical University,Sun Yat-sen University",Perception and Autonomy II,"3D coverage path planning for UAVs is a crucial problem in diverse practical applications. However, existing methods have shown unsatisfactory system simplicity, computation efficiency, and path quality in large and complex scenes. To address these challenges, we propose FC-Planner,a skeleton-guided planning framework that can achieve fastaerial coverage of complex 3D scenes without pre-processing.We decompose the scene into several simple subspaces by askeleton-based space decomposition (SSD). Additionally, theskeleton guides us to effortlessly determine free space. Weutilize the skeleton to efficiently generate a minimal set ofspecialized and informative viewpoints for complete cover age. Based on SSD, a hierarchical planner effectively divides the large planning problem into independent sub-problems, enabling parallel planning for each subspace. The carefully designed global and local planning strategies are then in corporated to guarantee both high quality and efficiency in path generation. We conduct extensive benchmark and real world tests, where FC-Planner computes over 10 times faster compared to state-of-the-art methods with shorter path and more complete coverage. The source code will be made publicly available to benefit the community3. Project page: https: //hkust-aerial-robotics.github.io/FC-Planner."
Multi-Robot Multi-Room Exploration with Geometric Cue Extraction and Circular Decomposition,"Seungchan Kim, Micah Corah, John Keller, Graeme Best, Sebastian Scherer","Carnegie Mellon University,Colorado School of Mines,University of Technology Sydney",Perception and Autonomy II,"This work proposes an autonomous multi-robot exploration pipeline that coordinates the behaviors of robots in an indoor environment composed of multiple rooms. Contrary to simple frontier-based exploration approaches, we aim to enable robots to methodically explore and observe an unknown set of rooms in a structured building, keeping track of which rooms are already explored and sharing this information among robots to coordinate their behaviors in a distributed manner. To this end, we propose (1) a geometric cue extraction method that processes 3D point cloud data and detects the locations of potential cues such as doors and rooms, (2) a circular decomposition for free spaces used for target assignment. Using these two components, our pipeline effectively assigns tasks among robots, and enables a methodical exploration of rooms. We evaluate the performance of our pipeline using a team of up to 3 aerial robots, and show that our method outperforms the baseline by 33.4% in simulation and 26.4% in real-world experiments."
Fast Multi-UAV Decentralized Exploration of Forests,"Luca Bartolomei, Lucas Teixeira, Margarita Chli","ETH Zurich,ETH Zurich & University of Cyprus",Perception and Autonomy II,"Efficient exploration strategies are vital in tasks such as search-and-rescue missions and disaster surveying. Unmanned Aerial Vehicles (UAVs) have become particularly popular in such applications, promising to cover large areas at high speeds. Moreover, with the increasing maturity of onboard UAV perception, research focus has been shifting toward higher-level reasoning for multi-robot missions. However, autonomous navigation and exploration of previously unknown large spaces still constitute an open challenge, especially when the environment is cluttered and exhibits large and frequent occlusions due to high obstacle density, as is the case of forests. Moreover, the problem of long-distance wireless communication in such scenes can become a limiting factor, especially when automating the navigation of a UAV fleet. In this spirit, this work proposes an exploration strategy that enables multiple UAVs to quickly explore complex scenes in a decentralized fashion. By providing the decision-making capabilities to each UAV to switch between different execution modes, the proposed strategy is shown to strike a great balance between cautious exploration of yet completely unknown regions and more aggressive exploration of smaller areas of unknown space. This results in full coverage of forest areas in multi-UAV setups up to 30% faster than the state of the art."
Reinforcement Learning for Collision-Free Flight Exploiting Deep Collision Encoding,"Mihir Kulkarni, Kostas Alexis","NTNU: Norwegian University of Science and Technology,NTNU - Norwegian University of Science and Technology",Perception and Autonomy II,This work contributes a novel deep navigation policy that enables collision-free flight of aerial robots based on a modular approach exploiting deep collision encoding and reinforcement learning. The proposed solution builds upon a deep collision encoder that is trained on both simulated and real depth images using supervised learning such that it compresses the high-dimensional depth data to a low-dimensional latent space encoding collision information while accounting for the robot size. This compressed encoding is combined with an estimate of the robot's odometry and the desired target location to train a deep reinforcement learning navigation policy that offers low-latency computation and robust sim2real performance. A set of simulation and experimental studies in diverse environments are conducted and demonstrate the efficiency of the emerged behavior and its resilience in real-life deployments.
Learning Agile Flights through Narrow Gaps with Varying Angles Using Onboard Sensing,"Yuhan Xie, Minghao Lu, Rui Peng, Peng Lu",The University of Hong Kong,Perception and Autonomy II,"This paper addresses the problem of traversing through unknown, tilted, and narrow gaps for quadrotors using Deep Reinforcement Learning (DRL). Previous learning-based methods relied on accurate knowledge of the environment, including the gap's pose and size. In contrast, we integrate onboard sensing and detect the gap from a single onboard camera. The training problem is challenging for two reasons: a precise and robust whole-body planning and control policy is required for variable-tilted and narrow gaps, and an effective Sim2Real method is needed to successfully conduct real-world experiments. To this end, we propose a learning framework for agile gap traversal flight, which successfully trains the vehicle to traverse through the center of the gap at an approximate attitude to the gap with aggressive tilted angles. The policy trained only in a simulation environment can be transferred into different domains with fine-tuning while maintaining the success rate. Our proposed framework, which integrates onboard sensing and a neural network controller, achieves a success rate of 87.36% in real-world experiments, with gap orientations up to 60deg. To the best of our knowledge, this is the first paper that performs the learning-based variable-tilted narrow gap traversal flight in the real world, without prior knowledge of the environment."
Osiris: Building Hierarchical Representations for Agricultural Environments,"Adam Mukuddem, Paul Amayo",University of Cape Town,Robotics and Automation in Agriculture and Forestry III,"3D scene graphs have recently emerged as a powerful and human-understandable way of representing complex 3D environments. These describes environments through a layered or hierarchical graph where nodes represent different spatial concepts (from low-level geometry to higher-level scene-scale reasoning) and the edges between them represent relationships. While these representations have shown great promise in indoor well-structured environments, their use in outdoor structured environments such as agricultural environment has been under-explored. A key challenge here is that concepts and structure often observed in urban indoor environments cannot be easily transferred to these novel scenes. Motivated by this challenge , this paper presents emph{Osiris} which is a 3D scene graph builder for agricultural environment. We first propose a structure of the hierarchical graph for agricultural environments consisting of rowed crops and through our proposed system emph{Osiris} incrementally construct a 3D scene graph of agricultural environments from data taken onboard a mobile robot. We validate and evaluate the performance of Osiris using real-world data collected at several farms and show that this system is able to accurately get to the underlying structure of these agricultural environments while presenting a metrically accurate and human-understandable representation."
Streamlined Acquisition of Large Sensor Data for Autonomous Mobile Robots to Enable Efficient Creation and Analysis of Datasets,"Mark Niemeyer, Julian Arkenau, Sebastian Pütz, Joachim Hertzberg","DFKI,German Research Center for Artificial Intelligence,University of Osnabrueck",Robotics and Automation in Agriculture and Forestry III,"The increasing usage of modern AI techniques represents a transforming shift in the robotics domain. Training and accessing new models requires substantial amounts of application-specific data, but the limited resources onboard mobile robots (like processing power, network bandwidth, etc.) pose a challenge for the development of efficient data recording and provisioning pipelines. Furthermore, accessing specific information based on a combination of spatial, temporal and semantic information is generally not supported by currently available tools. In this paper, we present a methodology which allows the efficient recording of robotic sensor data streams. We show that our approach reduces the overall time needed until the data can be served via the spatio-temporal-semantic query interface of the semantic environment representation SEEREP. We further present that the maximum sensor data rate which can be stored to disk in real-time is increased for large robotic data types like images and point clouds in comparison to frequently employed solutions within the ROS ecosystem."
Development of an Automatic Sweet Pepper Harvesting Robot and Experimental Evaluation,"Qinghui Pan, Dong Wang, Jie Lian, Yongxiang Dong, Chaochao Qiu",Dalian University of Technology,Robotics and Automation in Agriculture and Forestry III,"The aging population and diminishing working population in agriculture motivate the development of autonomous harvesting robots. Although autonomous harvesting is expanding rapidly, the commercial application of sweet pepper harvesting robots still faces challenges. This paper presents the development of a sweet pepper harvesting robot and reports its experimental verification, which mainly includes end-effector design, visual perception, and grasping pose control. The end-effector adopts electrical control, mainly composed of a servo-electric two-finger parallel clamping module, a swing-cutting module, and a fruit recovery device. Equipped with a tactile sensor array, it can accurately sense the sweet pepper peduncle position and the end-effector state (harvesting failure) to complete the precise cutting. An end-effector grasping pose control algorithm of the manipulator is proposed, which can control the end-effector to grasp along the direction of the fruit peduncle and perpendicular to the tangent direction of the picking point by estimating the pose of the sweet pepper peduncle. Finally, the robot and proposed method were verified in a plant factory. The experimental findings demonstrate that the developed harvesting robot can complete robust detection of fruit peduncles and non-destructive picking of sweet pepper, with an average picking time of about 15 seconds."
LiDAR-Based Robot Transplanter,"Masaki Asano, Takanori Fukao","University of Tokyo, Graduate School of Information Science and ,University of Tokyo",Robotics and Automation in Agriculture and Forestry III,"In Japan, labor shortage of agriculture is becoming increasingly severe due to the lack of farmers and aging. Therefore, the development of automation of vegetable production such as transplanting, harvesting and transporting is required. In this paper, a self-localization method by using LiDAR and a robust control method of a transplanter are proposed for accurate transplanting. In this system, the path of transplanter is generated by using 3D point cloud data, and the transplanting part follows it and plant seedlings of cabbage accurately. Path generation is performed considering vehicle tilt in the roll direction depending on the environment of grooves. An accurate calculation of lateral and angular position of the transplanting part is also proposed. For path following control, sliding-mode control and inverse optimal control are applied to transplanter. The experimental results demonstrated the effectiveness of these proposed methods and problems we have to tackle on. Basically, it was possible to perform automated transplanting accurately, but there was an occasional problem of offset error from 0. It was confirmed that inverse optimal control is superior to sliding-mode control and is more robust to environmental changes."
EdgeSoil 2.0 â€“ Soil Analyzer Using Convolutional Neural Network and Camera Imaging for Agricultural Robotics,"Roni Kasemi, Lara Lammer, Stefan Thalhammer, Markus Vincze","UBT,ACIN TU WIEN,TU Wien,Vienna University of Technology",Robotics and Automation in Agriculture and Forestry III,"Soil is the most important building element of agriculture and its analysis is crucial for healthy plants and a high crop yield. But apart from its importance, soil analysis is a tedious and time-consuming task. This paper presents EdgeSoil 2.0, a non-invasive, accurate, and real-time robotic system for soil pH prediction, a key parameter of soil status for farmers. The EdgeSoil 2.0 predicts the pH value of the soil in real-time, using a live video stream from a webcam with an average of 7 FPS. The method is suitable to be implemented on edge devices necessary for the application: we are using a mobile robot with the NVIDIA Jetson Nano module which is running a pH-estimator trained with a Convolutional Neural Network (CNN) on a novel dataset we built for this purpose. Predictions are performed while the robot is moving over the plowed field before the planting process starts. In order to achieve the best performance, we train the pH-estimator with different input modalities and validate each result using Mean Squared Error (MSE) and Standard Deviation (SD). We are able to achieve accurate results with the MSE value of 0.08, the SD value of 0.15, and with testing results from the field showing up to Â± 0.3 deviation from the GT value during prediction, which is sufficient to comply with agricultural standards."
Semiautonomous Precision Pruning of Upright Fruiting Offshoot Orchard Systems: An Integrated Approach,"Alexander You, Nidhi Parayil, Josyula Gopala Krishna, Uddhav Bhattarai, Ranjan Sapkota, Dawood Ahmed, Matthew Whiting, Manoj Karkee, Cindy Grimm, Joseph Davidson","Oregon State University,Oregon State Univeristy,Washington State University",Robotics and Automation in Agriculture and Forestry III,"Dormant pruning is an important orchard activity for maintaining tree health and producing high-quality fruit. Due to decreasing worker availability, pruning is a prime candidate for robotics. However, pruning also represents a uniquely difficult problem, requiring robust systems for perception, pruning point determination, and manipulation that must operate under variable lighting conditions and in complex, highly unstructured environments. In this article, we introduce a system for pruning modern, planar orchard architectures with simple pruning rules that combines various subsystems from our previous work on perception and manipulation. The integrated system demonstrates the ability to autonomously detect and cut pruning targets with minimal control of the environment, laying the groundwork for a fully autonomous system in the future. We validate the performance of our system through field trials in a sweet cherry orchard, ultimately achieving a cutting success rate of 58% across ten trees. Though not fully robust and requiring improvements in throughput, our system is the first to operate on fruit trees and represents a useful base platform to be improved in the future."
On-The-Go Tree Detection and Geometric Traits Estimation with Ground Mobile Robots in Fruit Tree Groves,"Dimitrios Chatziparaschis, Hanzhe Teng, Yipeng Wang, Pamodya Peiris, Elia Scudiero, Konstantinos Karydis","UC Riverside,University of California, Riverside",Robotics and Automation in Agriculture and Forestry III,"By-tree information gathering is an essential task in precision agriculture achieved by ground mobile sensors, but it can be time- and labor-intensive. In this paper we present an algorithmic framework to perform real-time and on-the-go detection of trees and key geometric characteristics (namely, width and height) with wheeled mobile robots in the field. Our method is based on the fusion of 2D domain-specific data (normalized difference vegetation index [NDVI] acquired via a red-green-near-infrared [RGN] camera) and 3D LiDAR point clouds, via a customized tree landmark association and parameter estimation algorithm. The proposed system features a multi-modal and entropy-based landmark correspondences approach, integrated into an underlying Kalman filter system to recognize the surrounding trees and jointly estimate their spatial and vegetation-based characteristics. Realistic simulated tests are used to evaluate our proposed algorithm's behavior in a variety of settings. Physical experiments in agricultural fields help validate our method's efficacy in acquiring accurate by-tree information on-the-go and in real-time by employing only onboard computational and sensing resources."
Autonomous Apple Fruitlet Sizing with Next Best View Planning,"Harry Freeman, George Kantor",Carnegie Mellon University,Robotics and Automation in Agriculture and Forestry III,"In this paper, we present a next-best-view planning approach to autonomously size apple fruitlets. State-of-the-art viewpoint planners in agriculture are designed to size large and more sparsely populated fruit. They rely on lower resolution maps and sizing methods that do not generalize to smaller fruit sizes. To overcome these limitations, our method combines viewpoint sampling around semantically labeled regions of interest, along with an attention-guided information gain mechanism to more strategically select viewpoints that target the small fruits' volume. Additionally, we integrate a dual-map representation of the environment that is able to both speed up expensive ray casting operations and maintain the high occupancy resolution required to informatively plan around the fruit. When sizing, a robust estimation and graph clustering approach is introduced to associate fruit detections across images. Through simulated experiments, we demonstrate that our viewpoint planner improves sizing accuracy compared to state of the art and ablations. We also provide quantitative results on data collected by a real robotic system in the field."
Gradient-Based Local Next-Best-View Planning for Improved Perception of Targeted Plant Nodes,"Akshay Kumar Burusa, Eldert J. Van Henten, Gert Kootstra","Wageningen University and Research,Wageningen University",Robotics and Automation in Agriculture and Forestry III,"Robots are increasingly used in tomato greenhouses to automate labour-intensive tasks such as selective harvesting and de-leafing. To perform these tasks, robots must be able to accurately and efficiently perceive the plant nodes that need to be cut, despite the high levels of occlusion from other plant parts. We formulate this problem as a local next-best-view (NBV) planning task where the robot has to plan an efficient set of camera viewpoints to overcome occlusion and improve the quality of perception. Our formulation focuses on quickly improving the perception accuracy of a single target node to maximise its chances of being cut. Previous methods of NBV planning mostly focused on global view planning and used random sampling of candidate viewpoints for exploration, which could suffer from high computational costs, ineffective view selection due to poor candidates, or non-smooth trajectories due to inefficient sampling. We propose a gradient-based NBV planner using differential ray sampling, which directly estimates the local gradient direction for viewpoint planning to overcome occlusion and improve perception. Through simulation experiments, we showed that our planner can handle occlusions and improve the 3D reconstruction and position estimation of nodes equally well as a sampling-based NBV planner, while taking ten times less computation and generating 28% more efficient trajectories."
A Vision-Centric Approach for Static Map Element Annotation,"Jiaxin Zhang, Chen Shiyuan, Haoran Yin, Ruohong Mei, Xuan Liu, Cong Yang, Qian Zhang, Wei Sui","Soochow University,Northeast Normal University,Horizon Robotics",Localization and Mapping I,"The recent development of online static map element (a.k.a. HD Map) construction algorithms has raised a vast demand for data with ground truth annotations. However, available public datasets currently cannot provide high-quality training data regarding consistency and accuracy. To this end, we present CAMA: a vision-centric approach for Consistent and Accurate Map Annotation. Without LiDAR inputs, our proposed framework can still generate high-quality 3D annotations of static map elements. Specifically, the annotation can achieve high reprojection accuracy across all surrounding cameras and is spatial-temporal consistent across the whole sequence. We apply our proposed framework to the popular nuScenes dataset to provide efficient and highly accurate annotations. Compared with the original nuScenes static map element, models trained with annotations from CAMA achieve lower reprojection errors (e.g., 4.73 vs. 8.03 pixels)."
VBR: A Vision Benchmark in Rome,"Leonardo Brizi, Emanuele Giacomini, Luca Di Giammarino, Simone Ferrari, Omar Ashraf Ahmed Khairy Salem, Lorenzo De Rebotti, Giorgio Grisetti",Sapienza University of Rome,Localization and Mapping I,"This paper presents a robotics perception research dataset collected in Rome, featuring RGB data, 3D point clouds, IMU, and GPS data. We introduce a new benchmark targeting visual odometry and SLAM, to advance the research in autonomous robotics. This work complements existing datasets by simultaneously addressing several issues, such as environment diversity, motion patterns, and sensor frequency. It uses up-to-date devices and presents effective procedures to accurately calibrate the intrinsic and extrinsic of the sensors while addressing temporal synchronization. During recording, we cover multi-floor buildings, gardens, urban and highway scenarios. Combining handheld and car-based data collections, our setup can simulate any robot (quadrupeds, quadrotors, autonomous vehicles). The dataset includes an accurate 6-dof groundtruth based on a novel methodology that refines the RTK-GPS estimate with LiDAR point clouds through Bundle Adjustment(BA). All sequences divided in training and validation are accessible at www.rvp-group.net/datasets/slam."
Spatial-Aware Dynamic Lightweight Self-Supervised Monocular Depth Estimation,"Linna Song, Dianxi Shi, Jianqiang Xia, Qianying Ouyang, Ziteng Qiao, Songchang Jin, Shaowu Yang","National University of Defense Technology,Defense Innovation Institute,National Innovation Institute of Defense Technology,Intelligent Game and Decision Lab;Tianjin Artificial Intelliegnc",Localization and Mapping I,"Self-supervised monocular depth estimation has attracted extensive attention in recent years. Lightweight depth estimation methods are crucial for resource-constrained edge devices. However, existing lightweight methods often encounter the challenge of limited representation capacity and increased computational resource consumption for image reconstruction. To alleviate these issues, we propose a novel spatial-aware dynamic lightweight monocular depth estimation method (SAD-Depth). Specifically, we propose a spatial-aware dynamic encoder, which can capture spatial information of the input and generate input-adaptive dynamic convolutions, thereby significantly enhancing the model's adaptability to complex scenes. Meanwhile, we propose a multi-scale sub-pixel lightweight decoder that generates high-quality depth maps while maintaining a lightweight design. Experimental results demonstrate that our proposed SAD-Depth exhibits superiority in both model size and inference speed, achieving state-of-the-art performance on the KITTI benchmark."
VDNA-PR: Using General Dataset Representations for Robust Sequential Visual Place Recognition,"Benjamin Ramtoula, Daniele De Martini, Matthew Gadd, Paul Newman","University of Oxford,Oxford University",Localization and Mapping I,"This paper adapts a general dataset representation technique to produce robust Visual Place Recognition (VPR) descriptors, crucial to enable real-world mobile robot localisation. Two parallel lines of work on VPR have shown, on one side, that general-purpose off-the-shelf feature representations can provide robustness to domain shifts, and, on the other, that fused information from sequences of images improves performance. In our recent work on measuring domain gaps between image datasets, we proposed a Visual Distribution of Neuron Activations (VDNA) representation to represent datasets of images. This representation can naturally handle image sequences and provides a general and granular feature representation derived from a general-purpose model. Moreover, our representation is based on tracking neuron activation values over the list of images to represent and is not limited to a particular neural network layer, therefore having access to high- and low-level concepts. This work shows how VDNAs can be used for VPR by learning a very lightweight and simple encoder to generate task-specific descriptors. Our experiments show that our representation can allow for better robustness than current solutions to serious domain shifts away from the training data distribution, such as to indoor environments and aerial imagery."
NISB-Map: Scalable Mapping with Neural Implicit Spatial Block,"Beichen Xiang, Yuxin Sun, Zhongqu Xie, Xiaolong Yang, Yulin Wang","Nanjing University of Science and Technology,Shanghai Jiao Tong University",Localization and Mapping I,
Regressing Transformers for Data-Efficient Visual Place Recognition,"Maria Leyva-vallina, Nicola Strisciuglio, Nicolai Petkov","University of Groningen,University of Twente",Localization and Mapping I,"Visual place recognition is a critical task in computer vision, especially for localization and navigation systems. Existing methods often rely on contrastive learning: image descriptors are trained to have small distance for similar images and larger distance for dissimilar ones in a latent space. However, this approach struggles to ensure accurate distance-based image similarity representation, particularly when training with binary pairwise labels, and complex re-ranking strategies are required. This work introduces a fresh perspective by framing place recognition as a regression problem, using camera field-of-view overlap as similarity ground truth for learning. By optimizing image descriptors to align directly with graded similarity labels, this approach enhances ranking capabilities without expensive re-ranking, offering data-efficient training and strong generalization across several benchmark datasets"
On the Study of Data Augmentation for Visual Place Recognition,"Suji Jang, Ue-Hwan Kim","Gwang-ju Institute of Science and Technology,Gwangju Institute of Science and Technology (GIST)",Localization and Mapping I,"In the field of robotics engineering and autonomous driving vehicles, precise estimation of positions through visual place recognition (VPR) is crucial not only for reducing localization errors caused by visual odometry but also for preventing the creation of ambiguous maps in unfamiliar environments. Despite numerous research efforts aimed at improving VPR performance by addressing challenges such as illumination variation, occlusions, and dynamic objects, contemporary approaches have primarily focused on model-based methods, with limited attention given to data augmentation (DA) methods. Therefore, there is a need to investigate the impact of DA on the generalization ability of VPR. To achieve this objective, this study compares VPR learning approaches, conducts a comprehensive empirical analysis, and presents crucial insights. The results of this study can provide useful guidance for the design of future VPR systems and contribute to the advancement of computer vision and robotics research."
Enhancing Visual Place Recognition with Multi-Modal Features and Time-Constrained Graph Attention Aggregation,"Zhuo Wang, Yunzhou Zhang, Xinge Zhao, Jian Ning, Dehao Zou, Meiqi Pei",Northeastern University,Localization and Mapping I,"Visual place recognition(VPR) is a crucial technology for autonomous driving and robotic navigation. However, severe appearance and perspective changes often lead to degradation of algorithm performance. Current methods mainly utilize single-modality RGB images, which are sensitive to environmental changes. To address this challenge, we propose a novel multi-modal visual place recognition method by incorporating depth information as auxiliary data to enhance the robustness of the VPR algorithm. The pipeline involves dual-branch feature extraction and shared multi-modal feature fusion based on transformer(SFFM) to enable full interaction between semantic and structural information. Furthermore, we introduce a time-constrained graph attention aggregation(TC-GAT) that propagates node information across time and space to deal with perceptual aliasing. Extensive experiments on the Oxford Robotcar and MSLS datasets demonstrate that the proposed algorithm is not only effective in appearance changes but also competitive in opposing viewpoints."
MBFusion: A New Multi-Modal BEV Feature Fusion Method for HD Map Construction,"Xiaoshuai Hao, Hui Zhang, Yifan Yang, Yi Zhou, Sangil Jung, Seung-in Park, Byungin Yoo","Samsung Research China - Beijing (SRC-B),Samsung Research,Samsung Advanced Institute of Technology",Localization and Mapping I,"Abstractâ€” HD map construction is a fundamental and challenging task in autonomous driving to understand the surrounding environment. Recently, Camera-LiDAR BEV feature fusion methods have attracted increasing attention in HD map construction task, which can significantly boost the benchmark. However, existing fusion methods ignore modal interaction and utilize very simple fusion strategy, which suffers from the problems of misalignment and information loss. To tackle this, we propose a novel Multi-modal BEV feature fusion method named MBFusion. Specifically, to solve the semantic misalignment problem between Camera and LiDAR features, we design Cross-modal Interaction Transform(CIT) module to make these two feature spaces interact knowledge with each other to enhance the feature representation by the cross-attention mechanism. Then, we propose a Dual Dynamic Fusion (DDF) module to automatically select valuable information from different modalities for better feature fusion. Moreover, MBFusion is simple, and can be plug-and-played into exist- ing pipelines. We evaluate MBFusion on three architectures, including HDMapNet, VectorMapNet, and MapTR, to show its versatility and effectiveness. Compared with the state-of- the-art methods, MBFusion achieves 3.6% and 4.1% absolute improvements on mAP on the nuScenes and the Argoverse2 datasets, respectively, demonstrating the superiority of our method."
HPF-SLAM: An Efficient Visual SLAM System Leveraging Hybrid Point Features,"Xin Su, Sebastian Eger, Adam Misik, Dong Yang, Rastin Pries, Eckehard Steinbach","Technical University of Munich,TUM,Siemens Technology, Technical University Munich,Nokia",SLAM V,"Visual SLAM is an essential tool in diverse applications such as robot perception and extended reality, where feature-based methods are prevalent due to their accuracy and robustness. However, existing methods employ either hand-crafted or solely learnable point features and are thus limited by the feature attributes. In this paper, we propose incorporating hybrid point features efficiently into a single system. By integrating hand-crafted and learnable features, we seek to capitalize on their complementary attributes in both key-point identification and descriptor expressiveness. To this purpose, we design a pre-processing module, which includes extraction, inter-class processing, and post-processing of hybrid point features. We present an efficient matching approach to exclusively perform the data association within the same class of features. Moreover, we design a Hybrid Bag-of-Words (H-BoW) model to deal with hybrid point features in matching and loop-closure-detection. By integrating the proposed framework into a modern feature-based system, we introduce HPF-SLAM. We evaluate the system on EuRoC-MAV and TUM-RGBD benchmarks. The experimental results show that our method consistently surpasses the baseline at comparable speed."
2D-3D Object Shape Alignment for Camera-Object Pose Compensation in Object-Visual SLAM,"Hanyeol Lee, Jaehyung Jung, Chan Gook Park","Seoul National University,Technical University of Munich",SLAM V,"In this study, we propose an object shape alignment method through a robust optimization scheme for 6-degrees-of-freedom (DOF) object pose compensation. Although the pose estimation of the 3D object by the camera has been rapidly improved in recent years with the development of deep learning, the estimate still contains errors due to several factors. To compensate for this, we perform a shape alignment between the 2D segmentation of the object and the projection of the 3D object in the image plane. To avoid convergence to a local minimum in nonlinear optimization, we separate the pose into translation and rotation. This approach derives the optimization of a linear form in terms of a translation with reduced computational cost. For the rotation, the parallel optimization is performed with multiple initial values, reflecting to the uncertainty of an initial value. We formulate an invariant extended Kalman filter (EKF)-based object-visual simultaneous localization and mapping (SLAM) with a camera-object relative pose as the measurement model. To verify the performance of the proposed algorithm, we present the improved results of camera-object relative pose accuracy and localization and mapping accuracy in the several sequences of YCB-video dataset."
Spectral Trade-Off for Measurement Sparsification of Pose-Graph SLAM,"Jiyeon Nam, Soojeong Hyeon, Youngjun Joo, Dongki Noh, Hyungbo Shim","ASRI, Seoul National University,Seoul National University,Sookmyung Women's University,LG Electronics Inc.",SLAM V,"In this paper, we propose a trade-off optimization algorithm to compute an appropriate number of edges for measurement (edge) sparsification in pose-graph SLAM. The greater the amount of measurement data, the larger is the computational burden. To reduce computational burden, one can remove a portion of measurements. However, reliable data, such as odometric measurements, can be lost if measurements are removed without any principle. To remove measurements which is redundant, we propose a trade-off optimization algorithm between maximization of the Fiedler value and minimization of the largest eigenvalue of adjacency matrix for measurement graph. This problem formulation gives virtues twofold. First, it is scalable. For any dataset, when a weight for trade-off is given, this algorithm determines the appropriate number of edges since this is a trade-off optimization problem. Second, the edges of the measurement graph can be distributed evenly. The algorithm considers the minimization of the largest eigenvalue of the adjacency matrix, so it suppresses the upper bound of the maximum degree of the measurement graph. It removes the redundant information concentrated on a few nodes, and improves the estimation accuracy of the sparsified graph. To validate the performance of the proposed trade-off optimization algorithm, we apply our approach to CSAIL, Intel, and Manhattan datasets."
Learning Covariances for Estimation with Constrained Bilevel Optimization,"Mohamad Qadri, Zachary Manchester, Michael Kaess",Carnegie Mellon University,SLAM V,"We consider the problem of learning error covariance matrices for robotic state estimation. The convergence of a state estimator to the correct belief over the robot state is dependent on the proper tuning of noise models. During inference, these models are used to weigh different blocks of the Jacobian and error vector resulting from linearization and hence, additionally affect the stability and convergence of the non-linear system. We propose a gradient-based method to estimate well-conditioned covariance matrices by formulating the learning process as a constrained bilevel optimization problem over factor graphs. We evaluate our method against baselines across a range of simulated and real-world tasks and demonstrate that our technique converges to model estimates that lead to better solutions as evidenced by the improved tracking accuracy on unseen test trajectories."
UWB Radar SLAM: An Anchorless Approach in Vision Denied Indoor Environments,"Gihan Charith Premachandra Hanchapola Appuha, Ran Liu, Chau Yuen, U-Xuan Tan","Singapore University of Technology and Design,Southwest University of Science and Technology,Nanyang Technological University,Singapore University of Techonlogy and Design",SLAM V,"LiDAR and cameras are frequently used as sensors for simultaneous localization and mapping (SLAM). However, these sensors are prone to failure under low visibility (e.g. smoke) or places with reflective surfaces (e.g. mirrors). On the other hand, electromagnetic waves exhibit better penetration properties when the wavelength increases, thus are not affected by low visibility. Hence, this letter presents ultra-wideband (UWB) radar as an alternative to the existing sensors. UWB is generally known to be used in anchor-tag SLAM systems. One or more anchors are installed in the environment and the tags are attached to the robots. Although this method performs well under low visibility, modifying the existing infrastructure is not always feasible. UWB has also been used in peer-to-peer ranging collaborative SLAM systems. However, this requires more than a single robot and does not include mapping in the mentioned environment like smoke. Therefore, the presented approach in this letter solely depends on the UWB transceivers mounted on-board. In addition, an extended Kalman filter (EKF) SLAM is used to solve the SLAM problem at the back-end. Experiments were conducted and demonstrated that the proposed UWB-based radar SLAM is able to map natural point landmarks inside an indoor environment while improving robot localization."
Less Is More: Physical-Enhanced Radar-Inertial Odometry,"Qiucan Huang, Yuchen Liang, Zhijian Qiao, Shaojie Shen, Huan Yin","Hong Kong University of Science and Technology,The Hong Kong University of Science and Technology",SLAM V,"Radar offers the advantage of providing additional physical properties related to observed objects. In this study, we design a physical-enhanced radar-inertial odometry system that capitalizes on the Doppler velocities and radar cross-section information. The filter for static radar points, correspondence estimation, and residual functions are all strengthened by integrating the physical properties. We conduct experiments on both public datasets and our self-collected data, with different mobile platforms and sensor types. Our quantitative results demonstrate that the proposed radar-inertial odometry system outperforms alternative methods using the physical-enhanced components. Our findings also reveal that using the physical properties results in fewer radar points for odometry estimation, but the performance is still guaranteed and even improved, thus aligning with the ""less is more"" principle."
Linear Four-Point LiDAR SLAM for Manhattan World Environments,"Eunju Jeong, Jina Lee, Suyoung Kang, Pyojin Kim","Sookmyung Women's University,sookmyung women's university,Gwangju Institute of Science and Technology (GIST)",SLAM V,"We present a new SLAM algorithm that utilizes an inexpensive four-point LiDAR to supplement the limitations of the short-range and viewing angles of RGB-D cameras. Herein, the four-point LiDAR can detect distances up to 40 m, and it senses only four distance measurements per scan. In open spaces, RGB-D SLAM approaches, such as L-SLAM, fail to estimate robust 6-DoF camera poses due to the limitations of the RGB-D camera. We detect walls beyond the range of RGB-D cameras using four-point LiDAR; subsequently, we build a reliable global Manhattan world (MW) map while simultaneously estimating 6-DoF camera poses. By leveraging the structural regularities of indoor MW environments, we overcome the challenge of SLAM with sparse sensing owing to the four-point LiDARs. We expand the application range of L-SLAM while preserving its strong performance, even in low-textured environments, using the linear Kalman filter (KF) framework. Our experiments in various indoor MW spaces, including open spaces, demonstrate that the performance of the proposed method is comparable to that of other state-of-the-art SLAM methods."
IBoW3D: Place Recognition Based on Incremental and General Bag of Words in 3D Scans,"Yuxiaotong Lin, Jiming Chen, Liang Li","ZJU,Zhejiang University,Zhejiang Univerisity",SLAM V,"Existing methods for place recognition in 3D point clouds either ignore partial structure information by converting 3D scans to 2D images or construct constrained bag-of-words (BoW) representations reliant on specific feature extraction algorithms. In this paper, we propose a novel method based on incremental and general bag of words. Incorporating an adaptable keypoint and 3D local feature extraction method, we employ an incremental BoW model that is updated regularly. This enables a coarse-to-fine candidate selection from the database. And a revisit can be identified following geometric verification. In addition, we propose a new supplementary metric that addresses the leaving-out issue of the conventional metric, enhancing the identification of true loops. Employing a state-of-the-art (SOTA) keypoint and feature extraction algorithm, we evaluate our method as well as SOTA place recognition methods using diverse datasets with varying qualities. Experimental results demonstrate that our method outperforms the baselines across all three datasets, showcasing robust performance and notable generalization capabilities."
Language-EXtended Indoor SLAM (LEXIS): A Versatile System for Real-Time Visual Scene Understanding,"Christina Kassab, Matias Mattamala, Lintong Zhang, Maurice Fallon",University of Oxford,SLAM V,"Versatile and adaptive semantic understanding would enable autonomous systems to comprehend and interact with their surroundings. Existing fixed-class models limit the adaptability of indoor mobile and assistive autonomous systems. In this work, we introduce LEXIS, a real-time indoor Simultaneous Localization and Mapping (SLAM) system that harnesses the open-vocabulary nature of Large Language Models (LLMs) to create a unified approach to scene understanding and place recognition. The approach first builds a topological SLAM graph of the environment (using visual-inertial odometry) and embeds Contrastive Language-Image Pretraining (CLIP) features in the graph nodes. We use this representation for flexible room classification and segmentation, serving as a basis for room-centric place recognition. This allows loop closure searches to be concentrated to semantically relevant places. Our proposed system is evaluated using both public, simulated data and real-world data, covering office and home environments. It successfully categorizes rooms with varying layouts and dimensions and outperforms the state-of-the-art (SOTA). For place recognition and trajectory estimation tasks we achieve equivalent performance to the SOTA, all while utilizing the same pre-trained model. Lastly, we demonstrate the systemâ€™s potential for planning."
Helical Control in Latent Space: Enhancing Robotic Craniotomy Precision in Uncertain Environments,"Yuanyuan Jia, Jessica Qu, Tadahiro Taniguchi","Ritsumeikan University,Canadian Academy",Dexterous Manipulation II,"In this paper, we introduce a double-stage transfer learning framework based on expert data. It employs probabilistic graphical models to effectively capture helical periodic features in the latent space, integrating Bayesian variational inference and neural networks for implementation. Compared to traditional methods, it achieves high precision and stable control even in environments with limited observation signals and high noise levels. We have successfully applied this method to a biomedical task of a simulated cranial window procedure. Preliminary results show promising performance comparable to those of human experts with only image information, further validating the efficacy of the proposed method."
1 kHz Behavior Tree for Self-Adaptable Tactile Insertion,"Yansong Wu, Fan Wu, Lingyun Chen, Kejia Chen, Samuel Schneider, Lars Johannsmeier, Zhenshan Bing, Fares Abu-Dakka, Zhenshan Bing, Sami Haddadin","Technische Universität München,Technical University of Munich,TUM,Franka Robotics GmbH,Mondragon University,Tech. Univ. Muenchen TUM",Dexterous Manipulation II,"Insertion is an essential skill for robots in both modern manufacturing and services robotics. In our previous study, we proposed an insertion skill framework based on force-domain wiggle motion. The main limitation of this method lies in the robot's inability to adjust its behavior according to changing contact state during interaction. In this paper, we extend the skill formalism by incorporating a behavior tree-based primitive switching mechanism that leverages high-frequency tactile data for the estimation of contact state. The efficacy of our proposed framework is validated with a series of experiments that involve the execution of tightly constrained peg-in-hole tasks. The experiment results demonstrate a significant improvement in performance, characterized by reduced execution time, heightened robustness, and superior adaptability when confronted with unknown tasks. Moreover, in the context of transfer learning, our paper provides empirical evidence indicating that the proposed skill framework contributes to enhanced transferability across distinct operational contexts and tasks."
DexDLO: Learning Goal-Conditioned Dexterous Policy for Dynamic Manipulation of Deformable Linear Objects,"Zhaole Sun, Jihong Zhu, Robert Fisher","Tsinghua University, the University of Edinburgh, Intel Lab Chin,University of York,University of Edinburgh",Dexterous Manipulation II,"Deformable linear object (DLO) manipulation is needed in many fields. Previous research on deformable linear object (DLO) manipulation has primarily involved parallel jaw gripper manipulation with fixed grasping positions. However, the potential for dexterous manipulation of DLOs using an anthropomorphic hand is under-explored. We present DexDLO, a model-free framework that learns dexterous dynamic manipulation policies for deformable linear objects with a fixed-base dexterous hand in an end-to-end way. By abstracting several common DLO manipulation tasks into goal-conditioned tasks, DexDLO can perform tasks such as DLO grabbing, DLO pulling, DLO end-tip position controlling, etc. Using the Mujoco physics simulator, we demonstrate that our framework can efficiently and effectively learn five different DLO manipulation tasks with the same framework parameters. We further provide a thorough analysis of learned policies, reward functions, and reduced observations for a comprehensive understanding of the framework."
Everyday Finger: A Robotic Finger That Meets the Needs of Everyday Interactive Manipulation,"Ruben Castro Ornelas, Tomas Cantu, Isabel Sperandio, Alexander Slocum, Pulkit Agrawal","Massachusetts Institute of Technology,MIT",Dexterous Manipulation II,"We provide the mechanical and dynamical requirements for a robotic finger capable of performing a large number of everyday tasks. To match these requirements, we present a novel actuator and finger design, the everyday finger, that comes close to many characteristics of the human fingers. In particular, we focus on minimizing the size of components to get proper performance without sacrificing compactness. A robotic hand that uses two Everyday fingers demonstrated an 80% success rate in picking up and placing dishes in a rack, and the ability to pick up flat objects like napkins and delicate ones like strawberries. Videos are available at the project website: https://sites.google.com/view/everydayfinger."
Quadratic Programming Based Inverse Kinematics for Precise Bimanual Manipulation,"Tomohiro Chaki, Tomohiro Kawakami","Honda R&D Co., Ltd.",Dexterous Manipulation II,"We discuss the precise cooperative motion of a dual manipulator. In the inverse kinematics of cooperative redundant manipulators, a hierarchical method using null space and an optimization method prioritizing the end-effectors relative position in the objective function have been proposed. However, there is no guarantee that the relative position will be maintained in regions subject to joint limits and task-space reachability constraints. As a result, unacceptable errors may occur, and some tasks cannot be accomplished. We propose designing the maximum permissible errors in advance by expressing the target relative position as inequality constraints in the Quadratic Programming (QP) problem. By extending its description to include a virtual spring, we have also achieved subtle force application by two cooperated manipulators. The proposed method was verified by simulation and experiments."
Model-Free 3D Shape Control of Deformable Objects Using Novel Features Based on Modal Analysis,"Bohan Yang, Bo Lu, Wei Chen, Fangxun Zhong, Yunhui Liu","The Chinese University of Hong Kong,Soochow University,Chinese University of Hong Kong",Dexterous Manipulation II,"Shape control of deformable objects is challenging and important. This paper proposes a model-free controller using novel 3D global deformation features based on modal analysis. Unlike most existing controllers using geometric features, our controller employs physically based deformation features designed by decoupling global deformation into low-frequency modes. Although modal analysis is widely adopted in computer vision and simulation, its usage in robotic deformation control is still an open topic. We develop a new model-free framework for the modal-based deformation control. Physical interpretation of the modes enables us to formulate an analytical deformation Jacobian matrix mapping the robot manipulation onto changes of the modal features. In the Jacobian matrix, unknown geometric and physical models of the object are treated as low-dimensional modal parameters which can be used to linearly parameterize the closed-loop system. Thus, an adaptive controller with proven stability can be designed to deform the object while online estimating the modal parameters. Simulations, experiments, and comparative studies are conducted for validation."
Global Planning for Contact-Rich Manipulation Via Local Smoothing of Quasi-Dynamic Contact Models,"Tao Pang, Hyung Ju Terry Suh, Lujie Yang, Russ Tedrake","Boston Dynamics AI Institute,Massachusetts Institute of Technology,MIT",Dexterous Manipulation II,"The empirical success of Reinforcement Learning (RL) in contact-rich manipulation leaves much to be understood from a model-based perspective, where key difficulties are often attributed to (i) the explosion of contact modes, (ii) stiff, non-smooth contact dynamics and the resulting exploding / discontinuous gradients, and (iii) the non-convexity of the planning problem. The stochastic nature of RL addresses (i) and (ii) by sampling and averaging contact modes. In contrast, model-based methods smooth contact dynamics analytically. Our first contribution establishes the theoretical equivalence of the two methods for simple systems, and shows empirical equivalence on several complex examples. To further alleviate (ii), our second contribution is a convex, differentiable and quasi-dynamic formulation of contact dynamics, which is amenable to both smoothing schemes. Our final contribution resolves (iii), where we show that with smoothing, classical sampling-based motion planning can be effective in global planning. Applying our method on challenging contact-rich manipulation tasks, we show that model-based motion planning can perform comparably to RL with dramatically"
Enhancing Dexterity in Robotic Manipulation Via Hierarchical Contact Exploration,"Xianyi Cheng, Sarvesh Bipin Patil, Zeynep Temel, Oliver Kroemer, Matthew T. Mason","Carnegie Mellon University,Carnegie Mellon University School of Computer Science",Dexterous Manipulation II,"Planning robot dexterity is challenging due to the non-smoothness introduced by contacts, intricate fine motions, and ever-changing scenarios. We present a hierarchical planning framework for dexterous robotic manipulation (HiDex). This framework explores in-hand and extrinsic dexterity by leveraging contacts. It generates rigid-body motions and complex contact sequences. Our framework is based on Monte-Carlo Tree Search and has three levels: 1) planning object motions and environment contact modes; 2) planning robot contacts; 3) path evaluation and control optimization. This framework offers two main advantages. First, it allows efficient global reasoning over high-dimensional complex space created by contacts. It solves a diverse set of manipulation tasks that require dexterity, both intrinsic (using the fingers) and extrinsic (also using the environment), mostly in seconds. Second, our framework allows the incorporation of expert knowledge and customizable setups in task mechanics and models. It requires minor modifications to accommodate different scenarios and robots. Hence, it provides a flexible and generalizable solution for various manipulation tasks. As examples, we analyze the results on 7 hand configurations and 15 scenarios. We demonstrate 8 tasks on two robot platforms."
Inter-Finger Small Object Manipulation with DenseTact Optical Tactile Sensor,"Won Kyung Do, Bianca Aumann, Camille Chungyoun, Monroe Kennedy",Stanford University,Dexterous Manipulation II,"The ability to grasp and manipulate small objects in cluttered environments remains a significant challenge. This letter introduces a novel approach that utilizes a tactile sensor-equipped gripper with eight degrees of freedom to overcome these limitations. We employ DenseTact 2.0 for the gripper, enabling precise control and improved grasp success rates, particularly for small objects ranging from 5 mm to 25 mm. Our integrated strategy incorporates the robot arm, gripper, and sensor to manipulate and orient small objects for subsequent classification, effectively. We contribute a specialized dataset designed for classifying these objects based on tactile sensor output and a new control algorithm for in-hand orientation tasks. Our system demonstrates 88% of successful grasp and successfully classified small objects in cluttered scenarios."
ViHOPE: Visuotactile In-Hand Object 6D Pose Estimation with Shape Completion,"Hongyu Li, Snehal Dikhale, Soshi Iba, Nawid Jamali","Brown University,Honda Research Institute USA,Honda Research Institute USA, Inc.",Perception for Grasping and Manipulation II,
VERGNet: Visual Enhancement Guided Robotic Grasp Detection under Low-Light Condition,"Mingdi Niu, Zhenyu Lu, Lu Chen, Jing Yang, Chenguang Yang","Shanxi University,Bristol Robotics Laboratory,University of Liverpool",Perception for Grasping and Manipulation II,"Although existing grasp detection methods have achieved encouraging performance under well-light conditions, repetitive experiments have found that the detection performance would deteriorate drastically under low-light conditions. Although supplementary information can be provided by additional sensors, such as depth camera, the sparse and weak visual features still hinder the improvement of detection accuracy. In order to address these, we propose a visual enhancement guided grasp detection model (VERGNet) to improve the robustness of robotic grasping in low-light conditions. Firstly, a simultaneous grasp detection and low-light feature enhancement framework is designed, which integrates residual blocks with coordinate attention to re-optimize grasping features. Then, the unsupervised low-light feature enhancement strategy is adopted to reduce the dependence on paired data as well as improve the algorithmic robustness to low-light conditions. Extensive experiments are finally conducted on two newly-constructed lowlight grasp datasets and the proposed method achieves 98.9% and 91.2% detection accuracy respectively, which are superior to comparative methods. Besides, the effectiveness in our method has also been validated in real-world low-light imaging scenarios."
TactileAR: Active Tactile Pattern Reconstruction,"Bing Wu, Qian Liu","Dalian University of Technology , China,Dalian University of Technology",Perception for Grasping and Manipulation II,"High-resolution (HR) contact surface information is essential for robotic grasping and precise manipulation tasks. However, it remains a challenge for current taxel-based sensors to obtain HR tactile information. In this paper, we focus on utilizing low-resolution (LR) tactile sensors to reconstruct the localized, dense, and HR representation of contact surfaces. In particular, we build a Gaussian triaxial tactile sensor degradation model and propose a tactile pattern reconstruction framework based on the Kalman filter. This framework enables the reconstruction of 2-D HR contact surface shapes using collected LR tactile sequences. In addition, we present an active exploration strategy to enhance the reconstruction efficiency. We evaluate the proposed method in real-world scenarios with comparison to existing priori-information-based approaches. Experimental results confirm the efficiency of the proposed approach and demonstrate satisfactory reconstructions of complex contact surface shapes."
Online Estimation of Articulated Objects with Factor Graphs Using Vision and Proprioceptive Sensing,"Russell Buchanan, Adrian Röfer, Joao Moura, Abhinav Valada, Sethu Vijayakumar","University of Edinburgh,University of Freiburg,The University of Edinburgh",Perception for Grasping and Manipulation II,"From dishwashers to cabinets, humans interact with articulated objects every day, and for a robot to assist in common manipulation tasks, it must learn a representation of articulation. Recent deep learning methods can provide powerful vision-based priors on the affordance of articulated objects from previous, possibly simulated, experiences. In contrast, many other works estimate articulation by observing the object in motion, requiring the robot to already be interacting with the object. In this work, we propose to use the best of both worlds by introducing an online estimation method that merges vision-based affordance predictions from a neural network with interactive kinematic sensing in an analytical model. Our work has the benefit of using vision to predict an articulation model before touching the object, while also being able to update the model quickly from kinematic sensing during the interaction. In this paper, we implement a full system using shared autonomy for robotic opening of articulated objects, in particular objects in which the articulation is not apparent from vision alone. We implemented our system on a real robot and performed several autonomous closed-loop experiments in which the robot had to open a door with unknown joint while estimating the articulation online. Our system achieved an 80% success rate for autonomous opening of unknown articulated objects."
Learning to Estimate Incipient Slip with Tactile Sensing to Gently Grasp Objects,"Dirk-jan Boonstra, Laurence Willemet, Jelle Luijkx, Michael Wiertlewski","Delft University of Technology,TU Delft",Perception for Grasping and Manipulation II,"To gently grasp objects, robots need to balance generating enough friction yet avoiding too much force that could damage the object. In practice, the force regulation is challenging to implement since it requires knowledge of the friction coefficient, which can vary from object to object and even from grasp to grasp. Tactile sensing offers a window in the contact mechanics and provides information about friction. Notably touch can detect the precursor of the object slipping away from the grasp. To find this information, tactile sensors measure the deformation field of an artificial skin in both the normal and tangential direction. However, current approaches only react to slip and therefore react too late to perturbations. The object slips, inducing a failure of the grasp and damage. In this study, we introduce a method that uses machine-learning to anticipate slip by computing the so-called safety margin of the grasp. This safety margin represents the extra lateral force that maintains the contact away from the frictional limit. To find this value, we use a high-density camera-based tactile sensor to measure the 3D deformation of the surface via the movement of 82 colored markers. We trained a Convolutional Neural Network (CNN) to estimate the safety margin from the tactile images. Because it gives a distance to slip, the safety margin is a powerful metric for regulating grasp forces. As a testament of this effectiveness, we show that a simple proportional controller can robustly grasp a wide variety of objects. The results show that this control method outperforms slip detection methods, by reducing regrasp reaction times while decreasing grasping forces to 1-3 N."
Learning Interaction Constraints for Robot Manipulation Via Set Correspondences,"Junyu Nan, Jessica Hodgins, Brian Okorn","Carnegie Mellon University,Boston Dynamics AI Institute",Perception for Grasping and Manipulation II,"Cross-pose estimation between rigid objects is a fundamental building block for robotic applications. In this paper, we propose a new cross-pose estimation method that predicts correspondences on a set level as opposed to a point level. This contrasts methods that predict cross-pose from per-point correspondences, which can encounter optimization problems for objects with symmetries, since each point may have multiple valid correspondences. Our method, SCAlign, consists of a Set Correspondence Network (SCN) which predicts these sets and their correspondences, and an alignment module to compute their relative cross-pose. Taking point clouds of two objects as input, SCN predicts a set label for each point such that such that points that share a set label form a cross object correspondence. The alignment module then computes the cross-pose as the SE(3) transformation that aligns these set correspondences. We compare SCAlign against other cross- pose estimation baselines on a synthetically generated dataset, SynWidth, which contains randomly generated width-mate objects with symmetric or near-symmetric intercepts. SCAlign significantly outperforms the baselines on this challenging dataset. Additionally, we show that set correspondences can be leveraged to distinguish positive and negative matches between pegs and holes. Robot experiments further validate the practical application of this approach."
Joint-Loss Enhanced Self-Supervised Learning for Refinement-Coupled Object 6D Pose Estimation,"Fengjun Mu, Shixiang Sun, Rui Huang, Chaobin Zou, Wenjiang Li, Huayi Zhan, Hong Cheng","University of Electronic Science and Technology of China,Changhong AI Lab (CHAIR), Sichuan Changhong Electronics Holding ,University of Electronic Science and Technology",Perception for Grasping and Manipulation II,"6D object pose estimation plays a crucial role in robot grasping and manipulation. However, the prevalent methods for 6D object pose estimation heavily rely on 6D annotated data to train deep neural networks, which poses challenges due to the difficulty in obtaining sufficient pose annotations. To address this limitation, this paper presents a self-supervised pose estimation method based on a novel pixel-wise weighted dense fusion architecture. This method allows for direct learning from unannotated RGB-D data facilitated by an Iterative Annotation Resolver. Furthermore, a self-supervised pose refinement method based on joint loss is proposed to enhance the pose estimation accuracy. This refinement method employs a differentiable renderer to construct joint optimization constraints. The experimental results demonstrate that our approach achieves a level of pose estimation accuracy that closely rivals that of supervised methods."
Force-Based Semantic Representation and Estimation of Feature Points for Robotic Cable Manipulation with Environmental Contacts,"Andrea Monguzzi, Yiannis Karayiannidis, Paolo Rocco, Andrea Maria Zanchettin","Politecnico di Milano,Lund University",Perception for Grasping and Manipulation II,"This work demonstrates the utility of dual-arm robots with dual-wrist force-torque sensors in manipulating a Deformable Linear Object (DLO) within an unknown environment that imposes constraints on the DLO's movement through contacts and fixtures. We propose a strategy to estimate the pose of unknown environmental contacts encountered during the manipulation of a DLO, classifying the induced constraints as unilateral, bilateral and fully constrained, exploiting the redundancy of force sensors. A semantic approach to define environmental constraints is introduced and incorporated into a graph-based model of the DLO. This model remains accurate as long as the DLO is under tension and is dynamically updated throughout the manipulation process, built by sequencing a set of primitives. The estimation strategy is validated through simulations and real-world experiments, demonstrating its potential in handling DLOs under various, possibly uncertain, constraints."
AnyGrasp: Robust and Efficient Grasp Perception in Spatial and Temporal Domains,"Hao-shu Fang, Chenxi Wang, Hongjie Fang, Minghao Gou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen Xie, Cewu Lu","Shanghai Jiao Tong University,Shanghai Jiaotong University,University of California, Berkeley,ShangHai Jiao Tong University",Perception for Grasping and Manipulation II,"As the basis for prehensile manipulation, it is vital to enable robots to grasp as robustly as humans. Our innate grasping system is prompt, accurate, flexible, and continuous across spatial and temporal domains. Few existing methods cover all these properties for robot grasping. In this paper, we propose AnyGrasp for grasp perception to enable robots these abilities using a parallel gripper. Specifically, we develop a dense supervision strategy with real perception and analytic labels in the spatial-temporal domain. Additional awareness of bjects' center-of-mass is incorporated into the learning process to help improve grasping stability. Utilization of grasp correspondence across observations enables dynamic grasp tracking. Our model can efficiently generate accurate, 7-DoF, dense, and temporally-smooth grasp poses and works robustly against large depth-sensing noise. Using AnyGrasp, we achieve a 93.3% success rate when clearing bins with over 300 unseen objects, which is on par with human subjects under controlled conditions. Over 900 mean-picks-per-hour is reported on a single-arm system. For dynamic grasping, we demonstrate catching swimming robot fish in the water."
Uplifting Range-View-Based 3D Semantic Segmentation in Real-Time with Multi-Sensor Fusion,"Shiqi Tan, Hamidreza Fazlali, Richard Xu, Yuan Ren, Bingbing Liu","University of Toronto, Huawei Technologies Canada, Co., Ltd.,Noah's Ark Lab,Huawei Technologies Canada Co., Ltd.,Noah's Ark Lab, Huawei Technologies Canada Inc,Huawei Technologies",Object Detection IV,"Range-View(RV)-based 3D point cloud segmentation is widely adopted due to its compact data form. However, RV-based methods fall short in providing robust segmentation for the occluded points and suffer from distortion of projected RGB images due to the sparse nature of 3D point clouds. To alleviate these problems, we propose a new LiDAR and Camera Range-view-based 3D point cloud semantic segmentation method (LaCRange)}. Specifically, a distortion-compensating knowledge distillation (DCKD) strategy is designed to remedy the adverse effect of RV projection of RGB images. Moreover, a context-based feature fusion module is introduced for robust and preservative sensor fusion. Finally, in order to address the limited resolution of RV and its insufficiency of 3D topology, a new point refinement scheme is devised for proper aggregation of features in 2D and augmentation of point features in 3D. We evaluated the proposed method on large-scale autonomous driving datasets i.e. SemanticKITTI and nuScenes. In addition to being real-time, the proposed method achieves state-of-the-art results on nuScenes benchmark."
Radar Tracker: Moving Instance Tracking in Sparse and Noisy Radar Point Clouds,"Matthias Zeller, Daniel Casado Herraez, Jens Behley, Michael Heidingsfeld, Cyrill Stachniss","CARIAD SE,University of Bonn & CARIAD SE,University of Bonn",Object Detection IV,"Robots and autonomous vehicles should be aware of what happens in their surroundings. The segmentation and tracking of moving objects are essential for reliable path planning, including collision avoidance. We investigate this estimation task for vehicles using radar sensing. We address moving instance tracking in sparse radar point clouds to enhance scene interpretation. We propose a learning-based radar tracker incorporating temporal offset predictions to enable direct center-based association and enhance segmentation performance by including additional motion cues. We implement attention-based tracking for sparse radar scans to include appearance features and enhance performance. The final association combines geometric and appearance features to overcome the limitations of center-based tracking to associate instances reliably. Our approach shows an improved performance on the moving instance tracking benchmark of the RadarScenes dataset compared to the current state of the art."
ProEqBEV: Product Group Equivariant BEV Network for 3D Object Detection in Road Scenes of Autonomous Driving,"Hongwei Liu, Jian Yang, Zhengyu Li, Ke Li, Jianzhang Zheng, Xihao Wang, Xuan Tang, Mingsong Chen, Xian Wei, Xiong You","Fuzhou University,Information Engineering University,East China Normal University,Fujian Institute of Research on the Structure of Matter, Chinese,Technische Universität München",Object Detection IV,"With the rapid development of autonomous driving systems, 3D object detection based on Bird's Eye View (BEV) in road scenes has witnessed great progress over the past few years.As a road scene exhibits a part-whole hierarchy between the within objects and the scene itself, simple parts (e.g., roads, lane lines, vehicles and pedestrians) can be assembled into progressively more complex shapes to form a BEV representation of the whole road scene.Therefore, a BEV often has multiple levels of freedom on motion, i.e., the rotation and the moving shift of the whole BEV, and the random movements of objects (e.g., pedestrians and vehicles) inside the BEV.However, most of the current single-sensor or multi-sensor fusion-based BEV object detection methods have not yet taken into account capturing such multi-level motion in a BEV. To address this problem, we propose a product group equivariant object detection network framework that is equivariant with respect to multiple levels of symmetry groups based on multi-sensor fusion. The proposed framework extracts local equivariant features of objects in point clouds, while global equivariant features are extracted in both point clouds and images. Furthermore, the network learns diverse rotation-equivariant features and mitigates a significant amount of detection errors caused by rotations of BEV and objects inside a BEV, thereby further enhancing the performance of object detection.The experiment results show that the network architecture significantly improves object detection on mAP and NDS, respectively.In addition, in order to demonstrate the effectiveness of the proposed local-multi-global equivariant components, we conduct sufficient ablation experiments. The results show that the individual components are indispensable for the object detection performance improvement of the overall network architecture."
Orientation-Aware Multi-Modal Learning for Road Intersection Identification and Mapping,"Qibin He, Zhongyang Xiao, Ze Huang, Hongyuan Yuan, Li Sun","University of Chinese Academy of Sciences,Autonomous Driving Division of NIO Inc., China,Fudan University,Autonomous Driving Division of NIO Inc,University of Sheffield",Object Detection IV,
Masked Gamma-SSL: Learning Uncertainty Estimation Via Masked Image Modeling,"David Williams, Matthew Gadd, Paul Newman, Daniele De Martini","University of Oxford,Oxford University",Object Detection IV,"This work proposes a semantic segmentation network that produces high-quality uncertainty estimates in a single forward pass. We exploit general representations from foundation models and unlabelled datasets through a Masked Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters and simpler than previous techniques. For neural networks used in safety-critical applications, bias in the training data can lead to errors; therefore it is crucial to understand a networkâ€™s limitations at run time and act accordingly. To this end, we test our proposed method on a number of test domains including the SAX Segmentation benchmark, which includes labelled test data from dense urban, rural and off-road driving domains. The proposed method consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark."
EfficientDPS: Efficient and End-To-End Depth-Aware Panoptic Segmentation,"Shengkai Wu, Liangliang Ren, Linfeng Gao, Yupeng Li, Wenyu Liu","CVTE,Huazhong University of Science and Technology",Object Detection IV,"Depth-aware panoptic segmentation (DPS) combines image segmentation and monocular depth estimation in a single model to achieve semantic and geometry perception simultaneously. DPS task has important applications in the robot area but the previous DPS models are too heavy to be applied. Thus, we propose EfficientDPS, an efficient, end-to-end, and unified model for DPS. In our method, query features extracted with convolution networks are used to represent things/stuff. In this way, different vision tasks such as classification, segmentation, and depth estimation can be realized in a unified manner, leading to a compact and efficient model. EfficientDPS can be trained and tested in an end-to-end manner via bipartite matching and complex post-process is not needed at inference. To enhance the supervision signal, group query representation is proposed, leading to better performance without affecting the inference speed. Extensive experiments on Cityscapes-DPS and SemKITTI-DPS show that EfficientDPS can achieve the best trade-off between speed and accuracy than the state-of-the-art methods."
Robust Collaborative Perception against Temporal Information Disturbance,"Xunjie He, Yiming Li, Te Cui, Meiling Wang, Tong Liu, Yufeng Yue","Beijing Institute of Technology,Beijing Institute of technology",Object Detection IV,"Collaborative perception facilitates a more comprehensive representation of the environment by leveraging complementary information shared among various agents and sensors. However, practical applications often encounter information disturbance which includes perception packet loss and time delays, and a comprehensive framework that can simultaneously address such issues is absent. In addition, the feature extraction process prior to fusion is not sufficient, as it lacks exploration of the local semantics and context dependencies of individual features. To enhance both accuracy and robustness, this paper introduces a novel framework named Robust Collaborative Perception against Temporal Information Disturbance, which predicts perception information when disturbance occurs. Specifically, the Historical Frame Prediction (HFP) module is introduced to make compensation for information loss with temporal association excavation of historical features. Based on the predicted features generated by the HFP module, the Pyramid Attention Integration (PAI) module is introduced to augment local semantics and incorporate global long-range dependencies through multi-scale window attention. Compared with existing methods on the publicly available dataset OPV2V, our approach exhibits superior performance and expanded robustness in the 3D object detection task. The code will be publicly available at https://github.com/ hexunjie/Ro- temd."
Concavity-Induced Distance for Unoriented Point Cloud Decomposition,"Ruoyu Wang, Yanfei Xue, Bharath Surianarayanan, Dong Tian, Chen Feng","New York University,InterDigital",Object Detection IV,"We propose Concavity-induced Distance (CID) as a novel way to measure the dissimilarity between a pair of points in an unoriented point cloud. CID indicates the likelihood of two points or two sets of points belonging to different convex parts of an underlying shape represented as a point cloud. After analyzing its properties, we demonstrate how CID can benefit point cloud analysis without the need for meshing or normal estimation, which is beneficial for robotics applications when dealing with raw point cloud observations. By randomly selecting very few points for manual labeling, a CID-based point cloud instance segmentation via label propagation achieves comparable average precision as recent supervised deep learning approaches, on S3DIS and ScanNet datasets. Moreover, CID can be used to group points into approximately convex parts whose convex hulls can be used as compact scene representations in robotics, and it outperforms the baseline method in terms of grouping quality. Our project website is available at: https://ai4ce.github.io/CID/"
FocoTrack: Multi Object Tracking by Focusing on Overlap at Low Frame Rate,"Jaehyeok Lee, Jae-Hyeon Park, Dong Eui Chang","Korea Advanced Institute of Science Technology,Korea Advanced Institute of Science and Technology (KAIST),KAIST",Object Detection IV,"Multi-object tracking (MOT) presents a crucial challenge in robotics. Due to limited resources embedded in robots, one time step per processing time for algorithms can be considerably large. This scenario necessitates the operation of MOT at a low-frame rate. However, algorithms within the MOT research field have been constructed around datasets functioning at 10--30 frames per second (fps) which can be difficult to operate in the limited resources. In response to it, we introduce a new algorithm, called FocoTrack, which maintains tracking ability in four situations, one of which is when objects are overlapped by each other. Our algorithm exhibits remarkable performance without using any deep appearance descriptor, surpassing existing MOT methods which even use the deep appearance descriptor on a 2.5 fps dataset. We also demonstrate strong results with our algorithm on DanceTrack dataset at 20 fps and provide comprehensive insights through detailed analysis of our tracking model."
Ethically Compliant Autonomous Systems under Partial Observability,"Qingyuan Lu, Justin Svegliato, Samer Nashed, Shlomo Zilberstein, Stuart Jonathan Russell","Massachusetts Institute of Technology,University of California Berkeley,University of Massachusetts Amherst,University of Massachusetts,University of California, Berkeley",AI-Enabled Robotics and Learning,"Ethically compliant autonomous systems (ECAS) are the prevailing approach to building robotic systems that perform sequential decision making subject to ethical theories in fully observable environments. However, in real-world robotics settings, these systems often operate under partial observability because of sensor limitations, environmental conditions, or limited inference due to bounded computational resources. Therefore, this paper proposes a partially observable ECAS (PO-ECAS), bringing this work one step closer to being a practical and useful tool for roboticists. First, we formally introduce the PO-ECAS framework and a MILP-based solution method for approximating an optimal ethically compliant policy. Next, we extend an existing ethical framework for prima facie duties to belief space and offer an ethical framework for virtue ethics inspired by Aristotle's Doctrine of the Mean. Finally, we demonstrate that our approach is effective in a simulated campus patrol robot domain."
"Prompt, Plan, Perform: LLM-Based Humanoid Control Via Quantized Imitation Learning","Jingkai Sun, Qiang Zhang, Yiqun Duan, Xiaoyang Jiang, Chong Cheng, Renjing Xu","The Hong Kong University of Science and Technology(GZ),The Hong Kong University of Science and Technology (Guangzhou),University of Technolgoy Sydney,Northeastern University,HKUST(GZ)",AI-Enabled Robotics and Learning,"In recent years, reinforcement learning and imitation learning have shown great potential for controlling humanoid robots' motion. However, these methods typically create simulation environments and rewards for specific tasks, resulting in the requirements of multiple policies and limited capabilities for tackling complex and unknown tasks. To overcome these issues, we present a novel approach that combines adversarial imitation learning with large language models (LLMs). This innovative method enables the agent to learn reusable skills with a single policy and solve zero-shot tasks under the guidance of LLMs. In particular, we utilize the LLM as a strategic planner for applying previously learned skills to novel tasks through the comprehension of task-specific prompts. This empowers the robot to perform the specified actions in a sequence. To improve our model, we incorporate codebook-based vector quantization, allowing the agent to generate suitable actions in response to unseen textual commands from LLMs. Furthermore, we design general reward functions that consider the distinct motion features of humanoid robots, ensuring the agent imitates the motion data while maintaining goal orientation without additional guiding direction approaches or policies. To the best of our knowledge, this is the first framework that controls humanoid robots using a single learning policy network and LLM as a planner. Extensive experiments demonstrate that our method exhibits efficient and adaptive ability in complicated motion tasks."
Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations Via Inverse Reinforcement Learning,"Feiyang Wu, Zhaoyuan Gu, Hanran Wu, Anqi Wu, Ye Zhao",Georgia Institute of Technology,AI-Enabled Robotics and Learning,"Enabling bipedal walking robots to learn how to maneuver over highly uneven, dynamically changing terrains is challenging due to the complexity of robot dynamics and interacted environments. Recent advancements in learning from demonstrations have shown promising results for robot learning in complex environments. While imitation learning of expert policies has been well-explored, the study of learning expert reward functions is largely under-explored in legged locomotion. This paper brings state-of-the-art Inverse Reinforcement Learning (IRL) techniques to solving bipedal locomotion problems over complex terrains. We propose algorithms for learning expert reward functions, and we subsequently analyze the learned functions. Through nonlinear function approximation, we uncover meaningful insights into the expert's locomotion strategies. Furthermore, we empirically demonstrate that training a bipedal locomotion policy with the inferred reward functions enhances its walking performance on unseen terrains, highlighting the adaptability offered by reward learning."
Online Distribution Shift Detection Via Recency Prediction,"Rachel Luo, Rohan Sinha, Yixiao Sun, Ali Hindy, Shengjia Zhao, Silvio Savarese, Edward Schmerling, Marco Pavone",Stanford University,AI-Enabled Robotics and Learning,"When deploying modern machine learning-enabled robotic systems in high-stakes applications, detecting distribution shift is critical. However, most existing methods for detecting distribution shift are not well-suited to robotics settings, where data often arrives in a streaming fashion and may be very high-dimensional. In this work, we present an online method for detecting distribution shift with guarantees on the false positive rate --- i.e., when there is no distribution shift, our system is very unlikely (with probability < epsilon) to falsely issue an alert; any alerts that are issued should therefore be heeded. Our method is specifically designed for efficient detection even with high dimensional data, and it empirically achieves up to 11x faster detection on realistic robotics settings compared to prior work while maintaining a low false negative rate in practice (whenever there is a distribution shift in our experiments, our method indeed emits an alert). We demonstrate our approach in both simulation and hardware for a visual servoing task, and show that our method indeed issues an alert before a failure occurs."
Simplified Continuous High Dimensional Belief Space Planning with Adaptive Probabilistic Belief-Dependent Constraints,"Andrey Zhitnikov, Vadim Indelman","Technion – Israel Institute of Technology,Technion - Israel Institute of Technology",AI-Enabled Robotics and Learning,"Online decision making under uncertainty in partially observable domains, also known as Belief Space Planning, is a fundamental problem in Robotics and Artificial Intelligence. Due to an abundance of plausible future unravelings, calculating an optimal course of action inflicts an enormous computational burden on the agent. Moreover, in many scenarios, e.g., Information gathering, it is required to introduce a belief-dependent constraint. Prompted by this demand, in this paper, we consider a recently introduced probabilistic belief-dependent constrained POMDP. We present a technique to adaptively accept or discard a candidate action sequence with respect to a probabilistic belief-dependent constraint, before expanding a complete set of sampled future observations episodes and without any loss in accuracy. Moreover, using our proposed framework, we contribute an adaptive method to find a maximal feasible return (e.g., Information Gain) in terms of Value at Risk and a corresponding action sequence, given a set of candidate action sequences, with substantial acceleration. On top of that, we introduce an adaptive simplification technique for a probabilistically constrained setting. Such an approach provably returns an identical-quality solution while dramatically accelerating the online decision making. Our universal framework applies to any belief-dependent constrained continuous POMDP with parametric beliefs, as well as nonparametric beliefs represented by particles."
Conformal Policy Learning for Sensorimotor Control under Distribution Shifts,"Huang Huang, Satvik Sharma, Antonio Loquercio, Anastasios Angelopoulos, Ken Goldberg, Jitendra Malik","University of California at Berkeley,University of California, Berkeley,UC Berkeley",AI-Enabled Robotics and Learning,"This paper focuses on the problem of detecting and reacting to changes in the distribution of a sensorimotor controllerâ€™s observables. The key idea is the design of policies that can take conformal quantiles as input, to detect distribution shifts with formal statistical guarantees, which we define as conformal policy learning. We show how to design such policies by using conformal quantiles to switch between base policies with different characteristics, e.g. safety or speed, or directly augmenting a policy observation with a quantile and training it with reinforcement learning. Theoretically, we show that such policies achieve the formal convergence guarantees in finite time. In addition, we thoroughly evaluate their advantages and limitations on two use cases: simulated autonomous driving and active perception with a physical quadruped. Empirical results demonstrate that our approach outperforms five baselines. It is also the simplest of the baseline strategies besides one ablation. Being easy to use, flexible, and with formal guarantees, our work demonstrates how conformal prediction can be an effective tool for sensorimotor learning under uncertainty."
Resampling-Free Particle Filters in High-Dimensions,"Akhilan Boopathy, Aneesh Muppidi, Peggy Yang, Abhiram Iyer, William Yue, Fiete Ila","Massachusetts Institute of Technology,Harvard,MIT",AI-Enabled Robotics and Learning,"State estimation is crucial for the performance and safety of numerous robotic applications. Among the suite of estimation techniques, particle filters have been identified as a powerful solution due to their non-parametric nature. Yet, in high-dimensional state spaces, these filters face challenges such as 'particle deprivation' which hinders accurate representation of the true posterior distribution. This paper introduces a novel resampling-free particle filter designed to mitigate particle deprivation by forgoing the traditional resampling step. This ensures a broader and more diverse particle set, especially vital in high-dimensional scenarios. Theoretically, our proposed filter is shown to offer a near-accurate representation of the desired posterior distribution in high-dimensional contexts. Empirically, the effectiveness of our approach is underscored through a high-dimensional synthetic state estimation task and a 6D pose estimation derived from videos. We posit that as robotic systems evolve with greater degrees of freedom, particle filters tailored for high-dimensional state spaces will be indispensable."
A New Perspective of DL Testing Framework: Human-Computer Interaction Based Neural Network Testing,"Wei Kong, Li Hu, Du Qianjin, Cao Huayang, Kuang Xiaohui","National Key Laboratory on Science and Technology of Information,Tsinghua University",AI-Enabled Robotics and Learning,"Deep learning models have revolutionized various domains but have also raised concerns regarding their security and reliability. Adversarial attacks and coverage-based testing have been extensively studied to assess and enhance the dependability of deep neural networks. However, current research in this area has reached a state of stagnation. Adversarial attacks focus on exploiting vulnerabilities in models, while coverage-based testing aims to achieve comprehensive testing but overlooks application scenarios. Moreover, evaluating test cases solely based on their fault-revealing capability is insufficient. To address these limitations, we propose an innovative interdisciplinary framework that incorporates human-computer interaction methods in deep learning security testing. By considering the attributes of model application scenarios, we can design more effective test suites. Additionally, we establish a comprehensive evaluation metric for test suite quality, considering factors such as diversity and naturalness. This framework promotes reliable and secure deployment of deep learning models, fostering interdisciplinary collaboration between artificial intelligence and human-computer interaction."
MARC: Multipolicy and Risk-Aware Contingency Planning for Autonomous Driving,"Tong Li, Lu Zhang, Sikang Liu, Shaojie Shen","Hong Kong University of Science and Technology,DJI",Autonomous Vehicle Navigation II,"Generating safe and non-conservative behaviors in dense, dynamic environments remains challenging for automated vehicles due to the stochastic nature of traffic participants' behaviors and their implicit interaction with the ego vehicle. This paper presents a novel planning framework, Multipolicy And Risk-aware Contingency planning (MARC), that systematically addresses these challenges by enhancing the multipolicy-based pipelines from both behavior and motion planning aspects. Specifically, MARC realizes a critical scenario set that reflects multiple possible futures conditioned on each semantic-level ego policy. Then, the generated policy-conditioned scenarios are further formulated into a tree-structured representation with a dynamic branchpoint based on the scene-level divergence. Moreover, to generate diverse driving maneuvers, we introduce risk-aware contingency planning, a bi-level optimization algorithm that simultaneously considers multiple future scenarios and user-defined risk tolerance levels. Owing to the more unified combination of behavior and motion planning layers, our framework achieves efficient decision-making and human-like driving maneuvers. Comprehensive experimental results demonstrate superior performance to other strong baselines in various driving environments."
Graph-Based Scenario-Adaptive Lane-Changing Trajectory Planning for Autonomous Driving,"Qing Dong, Zhanhong Yan, Kimihiko Nakano, Xuewu Ji, Yahui Liu","Tsinghua University,the University of Tokyo,The University of Tokyo",Autonomous Vehicle Navigation II,"Trajectory planning is one of the key challenges to the rapid and large-scale deployment of autonomous driving. The lane-changing trajectory planning algorithm for autonomous driving is typically formulated as a optimization process of a cost function, which can be challenging to manually tune for different traffic scenarios. This paper presents a graph-based scenario-adaptive lane-changing trajectory planning approach that overcomes this challenge. Specifically, the cost function recovery method based on maximum entropy inverse reinforcement learning (IRL) is proposed to recover the cost functions of the all demonstrated lane-changing trajectories, and the cost function database is constructed. Then, the scenario matching model based on spatial-temporal graph convolutional network (ST-GCN) is proposed to match the recovered cost functions with the traffic scenarios, making the lane-changing trajectory planning method scenario-adaptive. Our proposed method is evaluated through simulations on the well-known NGSIM dataset and experiments on two typical lane-changing scenarios on the autonomous driving platform. The results show that our method is capable of learning the lane-changing cost function from demonstration and performing scenario-adaptive lane-changing trajectory planning."
"Toward Wheeled Mobility on Vertically Challenging Terrain: Platforms, Datasets, and Algorithms","Aniket Datar, Chenhui Pan, Mohammad Nazeri, Xuesu Xiao","George Mason University,PhD Student at George Mason University",Autonomous Vehicle Navigation II,"Most conventional wheeled robots can only move in flat environments and simply divide their planar workspaces into free spaces and obstacles. Deeming obstacles as non- traversable significantly limits wheeled robotsâ€™ mobility in real- world, extremely rugged, off-road environments, where part of the terrain (e.g., irregular boulders and fallen trees) will be treated as non-traversable obstacles. To improve wheeled mobility in those environments with vertically challenging terrain, we present two wheeled platforms with little hardware modification compared to conventional wheeled robots; we collect datasets of our wheeled robots crawling over previously non-traversable, vertically challenging terrain to facilitate data-driven mobility; we also present algorithms and their experimental results to show that conventional wheeled robots have previously unrealized potential of moving through vertically challenging terrain. We make our platforms, datasets, and algorithms publicly available to facilitate future research on wheeled mobility."
Rethinking Social Robot Navigation: Leveraging the Best of Two Worlds,"Amir Hossain Raj, Zichao Hu, Haresh Karnan, Rohan Chandra, Amirreza Payandeh, Luisa Mao, Peter Stone, Joydeep Biswas, Xuesu Xiao","George Mason University,University of Texas at Austin,The University of Texas at Austin,UT Austin,george mason,University of Texas Austin",Autonomous Vehicle Navigation II,"Empowering robots to navigate in a socially compliant manner is essential for the acceptance of robots moving in human-inhabited environments. Previously, roboticists have developed geometric navigation systems with decades of empirical validation to achieve safety and efficiency. However, the many complex factors of social compliance make geometric navigation systems hard to adapt to social situations, where no amount of tuning enables them to be both safe (people are too unpredictable) and efficient (the frozen robot problem). With recent advances in deep learning approaches, the common reaction has been to entirely discard classical navigation systems and start from scratch, building a completely new learning based social navigation planner. In this work, we find that this reaction is unnecessarily extreme: using a large-scale real world social navigation dataset, SCAND, we find that geometric systems can produce trajectory plans that align with the human demonstrations in a large number of social situations. We, therefore, ask if we can rethink the social robot navigation problem by leveraging the advantages of both geometric and learning-based methods. We validate this hybrid paradigm through a proof-of-concept experiment, in which we develop a hybrid planner that switches between geometric and learning based planning. Our experiments on both SCAND and two physical robots show that the hybrid planner can achieve better social compliance compared to using either the geometric or learning-based approach alone."
Continuous Robotic Tracking of Dynamic Targets in Complex Environments Based on Detectability,"Zhihao Wang, Shixing Huang, Minghang Li, Junyuan Ouyang, Yu Wang, Haoyao Chen","Harbin Institute of Technology, Shenzhen,Harbin Institute of Technology, Shenzhen.,Harbin Institute of Technology,ShenZhen",Autonomous Vehicle Navigation II,"Target tracking is a fundamental task in the domain of robotics. The effectiveness of target tracking hinges upon various factors, such as tracking distance, occlusions, collision avoidance, etc. However, few existing works can simultaneously tackle these considerations of tracking single and multiple targets in complex environments. In this study, the interaction mechanism of target tracking between the robot, the environment and the targets is analyzed, and a general measure named detectability is introduced to correlate the tracking performance for guiding robotic motion planning. Based on the detectability measure, the robotic motion planning framework based on Model Predictive Control (MPC) is proposed to achieve continuous and robust tracking of single, two and three targets in complex environments. Simulations and experiments are performed and verify the performances of our method better than the state-of-the-art methods."
Talk2BEV: Language-Enhanced Birdâ€™s-Eye View Maps for Autonomous Driving,"Tushar Choudhary, Vikrant Dewangan, Shivam Chandhok, Shubham Priyadarshan, Anushka Jain, Arun Singh, Siddharth Srivastava, Krishna Murthy, Madhava Krishna","International Institute of Information Technology, Hyderabad,University of British Columbia,International Institute of Information Technology Hyderabad,University of Tartu,TensorTour Inc,MIT,IIIT Hyderabad",Autonomous Vehicle Navigation II,"This work introduces Talk2BEV, a large vision-language model (LVLM) interface for birdâ€™s-eye view (BEV) maps commonly used in autonomous driving. While existing perception systems for autonomous driving scenarios have largely focused on a pre-defined (closed) set of object categories and driving scenarios, Talk2BEV eliminates the need for BEV-specific training, relying instead on well-performing pre-trained LVLMs. This enables a single system to cater to a variety of autonomous driving tasks encompassing visual and spatial reasoning, predicting the intents of traffic actors, and decision-making based on visual cues. We extensively evaluate Talk2BEV on a large number of scene understanding tasks that rely on both the ability to interpret freeform natural language queries, and in grounding these queries to the visual context embedded into the language-enhanced BEV map. To enable further research in LVLMs for autonomous driving scenarios, we develop and release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV scenarios, with more than 20,000 questions and ground-truth responses from the NuScenes dataset."
DualAT: Dual Attention Transformer for End-To-End Autonomous Driving,"Zesong Chen, Ze Yu, Jun Li, Linlin You, Xiaojun Tan","Sun Yat-sen University,Sun Yat-Sen University",Autonomous Vehicle Navigation II,"The effective reasoning of integrated multimodal perception information is crucial for achieving enhanced end-to-end autonomous driving performance. In this paper, we introduce a novel multitask imitation learning framework for end-to-end autonomous driving that leverages a dual attention transformer (DualAT) to enhance the multimodal fusion and waypoint prediction processes. A self-attention mechanism captures global context information and models the long-term temporal dependencies of waypoints for multiple time steps. On the other hand, a cross-attention mechanism implicitly associates the latent feature representations derived from different modalities through a learnable geometrically linked positional embedding. Specifically, the DualAT excels at processing and fusing information from multiple camera views and LiDAR sensors, enabling comprehensive scene understanding for multitask learning. Furthermore, the DualAT introduces a novel waypoint prediction architecture that combines the temporal relationships between waypoints with the spatial features extracted from sensor inputs. We evaluate our approach on both the Town05 and Longest6 benchmarks using the closed-loop CARLA urban driving simulator and provide extensive ablation studies. The experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods."
SCALE: Self-Correcting Visual Navigation for Mobile Robots Via Anti-Novelty Estimation,"Chang Chen, Yuecheng Liu, Yuzheng Zhuang, Sitong Mao, Kaiwen Xue, Shunbo Zhou","The Chinese University of Hong Kong, Shenzhen,Huawei Noah's Ark Lab,Huawei Technologies Company,ShenZhen Huawei Cloud Computing Technologies Co., Ltd.,The Chinese University of Hong Kong",Autonomous Vehicle Navigation II,"Although visual navigation has been extensively studied using deep reinforcement learning, online learning for real-world robots remains a challenging task. Recent work directly learned from offline dataset to achieve broader generalization in the real-world tasks, which, however, faces the out-of-distribution (OOD) issue and potential robot localization failures in a given map for unseen observation. This significantly drops the success rates and even induces collision. In this paper, we present a self-correcting visual navigation method, SCALE, that can autonomously prevent the robot from the OOD situations without human intervention. Specifically, we develop an image-goal conditioned offline reinforcement learning method based on implicit Q-learning (IQL). When facing OOD observation, our novel localization recovery method generates the potential future trajectories by learning from the navigation affordance, and estimates the future novelty via random network distillation (RND). A tailored cost function searches for the candidates with the least novelty that can lead the robot to the familiar places. We collect offline data and conduct evaluation experiments in three real-world urban scenarios. Experiment results show that SCALE outperforms the previous state-of-the-art methods for open-world navigation with a unique capability of localization recovery, significantly reducing the need for human intervention. Code is available at https://github.com/KubeEdge4Robotics/ScaleNav."
UniGen: Unified Modeling of Initial Agent States and Trajectories for Generating Autonomous Driving Scenarios,"Reza Mahjourian, Rongbing Mu, Valerii Likhosherstov, Paul Mougin, Xiukun Huang, João Vicente Teixeira De Sousa Messias, Shimon Whiteson","Waymo,Waymo UK Ltd,Waymo UK",Autonomous Vehicle Navigation II,"This paper introduces UniGen, a novel approach to generating new traffic scenarios for evaluating and improving autonomous driving software through simulation. Our approach models all driving scenario elements in a unified model: the position of new agents, their initial state, and their future motion trajectories. By predicting the distributions of all these variables from a shared global scenario embedding, we ensure that the final generated scenario is fully conditioned on all available context in the existing scene. Our unified modeling approach, combined with autoregressive agent injection, conditions the placement and motion trajectory of every new agent on all existing agents and their trajectories, leading to realistic scenarios with low collision rates. Our experimental results show that UniGen outperforms prior state of the art on the Waymo Open Motion Dataset."
S2R-ViT for Multi-Agent Cooperative Perception: Bridging the Gap from Simulation to Reality,"Jinlong Li, Runsheng Xu, Xinyu Liu, Baolu Li, Qin Zou, Jiaqi Ma, Hongkai Yu","cleveland state university,UCLA,Cleveland State University,Wuhan University,University of California, Los Angeles",Image-Based Navigation I,"Due to the lack of enough real multi-agent data and time-consuming of labeling, existing multi-agent cooperative perception algorithms usually select the simulated sensor data for training and validating. However, the perception performance is degraded when these simulation-trained models are deployed to the real world, due to the significant domain gap between the simulated and real data. In this paper, we propose the first Simulation-to-Reality transfer learning framework for multi-agent cooperative perception using a novel Vision Transformer, named as S2R-ViT, which considers both the Deployment Gap and Feature Gap between simulated and real data. We investigate the effects of these two types of domain gaps and propose a novel uncertainty-aware vision transformer to effectively relief the Deployment Gap and an agent-based feature adaptation module with inter-agent and ego-agent discriminators to reduce the Feature Gap. Our intensive experiments on the public multi-agent cooperative perception datasets OPV2V and V2V4Real demonstrate that the proposed S2R-ViT can effectively bridge the gap from simulation to reality and outperform other methods significantly for point cloud-based 3D object detection."
Eliminating Cross-Modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection,"Jiahui Fu, Chen Gao, Zitian Wang, Lirong Yang, Xiaofei Wang, Beipeng Mu, Si Liu","Beihang University,Meituan",Image-Based Navigation I,"Recent 3D object detectors typically utilize multi-sensor data and unify multi-modal features in the shared birdâ€™s-eye view (BEV) representation space. However, our empirical findings indicate that previous methods have limitations in generating fusion BEV features free from cross-modal conflicts. These conflicts encompass extrinsic conflicts caused by BEV feature construction and inherent conflicts stemming from heterogeneous sensor signals.Therefore, we propose a novel Eliminating Conflicts Fusion (ECFusion) method to explicitly eliminate the extrinsic/inherent conflicts in BEV space and produce improved multi-modal BEV features. Specifically, we devise a Semantic-guided Flow-based Alignment (SFA) module to resolve extrinsic conflicts via unifying spatial distribution in BEV space before fusion. Moreover, we design a Dissolved Query Recovering (DQR) mechanism to remedy inherent conflicts by preserving objectness clues that are lost in the fusion BEV feature.In general, our method maximizes the effective information utilization of each modality and leverages inter-modal complementarity. Our method achieves state-of-the-art performance in the highly competitive nuScenes 3D object detection dataset."
EMIFF: Enhanced Multi-Scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection,"Zhe Wang, Siqi Fan, Xiaoliang Huo, Tongda Xu, Yan Wang, Jingjing Liu, Yilun Chen, Ya-qin Zhang","Institute for AI Industry Research, Tsinghua University,Tsinghua University,Beihang University,Institute for AI Industry Research (AIR), Tsinghua University,Institute for AI Industry Research(AIR), Tsinghua University",Image-Based Navigation I,"In autonomous driving, cooperative perception makes use of multi-view cameras from both vehicles and infrastructure, providing a global vantage point with rich semantic context of road conditions beyond a single vehicle viewpoint. Currently, two major challenges persist in vehicle-infrastructure cooperative 3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-view images, caused by time asynchrony across cameras; $2)$ information loss in transmission process resulted from limited communication bandwidth. To address these issues, we propose a novel camera-based 3D detection framework for VIC3D task, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit holistic perspectives from both vehicles and infrastructure, we propose Multi-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM) modules to enhance infrastructure and vehicle features at scale, spatial, and channel levels to correct the pose error introduced by camera asynchrony. We also introduce a Feature Compression (FC) module with channel and spatial compression blocks for transmission efficiency. Experiments show that EMIFF achieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous early-fusion and late-fusion methods with comparable transmission costs."
Vehicle Intention Classification Using Visual Clues,"Marvin Klemp, Royden Wagner, Kevin Rösch, Martin Lauer, Christoph Stiller","Karlsruhe Institute of Technology - KIT,KIT,FZI Forschungszentrum Informatik,Karlsruhe Institute of Technology",Image-Based Navigation I,"Classifying intentions of other traffic agents is an essential task for intelligent transportation systems. To simplify this task, vehicles are equipped with various illumination systems, including turn indicators, emergency lights, rear lights, and brake lights. We extend the Waymo open perception dataset with ground truth annotations for different visual intentions to develop methods designed to classify the state of such systems. Furthermore, we propose the visual intention former, a two-step transformer-based architecture to classify visual intentions in image sequences of tracked traffic participants. We use a vision transformer to extract image features, which are passed into a transformer encoder that reasons about temporal dependencies among them. We evaluate against different baseline architectures where our proposed method achieves state-of-the-art results. Additionally, we conduct an in-depth performance analysis of our method regarding different input sequence lengths, vehicle headings, and daytime conditions."
CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation,"Zihua Liu, Yizhou Li, Masatoshi Okutomi",Tokyo Institute of Technology,Image-Based Navigation I,"Stereo matching under foggy scenes remains a challenging task since the scattering effect degrades the visibility and results in less distinctive features for dense correspondence matching. While some previous learning-based methods integrated a physical scattering function for simultaneous stereo-matching and dehazing, simply removing fog might not aid depth estimation because the fog itself can provide crucial depth cues. In this work, we introduce a framework based on contrastive feature distillation(CFD). This strategy combines feature distillation from merged clean-fog features with contrastive learning, ensuring balanced dependence on fog depth hints and clean matching features. This framework helps enhance model generalization across both clean and hazy environments. Comprehensive experiments on synthetic and real-world datasets affirm the superior strength and adaptability of our method."
Multi-Class Road Defect Detection and Segmentation Using Spatial and Channel-Wise Attention for Autonomous Road Repairing,"Jongmin Yu, Chi Bene Chen, Sebastiano Fichera, Paolo Paoletti, Devansh Mehta, Shan Luo","King's College London,University of Liverpool,Robotiz,d",Image-Based Navigation I,"Road pavement detection and segmentation are critical for developing autonomous road repair systems. However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes. We propose a novel end-to-end method for multi-class road defect detection and segmentation. The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions. Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned. To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset annotated with nine road defect classes. The experiments show that our proposed method outperforms existing state-of-the-art methods for multi-class road defect detection and segmentation methods."
HPL-ViT: A Unified Perception Framework for Heterogeneous Parallel LiDARs in V2V,"Yuhang Liu, Sun Boyi, Yuke Li, Yuzheng Hu, Feiyue Wang","Chinese Academy of Science,University of Chinese Academic of Science,Waytous Co. Ltd.,University of Illinois Urbana-Champaign,Institute of automation, Chinese academy of sciences",Image-Based Navigation I,"To develop the next generation of intelligent LiDARs, we propose a novel framework of parallel LiDARs and construct a hardware prototype in our experimental platform, DAWN (Digital Artificial World for Natural). It emphasizes the tight integration of physical and digital space in LiDAR systems, with networking being one of its supported core features. In the context of autonomous driving, V2V (Vehicle-to-Vehicle) technology enables efficient information sharing between different agents which significantly promotes the development of LiDAR networks. However, current research operates under an ideal situation where all vehicles are equipped with identical LiDAR, ignoring the diversity of LiDAR categories and operating frequencies. In this paper, we first utilize OpenCDA and RLS (Realistic LiDAR Simulation) to construct a novel heterogeneous LiDAR dataset named OPV2V-HPL. Additionally, we present HPL-ViT, a pioneering architecture designed for robust feature fusion in heterogeneous and dynamic scenarios. It uses a graph-attention Transformer to extract domain-specific features for each agent, coupled with a cross-attention mechanism for the final fusion. Extensive experiments on OPV2V-HPL demonstrate that HPL-ViT achieves SOTA (state-of-the-art) performance in all settings and exhibits outstanding generalization capabilities."
FastOcc: Accelerating 3D Occupancy Prediction by Fusing the 2D Bird's-Eye View and Perspective View,"Jiawei Hou, Xiaoyan Li, Wenhao Guan, Gang Zhang, Di Feng, Yuheng Du, Xiangyang Xue, Jian Pu","Fudan University,Chinese Academy of Sciences,Damo Academy, Alibaba Group,Mogo Auto Intelligence and Telematics Information Technology Co.",Image-Based Navigation I,"In autonomous driving, 3D occupancy prediction outputs voxel-wise status and semantic labels for more comprehensive understandings of 3D scenes compared with traditional perception tasks, such as 3D object detection and bird's-eye view (BEV) semantic segmentation. Recent researchers have extensively explored various aspects of this task, including view transformation techniques, ground-truth label generation, and elaborate network design, aiming to achieve superior performance. However, the inference speed, crucial for running on an autonomous vehicle, is neglected. To this end, a new method, dubbed FastOcc, is proposed. By carefully analyzing the network effect and latency from four parts, including the input image resolution, image backbone, view transformation, and occupancy prediction head, it is found that the occupancy prediction head holds considerable potential for accelerating the model while keeping its accuracy. Targeted at improving this component, the time-consuming 3D convolution network is replaced with a novel residual-like architecture, where features are mainly digested by a lightweight 2D BEV convolution network and compensated by integrating the 3D voxel features interpolated from the original image features. Experiments on the Occ3D-nuScenes benchmark demonstrate that our FastOcc achieves state-of-the-art results with a fast inference speed."
LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network,"Dongqiangzi Ye, Yufei Xie, Weijia Chen, Zixiang Zhou, Lingting Ge, Hassan Foroosh","Tusimple,N/A,University of Central Florida,TuSimple Inc",Image-Based Navigation I,"Due to the difficulty of acquiring large-scale 3D human keypoint annotation, previous methods for 3D human pose estimation (HPE) have often relied on 2D image features and sequential 2D annotations. Furthermore, the training of these networks typically assumes the prediction of a human bounding box and the accurate alignment of 3D point clouds with 2D images, making direct application in real-world scenarios challenging. In this paper, we present the 1st framework for end-to-end 3D human pose estimation, named LPFormer, which uses only LiDAR as its input along with its corresponding 3D annotations. LPFormer consists of two stages: firstly, it identifies the human bounding box and extracts multi-level feature representations, and secondly, it utilizes a transformer-based network to predict human keypoints based on these features. Our method demonstrates that 3D HPE can be seamlessly integrated into a strong LiDAR perception network and benefit from the features extracted by the network. Experimental results on the Waymo Open Dataset demonstrate the state-of-the-art performance, and improvements even compared to previous multi-modal solutions."
A Tube-Based Reinforcement Learning Approach for Optimal Motion Planning in Unknown Workspaces,"Panagiotis Rousseas, Charalampos Bechlioulis, Kostas Kyriakopoulos","National Technical University of Athens,University of Patras,New York University - Abu Dhabi",Integrated Planning and Learning,"In this work, a tube-based nearly optimal solution to motion planning in unknown workspaces is presented. The advantages of reactive motion planning are combined with a Policy Iteration Reinforcement Learning scheme to yield a novel solution for unknown workspaces that inherits provable safety, convergence and optimality. Moreover, in simply-connected workspaces, our method is proven to asymptotically provide the globally optimal path. Our method is compared against a provably asymptotically optimal RRT* method, as well as a relevant reactive method and provides satisfactory performance, closely matching or outperforming the former."
Task-Oriented Active Learning of Model Preconditions for Inaccurate Dynamics Models,"Alex Lagrassa, Moonyoung Lee, Oliver Kroemer",Carnegie Mellon University,Integrated Planning and Learning,"When planning with an inaccurate dynamics model, a practical strategy is to restrict planning to regions of state-action space where the model is accurate: also known as a model precondition. Empirical real-world trajectory data is valuable for defining data-driven model preconditions regardless of the model form (analytical, simulator, learned, etc...). However, real-world data is often expensive and dangerous to collect. In order to achieve data efficiency, this paper presents an algorithm for actively selecting trajectories to learn a model precondition for an inaccurate pre-specified dynamics model. Our proposed techniques address challenges arising from the sequential nature of trajectories, and potential benefit of prioritizing task-relevant data. The experimental analysis shows how algorithmic properties affect performance in three planning scenarios: icy gridworld, simulated plant watering, and real-world plant watering. Results demonstrate an improvement of approximately 80% after only four real-world trajectories when using our proposed techniques. More material can be found on our project website: url{https://sites.google.com/view/active-mde}"
Risk-Predictive Planning for Off-Road Autonomy,"Lukas Lao Beyer, Gilhyun Ryou, Patrick Spieler, Sertac Karaman","Massachusetts Institute of Technology,JPL",Integrated Planning and Learning,"Efficiently navigating off-road environments presents a number of challenges arising from their unstructured nature. In the absence of high-fidelity maps, occlusions from obstacles and terrain lead to limited information available to inform planning decisions. Furthermore, resolution and latency limitations of real-world perception systems lead to potentially of degraded perception performance when traversing such environments at high speeds. We address these problems by proposing an algorithm which plans trajectories while anticipating future observations. In particular, we introduce a model which learns to predict the evolution of future riskmaps conditioned on the future path and speed profile of the vehicle. The model is trained in a self-supervised fashion using recordings of vehicle trajectories. We then present an algorithm which leverages a way to efficiently query the model along candidate paths and speed profiles to produce time-optimal trajectories while maintaining a bound on the future expected risk. We assess the predictive performance of our risk model through a comparison with real vehicle driving logs. Furthermore, our closed-loop simulations of several benchmark scenarios demonstrate how the behavior of our planner leads to qualitatively distinct trajectories, leading to improvements in both success rate and speed by up to 60%."
BeBOP -- Combining Reactive Planning and Bayesian Optimization to Solve Robotic Manipulation Tasks,"Jonathan Styrud, Matthias Mayr, Erik Hellsten, Volker Krueger, Claes Christian Smith","ABB,Lund University,KTH Royal Institute of Technology",Integrated Planning and Learning,"Robotic systems for manipulation tasks are increasingly expected to be easy to configure for new tasks. While in the past, robot programs were often written statically and tuned manually, the current, faster transition times call for robust, modular and interpretable solutions that also allow a robotic system to learn how to perform a task. We propose the method Behavior-based Bayesian optimization and Planning (BeBOP) that combines two approaches for generating behavior trees: we build the structure using a reactive planner and learn specific parameters with Bayesian optimization. The method is evaluated on a set of robotic manipulation benchmarks and is shown to outperform state-of-the-art reinforcement learning algorithms by being up to 46 times faster while simultaneously being less dependent on reward shaping. We also propose a modification to the uncertainty estimate for the Random Forest surrogate models that drastically improves the results."
Motion Memory: Leveraging past Experiences to Accelerate Future Motion Planning,"Dibyendu Das, Yuanjie Lu, Erion Plaku, Xuesu Xiao",George Mason University,Integrated Planning and Learning,"When facing a new motion-planning problem, most motion planners solve it from scratch, e.g., via sampling and exploration or starting optimization from a straight-line path. However, most motion planners have to experience a variety of planning problems throughout their lifetimes, which are yet to be leveraged for future planning. In this paper, we present a simple but efficient method called Motion Memory, which allows different motion planners to accelerate future planning using past experiences. Treating existing motion planners as either a closed or open box, we present a variety of ways that Motion Memory can contribute to reduce the planning time when facing a new planning problem. We provide extensive experiment results with three different motion planners on three classes of planning problems with over 30,000 problem instances and show that planning speed can be significantly reduced by up to 89% with the proposed Motion Memory technique and with increasing past planning experiences."
Mitigating Causal Confusion in Vector-Based Behavior Cloning for Safer Autonomous Planning,"Jiayu Guo, Mingyue Feng, Pengfei Zhu, Jinsheng Dou, Di Feng, Chengjun Li, Ru Wan, Jian Pu","Fudan University,Mogo Auto Intelligence and Telematics Information Technology Com,mogo ai,Mogo Auto Intelligence and Telematics Information Technology Co.,Mogo ai",Integrated Planning and Learning,"The utilization of vector-based deep learning techniques has great prospects in the realm of autonomous driving, particularly in the domains of prediction and planning tasks. However, the application of vector-based backbones for prediction and planning tasks may lead to the occurrence of causal confusion. Previous studies have explored the phenomenon of causal confusion, with a specific emphasis on the context of visual imitation learning. As for the vector-based model, we observe that the states of surrounding vehicles can be a nuisance shortcut. In our work, an off-policy approach is proposed to alleviate the issue by incorporating de-confounding supervision. Additionally, to better capture the environmental cues, such as route and traffic lights, in vectorized representation, a decoder utilizing iterative route fusion is devised. By incorporating auxiliary supervision and employing a dedicated decoder, we demonstrate the effectiveness of our methods in reducing causal confusion and improving performance in planning tasks through reactive and nonreactive closed-loop simulations on the nuPlan dataset."
Learning-Based Motion Planning with Mixture Density Networks,"Yinghan Wang, Jianping He, Xiaoming Duan",Shanghai Jiao Tong University,Integrated Planning and Learning,"The trade-off between computation time and path optimality is a key consideration in motion planning algorithms. While classical sampling based algorithms fall short of computational efficiency in high dimensional planning, learning based methods have shown great potential in achieving time efficient and optimal motion planning. The SOTA learning based motion planning algorithms utilize paths generated by sampling based methods as expert supervision data and train networks via regression techniques. However, these methods often overlook the important multimodal property of the optimal paths in the training set, making them incapable of finding good paths in some scenarios. In this paper, we propose a Multimodal Neuron Planner (MNP) based on the mixture density networks that explicitly takes into account the multimodality of the training data and simultaneously achieves time efficiency and path optimality. For environments represented by point clouds, MNP first efficiently compresses point clouds into a latent vector by encoding networks that are suitable for processing point clouds. We then design multimodal planning networks which enables MNP to learn and predict multiple optimal solutions. Simulation results show that our method outperforms SOTA learning based method MPNet and advanced sampling based methods IRRT* and BIT*."
Subgoal Diffuser: Coarse-To-Fine Subgoal Generation to Guide Model Predictive Control for Robot Manipulation,"Zixuan Huang, Yating Lin, Fan Yang, Dmitry Berenson",University of Michigan,Integrated Planning and Learning,"Manipulation of articulated and deformable objects can be difficult due to their compliant and under-actuated nature. Unexpected disturbances can cause the object to deviate from a predicted state, making it necessary to use Model-Predictive Control (MPC) methods to plan motion. However, these methods need a short planning horizon to be practical. Thus, MPC is ill-suited for long-horizon manipulation tasks due to local minima. In this paper, we present a diffusion-based method that guides an MPC method to accomplish long-horizon manipulation tasks by dynamically specifying sequences of subgoals for the MPC to follow. Our method, called Subgoal Diffuser, generates subgoals in a coarse-to-fine manner, producing sparse subgoals when the task is easily accomplished by MPC and more dense subgoals when the MPC method needs more guidance. The density of subgoals is determined dynamically based on a learned estimate of reachability, and subgoals are distributed to focus on challenging parts of the task. We evaluate our method on two robot manipulation tasks and find it improves the planning performance of an MPC method, and also outperforms prior diffusion-based methods."
Motion Planning As Online Learning: A Multi-Armed Bandit Approach to Kinodynamic Sampling-Based Planning,"Marco Faroni, Dmitry Berenson","Politecnico di Milano,University of Michigan",Integrated Planning and Learning,"Kinodynamic motion planners allow robots to perform complex manipulation tasks under dynamics constraints or with black-box models. However, they struggle to find high-quality solutions, especially when a steering function is unavailable. This paper presents a novel approach that adaptively biases the sampling distribution to improve the planner's performance. The key contribution is to formulate the sampling bias problem as a non-stationary multi-armed bandit problem, where the arms of the bandit correspond to sets of possible transitions. High-reward regions are identified by clustering transitions from sequential runs of kinodynamic RRT and a bandit algorithm decides what region to sample at each timestep. The paper demonstrates the approach on several simulated examples as well as a 7-degree-of-freedom manipulation task with dynamics uncertainty, suggesting that the approach finds better solutions faster and leads to a higher success rate in execution."
Quadcopter Trajectory Time Minimization and Robust Collision Avoidance Via Optimal Time Allocation,"Zhefan Xu, Kenji Shimada",Carnegie Mellon University,"Planning, Scheduling and Coordination","Autonomous navigation requires robots to generate trajectories for collision avoidance efficiently. Although plenty of previous works have proven successful in generating smooth and spatially collision-free trajectories, their solutions often suffer from suboptimal time efficiency and potential unsafety, particularly when accounting for uncertainties in robot perception and control. To address this issue, this paper presents the Robust Optimal Time Allocation (ROTA) framework. This framework is designed to optimize the time progress of the trajectories temporally, serving as a post-processing tool to enhance trajectory time efficiency and safety under uncertainties. In this study, we begin by formulating a non-convex optimization problem aimed at minimizing trajectory execution time while incorporating constraints on collision probability as the robot approaches obstacles. Subsequently, we introduce the concept of the trajectory braking zone and adopt the chance-constrained formulation for robust collision avoidance in the braking zones. Finally, the non-convex optimization problem is reformulated into a second-order cone programming problem to achieve real-time performance. Through simulations and physical flight experiments, we demonstrate that the proposed approach effectively reduces trajectory execution time while enabling robust collision avoidance in complex environments. Our software is available on GitHub, along with the developed autonomy framework, as open-source ROS packages."
Scaling Team Coordination on Graphs with Reinforcement Learning,"Manshi Limbu, Zechen Hu, Xuan Wang, Daigo Shishika, Xuesu Xiao",George Mason University,"Planning, Scheduling and Coordination","This paper studies Reinforcement Learning (RL) techniques to enable team coordination behaviors in graph environments with support actions among teammates to reduce the costs of traversing certain risky edges in a centralized manner. While classical approaches can solve this non-standard multi-agent path planning problem by converting the original Environment Graph (EG) into a Joint State Graph (JSG) to implicitly incorporate the support actions, those methods do not scale well to large graphs and teams. To address this curse of dimensionality, we propose to use RL to enable agents to learn such graph traversal and teammate supporting behaviors in a data-driven manner. Specifically, through a new formulation of the team coordination on graphs with risky edges problem into Markov Decision Processes (MDPs) with a novel state and action space, we investigate how RL can solve it in two paradigms: First, we use RL for a team of agents to learn how to coordinate and reach the goal with minimal cost on a single EG. We show that RL efficiently solves problems with up to 20/4 or 25/3 nodes/agents, using a fraction of the time needed for J S G to solve such complex problems; Second, we learn a general RL policy for any N-node EGs to produce efficient supporting behaviors. We present extensive experiments and compare our RL approaches against their classical counterparts."
Dynamic Crane Scheduling with Reinforcement Learning for a Steel Coil Warehouse,"Sang-hyun Cho, Woo-Jin Shin, Jeongsun Ahn, Sanghyun Joo, Hyun-Jung Kim","Korea Advanced Institute of Science and Technology,Korea Advanced Institute of Science & Technology,KAIST,Korea Advanced Institute of Science and Technology(KAIST)","Planning, Scheduling and Coordination","This paper tackles the dynamic crane scheduling problem in a steel coil warehouse, involving tasks, such as coil storage, retrieval, and shuffling. Tasks arrive dynamically with precedence relations, while multiple cranes share a track, necessitating collision avoidance. Our goal is to minimize the average task waiting time by allocating tasks to cranes and optimizing their execution sequence. Unlike prior research focusing on static scenarios or rule-based heuristics, we introduce a real-time, reinforcement learning-based algorithm. To effectively handle precedence relations and global information, we propose a policy network based on graph neural networks. Experimental results demonstrate its superiority over traditional heuristics such as dispatching rules in dynamic scenarios."
Tree-Based Representation of Locally Shortest Paths for 2D K-Shortest Non-Homotopic Path Planning,"Tong Yang, Li Huang, Yue Wang, Rong Xiong",Zhejiang University,"Planning, Scheduling and Coordination","A novel algorithm to solve the 2D ð‘˜-shortest non-homotopic path planning (ð‘˜-SNPP) task is proposed in this paper. The task is of practical significance as a sub-module for higher level planning and scheduling tasks, and is gaining increasing attention and focus in recent years. There have existed algorithms that explicitly characterised non-homotopic paths using topological invariants such as â„Ž-signature and winding number. However, these algorithms are inefficient due to their separate treatment of topology and geometry: Topological invariants are singularly utilised for distinguishing non-homotopic property among paths, which significantly increases the volume of the robot configuration space. Meanwhile, distance-optimal path planners search for locally shortest paths in the augmented space, which becomes extremely time-consuming. In this paper, a topological tree is proposed to simultaneously leverage topology and geometry. The tree grows from the starting location and explores all topological routes, until the best ð‘˜ of its leaves reach the goal. It is proven that different branches of the tree explore different homotopy classes of paths, and all the branches are locally shortest. Comparative experiments for ð‘˜-SNPP are conducted in challenging grid-based simulated environments to validate the performance of the proposed algorithm. The C++ implementation of the proposed algorithm is released for the benefit of the robotics community"
Well-Connected Set and Its Application to Multi-Robot Path Planning,"Teng Guo, Jingjin Yu",Rutgers University,"Planning, Scheduling and Coordination","Parking lots and autonomous warehouses for accommodating many vehicles/robots adopt designs in which the underlying graphs are well-connected to simplify planning and reduce congestion. In this study, we formulate and delve into the largest well-connected set (LWCS) problem and explore its applications in layout design for multi-robot path planning. Roughly speaking, a well-connected set over a connected graph is a set of vertices such that there is a path on the graph connecting any pair of vertices in the set without passing through any additional vertices of the set. Identifying an LWCS has many potential high-utility applications, e.g., for determining parking garage layout and capacity, as prioritized planning can be shown to be complete when start/goal configurations belong to an LWCS. In this work, we establish that computing an LWCS is NP-complete. We further develop optimal and near-optimal LWCS algorithms, with the near-optimal algorithm targeting large maps. A complete prioritized planning method is given for planning paths for multiple robots residing on an LWCS."
Dynamic Coalition Formation and Routing for Multirobot Task Allocation Via Reinforcement Learning,"Weiheng Dai, Aditya Bidwai, Guillaume Sartoretti","National University of Singapore,National University of Singapore (NUS)","Planning, Scheduling and Coordination","Many multi-robot deployments, such as automated construction of buildings, distributed search, or cooperative mapping, often require agents to intelligently coordinate their trajectories and form coalition over a large domain, to complete spatially distributed tasks as quickly as possible. We focus on scenarios involving homogeneous robots, but where tasks vary in the number of agents required to start them. For example, construction robots may need to collaboratively air-lift heavy objects at different locations (e.g., prefabricated rooms, crates of material/equipment), where the weight of each payload defines the required coalition size. To balance the total travel time of the agents and their waiting time (before task initiation), agents need to carefully sequence tasks but also dynamically form/disband coalitions. While simpler problems can be approached using heuristics or optimization, these methods struggle with more complex instances involving large task-to-agent ratios, where frequent coalition changes are needed. In this work, we propose to let agents learn to iteratively build cooperative schedules to solve such problems, by casting the problem in the reinforcement learning framework. Our approach relies on an attention-based neural network, allowing agents to reason about the current state of the system to sequence movement decisions that optimize short-term coalition formation and long-term task scheduling. We further propose a novel leader-follower technique to boost cooperation learning and compare our performance to conventional baselines in a wide variety of scenarios. There, our method closely matches or outperforms the baselines; in particular, it yields higher-quality solutions and is at least 2 orders of magnitude faster than exact solver in cases where frequent coalition updates are required."
Multi-Robot Task Allocation under Uncertainty Via Hindsight Optimization,"Neel Dhanaraj, Jeon Ho Kang, Anirban Mukherjee, Heramb Nemlekar, Stefanos Nikolaidis, Satyandra K. Gupta","University of Southern California,UNIVERSITY OF SOUTHERN CALIFORNIA","Planning, Scheduling and Coordination","Multi-robot systems are becoming increasingly prevalent in various real-world applications, such as manufacturing and warehouse logistics. These systems face complex challenges in 1) task allocation due to factors like time-extended tasks, and agent specialization, and 2) uncertainties in task execution. Potential task failures can add further contingency tasks to recover from the failure, thereby causing delays. This paper addresses the problem of Multi-Robot Task Allocation under Uncertainty by proposing a hierarchical approach that decouples the problem into two layers. We use a low-level optimization formulation to find the optimal solution for a deterministic multi-robot task allocation problem with known task outcomes. The higher-level search intelligently generates more likely combinations of failures and calls the inner-level search repeatedly to find the optimal task allocation sequence, given the known outcomes. We validate our results in simulation for a manufacturing domain and demonstrate that our method can reduce the effect of potential delays from contingencies. We show that our algorithm is computationally efficient while improving average makespan compared to other baselines."
Traffic Flow Learning Enhanced Large-Scale Multi-Robot Cooperative Path Planning under Uncertainties,"Xingyao Han, Siyuan Chen, Xinye Xiong, Qiming Liu, Shunbo Zhou, Heng Zhang, Zhe Liu","Shanghai Jiao Tong University,Shanghai JiaoTong University,Huawei,University of Cambridge","Planning, Scheduling and Coordination","Robotic systems with hundreds or even thousands of robots are widely implemented in logistic and industrial applications. In such systems, cooperative path planning is of great importance, as local congestion and motion conflict may greatly degrade system performance, especially in the presence of uncertainties. Our idea is to consider traffic flow equilibrium in path planning to relieve any potential congestion and increase efficiency. In this paper, we propose a hierarchical framework, which includes a traffic flow prediction layer, a sector-level planning layer, and a road-level coordination layer. In traffic flow prediction, we propose a spatio-temporal graph neural network that integrates local information to predict the evolution of future robot density distribution. In sectorlevel planning, we generate sector-level paths that consider travel distance and traffic flow equilibrium simultaneously. In road-level coordination, we implement the conflict-based search algorithm within each sector to ensure conflict-free local paths. In addition, we also explicitly consider motion/communication uncertainties that are unavoided in practical systems. We validate our effectiveness in simulations with over 1000 robots, whatâ€™s more, real experiments are provided."
Accounting for Travel Time and Arrival Time Coordination During Task Allocations in Legged-Robot Teams,"Shengqiang Chen, Yiyu Chen, Ronak Jain, Xiaopan Zhang, Quan Nguyen, Satyandra K. Gupta",University of Southern California,"Planning, Scheduling and Coordination","Many applications require the deployment of legged-robot teams to effectively and efficiently carry out missions. The use of multiple robots allows tasks to be executed concurrently, expediting mission completion. It also enhances resilience by enabling task transfer in case of a robot failure. This paper presents a formulation based on Mixed Integer Linear Programming (MILP) for allocating tasks to robots by taking into account travel time and ensuring efficient execution of collaborative tasks. We extended the MILP formulation to account for complexities with legged robot teams. Our results demonstrate that this approach leads to improved performance in terms of the makespan of the mission. We demonstrate the usefulness of this approach using a case study involving the disinfection of a building consisting of multiple rooms."
Adaptive Pedestrian Agent Modeling for Scenario-Based Testing of Autonomous Vehicles through Behavior Retargeting,"Golam Md Muktadir, Jim Whitehead","University of California, Santa Cruz",Autonomous Agents II,"This work proposes a new representation of pedestrian crossing scenarios and a hybrid modeling approach, RePed, that facilitates transferring microscopic behavior models from behavior research to higher-level trajectories. With this, real-world trajectory-based scenarios can be augmented with a diverse set of human crossing maneuvers, producing a wealth of new scenarios and addressing the scarcity of rare case data that existing works struggle to deal with. Leveraging the controllability of this modeling approach, perturbation-based augmentation can be applied to enrich scenarios further. In addition, the representation is rooted in the Ego vehicle's coordinate system with a logical representation of roads. This design enables scenario retargeting to various road structures, traffic conditions, and ego vehicle behaviors. Thus, it strongly supports scenario-based testing by forcing pedestrians to produce certain situations in simulation even when the Ego Vehicle tries to evade them."
KT-BT: A Framework for Knowledge Transfer through Behavior Trees in Multi-Robot Systems,"Sanjay Sarma Oruganti Venkata, Ramviyas Parasuraman, Ramana Pidaparti","Rensselaer Polytechnic Institute,University of Georgia",Autonomous Agents II,"Multi-Robot and Multi-Agent Systems demonstrate collective (swarm) intelligence through systematic and distributed integration of local behaviors in a group. Agents sharing knowledge about the mission and environment can enhance performance at individual and mission levels. However, this is difficult to achieve, partly due to the lack of a generic framework for transferring part of the known knowledge (behaviors) between agents. This paper presents a new knowledge representation framework and a transfer strategy called KT-BT: Knowledge Transfer through Behavior Trees. The KT-BT framework follows a query-response-update mechanism through an online Behavior Tree framework, where agents broadcast queries for unknown conditions and respond with appropriate knowledge using a condition-action-control sub-flow. We embed a novel grammar structure called stringBT that encodes knowledge, enabling behavior sharing. We theoretically investigate the properties of the KT-BT framework in achieving homogeneity of high knowledge across the entire group compared to a heterogeneous system without the capability of sharing their knowledge. We extensively verify our framework in a simulated multi-robot"
Distributed Matching-By-Clone Hungarian-Based Algorithm for Task Allocation of Multi-Agent Systems,"Arezoo Samiei, Liang Sun","USA,New Mexico State University",Autonomous Agents II,"In this article, we present a novel approach, namely distributed matching-by-clone hungarian-based algorithm (DMCHBA), to multiagent task-allocation problems, in which the number of agents is smaller than the number of tasks. The proposed DMCHBA assumes that agents employ an implicit coordination mechanism and consists of two iterative phases, i.e., the communication phase and the assignment phase. In the communication phase, agents communicate with their connected neighbors and exchange their local knowledge base until they converge on the global knowledge base. In the assignment phase, each agent builds a squared cost matrix by cloning agents adding pseudotasks when necessary, and applying the Hungarian method for task allocation. A local planning algorithm is then applied to identify the order of task execution for an agent. The proposed DMCHBA is proven to produce conflict-free assignments among agents in finite time. We compare the performance of DMCHBA with the consensus-based bundle algorithm, the distributed recursive Hungarian-based algorithms, and the cluster-based Hungarian algorithm (CBHA) in Monte-Carlo simulations with different numbers of agents and tasks. The numerical results reveal the superior convergence and optimality of DMCHBA over all other selected algorithms."
Convolutional Vision Transformer As a Path Following Controller for Omnidirectional Robots,"Sandesh Athni Hiremath, Chengyi Huang, Argtim Tika, Naim Bajcinca","TU Kaiserslautern,Rheinland-Pfälzische Technische Universität Kaiserslautern-Landa,Technische Universität Kaiserslautern",Autonomous Agents II,"A novel deep neural network (DNN) based controller for omnidirectional robots is proposed. The controller decomposes the prescribed reference path, corresponding to a fixed prediction horizon, into multiple shorter paths corresponding to shorter prediction horizons. This implicitly enforces a Hankel structure in the input and consequently also on the output. Taking advantage of this, a convolutional vision transformer model is used to realize the controller which is then trained to predict state and controls over multiple prediction horizons. Model training is performed in a self-supervised manner using a synthetic dataset. The proposed controller is shown to be more efficient than a model designed for a single prediction horizon. In comparison to a model predictive controller, the proposed approach exhibits competitive performance in path following tasks and is 3 times faster on average for the same prediction length."
Can an Embodied Agent Find Your â€œCat-Shaped Mugâ€? LLM-Based Zero-Shot Object Navigation,"Vishnu Sashank Dorbala, James Mullen, Dinesh Manocha","University of Maryland, College Park,University of Maryland",Autonomous Agents II,"We present LGX (Language-guided Exploration), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to an uniquely described target object in a previously unseen environment. Our approach makes use of Large Language Models (LLMs) for this task by leveraging the LLMâ€™s commonsense-reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via real-world experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects."
AutoExplorers: Autoencoder-Based Strategies for High-Entropy Exploration in Unknown Environments for Mobile Robots,"Lennart Puck, Maximilian Schik, Tristan Schnell, Timothee Buettner, Arne Roennau, Rüdiger Dillmann","FZI Forschungszentrum Informatik,FZI Research Center for Information Technology,FZI Forschungszentrum Informatik, Karlsruhe,FZI - Forschungszentrum Informatik - Karlsruhe",Autonomous Agents II,"Deciding where to go next is a challenging task for humans. However, for robots in unknown environments, this becomes even more demanding. In planetary explorations, the robots are continuously challenged with the task of exploring novel areas, yet so far, humans decide for the robots where to go. Even then, prioritizing the next target based on previous knowledge is complex. In our proposed work, the robot utilizes data about its surroundings from drone or satellite images. Alternatively, a volumetric representation can be reduced to form a suitable input. From the input, tiles are selected and embedded by different autoencoder variants. The robot can select the most promising next exploration goal through the distance in the embedding to the previous samples. In this work, a variational autoencoder, a Wasserstein autoencoder, and a spherical autoencoder are evaluated against each other. The latter two variants yield a high information gain when evaluated on satellite data from the Netherlands. Additionally, the framework was employed on data from an analog mission in the Tabernas desert. Through the framework, the robots get an understanding of which goals yield the most information gain and, therefore, can quickly improve their knowledge about their surroundings."
LLM-BT: Performing Robotic Adaptive Tasks Based on Large Language Models and Behavior Trees,"Haotian Zhou, Yunhan Lin, Longwu Yan, Jihong Zhu, Huasong Min","Wuhan University of Science and Technology,University of York,Robotics Institute of Beihang University of China",Autonomous Agents II,"Large Language Models (LLMs) have been widely utilized to perform complex robotic tasks. However, handling external disturbances during tasks is still an open challenge. This paper proposes a novel method to achieve robotic adaptive tasks based on LLMs and Behavior Trees (BTs). It utilizes ChatGPT to reason the descriptive steps of tasks. In order to enable ChatGPT to understand the environment, semantic maps are constructed by an object recognition algorithm. Then, we design a Parser module based on Bidirectional Encoder Representations from Transformers (BERT) to parse these steps into initial BTs. Subsequently, a BTs Update algorithm is proposed to expand the initial BTs dynamically to control robots to perform adaptive tasks. Different from other LLM based methods for complex robotic tasks, our method outputs variable BTs that can add and execute new actions according to environmental changes, which is robust to external disturbances. Our method is valid with simulation in different practical scenarios."
DyHGDAT: Dynamic Hypergraph Dual Attention Network for Multi-Agent Trajectory Prediction,"Weilong Lin, Xinhua Zeng, Jing Teng, Pang Chengxin, Jing Liu","Fudan University,North China Electric Power University,Shanghai University of Electric Power",Autonomous Agents II,"Modeling the interactions among agents based on their historical trajectories is key to precise multi-agent trajectory prediction. Hypergraph Convolutional Networks (HGCN) have become a proper choice for capturing highorder interactions among agents in this field. However, most existing works only consider static hypergraphs, and ignore that in a hypergraph, the power of influence varies between vertices (or hyperedges). Therefore, we propose DyHGDAT, a dynamic hypergraph dual attention network to capture the high-order interactions among agents, which not only models the evolution of hypergraph over time but also highlights the vertices and hyperedges with larger impacts. We apply DyHGDAT to a CVAE-based prediction system for predicting plausible trajectories. To validate the effectiveness of prediction, we evaluate our proposed method on two well-established trajectory prediction datasets: the ETH/UCY datasets and the Stanford Drone Dataset (SDD). The experimental results show that with DyHGDAT, the CVAE-based prediction system outperforms state-of-the-art methods by 12.5%/5.3% in ADE/FDE on ETH/UCY, and the improvement on SDD is 6.4%/7.4%."
LCCRAFT: LiDAR and Camera Calibration Using Recurrent All-Pairs Field Transforms without Precise Initial Guess,"Yu-chen Lee, Kuan-Wen Chen",National Yang Ming Chiao Tung University,Calibration and Identification II,"LiDAR-camera fusion plays a pivotal role in 3D reconstruction for self-driving applications. A fundamental prerequisite for effective fusion is the precise calibration be- tween LiDAR and camera systems. Many existing calibration methods are constrained by predefined mis-calibration ranges in the training data, essentially tying the network to a specific data distribution. However, if the range of evaluation data differs from what the network has been trained on, the resulting estimates may not meet expectations. Moreover, most methods require a precise initial guess for calibration to succeed. In this paper, we introduce LCCRAFT, an online calibration network designed for LiDAR and camera systems. Leveraging the 4D correlation volume and correlation lookup techniques inherited from RAFT, we apply them to correlate RGB images and depth maps derived from the projection of point clouds. Through weight sharing between update iterations and by enabling the update operator to learn from data with varying degrees of error, LCCRAFT demonstrates adaptability to diverse mis- calibration scenarios. This includes cases where the initial mis- calibration is even more severe than what the system encountered during training, demonstrating the robustness of the model. The calibration process executes in 93ms on a single GPU, meeting real-time requirements. Despite the modest 9M model parameters, LCCRAFT achieves competitive performance as compared to the state-of-the-art method, which entails 69M parameters."
Physics-Informed Neural Network for Model Prediction and Dynamics Parameter Identification of Collaborative Robot Joints,"Xingyu Yang, Yixiong Du, Leihui Li, Zhengxue Zhou, Xuping Zhang","Aarhus University,University of Liverpool",Calibration and Identification II,"Collaborative robots have promising potential for widespread use in small-and-medium-sized enterprise (SME) manufacturing and production due to the development of increasingly sophisticated Human-Robot Collaboration technologies. However, predicting and identifying the behavior of collaborative robots remains a challenging problem due to the significant non-linear properties of their unique gearbox, the harmonic drive. To tackle the engineering problem, this work proposes a physics-informed neural network (PINN) to predict and identify collaborative robot joint dynamics. The procedure involves deriving the state-space dynamic model, embedding the system's dynamics into a recurrent neural network (RNN) with customized Runge-Kutta cells, obtaining labeled training data, predicting system responses, and estimating dynamic parameters. The proposed method is applied to predict and identify collaborative robot joint dynamics, and the results are verified and validated through numerical simulations and experimental testing, respectively. The obtained results demonstrate a high level of agreement with the ground truth and exhibit superior performance compared to the conventional PINN and the non-linear grey-box state-space estimation algorithm when confronted with non-linearity and dynamic coupling. Moreover, the PINN exhibits the potential for extension to various dynamic systems."
Estimating Material Properties of Interacting Objects Using Sum-GP-UCB,"Muhammet Yunus Seker, Oliver Kroemer",Carnegie Mellon University,Calibration and Identification II,"Robots need to estimate the material and dynamic properties of objects from observations in order to simulate them accurately. We present a Bayesian optimization approach to identifying the material property parameters of objects based on a set of observations. Our focus is on estimating these properties based on observations of scenes with different sets of interacting objects. We propose an approach that exploits the structure of the reward function by modeling the reward for each observation separately and using only the parameters of the objects in that scene as inputs. The resulting lower-dimensional models generalize better over the parameter space, which in turn results in a faster optimization. To speed up the optimization process further, and reduce the number of simulation runs needed to find good parameter values, we also propose partial evaluations of the reward function, wherein the selected parameters are only evaluated on a subset of real world evaluations. The approach was successfully evaluated on a set of scenes with a wide range of object interactions, and we showed that our method can effectively perform incremental learning without resetting the rewards of the gathered observations."
LiDAR-Camera Extrinsic Calibration with Hierachical and Iterative Feature Matching,"Xuzhong Hu, Zaipeng Duan, Junfeng Ding, Zhe Zhang, Xiao Huang, Jie Ma","Huazhong University of Science and Technology,Huazhong university of science and technology,China ship Development and Design Center",Calibration and Identification II,"In autonomous driving, the LiDAR-Camera system plays a crucial role in a vehicle's perception of 3D environments. To effectively fuse information from both camera and LiDAR, extrinsic calibration is indispensable. Recently, some researchers have proposed deep learning-based methods that utilize convolutional networks to automatically extract features from LiDAR depth images and RGB images for calibration. However, these features do not sufficiently interact during feature matching, which limits the calibration accuracy. To this end, we introduce a novel extrinsic calibration network (HIFMNet) in this paper. It establishes a comprehensive connection between camera and LiDAR features by calculating a globally-aware map-to-map cost volume and hierachical point-to-map cost volumes. The former is used to regress large extrinsic offsets. The latter is employed to iteratively fine-tune extrinsic parameters, while the rigidity of LiDAR points is considered in each iteration to enhance regression robustness. Extensive experiments on the KITTI-odometry dataset demonstrate the superior performance of our HIFMNet compared to other state-of-the-art learning-based methods."
GBEC: Geometry-Based Hand-Eye Calibration,"Yihao Liu, Jiaming Zhang, Zhangcong She, Amir Kheradmand, Mehran Armand",Johns Hopkins University,Calibration and Identification II,"Hand-eye calibration is the problem of solving the transformation from the end-effector of a robot to the sensor attached to it. Commonly employed techniques, such as AXXB or AXZB formulations, rely on regression methods that require collecting pose data from different robot configurations, which can produce low accuracy and repeatability. However, the derived transformation should solely depend on the geometry of the end-effector and the sensor attachment. We propose Geometry-Based End-Effector Calibration (GBEC) that enhances the repeatability and accuracy of the derived transformation compared to traditional hand-eye calibrations. To demonstrate improvements, we apply the approach to two different robot-assisted procedures: Transcranial Magnetic Stimulation (TMS) and femoroplasty. We also discuss the generalizability of GBEC for camera-in-hand and marker-in-hand sensor mounting methods. In the experiments, we perform GBEC between the robot end-effector and an optical tracker's rigid body marker attached to the TMS coil or femoroplasty drill guide. Previous research documents low repeatability and accuracy of the conventional methods for robot-assisted TMS hand-eye calibration. Applying GBEC to repeated calibrations, we obtain transformations with standard deviations of 0.37mm, 0.65mm, and 0.40mm (translation) along x, y, and z axes of the end-effector, respectively. The tool alignment experiments after using GBEC achieve a mean accuracy around 0.2mm in Euclidean distance. When compared to some existing methods, the proposed method relies solely on the geometry of the flange and the pose of the rigid-body marker, making it independent of workspace constraints or robot accuracy, without sacrificing the orthogonality of the rotation matrix. Our results validate the accuracy and applicability of the approach, providing a new and generalizable methodology for obtaining the transformation from the end-effector to a sensor."
A Learning-Based Approach for Estimating Inertial Properties of Unknown Objects from Encoder Discrepancies,"Zizhou Lao, Yuanfeng Han, Yunshan Ma, Gregory Chirikjian","National University of Sinagpore,Johns Hopkins University,National University of Singapore",Calibration and Identification II,"Many robots utilize commercial force/torque sensors to identify inertial properties of unknown objects. However, such sensors can be difficult to apply to small-sized robots due to their weight, size, and cost. In this paper, we propose a learning-based approach for estimating the mass and center of mass (COM) of unknown objects without using force/torque sensors at the end effector or on the joints. In our method, a robot arm carries an unknown object as it moves through multiple discrete configurations. Measurements are collected when the robot reaches each discrete configuration and stops. A neural network then estimates joint torques from encoder discrepancies. Given multiple samples, we derive the closed-form relation between joint torques and the objectâ€™s inertial properties. Based on the derivation, the mass and COM of the object are identified by weighted least squares. In order to improve the accuracy of inferred inertial properties, an attention model is designed to generate the weights used in least squares, which indicate the relative importance for each joint. Our framework requires only encoder measurements without using any force/torque sensors, but still maintains accurate estimation capability. The proposed approach has been demonstrated on a 4 degree of freedom (DOF) robot arm."
CalibFormer: A Transformer-Based Automatic LiDAR-Camera Calibration Network,"Yuxuan Xiao, Yao Li, Chengzhen Meng, Xingchen Li, Jianmin Ji, Yanyong Zhang",University of Science and Technology of China,Calibration and Identification II,"The fusion of LiDARs and cameras has been increasingly adopted in autonomous driving for perception tasks. The performance of such fusion-based algorithms largely depends on the accuracy of sensor calibration, which is challenging due to the difficulty of identifying common features across different data modalities. Previously, many calibration methods involved specific targets and/or manual intervention, which has proven to be cumbersome and costly. Learning-based online calibration methods have been proposed, but their performance is barely satisfactory in most cases. These methods usually suffer from issues such as sparse feature maps, unreliable cross-modality association, inaccurate calibration parameter regression, etc. In this paper, to address these issues, we propose CalibFormer, an end-to-end network for automatic LiDAR-camera calibration. We aggregate multiple layers of camera and LiDAR image features to achieve high-resolution representations. A multi-head correlation module is utilized to identify correlations between features more accurately. Lastly, we employ transformer architectures to estimate accurate calibration parameters from the correlation information. Our method achieved a mean translation error of 0.8751cm and a mean rotation error of 0.0562â—¦ on the KITTI dataset, surpassing existing state-of-the-art methods and demonstrating strong robustness, accuracy, and generalization capabilities."
Target-Free Extrinsic Calibration of Event-LiDAR Dyad Using Edge Correspondences,"Wanli Xing, Shijie Lin, Lei Yang, Jia Pan","The University of Hong Kong,THE UNIVERSITY OF HONG KONG,University of Hong Kong",Calibration and Identification II,"Calibrating the extrinsic parameters of sensory devices is crucial for fusing multi-modal data. Recently, event cameras have emerged as a promising type of neuromorphic sensors, with many potential applications in fields such as mobile robotics and autonomous driving. When combined with LiDAR, they can provide more comprehensive information about the surrounding environment. Nonetheless, due to the distinctive representation of event cameras compared to traditional frame-based cameras, calibrating them with LiDAR presents a significant challenge. In this paper, we propose a novel method to calibrate the extrinsic parameters between a dyad of an event camera and a LiDAR without the need for a calibration board or other equipment. Our approach takes advantage of the fact that when an event camera is in motion, changes in reflectivity and geometric edges in the environment trigger numerous events, which can also be captured by LiDAR. Our proposed method leverages the edges extracted from events and point clouds and correlates them to estimate extrinsic parameters. Experimental results demonstrate that our proposed method is highly robust and effective in various scenes."
LB-R2R-Calib: Accurate and Robust Extrinsic Calibration of Multiple Long Baseline 4D Imaging Radars for V2X,"Jun Zhang, Zihan Yang, Fangwei Zhang, Zhenyu Wu, Guohao Peng, Yiyao Liu, Qiyang Lyu, Mingxing Wen, Danwei Wang","Nanyang Technological University,NANYANG Technological University,China-Singapore International Joint Research Center",Calibration and Identification II,"As a new sensor, 4D radar (x, y, z, velocity) has great potential for V2X, due to its 3D point cloud, direct doppler velocity output, long distance ranging, low-cost, and more importantly, robust perception in all weathers. However, the extrinsic calibration of multiple long baseline 4D radars is rarely researched in V2X, which is the key to fuse multi-radars. The main reasons are three-folds: (1) New sensor. Thus, it is not surprising that little related work can be found. (2) Long baseline and large viewpoint-difference. Current works are mainly focused on unmanned vehicles, which is short baseline and small viewpoint-difference. (3) Sparse, noisy, and very cluttered 4D radar point cloud. Thus, it is challenging to rapidly and accurately locate the target and extract the feature. In this paper, LB-R2R-Calib (Long Baseline Radar to Radar extrinsic Calibration) is proposed to address these problems. The novelties are: (1) A new target is introduced: an eight-quadrant corner reflector enclosed by a foam sphere. The benefit is the target center is a viewpoint-invariant feature. Thus, it is ideal for large viewpoint-difference calibration. (2) A new feature extraction algorithm is proposed to rapidly locate the target and extract the target center from a very cluttered point cloud, as we observed some important characteristics of 4D radar. Experiments with two 4D radars in real environments with four configurations demonstrate our method is highly accurate and robust."
AirTwins: Modular Bi-Copters Capable of Splitting from Their Combined Quadcopter in Midair,"Song Li, Fangyuan Liu, Yuzhe Gao, Jinwu Xiang, Zhan Tu, Daochun Li",Beihang University,Cooperating Cellular Robots,"Micro tandem bi-copters are capable of passing through narrow gaps owing to their particular slender shape. However, the introduction of the tilting servo motors leads to a non-minimum phase roll dynamics, which affects their flight stability when exploring environments with unpredictable disturbances. In this paper, we propose and design a re-configurable aerial platform consisting of two modular bi-copters with an undocking mechanism. In combined configuration, a crossover docking approach is employed to compensate for the poor stability in their servo-controlled attitude of each bi-copter. In bicopter configuration, the minimum size (equal to ideal passable gap's width) of the system was reduced by 58% through mid-air separation. In detail, to compare the attitude response of the two configurations, a dynamic model considering servo response and non-minimum phase is established and simulated, and flying poking experiments were also conducted on them respectively. On the other hand, the performance of single bi-copter including trajectory tracking and passing through narrow gaps was demonstrated through flight tests. Finally, the feasibility of the undocking mechanism was verified by mid-air separation experiments. The proposed system is promising to be applied in scenarios containing both complex perturbations and confined spaces, while also having the potential to improve exploration efficiency through collaborative work."
ArrayBot: Reinforcement Learning for Generalizable Distributed Manipulation through Touch,"Zhengrong Xue, Han Zhang, Cheng Jingwen, Zhengmao He, Yuanchen Ju, Changyi Lin, Gu Zhang, Huazhe Xu","Tsinghua University,Tsinghua University, Shanghai Qi Zhi Institute,Shanghai Qi Zhi Institute,Southwest University,Carnegie Mellon University,Shanghai Jiaotong University",Cooperating Cellular Robots,"We present ArrayBot, a distributed manipulation system consisting of a 16x16 array of vertically sliding pillars integrated with tactile sensors. Functionally, ArrayBot is designed to simultaneously support, perceive, and manipulate the tabletop objects. Towards generalizable distributed manipulation, we leverage reinforcement learning (RL) algorithms for the automatic discovery of control policies. In the face of the massively redundant actions, we propose to reshape the action space by considering the spatially local action patch and the low-frequency actions in the frequency domain. With this reshaped action space, we train RL agents that can relocate diverse objects through tactile observations only. Intriguingly, we find that the discovered policy can not only generalize to unseen object shapes in the simulator but also have the ability to transfer to the physical robot without any sim-to-real fine-tuning. Leveraging the deployed policy, we derive more real-world manipulation skills on ArrayBot to further illustrate the distinctive merits of our proposed system."
Optimizing Modular Robot Composition: A Lexicographic Genetic Algorithm Approach,"Jonathan Külz, Matthias Althoff","Technical University of Munich,Technische Universität München",Cooperating Cellular Robots,"Industrial robots are designed as general-purpose hardware with limited ability to adapt to changing task requirements or environments. Modular robots, on the other hand, offer flexibility and can be easily customized to suit diverse needs. The morphology, i.e., the form and structure of a robot, significantly impacts the primary performance metrics acquisition cost, cycle time, and energy efficiency. However, identifying an optimal module composition for a specific task remains an open problem, presenting a substantial hurdle in developing task-tailored modular robots. Previous approaches either lack adequate exploration of the design space or the possibility to adapt to complex tasks. We propose combining a genetic algorithm with a lexicographic evaluation of solution candidates to overcome this problem and navigate search spaces exceeding those in prior work by magnitudes in the number of possible compositions. We demonstrate that our approach outperforms a state-of-the-art baseline and is able to synthesize modular robots for industrial tasks in cluttered environments. â€‹"
WiBot 1.0: A Modular Reconfigurable Glass Cleaning Robot for High-Rise Buildings,"Sudheera Akalanka, Harith Sandeepa, Manu Athauda Pathirana, Ranjith Amarasinghe, A.G.B.P. Jayasekara, Gihan Charith Premachandra Hanchapola Appuha, U-Xuan Tan","University of Moratuwa,University of Moratuway,Singapore University of Technology and Design,Singapore University of Techonlogy and Design",Cooperating Cellular Robots,"Cleaning glass surfaces is a prevailing maintenance problem in high-rise buildings. In the traditional methods of cleaning windows, hanging on ropes poses significant occupational hazards to workers. Furthermore, most glass facades feature window frames to securely fasten the glass panels to the building structure, ensuring durability and elegance. In this context, existing robotic cleaning methods are limited by their capability to move-over window frames and need more flexibility to access tight corners and curved surfaces. This paper presents a novel reconfigurable glass cleaning robot called ""WiBot"" to address these limitations. WiBot is a kinematic chain comprising modular linkages with a prismatic joint and two revolute joints at each end. Each revolute joint has a suction unit that enables locomotion and adhesion. Window frames are detected using image processing with an onboard camera, and design optimizations were performed to improve the robot's capabilities. The prototype WiBot 1.0 was developed, and several experiments were conducted to evaluate the feasibility of the proposed system focusing on robot motion, window frame detection and move-over mechanism. The results show that WiBot can overcome the limitations of existing window cleaning solutions. Finally, several promising research directions are mentioned involving the proposed reconfigurable robot architecture in cleaning operations."
Collaborative Manipulation of Deformable Objects with Predictive Obstacle Avoidance,"Burak Aksoy, John Wen",Rensselaer Polytechnic Institute,Cooperating Cellular Robots,"Manipulating deformable objects arises in daily life and numerous applications. Despite phenomenal advances in industrial robotics, manipulation of deformable objects remains mostly a manual task. This is because of the high number of internal degrees of freedom and the complexity of predicting its motion.In this paper, we apply the computationally efficient position-based dynamics method to predict object motion and distance to obstacles. This distance is incorporated in a control barrier function for the resolved motion kinematic control for one or more robots to adjust their motion to avoid colliding with the obstacles. The controller has been applied in simulations to 1D and 2D deformable objects with varying numbers of assistant agents, demonstrating its versatility across different object types and multi-agent systems. Results indicate the feasibility of real-time collision avoidance through deformable object simulation, minimizing path tracking error while maintaining a predefined minimum distance from obstacles and preventing overstretching of the deformable object. The implementation is performed in ROS, allowing ready portability to different applications."
D-Lite: Navigation-Oriented Compression of 3D Scene Graphs for Multi-Robot Collaboration,"Yun Chang, Luca Ballotta, Luca Carlone","MIT,Delft University of Technology,Massachusetts Institute of Technology",Cooperating Cellular Robots,"For a multi-robot team that collaboratively explores an unknown environment, it is of vital importance that the collected information is efficiently shared among robots in order to support exploration and navigation tasks. Practical constraints of wireless channels, such as limited bandwidth, urge robots to carefully select information to be transmitted. In this paper, we consider the case where environmental information is modeled using a 3D Scene Graph, a hierarchical map representation that describes both geometric and semantic aspects of the environment. Then, we leverage graph-theoretic tools, namely graph spanners, to design greedy algorithms that efficiently compress 3D Scene Graphs with the aim of enabling communication between robots under bandwidth constraints. Our compression algorithms are navigation-oriented in that they are designed to approximately preserve shortest paths between locations of interest, while meeting a user-specified communication budget constraint. The effectiveness of the proposed algorithms is demonstrated in synthetic robot navigation experiments in a realistic simulator."
ColAG: A Collaborative Air-Ground Framework for Perception-Limited UGVs' Navigation,"Zhehan Li, Rui Mao, Nanhe Chen, Chao Xu, Fei Gao, Yanjun Cao","Zhejiang University,Sun Yat-sen University,Zhejiang University, Huzhou Institute of Zhejiang University",Cooperating Cellular Robots,"Perception is necessary for autonomous navigation in an unknown area crowded with obstacles. Itâ€™s challenging for a robot to navigate safely without any sensors that can sense the environment, resulting in a blind robot, and becomes more difficult when comes to a group of robots. However, it could be costly to equip all robots with expensive perception or SLAM systems. In this paper, we propose a novel system named ColAG, to solve the problem of autonomous navigation for a group of blind UGVs by introducing cooperation with one UAV, which is the only robot that has full perception capabilities in the group. The UAV uses SLAM for its odometry and mapping while sharing this information with UGVs via limited relative pose estimation. The UGVs plan their trajectories in the received map and predict possible failures caused by the uncertainty of its wheel odometry and unknown risky areas. The UAV dynamically schedules waypoints to prevent UGVs from collisions, formulated as a Vehicle Routing Problem with Time Windows to optimize the UAVâ€™s trajectories and minimize time when UGVs have to wait to guarantee safety. We validate our system through extensive simulation with 7 UGVs and real-world experiments with 3 UGVs."
GRF-Based Predictive Flocking Control with Dynamic Pattern Formation,"Chenghao Yu, Dengyu Zhang, Dr. Qingrui Zhang","Sun Yat-sen University,Sun Yat-Sen University",Cooperating Cellular Robots,"It is promising but challenging to design flocking control for a robot swarm to autonomously follow changing patterns or shapes in a optimal distributed manner. The optimal flocking control with dynamic pattern formation is, therefore, investigated in this paper. A predictive flocking control algorithm is proposed based on a Gibbs random field (GRF), where bio-inspired potential energies are used to charaterize ``robot-robot'' and ``robot-environment'' interactions. Specialized performance-related energies, e.g., motion smoothness, are introduced in the proposed design to improve the flocking behaviors. The optimal control is obtained by maximizing a posterior distribution of a GRF. A region-based shape control is accomplished for pattern formation in light of a mean shift technique. The proposed algorithm is evaluated via the comparison with two state-of-the-art flocking control methods in an environment with obstacles. Both numerical simulations and real-world experiments are conducted to demonstrate the efficiency of the proposed design."
From Birdâ€™s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model,"Xiaojie Xu, Tianshuo Xu, Fulong Ma, Yingcong Chen","The Hong Kong University of Science and Technology(Guangzhou),Hongkong University of Science and Technology (Guangzhou),The Hong Kong University of Science and Technology,The University of Science and Technology (Guangzhou)",Visual Learning III,"We explore Birdâ€™s-Eye View (BEV) generation, converting a BEV map into its corresponding multi-view street images. Valued for its unified spatial representation aiding multi-sensor fusion, BEV is pivotal for various autonomous driving applications. Creating accurate street-view images from BEV maps is essential for portraying complex traffic scenarios and enhancing driving algorithms. Concurrently, diffusion based conditional image generation models have demonstrated remarkable outcomes, adept at producing diverse, high-quality, and condition-aligned results. Nonetheless, the training of these models demands substantial data and computational resources. Hence, exploring methods to fine-tune these advanced models, like Stable Diffusion, for specific conditional generation tasks emerges as a promising avenue. In this paper, we introduce a practical framework for generating images from a BEV layout.Our approach comprises two main components: the Neural View Transformation and the Street Image Generation. The Neural View Transformation phase converts the BEV map into aligned multi-view semantic segmentation maps by learning the shape correspondence between the BEV and perspective views. Subsequently, the Street Image Generation phase utilizes these segmentations as a condition to guide a fine-tuned latent diffusion model. This finetuning process ensures both view and style consistency. Our model leverages the generative capacity of large pretrained diffusion models within traffic contexts, effectively yielding diverse and condition-coherent street view images."
Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous Driving,"Junyi Cao, Zhichao Li, Naiyan Wang, Chao Ma","Shanghai Jiao Tong University,tusimple.ai,TuSimple",Visual Learning III,"Recent studies have highlighted the promising application of NeRF in autonomous driving contexts. However, the complexity of outdoor environments, combined with the restricted viewpoints in driving scenarios, complicates the task of precisely reconstructing scene geometry. Such challenges often lead to diminished quality in reconstructions and extended durations for both training and rendering. To tackle these challenges, we present Lightning NeRF. It uses an efficient hybrid scene representation that effectively utilizes the geometry prior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly improves the novel view synthesis performance of NeRF and reduces computational overheads. Through evaluations on real-world datasets, such as KITTI-360, Argoverse2, and our private dataset, we demonstrate that our approach not only exceeds the current state-of-the-art in novel view synthesis quality but also achieves a five-fold increase in training speed and a ten-fold improvement in rendering speed. Codes are available at https://github.com/VISION-SJTU/Lightning-NeRF."
Physical Priors Augmented Event-Based 3D Reconstruction,"Jiaxu Wang, Junhao He, Ziyi Zhang, Renjing Xu","Hong Kong University of Science and Technology (Guangzhou),The Hong Kong University of Science and Technology (Guangzhou)",Visual Learning III,"3D Neural implicit representations play a significant component in many robotic applications. However, reconstructing neural radiance fields (NeRF) from realistic event data remains a challenge due to the sparsities and the lack of information when only event streams are available. In this paper, we utilize motion, geometry, and density priors behind event data to impose strong physical constraints to augment NeRF training. The proposed novel pipeline can directly benefit from those priors to reconstruct 3D scenes without additional inputs. Moreover, we present a novel density-guided patch-based sampling strategy for robust and efficient learning, which not only accelerates training procedures but also conduces to expressions of local geometries. More importantly, we establish the first large dataset for event-based 3D reconstruction, which contains 101 objects with various materials and geometries, along with the groundtruth of images and depth maps for all camera viewpoints, which significantly facilitates other research in the related fields. The code and dataset will be publicly available at https://github.com/Zerory1/Ev3D."
SLAM Based on Camera-2D LiDAR Fusion,Guoyu Lu,University of Georgia,Visual Learning III,"The SLAM system plays a pivotal role in robotic mapping and localization, leveraging various sensor technologies to achieve precision. Traditional passive sensors, such as RGB cameras, offer high-resolution imagery at a lower cost for SLAM applications, yet they fall short in accurately estimating 3D positions and camera orientations. On the other hand, LiDARs excel in generating accurate 3D maps but often come at a higher price and lower resolution. While active illumination sensors like LiDAR provide precise depth estimation, the prohibitive cost of high-resolution LiDAR systems restricts their widespread adoption across diverse applications. Although single-beam LiDAR is more affordable, its limited depth sensing capability hampers comprehensive environmental perception. Addressing these limitations, this study introduces a deep learning framework aimed at enhancing SLAM performance through the strategic fusion of camera and 2D LiDAR data. Our approach employs a novel self-supervised network alongside an economical single-beam LiDAR, striving to achieve or surpass the performance of more expensive LiDAR systems. The integration of single-beam LiDAR with our system allows for dynamic adjustment of scale uncertainty in depth maps generated by monocular camera systems within SLAM. Consequently, this fusion method enjoys the high-resolution and accuracy benefits of advanced LiDAR systems with the cost-effectiveness of single-beam LiDAR technology. Through this innovative combination, we demonstrate a SLAM system that not only maintains high fidelity in mapping and localization but also ensures affordability and broad applicability."
NeRF-Enhanced Outpainting for Faithful Field-Of-View Extrapolation,"Rui Yu, Jiachen Liu, Zihan Zhou, Sharon X. Huang","University of Louisville,Pennsylvania State University,Manycore Tech Inc.,The Pennsylvania State University",Visual Learning III,"In various applications, such as robotic navigation and remote visual assistance, expanding the field of view (FOV) of the camera proves beneficial for enhancing environmental perception. Unlike image outpainting techniques aimed solely at generating aesthetically pleasing visuals, these applications demand an extended view that faithfully represents the scene. To achieve this, we formulate a new problem of faithful FOV extrapolation that utilizes a set of pre-captured images as prior knowledge of the scene. To address this problem, we present a simple yet effective solution called NeRF-Enhanced Outpainting (NEO) that uses extended-FOV images generated through NeRF to train a scene-specific image outpainting model. To assess the performance of NEO, we conduct comprehensive evaluations on three photorealistic datasets and one real-world dataset. Extensive experiments on the benchmark datasets showcase the robustness and potential of our method in addressing this challenge. We believe our work lays a strong foundation for future exploration within the research community."
DL-PoseNet: A Differential Lightweight Network for Pose Regression Over SE(3),"Wenjie Li, Jia Liu, Yanyan Wang, Dayong Ren, Wei Hao, Lijun Chen","Nanjing University,Hohai University",Visual Learning III,"Accurate pose estimation over $SE(3)$ is fundamentally crucial for numerous perception tasks, including camera re-localization. While existing learning-based methods estimated from a series of RGB images have significantly improved the accuracy of pose, the majority of models still face one or two limitations. First, few representations on $SE(3)$ are smooth and differential, making them difficult to apply in deep learning frameworks. Second, they often require high computational resources due to complex deep network designs. We in this paper propose the DL-PoseNet to address these issues. Specifically, we present a novel representation for $SE(3)$ which follows the property of smoothness of the pose. We then design a lightweight neural network to regress the pose by developing a differential pose layer. Finally, we introduce a novel loss function and gradient descent method to better supervise the proposed lightweight pose network. Extensive experiments on the camera re-localization task on the Cambridge Landmarks and 7-Scenes datasets demonstrate the superior predictive accuracy and benefits of our method in comparison with the state-of-the-art."
Crossway Diffusion: Improving Diffusion-Based Visuomotor Policy Via Self-Supervised Learning,"Xiang Li, Varun Belagali, Jinghuan Shang, Michael S Ryoo","Stony Brook University,Google, Stony Brook University",Visual Learning III,"Diffusion models have been adopted for behavioral cloning in a sequence modeling fashion, benefiting from their exceptional capabilities in modeling complex data distributions. The standard diffusion-based policy iteratively denoises action sequences from random noise conditioned on the input states and the model is typically trained with a singular diffusion loss. This paper explores the potential enhancements in such models when the denoising process is informed by a better visual representation. We study the scenario where the model is jointly optimized using the standard diffusion loss alongside an auxiliary objective based on self-supervised learning. After experimenting with various objectives, we introduce Crossway Diffusion, a simple yet effective way to enhance diffusion-based visuomotor policy learning via a state decoder and an auxiliary reconstruction objective. During training, the state decoder reconstructs raw image pixels and other states from the intermediate representations of the model. Experiments demonstrate the effectiveness of our method in various simulated and real-world tasks, confirming its consistent advantages over the standard diffusion-based policy and other baselines."
Bi-KVIL: Keypoints-Based Visual Imitation Learning of Bimanual Manipulation Tasks,"Jianfeng Gao, Xiaoshu Jin, Franziska Krebs, Noémie Jaquier, Tamim Asfour","Karlsruhe Institute of Technology (KIT),Karlrsuhe Institute of Technology",Visual Learning III,"Visual imitation learning has achieved impressive progress in learning unimanual manipulation tasks from a small set of visual observations, thanks to the latest advances in computer vision. However, learning bimanual coordination strategies and complex object relations from bimanual visual demonstrations, as well as generalizing them to categorical objects in novel cluttered scenes remain unsolved challenges. In this paper, we extend our previous work on keypoints-based visual imitation learning (K-VIL) to bimanual manipulation tasks. The proposed Bi-KVIL jointly extracts so-called Hybrid Master-Slave Relationships (HMSR) among objects and hands, bimanual coordination strategies, and sub-symbolic task representations. Our bimanual task representation is object-centric, embodiment-independent, and viewpoint-invariant, thus generalizing well to categorical objects in novel scenes. We evaluate our approach in various real-world applications, showcasing its ability to learn fine-grained bimanual manipulation tasks from a small number of human demonstration videos. Videos and source code are available at https://sites.google.com/view/bi-kvil."
Neural Radiance Fields for Unbounded Lunar Surface Scene,"Xu Zhang, Linyan Cui, Jihao Yin","Beihang University,Beihang Universitity",Visual Learning III,"Accurate understanding of lunar surface topography is vital for effective decision-making and remote control of lunar rovers during exploration missions. Conventional sensing methods often struggle to capture the intricate details of the lunar landscape. In response, we propose an innovative approach that leverages NeRF to synthesize new viewpoints within the expansive lunar environment. By blending 3D hash grids and 2D plane grids representations, our approach provides a comprehensive scene representation. We employ the technique of spiral sampling and feature rendering to enhance rendering quality while simultaneously reducing training time.Additionally, we leverage sparse point cloud to aid the model in better learning the geometric structure of the lunar environment. Through experimentation, we have demonstrated that our method is capable of synthesizing realistic images of lunar environments."
A Convex Formulation of Frictional Contact between Rigid and Deformable Bodies,"Xuchen Han, Joseph Masterjohn, Alejandro Castro",Toyota Research Institute,Simulation and Animation,"We present a novel convex formulation that models rigid and deformable bodies coupled through frictional contact. The formulation incorporates a new corotational material model with positive semi-definite Hessian, which allows us to extend our previous work on the convex formulation of compliant contact to model large body deformations. We rigorously characterize our approximations and present implementation details. With proven global convergence, effective warm-start, the ability to take large time steps, and specialized sparse algebra, our method runs robustly at interactive rates. We provide validation results and performance metrics on challenging simulations relevant to robotics applications. Our method is made available in the open-source robotics toolkit Drake."
SocialGAIL: Faithful Crowd Simulation for Social Robot Navigation,"Bo Ling, Yan Lyu, Dongxiao Li, Guanyu Gao, Yi Shi, Xueyong Xu, Weiwei Wu","Southeast University,Nanjing University of Science and Technology,North Information Control Research Academy Group Co., Ltd.",Simulation and Animation,"Navigation through crowded human environments is challenging for social robots. While reinforcement learning has been adopted for its capacity to capture complex interactions, the training process often relies on simulators to replicate realistic crowd behaviors, ensuring cost-efficiency. Existing crowd simulation methods typically rely on either handcrafted rules, which may lead to overly aggressive navigation, or learning from human trajectory demonstrations, which can be challenging to generalize effectively. In this paper, we introduce a data-driven crowd simulation method called SocialGAIL, which leverages Generative Adversarial Imitation Learning (GAIL) to emulate real pedestrian navigation in crowded environments. SocialGAIL utilizes an attention-based graph neural network to encode observations and employs a generator-discriminator architecture to closely mimic pedestrian behavior. We also propose a set of metrics to evaluate the faithfulness of crowd simulation. Experimental results demonstrate that SocialGAIL outperforms baseline methods in terms of goal-reaching, intermediate state faithfulness, trajectory faithfulness, and adherence to global trajectory patterns."
MuRoSim â€“ a Fast and Efficient Multi-Robot Simulation for Learning-Based Navigation,"Christian Jestel, Karol Rösner, Niklas Dietz, Nicolas Bach, Julian Eßer, Jan Finke, Oliver Urbann",Fraunhofer IML,Simulation and Animation,"Multi-robot navigation and dynamic obstacle avoidance are challenging problems in robot learning. Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated great potential in this area. Nonetheless, they often face challenges related to low sample efficiency. To overcome this challenge, some research proposes simulators that incorporate hardware acceleration. Although these simulators improve efficiency, they often lack the flexibility to generate diverse learning scenarios as often needed in multi-robot scenarios, where the different environments have varying numbers of agents. In this paper, we introduce MuRoSim, a multi-robot simulation for lidar-based navigation specifically designed for DRL applications. Due to its high level of abstraction, complete implementation in C++, and rigorous thread pool utilization, MuRoSim achieves high computational performance. We apply MuRoSim for training navigation policies for omnidirectional mobile robots equipped with lidar sensors using DRL. Finally, we conduct extensive Sim-to-Real experiments to confirm the realism of the simulator, by deploying the learned policy for dynamic navigation with up to six robots in numerous of real-world experiments."
STARK: A Unified Framework for Strongly Coupled Simulation of Rigid and Deformable Bodies with Frictional Contact,"José Antonio Fernández-fernández, Ralph Lange, Stefan Laible, Kai Oliver Arras, Jan Bender","RWTH Aachen University,Robert Bosch GmbH,University of Tuebingen,University of Stuttgart",Simulation and Animation,"The use of simulation in robotics is increasingly widespread for the purpose of testing, synthetic data generation and skill learning. A relevant aspect of simulation for a variety of robot applications is physics-based simulation of robot-object interactions. This involves the challenge of accurately modeling and implementing different mechanical systems such as rigid and deformable bodies as well as their interactions via constraints, contact or friction. Most state-of-the-art physics engines commonly used in robotics either cannot couple deformable and rigid bodies in the same framework, lack important systems such as cloth or shells, have stability issues in complex friction-dominated setups or cannot robustly prevent penetrations. In this paper, we propose a framework for strongly coupled simulation of rigid and deformable bodies with focus on usability, stability, robustness and easy access to state-of-the-art deformation and frictional contact models. Our system uses the Finite Element Method (FEM) to model deformable solids, the Incremental Potential Contact (IPC) approach for frictional contact and a robust second order optimizer to ensure stable and penetration-free solutions to tight tolerances. It is a general purpose framework, not tied to a particular use case such as grasping or learning, it is written in C++ and comes with a Python interface. We demonstrate our systemâ€™s ability to reproduce complex real-world experiments where a mobile vacuum robot interacts with a towel on different floor types and towel geometries. Our system is able to reproduce 100% of the qualitative outcomes observed in the laboratory environment. The simulation pipeline, named Stark (the German word for strong, as in strong coupling) is made open-source."
Hydrodynamic Interactions in Schooling Fish: Prioritizing Real Fish Kinematics Over Travelling-Wavy Undulation,"Li-ming Chao, Liang Li","Max Planck Institute of Animal Behavior,Max-Planck Institute of Animal Behavior",Simulation and Animation,"Hydrodynamic interactions are crucial for understanding fish movement, particularly within the realm of robotic applications. Traditionally, many studies have favoured simplified travelling-wavy undulations derived from observed real fish kinematics. This approach often neglects higher-order undulations, thereby missing the subtleties of authentic fish movements. In this study, we utilised Computational Fluid Dynamics (CFD) to investigate the implications of using real fish kinematics in hydrodynamic interactions among schooling fish. We analysed two scenarios: one driven by real fish kinematics in spatiotemporal formations, and the other by travelling-wavy undulations inferred from the same real fish kinematics. Our results highlight the advantages of using real fish body kinematics for a more accurate representation of hydrodynamics in fish swimming. In contrast, the idealised travelling-wavy undulations tend to apply excessive force, displacing real fish more than expected. Additionally, the vortices and corresponding flow fields generated by real fish kinematics were found to be more stable than those arising from simplified travelling-wavy undulations. Our study underscores the significance of integrating real fish kinematics into robotic fish design and hydrodynamic studies in schooling fish."
OmniLRS: A Photorealistic Simulator for Lunar Robotics,"Antoine Richard, Junnosuke Kamohara, Kentaro Uno, Shreya Santra, Dave Van Der Meer, Miguel Olivares-Mendez, Kazuya Yoshida","University of Luxembourg,Tohoku University,Interdisciplinary Centre for Security, Reliability and Trust - U",Simulation and Animation,"Developing algorithms for extra-terrestrial robotic exploration has always been challenging. Along with the complexity associated with these environments, one of the main issues remains the evaluation of said algorithms. With the regained interest in lunar exploration, there is also a demand for quality simulators that will enable the development of lunar robots. In this paper, we propose Omniverse Lunar Robotic-Sim (OmniLRS) that is a photorealistic Lunar simulator based on Nvidia's robotic simulator. This simulation provides fast procedural environment generation, multi-robot capabilities, along with synthetic data pipeline for machine-learning applications. It comes with ROS1 and ROS2 bindings to control not only the robots, but also the environments. This work also performs sim-to-real rock instance segmentation to show the effectiveness of our simulator for image-based perception. Trained on our synthetic data, a yolov8 model achieves performance close to a model trained on real-world data, with 5% performance gap. When finetuned with real data, the model achieves 14% higher average precision than the model trained on real-world data, demonstrating our simulator's photorealism. The code is fully open-source, accessible here: https://github.com/AntoineRichard/LunarSim, and comes with demonstrations."
SceneControl: Diffusion for Controllable Traffic Scene Generation,"Jack Lu, Kelvin Wong, Chris Zhang, Shun Da Suo, Raquel Urtasun","University of Waterloo,University of Toronto,Waabi / University of Toronto,Waabi",Simulation and Animation,"We consider the task of traffic scene generation. A common approach in the self-driving industry is to use manual creation to generate scenes with specific characteristics and automatic generation to generate canonical scenes at scale. However, manual creation is not scalable, and automatic generation typically use rules-based algorithms that lack realism. In this paper, we propose SceneControl, a framework for controllable traffic scene generation. To capture the complexity of real traffic, SceneControl learns an expressive diffusion model from data. Then, using guided sampling, we can flexibly control the sampling process to generate scenes that exhibit desired characteristics. Our experiments show that SceneControl achieves greater realism and controllability than the existing state-of-the-art. We also illustrate how SceneControl can be used as a tool for interactive traffic scene generation."
Jade: A Differentiable Physics Engine for Articulated Rigid Bodies with Intersection-Free Frictional Contact,"Gang Yang, Siyuan Luo, Yunhai Feng, Zhixin Sun, Chenrui Tie, Lin Shao","National University of Singapore,Xi'an Jiaotong University,University of California, San Diego,Nanjing University,Peking University",Simulation and Animation,"We present Jade, a differentiable physics engine for articulated rigid bodies. Jade models contacts as the Linear Complementarity Problem (LCP). Compared to existing differentiable simulations, Adams offers features including intersection-free collision simulation and stable LCP solutions for multiple frictional contacts. We use continuous collision detection to detect the time of impact and adopt the backtracking strategy to prevent intersection between bodies with complex geometry shapes. We derive the gradient calculation to ensure the whole simulation process is differentiable under the backtracking mechanism. We modify the popular Dantzig algorithm to get valid solutions under multiple frictional contacts. We conduct extensive experiments to demonstrate the effectiveness of our differentiable physics simulation over a variety of contact-rich tasks. Supplemental materials and videos are available on our project webpage."
Simulation Modeling of Highly Dynamic Omnidirectional Mobile Robots Based on Real-World Data,"Marvin Wiedemann, Ossama Ahmed, Anna Dieckhoefer, Renato Gasoto, Sören Kerner","Fraunhofer Institute for Material Flow and Logistics,Frauhofer IML,Worcester Polytechnic Institute, NVIDIA,Fraunhofer IML",Simulation and Animation,"Simulation is a key technology in robotics as it enables the generation of environmental data and testing scenarios for development and maintenance purposes. However, simulations are an imperfect representation of the real world and the so-called sim-to-real gap between simulation and reality hinders the deployment of virtual developed solutions without additional effort. Modeling complex systems like highly dynamic and holonomic mobile robots presents additional complexities in simulation. This paper addresses these challenges through a case study on creating a model for a highly dynamic logistics robot. The study breaks down the modeling of the whole system down to creating appropriate colliders for the rollers of a Mecanum wheel. Additionally, the impact of significant physics parameters is presented. To bridge the sim-to-real gap, a pipeline is developed that utilizes a Motion Capture system to compare the behavior of a real robot with its simulated counterpart across various motions. By leveraging expert knowledge gained from the real-world data, the simulation model is manually tuned to replicate complex system behaviors, such as sliding effects."
Sim-To-Real Learning for Humanoid Box Loco-Manipulation,"Jeremy Dao, Helei Duan, Alan Fern",Oregon State University,Machine Learning for Robot Control II,"In this work we propose a learning-based approach to box loco-manipulation for a humanoid robot. This is a particularly challenging problem due to the need for whole-body coordination in order to lift boxes of varying weight, position, and orientation while maintaining balance. To address this challenge, we present a sim-to-real reinforcement learning approach for training general box pickup and carrying skills for the bipedal robot Digit. Our reward functions are designed to produce the desired interactions with the box while also valuing balance and gait quality. We combine the learned skills into a full system for box loco-manipulation to achieve the task of moving boxes from one table to another with a variety of sizes, weights, and initial configurations. In addition to quantitative simulation results, we demonstrate successful sim-to-real transfer on the humanoid robot Digit. To our knowledge this is the first demonstration of a learned controller for such a task on real world hardware."
Hamiltonian Dynamics Learning from Point Cloud Observations for Nonholonomic Mobile Robot Control,"Abdullah Altawaitan, Jason Stanley, Sambaran Ghosal, Thai Duong, Nikolay A. Atanasov","University of California San Diego,University of California, San Diego",Machine Learning for Robot Control II,"Reliable autonomous navigation requires adapting the control policy of a mobile robot in response to dynamics changes in different operational conditions. Hand-designed dynamics models may struggle to capture model variations due to a limited set of parameters. Data-driven dynamics learning approaches offer higher model capacity and better generalization but require large amounts of state-labeled data. This paper develops an approach for learning robot dynamics directly from point-cloud observations, removing the need and associated errors of state estimation, while embedding Hamiltonian structure in the dynamics model to improve data efficiency. We design an observation-space loss that relates motion prediction from the dynamics model with motion prediction from point-cloud registration to train a Hamiltonian neural ordinary differential equation. The learned Hamiltonian model enables the design of an energy-shaping model-based tracking controller for rigid-body robots. We demonstrate dynamics learning and tracking control on a real nonholonomic wheeled robot."
Deep Model Predictive Optimization,"Jacob Sacks, Rwik Rana, Kevin Huang, Alexander Spitzer, Guanya Shi, Byron Boots","University of Washington,Carnegie Mellon University",Machine Learning for Robot Control II,"A major challenge in robotics is to design robust policies which enable complex and agile behaviors in the real world. On one end of the spectrum, we have model-free reinforcement learning (MFRL), which is incredibly flexible and general but often results in brittle policies. In contrast, model predictive control (MPC) continually re-plans at each time step to remain robust to perturbations and model inaccuracies. However, despite its real-world successes, MPC often under-performs the optimal strategy. This is due to model quality, myopic behavior from short planning horizons, and approximations due to computational constraints. And even with a perfect model and enough compute, MPC can get stuck in bad local optima, depending heavily on the quality of the optimization algorithm. To this end, we propose Deep Model Predictive Optimization (DMPO), which learns the inner-loop of an MPC optimization algorithm directly via experience, specifically tailored to the needs of the control problem. We evaluate DMPO on a real quadrotor agile trajectory tracking task, on which it improves performance over a baseline MPC algorithm for a given computational budget. It can outperform the best MPC algorithm by up to 27% with fewer samples and an end-to-end policy trained with MFRL by 19%. Moreover, because DMPO requires fewer samples, it can also achieve these benefits with 4.3X less memory. When we subject the quadrotor to turbulent wind fields with an attached drag plate, DMPO can adapt zero-shot while still outperforming all baselines. Additional results can be found at https://tinyurl.com/mr2ywmnw."
Pay Attention to How You Drive: Safe and Adaptive Model-Based Reinforcement Learning for Off-Road Driving,"Sean J. Wang, Honghao Zhu, Aaron Johnson","Carnegie Mellon University,CMU",Machine Learning for Robot Control II,"Autonomous off-road driving is challenging as unsafe actions may lead to catastrophic damage. As such, developing controllers in simulation is often desirable. However, robot dynamics in unstructured off-road environments can be highly complex and difficult to simulate accurately. Domain randomization addresses this problem by randomizing simulation dynamics to train policies that are robust towards modeling errors. While these policies are robust across a range of dynamics, they are sub-optimal for any particular system dynamics. We introduce a novel model-based reinforcement learning approach that aims to balance robustness with adaptability. We train a System Identification Transformer (SIT) and an Adaptive Dynamics Model (ADM) under a variety of simulated dynamics. The SIT uses attention mechanisms to distill target system state-transition observations into a context vector, which provides an abstraction for the target dynamics. Conditioned on this, the ADM probabilistically models the system's dynamics. Online, we use a Risk-Aware Model Predictive Path Integral controller to safely control the robot under its current understanding of dynamics. We demonstrate in simulation and in the real world that this approach enables safer behaviors upon initialization and becomes less conservative (i.e. faster) as its understanding of the target system dynamics improves with more observations. In particular, our approach results in an approximately 41% improvement in lap-time over the non-adaptive baseline while remaining safe across different environments."
SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning,"Jianlan Luo, Zheyuan Hu, Charles Xu, You Liang Tan, Jacob Herman Berg, Archit Sharma, Stefan Schaal, Chelsea Finn, Abhishek Gupta, Sergey Levine","UC Berkeley,University of California, Berkeley,Georgia Institute of Technology,University of Washington,Stanford University,Google X",Machine Learning for Robot Control II,"Recent years have seen the development of many methods for robotic reinforcement learning (RL), some of which can even operate on complex image observations, run directly in the real world, and incorporate auxiliary data, such as demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is acknowledged among practitioners that, separately from the fundamental technical ideas, the particular implementation details of these algorithms are often just as important (if not moreso) for performance as the choice of algorithm that is actually used. We posit that a significant challenge to widespread adoption of robotic RL, as well as further development of robotic RL methods, is comparative inaccessibility of such methods. To address this challenge, we developed a carefully implemented library containing a sample efficient modern deep RL method, together with frameworks for computing rewards and resetting the environment, high-quality controllers for a few common robots, and a number of challenging example tasks. We provide this library as a resource for the community, describe its design choices, and present experimental results. Perhaps surprisingly, we find that our implementation can achieve very efficient learning, acquiring policies for PCB board assembly, cable routing, and object relocation in less than an hour of training per policy, comparing very favorably to state-of-the-art results reported for similar tasks in the literature. We hope that these promising results and our high-quality open-source implementation will provide a tool for the robotics community to study new developments in robotic RL. Our code and videos can be found at https://serl-robot.github.io."
Improving Out-Of-Distribution Generalization of Learned Dynamics by Learning Pseudometrics and Constraint Manifolds,"Yating Lin, Glen Chou, Dmitry Berenson","University of Michigan,MIT",Machine Learning for Robot Control II,"We propose a method for improving the prediction accuracy of learned robot dynamics models on out-of-distribution (OOD) states. We achieve this by leveraging two key sources of structure often present in robot dynamics: 1) sparsity, i.e., some components of the state may not affect the dynamics, and 2) physical limits on the set of possible motions, in the form of nonholonomic constraints. Crucially, we do not assume this structure is known a priori, and instead learn it from data. We use contrastive learning to obtain a distance pseudometric that uncovers the sparsity pattern in the dynamics, and use it to reduce the input space when learning the dynamics. We then learn the unknown constraint manifold by approximating the normal space of possible motions from the data, which we use to train a Gaussian process (GP) representation of the constraint manifold. We evaluate our approach on a physical differential-drive robot and a simulated quadrotor, showing improved prediction accuracy on OOD data relative to baselines."
Robotic Offline RL from Internet Videos Via Value-Function Learning,"Chethan Bhateja, Derek Guo, Dibya Ghosh, Anikait Singh, Manan Tomar, Quan Vuong, Yevgen Chebotar, Sergey Levine, Aviral Kumar","Stanford University,UC Berkeley,University of Alberta,UC San Diego,Google,CMU",Machine Learning for Robot Control II,"Pre-training on Internet data has proven to be a key ingredient for broad generalization in many modern ML systems. What would it take to enable such capabilities in robotic reinforcement learning (RL)? Offline RL methods, which learn from datasets of robot experience, offer one way to leverage prior data into the robotic learning pipeline. However, these methods have a ""type mismatch"" with video data (such as Ego4D), which are the largest prior datasets available for robotics, since video offers observation-only experience without the action or reward annotations needed for RL methods. In this paper, we develop a system for leveraging large-scale human video datasets in robotic offline RL, based entirely on learning value functions via temporal-difference learning. We show that value learning on video datasets learns representations that are more conducive to downstream robotic offline RL than other approaches for learning from video data. Our system, called V-PTR, combines the benefits of pre-training on video data with robotic offline RL approaches that train on diverse robot data, resulting in value functions and policies for manipulation tasks that perform better, act robustly, and generalize broadly. On several manipulation tasks on a real WidowX robot and in simulated settings, our framework produces policies that greatly improve over other prior methods. Our video and additional details can be found at https://dibyaghosh.com/vptr/."
Learning Manipulation of Steep Granular Slopes for Fast Mini Rover Turning,"Deniz Kerimoglu, Daniel Soto, Malone Lincoln Hemsley, Joseph Brunner, Sehoon Ha, Tingnan Zhang, Daniel Goldman","Georgia Institute of Technology,Morehouse College,Google",Machine Learning for Robot Control II,"Future planetary exploration missions will require reaching challenging regions such as craters and steep slopes. Such regions are ubiquitous and present science-rich targets potentially containing information regarding the planetâ€™s internal structure. Steep slopes consisting of low-cohesion regolith are prone to flow downward under small disturbances, making it challenging for autonomous rovers to traverse. Moreover, the navigation trajectories of rovers are heavily limited by the terrain topology and future systems will need to maneuver on flowable surfaces without getting trapped, allowing them to further expand their reach and increase mission efficiency. In this work, we used a robophysical rover model and performed maneuvering experiments on a steep granular slope of poppy seeds to explore the rover's turning capabilities. The rover is capable of lifting, sweeping, and spinning its wheels, allowing it to execute leg-like gait patterns. The high-dimensional actuation capabilities of the rover facilitate effective manipulation of the underlying granular surface. We used Bayesian Optimization (BO) to gain insight into successful turning gaits in high dimensional search space and found strategies such as differential wheel spinning and pivoting around a single sweeping wheel. We then used these insights to further fine-tune the turning gait, enabling the rover to turn nearly 90 degrees at just above 4 seconds with minimal downhill slip. Combining gait optimization and human-tuning approaches, we found that fast turning is empowered by creating anisotropic torques with the sweeping wheel."
Safe Reinforcement Learning with Dead-Ends Avoidance and Recovery,"xiao zhang, Hai Zhang, Hongtu Zhou, Chang Huang, Di Zhang, Chen Ye, Junjiao Zhao","Tongji University,TongJi University,Tongji university",Machine Learning for Robot Control II,"Safety is one of the main challenges in applying reinforcement learning to realistic environmental tasks. To ensure safety during and after training process, existing methods tend to adopt overly conservative policy to avoid unsafe situations. However, overly conservative policy severely hinders the exploration, and makes the algorithms substantially less rewarding. In this paper, we propose a method to construct a boundary that discriminates safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pretrained on an offline dataset, in which the safety critic evaluates upper bound of safety in each state as awareness of environmental safety for the agent. During online training, a behavior correction mechanism is adopted, ensuring the agent to interact with the environment using safe actions only. Finally, experiments of continuous control tasks demonstrate that our approach has better task performance with less safety violations than state-of-the-art algorithms."
Exploring the Needle Tip Interaction Force with Retinal Tissue Deformation in Vitreoretinal Surgery,"Simon Marc Pannek, Shervin Dehghani, Michael Sommersperger, Peiyao Zhang, Peter Gehlbach, M. Ali Nasseri, Iulian Iordachita, Nassir Navab","TUM,Technical University of Munich,Johns Hopkins University,Johns Hopkins Medical Institute,Technische Universitaet Muenchen,TU Munich",Data Sets for Robotic Vision II,"Recent advancements in age-related macular degeneration treatments necessitate precision delivery into the subretinal space, emphasizing minimally invasive procedures targeting the retinal pigment epithelium (RPE)-Bruchâ€™s membrane complex without causing trauma. Even for skilled surgeons, the inherent hand tremors during manual surgery can jeopardize the safety of these critical interventions. This has fostered the evolution of robotic systems designed to prevent such tremors. These robots are enhanced by FBG sensors, which sense the small force interactions between the surgical instruments and retinal tissue. To enable the community to design algorithms taking advantage of such force feedback data, this paper focuses on the need to provide a specialized dataset, integrating optical coherence tomography (OCT) imaging together with the aforementioned force data. We introduce a unique dataset, integrating force sensing data synchronized with OCT B-scan images, derived from a sophisticated setup involving robotic assistance and OCT integrated microscopes. Furthermore, we present a neural network model for image-based force estimation to demonstrate the datasetâ€™s applicability."
PanNote: An Automatic Tool for Panoramic Image Annotation of People's Positions,"Alberto Bacchin, Leonardo Barcellona, Sepideh Shamsizadeh, Emilio Olivastri, Alberto Pretto, Emanuele Menegatti","University of Padua,University of Padova,The University of Padua",Data Sets for Robotic Vision II,"Panoramic cameras offer a 4Ï€ steradian field of view, which is desirable for tasks like people detection and tracking since nobody can exit the field of view. Despite the recent diffusion of low-cost panoramic cameras, their usage in robotics remains constrained by the limited availability of datasets featuring annotations in the robot space, including people's 2D or 3D positions. To tackle this issue, we introduce PanNote, an automatic annotation tool for people's positions in panoramic videos. Our tool is designed to be cost-effective and straightforward to use without requiring human intervention during the labeling process and enabling the training of machine learning models with low effort. The proposed method introduces a calibration model and a data association algorithm to fuse data from panoramic images and 2D LiDAR readings. We validate the capabilities of PanNote by collecting a real-world dataset. On these data, we compared manual labels, automatic labels and the predictions of a baseline deep neural network. Results clearly show the advantage of using our method, with a 15-fold speed up in labeling time and a considerable gain in performance while training deep neural models on automatically labelled data."
A Multimodal Handover Failure Detection Dataset and Baselines,"Santosh Thoduka, Nico Hochgeschwender, Juergen Gall, Paul G. Plöger","Hochschule Bonn-Rhein-Sieg,University of Bremen,University of Bonn,Hochschule Bonn Rhein Sieg",Data Sets for Robotic Vision II,"An object handover between a robot and a human is a coordinated action which is prone to failure for reasons such as miscommunication, incorrect actions and unexpected object properties. Existing works on handover failure detection and prevention focus on preventing failures due to object slip or external disturbances. However, there is a lack of datasets and evaluation methods that consider unpreventable failures caused by the human participant. To address this deficit, we present the multimodal Handover Failure Detection dataset, which consists of failures induced by the human participant, such as ignoring the robot or not releasing the object. We also present two baseline methods for handover failure detection: (i) a video classification method using 3D CNNs and (ii) a temporal action segmentation approach which jointly classifies the human action, robot action and overall outcome of the action. The results show that video is an important modality, but using force-torque data and gripper position help improve failure detection and action segmentation accuracy."
Introducing CEA-IMSOLD: An Industrial Multi-Scale Object Localization Dataset,"Boris Meden, Emanuel Pablo Vega, Fabrice Mayran De Chamisso, Steve Bourgeois","Université Paris Saclay, CEA, LIST, F-,,,,, Palaiseau, France.,CEA, LIST, F-,,,,, Gif-sur-Yvette Cedex,CEA LIST",Data Sets for Robotic Vision II,"We introduce the CEA Industrial Multi-Scale Object Localization Dataset (CEA-IMSOLD), a new BOP format dataset for 6-DoF object localization, crucial for robotics. This dataset aims to evaluate the current localization methods with respect to a new difficulty: large variations in observation distance and, consequently, large variations in image appearance. Compared to the other publicly available datasets, our dataset provides both images with objects small and completely visible in the image, and images where objects are observed close enough so they appear larger than the field of view of the camera. We also propose to consider the observation distance in the evaluation process and introduce new metrics to do so. Finally, our dataset contains a large variety of industrial objects, from small and simple objects such as bolts to sizable and complex ones such as large car parts. We provide baseline results and the dataset is made publicly available to support the community at https://cea-list.github.io/CEA-IMSOLD/."
PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion,"Yuxiang Yan, Boda Liu, Jianfei Ai, Qinbu Li, Ru Wan, Jian Pu","Fudan University,Moo Auto Intelligence and Telematics Information Technology Comp,MOGO,Mogo ai",Data Sets for Robotic Vision II,"Semantic Scene Completion (SSC) aims to jointly generate space occupancies and semantic labels for complex 3D scenes. Most existing SSC models focus on volumetric representations, which are memory-inefficient for large outdoor spaces. Point clouds provide a lightweight alternative but existing benchmarks lack outdoor point cloud scenes with semantic labels. To address this, we introduce PointSSC, the first cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion. These scenes exhibit long-range perception and minimal occlusion. We develop an automated annotation pipeline leveraging Semantic Segment Anything to efficiently assign semantics. To benchmark progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for global and local feature extraction and a Completion and Segmentation Cooperative Module for joint completion and segmentation. PointSSC provides a challenging testbed to drive advances in semantic point cloud completion for real-world navigation. The code and datasets are available at https://github.com/yyxssm/PointSSC."
Close the Sim2real Gap Via Physically-Based Structured Light Synthetic Data Simulation,"Kaixin Bai, Lei Zhang, Zhaopeng Chen, Fang Wan, Jianwei Zhang","University of Hamburg,Southern University of Science and Technology",Data Sets for Robotic Vision II,"Despite the substantial progress in deep learning, its adoption in industrial robotics projects remains limited, primarily due to challenges in data acquisition and labeling. Previous sim2real approaches using domain randomization re- quire extensive scene and model optimization. To address these issues, we introduce an innovative physically-based structured light simulation system, generating both RGB and physically realistic depth images, surpassing previous dataset generation tools. We create an RGBD dataset tailored for robotic industrial grasping scenarios and evaluate it across various tasks, including object detection, instance segmentation, and embedding sim2real visual perception in industrial robotic grasping. By reducing the sim2real gap and enhancing deep learning training, we facilitate the application of deep learning models in industrial settings. Project details are available at https://baikaixin-github.io/structured light 3D synthesizer/."
Interacting Objects: A Dataset of Object-Object Interactions for Richer Dynamic Scene Representations,"Asim Unmesh, Rahul Jain, Jingyu Shi, V. K. Chaithanya Manam, Hyung-gun Chi, Subramanian Chidambaram, Alexander Quinn, Karthik Ramani",Purdue University,Data Sets for Robotic Vision II,"Dynamic environments in factories, surgical robotics, and warehouses increasingly involve humans, ma- chines, robots, and various other objects such as tools, fixtures, conveyors, and assemblies. In these environments, numerous interactions occur not just between humans and objects but also between objects themselves. However, current scene-graph datasets predominantly focus on human-object interactions (HOI) and overlook object-object interactions (OOIs) despite the necessity of OOIs in effectively representing dynamic environments. This oversight creates a significant gap in the coverage of interactive elements in dynamic scenes. We address this gap by proposing, to the best of our knowledge, the first dataset* annotating for OOIs in dynamic scenes. To model OOIs, we establish a classification taxonomy for spatio- temporal interactions. We use our taxonomy to annotate OOIs in video clips of dynamic scenes. Then, we introduce spatio- temporal OOI classification task which aims at identifying interaction categories between two given objects in a video clip. Further, we benchmark our dataset for the spatio-temporal OOI classification task by adopting state-of-the-art approaches from related areas of Human-Object Interaction Classification, Visual Relationship Classification, and Scene-Graph Gener- ation. Additionally, we utilize our dataset to examine the effectiveness of OOI and HOI-based features in the context of Action Recognition. Notably, our experimental results show th"
RTS-GT: Robotic Total Stations Ground Truthing Dataset,"Maxime Vaidis, Mohsen Hassanzadeh Shahraji, Effie Daum, William Dubois, Philippe Giguere, Francois Pomerleau",Université Laval,Data Sets for Robotic Vision II,"Numerous datasets and benchmarks exist to assess and compare Simultaneous Localization and Mapping (SLAM) algorithms. Nevertheless, their precision must follow the rate at which SLAM algorithms improved in recent years. Moreover, current datasets fall short of comprehensive data-collection protocol for reproducibility and the evaluation of the precision or accuracy of the recorded trajectories. With this objective in mind, we proposed the Robotic Total Stations Ground Truthing dataset (RTS-GT) dataset to support localization research with the generation of six-Degrees Of Freedom (DOF) ground truth trajectories. This novel dataset includes six-DOF ground truth trajectories generated using a system of three Robotic Total Stations (RTSs) tracking moving robotic platforms. Furthermore, we compare the performance of the RTS-based system to a Global Navigation Satellite System (GNSS)-based setup. The dataset comprises around sixty experiments conducted in various conditions over a period of 17 months, and encompasses over 49 kilometers of trajectories, making it the most extensive dataset of RTS-based measurements to date. Additionally, we provide the precision of all poses for each experiment, a feature not found in the current state-of-the-art datasets. Our results demonstrate that RTSs provide measurements that are 22 times more stable than GNSS in various environmental settings, making them a valuable resource for SLAM benchmark development."
RaSim: A Range-Aware High-Fidelity RGB-D Data Simulation Pipeline for Real-World Applications,"Xingyu Liu, Chenyangguang Zhang, Gu Wang, Ruida Zhang, Xiangyang Ji",Tsinghua University,Data Sets for Robotic Vision II,"In robotic vision, a de-facto paradigm is to learn in simulated environments and then transfer to real-world applications, which poses an essential challenge in bridging the sim-to-real domain gap. While mainstream works tackle this problem in the RGB domain, we focus on depth data synthesis and develop a Range-aware RGB-D data Simulation pipeline (RaSim). In particular, high-fidelity depth data is generated by imitating the imaging principle of real-world sensors. A range-aware rendering strategy is further introduced to enrich data diversity. Extensive experiments show that models trained with RaSim can be directly applied to real-world scenarios without any finetuning and excel at downstream RGB-D perception tasks. Data and code are available at https://github.com/shanice-l/RaSim."
When Prolog Meets Generative Models: A New Approach for Managing Knowledge and Planning in Robotic Applications,"Enrico Saccon, Ahmet Tikna, Davide De Martini, Edoardo Lamon, Luigi Palopoli, Marco Roveri","University of Trento,Università degli Studi di Trento",Task and Motion Planning II,"In this paper, we propose a robot oriented knowledge representation system based on the use of the Prolog language. Our framework hinges on a special organisation of knowledge base that enables: 1) its efficient population from natural language texts using semi-automated procedures based on Large Language Models (LLMs); 2) the seamless generation of temporal parallel plans for multi-robot systems through a sequence of transformations; 3) the automated translation of the plan into an executable formalism. The framework is supported by a set of open source tools and its functionality is shown with a realistic application."
HAPFI: History-Aware Planning Based on Fused Information,"Sujin Jeon, Suyeon Shin, Byoung-Tak Zhang",Seoul National University,Task and Motion Planning II,"Embodied Instruction Following (EIF) is a task of planning a long sequence of sub-goals given high-level natural language instructions, such as ''Rinse a slice of lettuce and place on the white table next to the fork''. To successfully execute these long-term horizon tasks, we argue that an agent must consider its past, i.e., historical data, when making decisions in each step. Nevertheless, recent approaches in EIF often neglects the knowledge from historical data and also do not effectively utilize information across the modalities. To this end, we propose History-Aware Planning based on Fused Information(HAPFI), effectively leveraging the historical data from diverse modalities that agents collect while interacting with the environment. Specifically, HAPFI integrates multiple modalities, including historical RGB observations, bounding boxes, sub-goals, and high-level instructions, by effectively fusing modalities via our Mutually Attentive Fusion method. Through experiments with diverse comparisons, we show that an agent utilizing historical multi-modal information surpasses all the compared methods that neglect the historical data in terms of action planning capability, enabling the generation of well-informed action plans for the next step. Moreover, we provided qualitative evidence highlighting the significance of leveraging historical multi-modal data, particularly in scenarios where the agent encounters intermediate failures, showcasing its robust re-planning capabilities."
Non-Axiomatic Reasoning for an Autonomous Mobile Robot,"Patrick Hammer, Peter Isaev, Lei Feng, Robert Johansson, Jana Tumova","KTH Royal Institute of Technology,Temple University,Stockholm University",Task and Motion Planning II,"We present the integration of a Non-Axiomatic Reasoning System (NARS) with mobile robots for planning and decision making. NARS enables robots to effectively handle uncertainty in real-time with complete sensor and actuator integration, thereby ensuring adaptability to evolving scenarios. We discuss essential parts of the logic, the architecture and working principles of NARS, and the integration of NARS as a ROS node. A case study is provided demonstrating the system's proficiency to carry out a garbage collection task in an open-air environment by operating a mobile robot with manipulator arm, and we demonstrate its ability to learn about the place-dependent accumulation of garbage items. Case study also reveals that our approach performs more effectively on the overall task than the Belief-Desire-Intention model we compared with."
Asynchronous Task Plan Refinement for Multi-Robot Task and Motion Planning,"Yoonchang Sung, Rahul Shome, Peter Stone","The University of Texas at Austin,The Australian National University,University of Texas at Austin",Task and Motion Planning II,"This paper explores general multi-robot task and motion planning, where multiple robots in close proximity manipulate objects while satisfying constraints and a given goal. In particular, we formulate the plan refinement problemâ€”which, given a task plan, finds valid assignments of variables corresponding to solution trajectoriesâ€”as a hybrid constraint satisfaction problem. The proposed algorithm follows several design principles that yield the following features: (1) efficient solution finding due to sequential heuristics and implicit time and roadmap representations, and (2) maximized feasible solution space obtained by introducing minimally necessary coordination-induced constraints and not relying on prevalent simplifications that exist in the literature. The evaluation results demonstrate the planning efficiency of the proposed algorithm, outperforming the synchronous approach in terms of makespan."
Optimal Planning for Timed Partial Order Specifications,"Kandai Watanabe, Georgios Fainekos, Bardh Hoxha, Morteza Lahijanian, Hideki Okamoto, Sriram Sankaranarayanan","University of Colorado Boulder,Toyota NA-R&D,Southern Illinois University,Toyota Motor North America,University of Colorado, Boulder",Task and Motion Planning II,"This paper addresses the challenge of planning a sequence of tasks to be performed by multiple robots while minimizing the overall completion time subject to timing and precedence constraints. Our approach uses the Timed Partial Orders (TPO) model to specify these constraints. We translate this problem into a Traveling Salesman Problem (TSP) variant with timing and precedent constraints, and we solve it as a Mixed Integer Linear Programming (MILP) problem. Our contributions include a general planning framework for TPO specifications, a MILP formulation accommodating time windows and precedent constraints, its extension to multi-robot scenarios, and a method to quantify plan robustness. We demonstrate our framework on several case studies, including an aircraft turnaround task involving three Jackal robots, highlighting the approach's potential applicability to important real-world problems. Our benchmark results show that our MILP method outperforms state-of-the-art open-source TSP solvers OR-Tools."
On the Convergence of a Closed-Loop Inverse Kinematics Solver with Time-Varying Task Functions,"Mario Fiore, Ciro Natale","Università degli Studi della Campania ""Luigi Vanvitelli""",Task and Motion Planning II,
PROTAMP-RRT: A Probabilistic Integrated Task and Motion Planner Based on RRT,"Alessio Saccuti, Riccardo Monica, Jacopo Aleotti",University of Parma,Task and Motion Planning II,"Solving complex robot manipulation tasks requires a Task and Motion Planner (TAMP) that searches for a sequence of symbolic actions, i.e. a task plan, and also computes collision-free motion paths. As the task planner and the motion planner are closely interconnected TAMP is considered a challenging problem. In this paper, a Probabilistic Integrated Task and Motion Planner (PROTAMP-RRT) is presented. The proposed method is based on a unified Rapidly-exploring Random Tree (RRT) that operates on both the geometric space and the symbolic space. The RRT is guided by the task plan and it is enhanced with a probabilistic model that estimates the probability of sampling a new robot configuration towards the next sub-goal of the task plan. When the RRT is extended, the probabilistic model is updated alongside. The probabilistic model is used to generate a new task plan if the feasibility of the previous one is unlikely. The performance of PROTAMP-RRT was assessed in simulated pick-and-place tasks, and it was compared against state-of-the-art approaches TM-RRT and Planet, showing better performance."
Intrinsic Contact Sensing and Object Perception of an Adaptive Fin-Ray Gripper Integrating Compact Deflection Sensors,"Genliang Chen, Shujie Tang, Shaoqiu Xu, Tong Guan, Yuanhao Xun, Zhuang Zhang, Hao Wang, Zhongqin Lin","Shanghai Jiao Tong University,Westlake University,SJTU",Contact Modeling,"Owing to their tremendous adaptability to free-form objects, soft grippers with fin-ray structures have a wide range of applications. However, kinetostatic analysis and contact sensing for such grippers are still challenging due to large structural deformations. In this paper, a model-based method for intrinsic contact sensing, object perception, and interactive manipulation, is proposed for these adaptive grippers. The contributions arise from the integration of compact deflection sensors, that are particularly fabricated for large deformations of flexible beams. Using a discretization-based approach, the contact condition can be identified via the local deformations from the deflection sensors. Prototypes are developed using simple materials and manufacturing methods, on which various validation experiments are conducted. Based on contact sensing, the developed adaptive gripper can perceive the boundary geometry and structural compliance of unstructured objects. Moreover, sensor-based feedback control can be accomplished to perform interactive manipulation, in which the contact force between the finger and object can be regulated precisely (5% RMS error) in realtime."
Incipient Slip-Based Rotation Measurement Via Visuotactile Sensing During In-Hand Object Pivoting,"Mingxuan Li, Yen Hang Zhou, Tiemin Li, Yao Jiang","Tsinghua University,Tsinghua university",Contact Modeling,"In typical in-hand manipulation tasks represented by object pivoting, the real-time perception of rotational slippage has been proven beneficial for improving the dexterity and stability of robotic hands. An effective strategy is to obtain the contact properties for measuring rotation angle through visuotactile sensing. However, existing methods for rotation estimation did not consider the impact of the incipient slip during the pivoting process, which introduces measurement errors and makes it hard to determine the boundary between stable contact and macro slip. This paper describes a generalized 2-d contact model under pivoting, and proposes a rotation measurement method based on the line-features in the stick region. The proposed method was applied to the Tac3D vision-based tactile sensors using continuous marker patterns. Experiments show that the rotation measurement system could achieve an average static measurement error of 0.17Â°Â±0.15Â° and an average dynamic measurement error of 1.34Â°Â±0.48Â°. Besides, the proposed method requires no training data and can achieve real-time sensing during the in-hand object pivoting."
Leveraging Compliant Tactile Perception for Haptic Blind Surface Reconstruction,"Laurent Yves Emile Ramos Cheret, Vinicius Prado Da Fonseca, Thiago Eustaquio Alves De Oliveira","Lakehead University,Memorial University of Newfoundland",Contact Modeling,"Non-flat surfaces pose difficulties for robots operating in unstructured environments. Reconstructions of uneven surfaces may only be partially possible due to non-compliant end-effectors and limitations on vision systems such as transparency, reflections, and occlusions. This study achieves blind surface reconstruction by harnessing the robotic manipulator's kinematic data and a compliant tactile sensing module, which incorporates inertial, magnetic, and pressure sensors. The module's flexibility enables us to estimate contact positions and surface normals by analyzing its deformation during interactions with unknown objects. While previous works collect only positional information, we include the local normals in a geometrical approach to estimate curvatures between adjacent contact points. These parameters then guide a spline-based patch generation, which allows us to recreate larger surfaces without an increase in complexity while reducing the time-consuming step of probing the surface. Experimental validation demonstrates that this approach outperforms an off-the-shelf vision system in estimation accuracy. Moreover, this compliant haptic method works effectively even when the manipulator's approach angle is not aligned with the surface normals, which is ideal for unknown non-flat surfaces."
Differentiable Compliant Contact Primitives for Estimation and Model Predictive Control,"Kevin Haninger, Kangwagye Samuel, Filippo Rozzi, Sehoon Oh, Loris Roveda","Fraunhofer IPK,DGIST,Politecnico di Milano,SUPSI-IDSIA",Contact Modeling,"Control techniques like MPC can realize contact-rich manipulation which exploits dynamic information, maintaining friction limits and safety constraints. However, contact geometry and dynamics are required to be known. This information is often extracted from CAD, limiting scalability and the ability to handle tasks with varying geometry. To reduce the need for a priori models, we propose a framework for estimating contact models online based on torque and position measurements. To do this, compliant contact models are used, connected in parallel to model multi-point contact and constraints such as a hinge. They are parameterized to be differentiable with respect to all of their parameters (rest position, stiffness, contact location), allowing the coupled robot/environment dynamics to be linearized or efficiently used in gradient-based optimization. These models are then applied for: offline gradient-based parameter fitting, online estimation via an extended Kalman filter, and online gradient-based MPC. The proposed approach is validated on two robots, showing the efficacy of sensorless contact estimation and the effects of online estimation on MPC performance."
"TacShade: A New 3D-Printed Soft Optical Tactile Sensor Based on Light, Shadow and Grey Scale for Shape Reconstruction","Zhenyu Lu, Jialong Yang, Haoran Li, Yifan Li, Weiyong Si, Nathan Lepora, Chenguang Yang","Bristol Robotics Laboratory,South China University of Technology; Peng Cheng Laboratory,University of Bristol,University of Essex,University of Liverpool",Contact Modeling,"In this paper, we present the TacShade: a newly designed 3D-printed soft optical tactile sensor. The sensor is developed for shape reconstruction under the inspiration of sketch drawing that uses the density of sketch lines to draw light and shadow, resulting in the creation of a 3D-view effect. TacShade, building upon the strengths of the TacTip, a single-camera tactile sensor of large in-depth deformation and being sensitive to edge and surface following, improves the structure in that the markers are distributed within the gap of papillae pins. Variations in light, dark and grey effects can be generated inside the sensor under the external contact interactions. The contours of the contacting objects are outlined by white markers, while the contact depth characteristics can be indirectly obtained from the distribution of black pins and white markers, creating a 2.5D visualization. Based on the imaging effect, we improve the Shape from Shading (SFS) algorithm to process tactile images, enabling a coarse but fast reconstruction for the contact objects. Two experiments are performed. The first verifies TacShadeâ€™s ability to reconstruct the shape of the contact objects through one image for object distinction to avoid the long-term deep learning process. The second experiment shows the shape reconstruction capability of TacShade for a large panel with ridged patterns based on the location of robots and image splicing technology."
Physics-Encoded Graph Neural Networks for Deformation Prediction under Contact,"Mahdi Saleh, Michael Sommersperger, Nassir Navab, Federico Tombari","Technical University Munich,Technical University of Munich,TU Munich,Technische Universität München",Contact Modeling,"In robotics, it's crucial to understand object deformation during tactile interactions. A precise understanding of deformation can elevate robotic simulations and have broad implications across different industries. We introduce a method using Physics-Encoded Graph Neural Networks (GNNs) for such predictions. Similar to robotic grasping and manipulation scenarios, we focus on modeling the dynamics between a rigid mesh contacting a deformable mesh under external forces. Our approach represents both the soft body and the rigid body within graph structures, where nodes hold the physical states of the meshes. We also incorporate cross-attention mechanisms to capture the interplay between the objects. By jointly learning geometry and physics, our model reconstructs consistent and detailed deformations. We've made our code and dataset public to advance research in robotic simulation and grasping."
Unwieldy Object Delivery with Nonholonomic Mobile Base: A Stable Pushing Approach,"Yujie Tang, Hai Zhu, Susan Potters, Martijn Wisse, Wei Pan","Delft University of Technology,Defense Innovation Institute,The University of Manchester",Contact Modeling,"This paper addresses the problem of pushing manipulation with nonholonomic mobile robots. Pushing is a fundamental skill that enables robots to move unwieldy objects that cannot be grasped. We propose a stable pushing method that maintains stiff contact between the robot and the object to avoid consuming repositioning actions. We prove that a line contact, rather than a single point contact, is necessary for nonholonomic robots to achieve stable pushing. We also show that the stable pushing constraint and the nonholonomic constraint of the robot can be simplified as a concise linear motion constraint. Then, the pushing planning problem can be formulated as a constrained optimization problem using nonlinear model predictive control (NMPC). According to the experiments, our NMPC-based planner outperforms a reactive pushing strategy in terms of efficiency, reducing the robotâ€™s travelled distance by 23.8% and time by 77.4%. Furthermore, our method requires four fewer hyperparameters and decision variables than the Linear Time-Varying (LTV) MPC approach, making it easier to implement. Real-world experiments are carried out to validate the proposed method with two differential-drive robots, Husky and Boxer, under different friction conditions."
Robotic Contact Juggling,"J. Zachary Woodruff, Kevin Lynch",Northwestern University,Contact Modeling,"In this article, we define ""robotic contact juggling"" to be the purposeful control of the motion of a 3-D smooth object as it rolls freely on a motion-controlled robot manipulator, or ""hand."" While specific examples of robotic contact juggling have been studied before, in this article, we provide the first general formulation and solution method for the case of an arbitrary smooth object in a single-point rolling contact on an arbitrary smooth hand. Our formulation splits the problem into four subproblems: deriving the second-order rolling kinematics; deriving the 3-D rolling dynamics; planning rolling motions that satisfy the rolling dynamics and achieve the desired goal; and stabilization of planned rolling trajectories. The theoretical results are demonstrated in 3-D simulations and 2-D experiments using feedback from a high-speed vision system."
Beyond Coulomb: Stochastic Friction Models for Practical Grasping and Manipulation,"Zixi Liu, Robert D. Howe",Harvard University,Contact Modeling,"Reliable grasping and manipulation in daily tasks and unstructured environments require accurate contact modeling and grasp stability estimation. One key component is the coefficient of friction, which is typically estimated in robotics applications using Coulomb's law of friction as a constant coefficient of friction from the literature, even though actual friction behavior is variable and depends on many factors. In this work, we conducted sliding experiments with robot fingers and a hand, and show that rubber friction varies strongly with normal force Fn and contact velocity v, and includes a significant stochastic component. We present a framework for modeling the coefficient of friction as a distribution rather than a single constant, and show how this distribution can be narrowed when given a prior on Fn or v. For a given distribution, the likelihood of slipping is a continuous function with respect to the tangential-to-normal force ratio, instead of a step function according to Coulomb's law. By modeling friction as a function of Fn and v, we demonstrate that friction parameters can be estimated using regression models from a single sliding stroke of the fingertip against the object surface, and that strokes that span a larger range of Fn-v space provide better friction estimates. These results can be applied to grasp control to enable a quantitative trade-off between the likelihood of slipping vs. grasp force levels, and to sliding manipulation planning by clarifying"
Learning Barrier-Certified Polynomial Dynamical Systems for Obstacle Avoidance with Robots,"Martin Schonger, Hugo Tadashi Kussaba, Lingyun Chen, Luis Felipe Cruz Figueredo, Abdalla Swikir, Aude G. Billard, Sami Haddadin","Technical University of Munich,Technical University of Munich (TUM),EPFL",Learning from Demonstration III,"Established techniques that enable robots to learn from demonstrations are based on learning a stable dynamical system (DS). To increase the robots' resilience to perturbations during tasks that involve static obstacle avoidance, we propose incorporating barrier certificates into an optimization problem to learn a stable and barrier-certified DS. Such optimization problem can be very complex or extremely conservative when the traditional linear parameter-varying formulation is used. Thus, different from previous approaches in the literature, we propose to use polynomial representations for DSs, which yields an optimization problem that can be tackled by sum-of-squares techniques. Finally, our approach can handle obstacle shapes that fall outside the scope of assumptions typically found in the literature concerning obstacle avoidance within the DS learning framework."
Domain Adaptation of Visual Policies with a Single Demonstration,"Weiyao Wang, Gregory Hager","The Johns Hopkins University,Johns Hopkins University",Learning from Demonstration III,"Deploying machine learning algorithms for robot tasks in real-world applications presents a core challenge: overcoming the domain gap between the training and the deployment environment. This is particularly difficult for visuomotor policies that utilize high-dimensional images as input, particularly when those images are generated via simulation. A common method to tackle this issue is through domain randomization, which aims to broaden the span of the training distribution to cover the test-time distribution. However, this approach is only effective when the domain randomization encompasses the actual shifts in the test-time distribution. We take a different approach, where we make use of a single demonstration (a prompt) to learn policy that adapts to the testing target environment. Our proposed framework, PromptAdapt, leverages the Transformer architecture's capacity to model sequential data to learn demonstration-conditioned visual policies, allowing for in-context adaptation to a target domain that is distinct from training. Our experiments in both simulation and real-world settings show that PromptAdapt is a strong domain-adapting policy that outperforms baseline methods by a large margin under a range of domain shifts, including variations in lighting, color, texture, and camera pose."
Learning Complex Motion Plans Using Neural ODEs with Safety and Stability Guarantees,"Farhad Nawaz Savvas Sadiq Ali, Tianyu Li, Nikolai Matni, Nadia Figueroa",University of Pennsylvania,Learning from Demonstration III,"We propose a Dynamical System (DS) approach to learn complex, possibly periodic motion plans from kinesthetic demonstrations using Neural Ordinary Differential Equations (NODE). To ensure reactivity and robustness to disturbances, we propose a novel approach that selects a target point at each time step for the robot to follow, by combining tools from control theory and the target trajectory generated by the learned NODE. A correction term to the NODE model is computed online by solving a quadratic program that guarantees stability and safety using control Lyapunov functions and control barrier functions, respectively. Our approach outperforms baseline DS learning techniques on the LASA handwriting dataset and complex periodic trajectories. It is also validated on the Franka Emika robot arm to produce stable motions for wiping and stirring tasks that do not have a single attractor, while being robust to perturbations and safe around humans and obstacles."
Learning a Stable Dynamic System with a Lyapunov Energy Function for Demonstratives Using Neural Networks,"Yu Zhang, Yongxiang Zou, Haoyu Zhang, Xiuze Xia, Long Cheng","University of Chinese Academy of Sciences,Institute of Automation, Chinese Academy of Sciences,Chinese Academy of Sciences",Learning from Demonstration III,"Autonomous Dynamic System (DS)-based algorithms hold a pivotal and foundational role in the field of Learning from Demonstration (LfD). Nevertheless, they confront the formidable challenge of striking a delicate balance between achieving precision in learning and ensuring the overall stability of the system. In response to this substantial challenge, this paper introduces a novel DS algorithm rooted in neural network technology. This algorithm not only possesses the capability to extract critical insights from demonstration data but also demonstrates the capacity to learn a candidate Lyapunov energy function that is consistent with the provided demonstrations. The model presented in this paper employs a simplistic neural network architecture that excels in fulfilling a dual objective: optimizing accuracy while simultaneously preserving global stability. To comprehensively evaluate the effectiveness of the proposed algorithm, rigorous assessments are conducted using the LASA dataset, further reinforced by empirical validation through a robotic experiment."
Learning a Flexible Neural Energy Function with a Unique Minimum for Globally Stable and Accurate Demonstration Learning,"Zhehao Jin, Weiyong Si, Andong Liu, Wen-an Zhang, Li Yu, Chenguang Yang","Zhejiang University of Technology,University of Essex,Zhejiang University of Technology, China,University of Liverpool",Learning from Demonstration III,"Learning a stable autonomous dynamic system (ADS) encoding human motion rules has been shown as an effective way for demonstration learning. However, the stability guarantee may sacrifice the demonstration learning accuracy. This article solves the issue by learning a stability certificate, represented by a neural energy function, on the demonstration set. We propose a Polar-like space analysis approach to derive parameter constraints to guarantee the unique-minimum property of the neural energy function, which is essential for it to be a cogent stability certificate. Then, the neural energy function is learned to capture the demonstration preferences via constrained optimization algorithms. With the learned neural energy function, a globally asymptotically stable ADS with predefined position constraint is further formulated. We also quantitatively analyze the generalization ability of the learned ADS by utilizing the substantial flexibility of the neural energy function. The effectiveness of the proposed approach is validated on the LASA data set and two representative robotic experiments."
Inverse Constraint Learning and Generalization by Transferable Reward Decomposition,"Jaehwi Jang, Minjae Song, Daehyung Park","Korea Advanced Institute of Science and Technology,KAIST,Korea Advanced Institute of Science and Technology, KAIST",Learning from Demonstration III,"We present the problem of inverse constraint learning (ICL), which recovers constraints from demonstrations to autonomously reproduce constrained skills in new scenarios. However, ICL suffers from an ill-posed nature, leading to inaccurate inference of constraints from demonstrations. To figure it out, we introduce a transferable constraint learning (TCL) algorithm that jointly infers a task-oriented reward and a task-agnostic constraint, enabling the generalization of learned skills. Our method TCL additively decomposes the overall reward into a task reward and its residual as soft constraints, maximizing policy divergence between task- and constraint-oriented policies to obtain a transferable constraint. Evaluating our method and five baselines in three simulated environments, we show TCL outperforms state-of-the-art IRL and ICL algorithms, achieving up to a 72% higher task-success rates with accurate decomposition compared to the next best approach in novel scenarios. Further, we demonstrate the robustness of TCL on two real-world robotic tasks."
Learning Robot Motion in a Cluttered Environment Using Unreliable Human Skeleton Data Collected by a Single RGB Camera,"Ryota Takamido, Jun Ota","Research into Artifacts, Center for Engineering (RACE), School o,The University of Tokyo",Learning from Demonstration III,"Current learning from demonstration (LfD) frameworks have difficulty dealing with an unreliable, limited number of demonstrations. To address this issue, we proposed a novel motion planning framework referred to as experience-driven random tree connect with human demonstration (ERTC-HD), which can facilitate the identification of valid motions in cluttered environments by only using human skeleton information extracted from a single red, green, and blue (RGB) camera. The point of this framework is to only extract the comprehensive features of human motion from unreliable demonstrations and use them as a rough estimate for solving complex planning problems instead of as a strict solution. During the process of ERTC-HD, robot motions generated from extracted comprehensive features of human motion are saved as a path experience and modified through the path adaptation process of an existing ERTC planner when transferring it to the new problem. The results of three simulation experiments revealed that the ERTC-HD could identify valid motion in cluttered environments within shorter time periods than other state-of-the-art planners even when using unreliable demonstration data collected by a single RGB camera. The reduction of the required accuracy of the original information resources can extend the range of applications of this LfD framework."
Robot Interaction Behavior Generation Based on Social Motion Forecasting for Human-Robot Interaction,"Esteve Valls Mascaro, Yashuai Yan, Dongheui Lee","Technische Universitat Wien,Vienna University of Technology,Technische Universität Wien (TU Wien)",Deep Learning III,"Integrating robots into populated environments is a complex challenge that requires an understanding of human social dynamics. In this work, we propose to model social motion forecasting in a shared human-robot representation space, which facilitates us to synthesize robot motions that interact with humans in social scenarios despite not observing any robot in the motion training. We develop a transformer-based architecture called ECHO, which operates in the aforementioned shared space to predict the future motions of the agents encountered in social scenarios. Contrary to prior works, we reformulate the social motion problem as the refinement of the predicted individual motions based on the surrounding agents, which facilitates the training while allowing for single-motion forecasting when only one human is in the scene. We evaluate our model in multi-person and human-robot motion forecasting tasks and obtain state-of-the-art performance by a large margin while being efficient and performing in real-time. Additionally, our qualitative results showcase the effectiveness of our approach in generating human-robot interaction behaviors that can be controlled via text commands."
SPCGC: Scalable Point Cloud Geometry Compression for Machine Vision,"Liang Xie, Wei Gao, Huiming Zheng, Ge Li","Peking University,Peking University Shenzhen Graduate School",Deep Learning III,"With the proliferation of sensor devices, the extensive utilization of three-dimensional data in multimedia continues to grow. Point clouds are widely adopted within this domain because they are one of the most intuitive representations of three-dimensional data. However, the substantial volume of point cloud data poses significant challenges for storage and transmission. Moreover, a considerable portion of the data loses its semantic information during transmission. Consequently, how can we ensure both the perceptual quality for the human and the performance of downstream tasks during the transmission? To address this issue, we propose a scalable point cloud geometry compression framework (SPCGC) for machine perception. This framework tackles the fidelity issues associated with point cloud compression and preserves more semantic information, enhancing the performance of machine vision tasks. Our solution consists of a base layer bitstream and an enhancement layer bitstream. The base layer bitstream contains geometry data, while the enhancement layer bitstream utilizes semantic-guided residual data. Additionally, we introduce two modules for extracting and coding residual features. And incorporate classification and segmentation losses from downstream tasks into the Rate-Distortion (RD) optimization. Our approach outperforms existing learning-based lossy point cloud coding methods through empirical validation in downstream tasks without sacrificing point cloud compression performance."
CppFlow: Generative Inverse Kinematics for Efficient and Robust Cartesian Path Planning,"Jeremy Morgan, David Millard, Gaurav Sukhatme",University of Southern California,Deep Learning III,"In this work we present CppFlow - a novel and performant planner for the Cartesian Path Planning problem, which finds valid trajectories up to 129x faster than current methods, while also succeeding on more difficult problems where others fail. At the core of the proposed algorithm is the use of a learned, generative Inverse Kinematics solver, which is able to efficiently produce promising entire candidate solution trajectories on the GPU. Precise, valid solutions are then found through classical approaches such as differentiable programming, global search, and optimization. In combining approaches from these two paradigms we get the best of both worlds - efficient approximate solutions from generative AI which are made exact using the guarantees of traditional planning and optimization. We evaluate our system against other state of the art methods on a set of established baselines as well as new ones introduced in this work and find that our method significantly outperforms others in terms of the time to find a valid solution and planning success rate, and performs comparably in terms of trajectory length over time. Additional results and an open source implementation is available at https://jstmn.github.io/cppflow-website"
Safe Deep Policy Adaptation,"Wenli Xiao, Tairan He, John Dolan, Guanya Shi",Carnegie Mellon University,Deep Learning III,"A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors and extra perturbations. Comprehensive experiments on (1) classic control problems (Inverted Pendulum), (2) simulation benchmarks (Safety Gym), and (3) a real-world agile robotics platform (RC Car) demonstrate great superiority of SafeDPA in both safety and task performance, over state-of-the-art baselines. Particularly, SafeDPA demonstrates notable generalizability, achieving a 300% increase in safety rate compared to the baselines, under unseen disturbances in real-world experiments."
Physics-Informed Neural Networks for Continuum Robots: Towards Fast Approximation of Static Cosserat Rod Theory,"Martin Bensch, Tim-David Job, Tim-Lukas Habich, Thomas Seel, Moritz Schappler","Leibniz University Hanover,Leibniz University Hannover,Leibniz Universität Hannover,Institute of Mechatronic Systems, Leibniz Universitaet Hannover",Deep Learning III,"Sophisticated models can accurately describe deformations of continuum robots while being computationally demanding, which limits their application. Especially when considering sampling-based path planning, the model has to be evaluated frequently, which can lead to substantially increased computation times. We present a new approach to compute the entire shape of a tendon-driven continuum robot by a physics-informed neural network (PINN). The underlying physics is modelled with the Cosserat rod theory and incorporated into the PINNâ€™s loss function. The boundary values for the training are obtained from a reference model, solved by the shooting method. Our approach allows for a computation of the learned Cosserat rod model multiple orders of magnitude faster than a publicly available reference model. The median position deviation from the reference model lies below 1mm (0.5% of the simulated robot length) for each of the robotâ€™s 20 disks."
Fast Kinodynamic Planning on the Constraint Manifold with Deep Neural Networks,"Piotr Kicki, Puze Liu, Davide Tateo, Haitham Bou Ammar, Krzysztof Walas, Piotr Skrzypczynski, Jan Peters","Poznan University of Technology,Technische Universität Darmstadt,Huawei",Deep Learning III,"Motion planning is a mature area of research in robotics with many well-established methods based on optimization or sampling the state space, suitable for solving kinematic motion planning. However, when dynamic motions under constraints are needed and computation time is limited, fast kinodynamic planning on the constraint manifold is indispensable. In recent years, learning-based solutions have become alternatives to classical approaches, but they still lack comprehensive handling of complex constraints, such as planning on a lower-dimensional manifold of the task space while considering the robot's dynamics. This paper introduces a novel learning-to-plan framework that exploits the concept of constraint manifold, including dynamics, and neural planning methods. Our approach generates plans satisfying an arbitrary set of constraints and computes them in a short constant time, namely the inference time of a neural network. This allows the robot to plan and replan reactively, making our approach suitable for dynamic environments. We validate our approach on~two~simulated tasks and in a demanding real-world scenario, where we use a Kuka LBR Iiwa 14 robotic arm to perform the hitting movement in robotic Air Hockey."
MANER: Multi-Agent Neural Rearrangement Planning of Objects in Cluttered Environments,"Vivek Gupta, Prabhpreet Dhir, Jeegn Dani, Ahmed H. Qureshi","Purdue University, West Lafayette,Purdue University",Deep Learning III,"Object rearrangement is a fundamental problem in robotics with various practical applications ranging from managing warehouses to cleaning and organizing home kitchens. While existing research has primarily focused on single-agent solutions, real-world scenarios often require multiple robots to work together on rearrangement tasks. This paper proposes a comprehensive learning-based framework for multi-agent object rearrangement planning, addressing the challenges of task sequencing and path planning in complex environments. The proposed method iteratively selects objects, determines their relocation regions, and pairs them with available robots under kinematic feasibility and task reachability for execution to achieve the target arrangement. Our experiments on a diverse range of simulated and real-world environments demonstrate the effectiveness and robustness of the proposed framework. Furthermore, results indicate improved performance in terms of traversal time and success rate compared to baseline approaches."
Benchmarking Actor-Critic Deep Reinforcement Learning Algorithms for Robotics Control with Action Constraints,"Kazumi Kasaura, Shuwa Miura, Tadashi Kozuno, Ryo Yonetani, Kenta Hoshino, Yohei Hosoe","OMRON SINIC X,University of Massachusetts, Amherst,Omron Sinic X,CyberAgent,Kyoto University",Deep Learning III,"This study presents a benchmark for evaluating action-constrained reinforcement learning (RL) algorithms. In action-constrained RL, each action taken by the learning system must comply with certain constraints. These constraints are crucial for ensuring the feasibility and safety of actions in real-world systems. We evaluate existing algorithms and their novel variants across multiple robotics control environments, encompassing multiple action constraint types. Our evaluation provides the first in-depth perspective of the field, revealing surprising insights, including the effectiveness of a straightforward baseline approach. The benchmark problems and associated code utilized in our experiments are made available online at github.com/omron-sinicx/action-constrained-RL-benchmark for further research and development."
Robust and Dexterous Dual-Arm Tele-Cooperation Using Adaptable Impedance Control,"Keyhan Kouhkiloui Babarahmati, Mohammadreza Kasaei, Carlo Tiseo, Michael Mistry, Sethu Vijayakumar","University of Edinburgh,University fo Sussex",Human-Robot Collaboration VI,"In recent years, the need for robots to transition from isolated industrial tasks to shared environments, including human-robot collaboration and teleoperation, has become increasingly evident. Building on the foundation of Fractal Impedance Control (FIC) introduced in our previous work, this paper presents a novel extension to dual-arm tele-cooperation, leveraging the non-linear stiffness and passivity of FIC to adapt to diverse cooperative scenarios. Unlike traditional impedance controllers, our approach ensures stability without relying on energy tanks, as demonstrated in our prior research. In this paper, we further extend the FIC framework to bimanual operations, allowing for stable and smooth switching between different dynamic tasks without gain tuning. We also introduce a telemanipulation architecture that offers higher transparency and dexterity, addressing the challenges of signal latency and low-bandwidth communication. Through extensive experiments, we validate the robustness of our method and the results confirm the advantages of the FIC approach over traditional impedance controllers, showcasing its potential for applications in planetary exploration and other scenarios requiring dexterous telemanipulation. This paper's contributions include the seamless integration of FIC into multi-arm systems, the ability to perform robust interactions in highly variable environments, and the provision of a comprehensive comparison with competing approaches, thereby significantly enhancing the robustness and adaptability of robotic systems."
PlanCollabNL: Leveraging Large Language Models for Adaptive Plan Generation in Human-Robot Collaboration,"Silvia Izquierdo-badiola, Gerard Canal, Carlos Rizzo, Guillem Alenyà","Eurecat,King's College London,University of Zaragoza,CSIC-UPC",Human-Robot Collaboration VI,"""Hey, robot. Let's tidy up the kitchen. By the way, I have back pain today"". How can a robotic system devise a shared plan with an appropriate task allocation from this abstract goal and agent condition? Classical AI task planning has been explored for this purpose, but it involves a tedious definition of an inflexible planning problem. Large Language Models (LLMs) have shown promising generalisation capabilities in robotics decision-making through knowledge extraction from Natural Language (NL). However, the translation of NL information into constrained robotics domains remains a challenge. In this paper, we use LLMs as translators between NL information and a structured AI task planning problem, targeting human-robot collaborative plans. The LLM generates information that is encoded in the planning problem, including specific subgoals derived from an NL abstract goal, as well as recommendations for subgoal allocation based on NL agent conditions. The framework, PlanCollabNL, is evaluated for a number of goals and agent conditions, and the results show that correct and executable plans are found in most cases. With this framework, we intend to add flexibility and generalisation to HRC plan generation, eliminating the need for a manual and laborious definition of restricted planning problems and agent models."
Multi-Agent Strategy Explanations for Human-Robot Collaboration,"Ravi Pandya, Michelle Zhao, Changliu Liu, Reid Simmons, Henny Admoni",Carnegie Mellon University,Human-Robot Collaboration VI,"As robots are deployed in human spaces, it is important that they are able to coordinate their actions with the people around them. Part of such coordination involves ensuring that people have a good understanding of how a robot will act in the environment. This can be achieved through explanations of the robot's policy. Much prior work in explainable AI and RL focuses on generating explanations for single-agent policies, but little has been explored in generating explanations for collaborative policies. In this work, we investigate how to generate multi-agent strategy explanations for human-robot collaboration. We formulate the problem using a generic multi-agent planner, show how to generate visual explanations through strategy-conditioned landmark states and generate textual explanations by giving the landmarks to an LLM. Through a user study, we find that when presented with explanations from our proposed framework, users are able to better explore the full space of strategies and collaborate more efficiently with new robot partners."
Efficient ISO/TS 15066 Compliance through Model Predictive Control,"Andrea Pupa, Cristian Secchi","University of Modena and Reggio Emilia,Univ. of Modena & Reggio Emilia",Human-Robot Collaboration VI,"In the actual industrial scenarios, human operators and robots work together sharing the workspace. Such proximity requires special attention in ensuring safety for the human operator, which is often translated in collision avoidance behaviour or high speed reduction. Adhering safety however is not the only aspect that must be taken into account. For many tasks, such as welding, it is crucial to ensure that the robot performs exactly the planned path. To optimize robot performance while complying with safety regulations, this work introduces a novel optimal nonlinear control problem. It prioritizes path preservation, exploiting redundancy to minimize task execution time, while explicitly adhering to the constraints imposed by ISO/TS 15066. To achieve high-performance outcomes, the control problem is addressed using the Model Predictive Control (MPC) approach. The proposed strategy has been experimentally validated in both simulations and a real-world industrial task involving a Kuka LWR4+ robot."
Dual-Mode Human-Robot Collaboration with Guaranteed Safety Using Time-Varying Zeroing Control Barrier Functions and Quadratic Program,"Kaige Shi, Guoqiang Hu","Nanyang Technological University,Nanyang Technological University,",Human-Robot Collaboration VI,"Safety and efficiency are two important aspects of human-robot collaboration (HRC). Most existing control methods for HRC consider either contactless HRC or physical HRC, hindering more efficient HRC. The proposed control framework enables dual-mode HRC, filling the gap between contactless and physical HRCs. With the framework, the robot can perform contactless HRC under safety regulations regarding the co-working human. Meanwhile, the human can safely interrupt the robot via physical contact to enter physical HRC, in which he/she can hand guide the robot or take over its gripped object. First, human safety is defined as bounded approaching velocities between human and multiple robot links based on ISO/TS 15066, allowing gradual establishing of physical contact. Then, the time-varying zeroing control barrier function is proposed and defined to guarantee the bounded approaching velocities by a safety control set. Second, a unified task control set is designed to achieve different robot tasks for different HRC modes in a unified manner. The unified task control set enables the robot to switch smoothly between the two HRC modes. An optimal final control input is determined by a quadratic program (QP) based on different control sets. Experiments were conducted to verify the proposed framework and compare the proposed framework with existing methods. An application example is presented to show the versatility of the proposed framework."
A Time-Optimal Energy Planner for Safe Human-Robot Collaboration,"Andrea Pupa, Marco Minelli, Cristian Secchi","University of Modena and Reggio Emilia,Univ. of Modena & Reggio Emilia",Human-Robot Collaboration VI,"The human-robot collaboration scenarios are characterized by the presence of human operators and robots that work in close contact with each other. As a consequence, the safety regulations have been updated in order to provide guidelines on how to asses safety in these new scenarios. In particular, Power and Force Limiting (PFL) collaborative mode describes how the energy should be regulated during the collaboration. Based on these guidelines, we propose a new optimal trajectory planner which, by exploiting the variability of the robot's inertia as a function of its configuration, is able to return trajectories that can be travelled at greater speed and in less time, while guaranteeing the safety limits according to the standard. The proposed planner was validated first in simulation, comparing completion times with other state-of-the-art planning algorithms, and then experimentally, demonstrating the performance of the planned trajectories during physical interaction with the environment. Both validations confirm the effectiveness of the proposed planner, which returns shorter completion times while ensuring safe interaction."
Discuss before Moving: Visual Language Navigation Via Multi-Expert Discussions,"Yuxing Long, Xiaoqi Li, Wenzhe Cai, Hao Dong","Peking University,Southeast University",Human-Robot Collaboration VI,"Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model's own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting, we introduce a novel zero-shot VLN framework. Within this framework, large models possessing distinct abilities are served as domain experts. Our proposed navigation agent, namely DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks like instruction understanding, environment perception, and completion estimation. Through comprehensive experiments, we demonstrate that discussions with domain experts can effectively facilitate navigation by perceiving instruction-relevant information, correcting inadvertent errors, and sifting through in-consistent movement decisions. The performances on the representative VLN task R2R show that our method surpasses the leading zero-shot VLN model by a large margin on all metrics. Additionally, real-robot experiments display the obvious advantages of our method over single-round self-thinking."
Personality and Memory-Based Software Framework for Human-Robot Interaction,"Alice Nardelli, Antonio Sgorbissa, Carmine Tommaso Recchiuto","University of Genoa,University of Genova",Human-Robot Collaboration VI,"The synergic orchestration of the cognitive and psychological dimensions characterizes human intelligence. Accordingly, carefully designing this mechanism in artificial intelligence can be a successful strategy to increase human likeness in a robot, enhancing mutual understanding and building a more natural and intuitive interaction. For this purpose, the main contribution of this work is a psychological and cognitive architecture tailored for HRI based on the interplay between robotic personality and memory-based cognitive processes. Indeed, the artificial personality manifests itself not only in various aspects of the behavior but also within the action selection process, which is closely intertwined with personality-dependent hedonic experiences linked to memories. Within this paper, we propose a task- and platform-independent framework, evaluated in a multiparty collaborative scenario. Obtained results show that a robot connected to our proposed framework is perceived as a cognitive agent capable of manifesting perceivable and distinguishable personality traits."
CalliRewrite: Recovering Handwriting Behaviors from Calligraphy Images without Supervision,"Yuxuan Luo, Zekun Wu, Zhouhui Lian",Peking University,Service Robotics,"Human-like planning skills and dexterous manipulation have long posed challenges in the fields of robotics and artificial intelligence (AI). The task of reinterpreting calligraphy presents a formidable challenge, as it involves the decomposition of strokes and dexterous utensil control. Previous efforts have primarily focused on supervised learning of a single instrument, limiting the performance of robots in the realm of cross-domain text replication. To address these challenges, we propose CalliRewrite: a coarse-to-fine approach for robot arms to discover and recover plausible writing orders from diverse calligraphy images without requiring labeled demonstrations. Our model achieves fine-grained control of various writing utensils. Specifically, an unsupervised image-to-sequence model decomposes a given calligraphy glyph to obtain a coarse stroke sequence. Using an RL algorithm, a simulated brush is fine-tuned to generate stylized trajectories for robotic arm control. Evaluation in simulation and physical robot scenarios reveals that our method successfully replicates unseen fonts and styles while achieving integrity in unknown characters. To access our code and supplementary materials, please visit our project page: https://luoprojectpage.github.io/callirewrite/."
Efficient and Accurate Transformer-Based 3D Shape Completion and Reconstruction of Fruits for Agricultural Robots,"Federico Magistri, Rodrigo Marcuzzi, Elias Ariel Marks, Matteo Sodano, Jens Behley, Cyrill Stachniss","University of Bonn,Photogrammetry and Robotics Lab, University of Bonn",Service Robotics,"Robots that operate in agricultural environments need a robust perception system that can deal with occlusions, which are naturally present in agricultural scenarios. In this paper, we address the problem of estimating 3D shapes of fruits when only partial observations are available. Generally speaking, such a shape completion can be realized by exploiting prior knowledge about the geometry of the fruit. This is typically done by template matching using traditional optimization algorithms, which are slow but accurate, or by encoding such knowledge into the weights of a neural network, leading to faster but often less accurate estimates. Our approach combines the best of both worlds. It exploits the benefit of having a template representing our object of interest with the advantages of using a neural network to learn how to deform a template. Our experimental evaluation demonstrates that our approach yields accurate estimation at a competitively low inference time in challenging greenhouse environments."
Censible: A Robust and Practical Global Localization Framework for Planetary Surface Missions,"Jeremy Nash, Quintin Dwight, Lucas Saldyt, Haoda Wang, Steven Myint, Adnan Ansar, Vandi Verma","Jet Propulsion Laboratory,University of Michigan,Jet Propulsion Laboratory, California Institute of Technology,NASA Jet Propulsion Laboratory,NASA Jet Propulsion Laboratory, California Institute of",Service Robotics,"To achieve longer driving distances, planetary robotics missions require accurate localization to counteract position uncertainty. Freedom and precision in driving allows scientists to reach and study sites of interest. Typically, rover global localization has been performed manually by humans, which is accurate but time-consuming as data is relayed between planets. This paper describes a global localization algorithm that is run onboard the Perseverance Mars rover. Our approach matches rover images to orbital maps using a modified census transform to achieve sub-meter accurate, near-human localization performance on a real dataset of 264 Mars rover panoramas. The proposed solution has also been successfully executed on the Perseverance Mars Rover, demonstrating the practicality of our approach."
Learning to Walk in Confined Spaces Using 3D Representation,"Takahiro Miki, Joonho Lee, Lorenz Wellhausen, Marco Hutter","ETH Zurich,ETH Zürich",Service Robotics,"Legged robots have the potential to traverse complex terrain and access confined spaces beyond the reach of traditional platforms thanks to their ability to carefully select footholds and flexibly adapt their body posture while walking. However, robust deployment in real-world applications is still an open challenge. In this paper, we present a method for legged locomotion control using reinforcement learning and 3D volumetric representations to enable robust and versatile locomotion in confined and unstructured environments. By employing a two-layer hierarchical policy structure, we exploit the capabilities of a highly robust low-level policy to follow 6D commands and a high-level policy to enable three-dimensional spatial awareness for navigating under overhanging obstacles. Our study includes the development of a procedural terrain generator to create diverse training environments. We present a series of experimental evaluations in both simulation and real-world settings, demonstrating the effectiveness of our approach in controlling a quadruped robot in confined, rough terrain. By achieving this, our work extends the applicability of legged robots to a broader range of scenarios."
Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery,"Mimo Shirasaka, Tatsuya Matsushima, Soshi Tsunashima, Yuya Ikeda, Aoi Horo, So Ikoma, Chikaha Tsuji, Hikaru Wada, Tsunekazu Omija, Dai Komukai, Yutaka Matsuo, Yusuke Iwasawa","The University of Tokyo,University of Tokyo,The Univeristy of Tokyo",Service Robotics,"A general-purpose service robot (GPSR), which can execute diverse tasks in various environments, requires a system with high generalizability and adaptability to tasks and environments. In this paper, we first developed a top-level GPSR system for worldwide competition (RoboCup@Home 2023) based on multiple foundation models. This system is both generalizable to variations and adaptive by prompting each model. Then, by analyzing the performance of the developed system, we found three types of failure in more realistic GPSR application settings: insufficient information, incorrect plan generation, and plan execution failure. We then propose the self-recovery prompting pipeline, which explores the necessary information and modifies its prompts to recover from failure. We experimentally confirm that the system with the self-recovery mechanism can accomplish tasks by resolving various failure cases."
Autonomous Quilt Spreading for Caregiving Robots,"Yuchun Guo, Zhiqing Lu, Yanling Zhou, Xin Jiang","Harbin Institute of Technology, Shenzhen,Harbin Institute of Technologyï¼ŒShenzhen,Harbin Institute of Technology (Shenzhen)",Service Robotics,"In this work, we propose a novel strategy to ensure infants, who inadvertently displace their quilts during sleep, are promptly and accurately re-covered. Our approach is formulated into two subsequent steps: interference resolution and quilt spreading. By leveraging the DWPose human skeletal detection and the Segment Anything instance segmentation models, the proposed method can accurately recognize the states of the infant and the quilt over her, which involves addressing the interferences resulted from an infant's limbs laid on part of the quilt. Building upon prior research, the EM*D deep learning model is employed to forecast quilt state transitions before and after quilt spreading actions. To improve the sensitivity of the network in distinguishing state variation of the handled quilt, we introduce an enhanced loss function that translates the voxelized quilt state into a more representative one. Both simulation and real-world experiments validate the efficacy of our method, in spreading and recover a quilt over an infant."
CNS: Correspondence Encoded Neural Image Servo Policy,"Anzhe Chen, Hongxiang Yu, Yue Wang, Rong Xiong",Zhejiang University,Service Robotics,"Image servo is an indispensable technique in robotic applications that helps to achieve high precision positioning. The intermediate representation of image servo policy is important to sensor input abstraction and policy output guidance. Classical approaches achieve high precision but require clean keypoint correspondence, and suffer from limited convergence basin or weak feature error robustness. Recent learning-based methods achieve moderate precision and large convergence basin on specific scenes but face issues when generalizing to novel environments. In this paper, we encode keypoints and correspondence into a graph and use graph neural network as architecture of controller. This design utilizes both advantages: generalizable intermediate representation from keypoint correspondence and strong modeling ability from neural network. Other techniques including realistic data generation, feature clustering and distance decoupling are proposed to further improve efficiency, precision and generalization. Experiments in simulation and real-world verify the effectiveness of our method in speed (maximum 40fps along with observer), precision ("
Adapting for Calibration Disturbances: A Neural Uncalibrated Visual Servoing Policy,"Hongxiang Yu, Anzhe Chen, Kechun Xu, Dashun Guo, Zhongxiang Zhou, Yufei Wei, Xuebo Zhang, Yue Wang, Rong Xiong","Zhejiang University,Nankai University,",Service Robotics,"Visual servoing (VS) is a widely used technique in industries where there are hundreds of robots, but it requires accurate camera calibration including camera intrinsic and extrinsic parameters. However, it is labour-intensive to calibrate robots one-by-one in practical use. In this paper, we propose a neural uncalibrated VS policy (NUVS) that can adapt to calibration disturbances with an adaption mechanism and a control-oriented guidance. It bridges the disturbance adaption of classical VS methods and the large convergence of learning-based VS methods. NUVS estimates the calibration embedding from past observations and servos to the desired pose under the supervision of a PBVS that can access the ground truth in simulation. With this adaption mechanism, NUVS outperforms the classical IBUVS algorithm when facing large initial camera pose offsets under the calibration disturbance. Supplementary material in: https://sites.google.com/view/neural-uncalibrated-vs"
ARIS 1.0: An Autonomous Multitasking Medical Service Robot for Hospital Environments,"Anurisha Piyathma Dunuwila, Lahiru Gunawardhana, Hirantha Basnayake, Ranjith Amarasinghe, A.G.B.P. Jayasekara, Gihan Charith Premachandra Hanchapola Appuha, Hiroki Tamura, U-Xuan Tan","University of Moratuwa,University of Moratuwa, Sri Lanka,University of Moratuway,Singapore University of Technology and Design,University of Miyazaki,Singapore University of Techonlogy and Design",Service Robotics,"Introducing robotics in the healthcare sector revolutionizes medical services by providing advanced treatments, medication management, and robotic assistance while overcoming resource limitations. In the current healthcare domain, an intermediate robotic communication platform is essential for distributing equal medical services, facilitating remote consultations, and maintaining the integrity of medical education, especially in rural areas and during pandemics. This work introduces ARIS, a multitasking medical service robot designed for telemedicine aspects and to facilitate remote medical education activities such as ward rounds. The prototype called ARIS 1.0 was developed, including a three-wheeled omnidirectional mobile platform, a torso and a novel movable neck mechanism with a face. The prototype robot can generate an online summarized report using its integrated language interaction and IoT-based vital sign extraction modules. The ROS-based semi-autonomous navigation facilitates the robot to be an assistive agent, allowing it to either accompany doctors or visit patients individually. Ultimately, ARIS 1.0 serves telepresence and novel regional language capabilities, specifically Sinhala-based self-communication features. This enables inter-party communication among doctors, medical students, and patients. The functionalities of ARIS 1.0 were validated in an emulated indoor environment to evaluate their feasibility. The results indicate that ARIS 1.0 is feasible for providing remote medical services. Furthermore, the paper discusses several promising research directions related to the proposed concept."
"Design, Modeling and Analysis of a Spherical Parallel Continuum Manipulator for Nursing Robots","Zhenhua Gong, Chuanxin Ning, Jiejunyi Liang, Ting Zhang","Soochow University,Huazhong university of science and technology",Service Robotics,"In the healthcare industry, nursing robots have made great contributions, assisting in the delivery of food and medicine as well as the movement and transfer of patients. However, the traditional continuum manipulator often has the problems of limited workspace and weak carrying capacity. Compared with traditional manipulator, the continuum manipulator has the advantages of a small moment of inertia and high dexterity. This paper proposes a original cable-driven parallel continuum manipulator with a spherical parallel mechanism as the continuous segments. Due to the spherical parallel mechanismsâ€™ characteristics, the proposed cable-driven spherical parallel continuum manipulator offers many inherent advantages for nursing robots. The prototype is tested and analyzed, and the kinematics and statics are verified. The results show that the cable-driven spherical parallel continuum manipulator for nursing robots has low requirements for workspace, suitable for complex spaces and can have a large carrying capacity."
LeagTag: An Elongated High-Accuracy Fiducial Marker for Tight Spaces,"Hideyuki Tanaka, Kunihiro Ogata","National Institute of AIST,National Institute of Advanced Industrial Science and Technology",Service Robotics,"Fiducial markers enable reliable service robot control. In human-robot coexistence environments, efficient placement of square or circular markers can be challenging due to limited space. In this study, we developed a world-first, elongated fiducial marker, capable of high-accuracy 6-DoF measurements, designed to be installable in tight spaces. We introduced two types of lenticular angle gauges to enhance pose estimation and developed new marker patterns and measurement algorithms to maintain recognition distance and accuracy. The proposed marker achieved a measurement accuracy of 0.1% position error and 0.5 deg orientation error. This technology will enhance the practicality and applicability of fiducial markers, contributing to the creation of robot-friendly space for future service robots."
An Open and Flexible Robot Perception Framework for Mobile Manipulation Tasks,"Patrick Mania, Simon Stelter, Gayane Kazhoyan, Michael Beetz","University of Bremen,Universität Bremen",Service Robotics,"Over the last years, powerful methods for solving specific perception problems such as object detection, pose estimation or scene understanding have been developed. While performing mobile manipulation actions, a robot's perception framework needs to execute a series of these methods in a specific sequence each time it receives a new perception task. Generating proficient combinations of vision methods to solve individual perception tasks remains a challenge, as the combination depends on the requirements of the task and the capabilities of the robot's hardware. In this paper, we propose RoboKudo, an open-source knowledge-enabled perception framework that leverages the strengths of the Unstructured Information Management (UIM) principle and the flexibility of Behavior Trees to model task-specific perception processes. The framework can combine state-of-the-art computer vision methods to satisfy the requirements of each perception task and scales to different robot platforms. The generality and effectiveness of the framework are evaluated in real world experiments where it solves various perception tasks in the context of mobile manipulation actions in a household domain. Code and additional material are available at https://robokudo.ai.uni-bremen.de/rkop."
Toward Mass Customization of a Robot's Morphology Design for Improving Area Coverage,"M. A. Viraj J. Muthugala, Bhagya Samarakoon, Raihan Enjikalayil Abdulkader, Rajesh Elara Mohan",Singapore University of Technology and Design,Service Robotics,"Floor cleaning robots have been developed to cater to building maintenance needs. Complete area coverage is crucial for a floor cleaning robot, and its morphology design plays a vital role in realizing complete area coverage. However, floor cleaning robots with fixed morphologies have difficulty in achieving a high area coverage performance. Mass customization of a robotâ€™s morphology would improve its productivity in terms of area coverage. This paper proposes a novel system that can be used for mass customizing the morphology of a robot to improve area coverage performance in an environment of interest. The customized morphology is determined through an optimization technique by considering an environment of interest and design constraints. The area coverage of a candidate morphology design is evaluated by simulating the robot navigation in an environment of interest. Generalized pattern search, particle swarm optimization, and surrogate optimization are independently considered optimization techniques. Experiments have been conducted considering the cases of robot deployments. The statistical conclusions on experimental results validate that the proposed system can synthesize a morphology that significantly improves the area coverage performance in an environment of interest."
CoPAL: Corrective Planning of Robot Actions with Large Language Models,"Frank Joublin, Antonello Ceravola, Pavel Smirnov, Felix Ocker, Joerg Deigmoeller, Anna Belardinelli, Chao Wang, Stephan Hasler, Daniel Tanneberg, Michael Gienger","Honda Research Institute Europe,Honda Research Institute Europe GmbH,Honda,Honda Research Institute",Service Robotics,"In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation."
Leaf-Inspired FSR Array and Insole-Type Sensor Module for Mobile Three-Dimensional Ground Reaction Force Estimation,"Taeyeon Kim, Eunseok Song, Seongbin An, Hyunjin Choi, Kyoungchul Kong","Korea Advanced Institute of Science and Technology,Korea Advanced Institute of Science and Technology (KAIST),KAIST,Sangmyung University",Wearable Robotics II,"This paper presents an insole-type sensor module with a novel leaf-inspired force-sensitive resistor (FSR) array for accurate three-dimensional ground reaction force (GRF) estimation during human's various motions. Joint torque analysis, essential for numerous applications in biomechanics and wearable robotics, necessitates the measurement of three-dimensional GRF vector information, traditionally achieved in indoor environments using costly force plates. To overcome these limitations, this study proposes an alternative method by incorporating FSRs on three inclined planes within the insole. A vector scaling process transforms the force values from the FSRs into the three-dimensional force vector, enabling continuous and user-independent estimation of GRF. The sensor module is integrated with machine learning, demonstrating its accuracy and usability in various motion scenarios. The results confirm the effectiveness of the leaf-inspired FSR array, giving the possibilities for portable and cost-effective motion analysis systems."
Human-Exoskeleton Locomotion Interaction Experience Transfer: Speeding up and Improving the Performance of Preference-Based Optimizations of Exoskeleton Assistance During Walking,"Hongwu Li, Junchen Liu, Ziqi Wang, Haotian Ju, Tianjiao Zheng, Yongsheng Gao, Jie Zhao, Yanhe Zhu",Harbin Institute of Technology,Wearable Robotics II,"Preference-based optimizing methods have shown their advantages and potential in exploring individual, comfortable, and effective control strategies and assistance parameters of exoskeletons during locomotion. Research indicates that compared with naive wearers, knowledgeable wearers with abundant exoskeleton assistance experience have obvious advantages in speeding up the parameters exploration process and improving the assistant performance. However, there is no existing method that could utilize the human-exoskeleton locomotion interaction experience (HELIE) to assist naive wearers during the exploration process. In this work, we propose a novel preference-based human-exoskeleton locomotion interaction experience transfer (LIET) framework, which could speed up the exploration of human-preferred parameters and acquire more satisfying results for naive wearers via the HELIE acquired from knowledgeable wearers. In addition, based on the proposed LIET framework, we establish the mathematical expression of the HELIE transfer during exoskeleton assistance. This will promote the research that concerns utilizing HELIE for exoskeleton control parameters optimizations in the future. Finally, experiments demonstrate the proposed LIET framework could speed up the exploration process and acquire more satisfying optimized results for naive wearers."
Design of a Knee-Joint Exoskeleton to Reduce Misalignment in Both the Sagittal and Coronal Planes,"Shubhranil Sengupta, Jee-Hwan Ryu",Korea Advanced Institute of Science and Technology,Wearable Robotics II,"Many individuals experience knee dysfunctions attributed to the natural aging process and degenerative con- ditions. To aid individuals in regaining knee functionality, supportive exoskeletons were designed to be affixed to both the shin and thigh. However, a common issue encountered in knee exoskeletons involves the misalignment of joints between the exoskeleton and the user, resulting in discomfort and potential injuries. To reduce misalignment with the knee joint, it is essential for the thigh and shin harnesses of the exoskeleton to replicate the natural trajectories of the knee. However, achieving this is a complex task due to the shifting center of rotation of the knee in both the Sagittal and Coronal planes. Previous knee exoskeletons primarily focus on aligning the joint in the Sagittal plane, neglecting alignment in the other dimension due to inherent design constraints. For the first time, this study introduces a knee-joint exoskeleton capable of conforming to the natural movement of the knee in both the Sagittal and Coronal planes, with the aim of minimizing joint misalignment without the use of inherently soft materials. A spherical scissor linkage mechanism (SSLM) was utilized in conjunction with a customized guide rail to adjust the center of rotation of the SSLM. This configuration facilitates knee flexion/extension while accommodating the knee jointâ€™s center of rotation in both the Sagittal and Coronal planes. The experimental outcomes demonstrated a substantial reduction in misalignment with the knee when compared to a commercial knee-support brace with a one-degree-of-freedom revolute joint."
Adaptive Active Disturbance Rejection Control of an Actuated Ankle Foot Orthosis for Ankle Movement Assistance,"Rami Jradi, Hala Rifai, Samer Mohammed","UPEC,University of Paris Est Créteil,University of Paris Est Créteil - (UPEC)",Wearable Robotics II,"Foot-drop (FD) is a post-stroke gait disorder characterized by impaired foot lifting during the swing phase. This paper focuses on providing a continuous ankle joint assistance throughout the gait cycle using an actuated ankle foot orthosis (AAFO). The control strategy is based on an adaptive active disturbance rejection controller (AADRC) such that the orthosis provides the only required amount of assistance to complement the human effort needed to perform the walking activity. The proposed controller exhibits adaptability, making it suitable for various subjects without the need for prior parameter identification. To demonstrate its effectiveness, the control strategy is experimentally validated with five healthy subjects and compared to state-of-the-art controllers."
A Novel Funnel-Based L1 Adaptive Fuzzy Approach for the Control of an Actuated Ankle Foot Orthosis,"Oussama Bey, Rami Jradi, Huiseok Moon, Hala Rifai, Kaushik Das Sharma, Yacine Amirat, Samer Mohammed","University Paris-Est Créteil - UPEC,UPEC,LISSI-lab, Universite de Paris-Est Creteil (UPEC),University of Paris Est Créteil,University of Calcutta,University of Paris Est Créteil (UPEC),University of Paris Est Créteil - (UPEC)",Wearable Robotics II,"This paper introduces a novel funnel-based adaptive L1 fuzzy control strategy for assisting ankle joint movement during walking with the use of an actuated ankle foot orthosis (AAFO). A projection-based adaptation mechanism employing a fuzzy system is used to estimate the unknown time-varying parameters of the L1 control law, ensuring precise tracking of the AAFO-wearer system by the state estimator. The projection operator guarantees the convergence of the parameters while offering a limited amount of assistance torque. Funnel-based feedback control is used to mitigate the typical time lag seen when using L1-based approaches due to the presence of a low-pass filter commonly used in this type of approach. The effectiveness of the proposed control strategy is demonstrated through experiments involving five healthy subjects."
CITR: A Coordinate-Invariant Task Representation for Robotic Manipulation,"Peter So, Rafael Ignacio Cabral Muchacho, Robin Kirschner, Abdalla Swikir, Luis Felipe Cruz Figueredo, Fares Abu-Dakka, Sami Haddadin","Technical University of Munich,KTH Royal Institute of Technology,TU Munich, Institute for Robotics and Systems Intelligence,Technical University of Munich (TUM),Mondragon University",Representation Learning II,"The basis for robotics skill learning is an adequate representation of manipulation tasks based on their physical properties. As manipulation tasks are inherently invariant to the choice of reference frame, an ideal task representation would also exhibit this property. Nevertheless, most robotic learning approaches use unprocessed, coordinate-dependent robot state data for learning new skills, thus inducing challenges regarding the interpretability and transferability of the learned models. In this paper, we propose a transformation from spatial measurements to a coordinate-invariant feature space, based on the pairwise inner product of the input measurements. We describe and mathematically deduce the concept, establish the task fingerprints as an intuitive image-based representation, experimentally collect task fingerprints, and demonstrate the usage of the representation for task classification. This representation motivates further research on data-efficient and transferable learning methods for online manipulation task classification and task-level perception."
SlotGNN: Unsupervised Discovery of Multi-Object Representations and Visual Dynamics,"Alireza Rezazadeh, Athreyi Badithela, Karthik Desingh, Changhyun Choi","University of Minnesota,University of Minnesota - Twin Cities,University of Minnesota, Twin Cities",Representation Learning II,"Learning multi-object dynamics from visual data using unsupervised techniques is challenging due to the need for robust, object representations that can be learned through robot interactions. This paper presents a novel framework with two new architectures: SlotTransport for discovering object representations from RGB images and SlotGNN for predicting their collective dynamics from RGB images and robot interactions. Our SlotTransport architecture is based on slot attention for unsupervised object discovery and uses a feature transport mechanism to maintain temporal alignment in object-centric representations. This enables the discovery of slots that consistently reflect the composition of multi-object scenes. These slots robustly bind to distinct objects, even under heavy occlusion or absence. Our SlotGNN, a novel unsupervised graph-based dynamics model, predicts the future state of multi-object scenes. SlotGNN learns a graph representation of the scene using the discovered slots from SlotTransport and performs relational and spatial reasoning to predict the future appearance of each slot conditioned on robot actions. We demonstrate the effectiveness of SlotTransport in learning object-centric features that accurately encode both visual and positional information. Further, we highlight the accuracy of SlotGNN in downstream robotic tasks, including challenging multi-object rearrangement and long-horizon prediction. Finally, our unsupervised approach proves effective in the real world. With only minimal additional data, our framework robustly predicts slots and their corresponding dynamics in real-world control tasks."
What Do We Learn from a Large-Scale Study of Pre-Trained Visual Representations in Sim and Real Environments?,"Sneha Silwal, Karmesh Yadav, Tingfan Wu, Jay Vakil, Arjun Majumdar, Sergio Arnaud, Claire Chen, Vincent-pierre Berges, Dhruv Batra, Aravind Rajeswaran, Mrinal Kalakrishnan, Franziska Meier, Oleksandr Maksymets","Meta,Georgia Tech,Meta AI,Georgia Institute of Technology,Stanford University,Meta AI Research,Georgia Tech / Facebook AI Research,Facebook,Facebook AI Research",Representation Learning II,"We present a large empirical investigation on the use of pre-trained visual representations (PVRs) for training downstream policies that execute real-world tasks. Our study involves five different PVRs, each trained for five distinct manipulation or indoor navigation tasks. We performed this evaluation using three different robots and two different policy learning paradigms. From this effort, we can arrive at three insights: 1) the performance trends of PVRs in the simulation are generally indicative of their trends in the real world, 2) the use of PVRs enables a first-of-its-kind result with indoor ImageNav (zero-shot transfer to a held-out scene in the real world), and 3) the benefits from variations in PVRs, primarily data-augmentation and fine-tuning, also transfer to the real-world performance."
L-DYNO: Framework to Learn Consistent Visual Features Using Robotâ€™s Motion,"Kartikeya Singh, Charuvahan Adhivarahan, Karthik Dantu","university at buffalo,University at Buffalo, State University of New York,University of Buffalo",Representation Learning II,"Historically, feature-based approaches have been used extensively for camera-based robot perception tasks such as localization, mapping, tracking, and others. Several of these approaches also combine other sensors (inertial sensing, for example) to perform combined state estimation. Our work rethinks this approach; we present a representation learning mechanism that identifies visual features that best correspond to robot motion as estimated by an external signal. Specifically, we utilize the robotâ€™s transformations through an external signal (inertial sensing, for example) and give attention to image space that is most consistent with the external signal. We use a pairwise consistency metric as a representation to keep the visual features consistent through a sequence with the robotâ€™s relative pose transformations. This approach enables us to incorporate information from the robotâ€™s perspective instead of solely relying on the image attributes. We evaluate our approach on real-world datasets such as KITTI & EuRoC and compare the refined features with existing feature descriptors. We also evaluate our method using our real robot experiment. We notice an average of 49% reduction in the image search space without compromising the trajectory estimation accuracy. Our method reduces the execution time of visual odometry by 4.3% and also reduces reprojection errors. We demonstrate the need to select only the most important features and show the competitiveness using various feature detection baselines."
Point Cloud Models Improve Visual Robustness in Robotic Learners,"Skand Peri, Iain Lee, Chanho Kim, Fuxin Li, Tucker Hermans, Stefan Lee","Oregon State University,University of Utah",Representation Learning II,"Visual control policies can encounter significant performance degradation when visual conditions like lighting or camera position differ from those seen during training -- often exhibiting sharp declines in capability even for minor differences. In this work, we examine robustness to a suite of these types of visual changes for RGB-D and point cloud based visual control policies. To perform these experiments on both model-free and model-based reinforcement learners, we introduce a novel Point Cloud World Model (PCWM) and point cloud based control policies. Our experiments show that policies that explicitly encode point clouds are significantly more robust than their RGB-D counterparts. Further, we find our proposed PCWM significantly outperforms prior works in terms of sample efficiency during training. Taken together, these results suggest reasoning about the 3D scene through point clouds can improve performance, reduce learning time, and increase robustness for robotic learners."
HIO-SDF: Hierarchical Incremental Online Signed Distance Fields,"Vasileios Vasilopoulos, Suveer Garg, Jinwook Huh, Bhoram Lee, Volkan Isler","Samsung Research America,University of Pennsylvania,Samsung,SRI International,University of Minnesota",Representation Learning II,"A good representation of a large, complex mobile robot workspace must be space-efficient yet capable of encoding relevant geometric details. When exploring unknown environments, it needs to be updatable incrementally in an online fashion. We introduce HIO-SDF, a new method that represents the environment as a Signed Distance Field (SDF). State of the art representations of SDFs are based on either neural networks or voxel grids. Neural networks are capable of representing the SDF continuously. However, they are hard to update incrementally as neural networks tend to forget previously observed parts of the environment unless an extensive sensor history is stored for training. Voxel-based representations do not have this problem but they are not space-efficient especially in large environments with fine details. HIO-SDF combines the advantages of these representations using a hierarchical approach which employs a coarse voxel grid that captures the observed parts of the environment together with high-resolution local information to train a neural network. HIO-SDF achieves a 46% lower mean global SDF error across all test scenes than a state of the art continuous representation, and a 30% lower error than a discrete representation at the same resolution as our coarse global SDF grid. Videos and code are available at: https://samsunglabs.github.io/HIO-SDF-project-page/"
Recasting Generic Pretrained Vision Transformers As Object-Centric Scene Encoders for Manipulation Policies,"Jianing Qian, Anastasios Panagopoulos, Dinesh Jayaraman",University of Pennsylvania,Representation Learning II,"Generic re-usable pre-trained image representation encoders have become a standard component of methods for many computer vision tasks. As visual representations for robots however, their utility has been limited, leading to a recent wave of efforts to pre-train robotics-specific image encoders that are better suited to robotic tasks than their generic counterparts. We propose Scene Objects From Transformers, abbreviated as SOFT, a wrapper around pre-trained vision transformer (PVT) models that bridges this gap without any further training. Rather than construct representations out of only the final layer activations, SOFT individuates and locates object-like entities from PVT attentions, and describes them with PVT activations, producing an object-centric representation. Across standard choices of generic pre-trained vision transformers PVT, we demonstrate in each case that policies trained on SOFT(PVT) far outstrip standard PVT representations for manipulation tasks in simulated and real settings, approaching the state-of-the-art robotics-aware representations. Appendix and videos: https://sites.google.com/view/robot-soft/"
NeRF-Loc: Transformer-Based Object Localization within Neural Radiance Fields,"Jiankai Sun, Yan Xu, Mingyu Ding, Hongwei Yi, Chen Wang, Jingdong Wang, Liangjun Zhang, Mac Schwager","Stanford University,The Chinese University of Hong Kong,UC Berkeley,Max Planck Institute for Intelligent Systems,Baidu",Representation Learning II,
Iterative PnP and Its Application in 3D-2D Vascular Image Registration for Robot Navigation,"Jingwei Song, Keke Yang, Zheng Zhang, Meng Li, Tuoyu Cao, Maani Ghaffari","University of Michigan,United Imaging,,. The Institute of Medical Imaging Technology, School of Biomed,Shanghai United Imaging Healthcare Co., Ltd.,United Imaging Healthcare",Surgical Robotics III,"This paper reports on a new real-time robot-centered 3D-2D vascular image alignment algorithm, which is robust to outliers and can align nonrigid shapes. Few works have managed to achieve both real-time and accurate performance for vascular intervention robots. This work bridges high-accuracy 3D-2D registration techniques and computational efficiency requirements in intervention robot applications. We categorize centerline-based vascular 3D-2D image registration problems as an iterative Perspective-n-Point (PnP) problem and propose using the Levenberg-Marquardt solver on the Lie manifold. Then, the recently developed Reproducing Kernel Hilbert Space (RKHS) algorithm is introduced to overcome the ``big-to-small'' problem in typical robotic scenarios. Finally, an iterative reweighted least squares is applied to solve RKHS-based formulation efficiently. Experiments indicate that the proposed algorithm processes registration over 50 Hz (rigid) and 20 Hz (nonrigid) and obtains competing registration accuracy similar to other works. Results indicate that our Iterative PnP is suitable for future vascular intervention robot applications."
Sim2Real Transfer of Reinforcement Learning for Concentric Tube Robots,"Keshav Iyengar, S.m.hadi Sadati, Christos Bergeles, Sarah Spurgeon, Danail Stoyanov","University College London,King's College London",Surgical Robotics III,"Concentric Tube Robots (CTRs) are promising for minimally invasive interventions due to their miniature diameter, high dexterity, and compliance with soft tissue. CTRs comprise individual pre-curved tubes usually composed of NiTi and are arranged concentrically. As each tube is relatively rotated and translated, the backbone elongates, twists, and bends with a dexterity that is advantageous for confined spaces. Tube interactions, unmodelled phenomena, and inaccurate tube parameter estimation make physical modeling of CTRs challenging, complicating in turn kinematics and control. Deep reinforcement learning (RL) has been investigated as a solution. However, hardware validation has remained a challenge due to differences between the simulation and hardware domains. With simulation-only data, in this work, domain randomization is proposed as a strategy for translation to hardware of a simulation policy with no additionally acquired physical training data. The differences in simulation and hardware forward kinematics accuracy and precision are characterized by errors of 14.74 +/- 8.87 mm or 26.61 +/- 17.00 % robot length. We showcase that the proposed domain randomization approach reduces errors by 56% in mean errors as compared to no domain randomization. Furthermore, we demonstrate path following capability in hardware with a line path with resulting errors of 4.37 +/- 2.39 mm or 5.61 +/- 3.11 % robot length."
A Kinetostatic Model for Concentric Push-Pull Robots,"Jake Childs, Caleb Rucker","EndoTheia, Inc.,University of Tennessee",Surgical Robotics III,"Concentric push-pull robots (CPPR) operate through the mechanical interactions of concentrically nested, laser-cut tubes with offset stiffness centers. The distal tips of the tubes are attached to each other, and relative displacement of the tube bases generates bending in the CPPR. Previous CPPR kinematic models assumed two tubes, planar shapes, no torsion, and no external loads. In this paper, we develop a new, more general CPPR model accounting for any number of tubes, describing their variable-curvature 3D shape when actuated, including the effects of torsion and external loads. To accomplish this, we employ a modified Kirchhoff rod model for each tube (with offset stiffness center) and embed the constraints of concentricity. We use an energy method to determine robot shape as a function of actuation and external loading. We experimentally validate this kinetostatic model on prototype CPPRs with two tubes and three tubes and non-constant laser-cut patterns that create variable curvature and stiffness. Experimental results agree with the model, paving the way for use of this model in design optimization, planning, and control of CPPRs."
Fully Distributed Shape Sensing of a Flexible Surgical Needle Using Optical Frequency Domain Reflectometry for Prostate Interventions,"Jacynthe Francoeur, Dimitri A. Lezcano, Yernar Zhetpissov, Raman Kashyap, Iulian Iordachita, Samuel Kadoury","Polytechnique Montréal,Johns Hopkins University,Polytechnique Montreal",Surgical Robotics III,"In minimally invasive procedures such as biopsies and prostate cancer brachytherapy, accurate needle placement remains challenging due to limitations in current tracking methods related to interference, reliability, resolution or image contrast. This often leads to frequent needle adjustments and reinsertions. To address these shortcomings, we introduce an optimized needle shape-sensing method using a fully distributed grating-based sensor. The proposed method uses simple trigonometric and geometric modeling of the fiber using optical frequency domain reflectometry (OFDR), without requiring prior knowledge of tissue properties or needle deflection shape and amplitude. Our optimization process includes a reproducible calibration process and a novel tip curvature compensation method. We validate our approach through experiments in artificial isotropic and inhomogeneous animal tissues, establishing ground truth using 3D stereo vision and cone beam computed tomography (CBCT) acquisitions, respectively. Our results yield an average RMSE ranging from 0.58 Â± 0.21 mm to 0.66 Â± 0.20 mm depending on the chosen spatial resolution, achieving the submillimeter accuracy required for interventional procedures."
Integrated Magnetic Location Sensing and Actuation of Steerable Robotic Catheters for Peripheral Arterial Disease Treatment,"Jingjie Wu, Kevin Yu, Ithza Lopez, Alexa Aguilar Izquierdo, Hamidreza Saber, Farshid Alambeigi, Lei Zhou","The University of Texas at Austin,University of Texas at Austin,University of Wisconsin-Madison",Surgical Robotics III,"Magnetically steerable robotic catheters (MSRC) are a promising technology for percutaneous endovascular intervention procedures to treat peripheral arterial diseases, where magnetic actuation is used to steer the catheter tip during navigation. However, today's MSRC systems require fluoroscopic imaging for catheter localization during navigation, which risks creating radiation-induced injuries to both the patient and the surgeon. Aiming to reduce the duration of x-ray radiation in interventions using MSRCs, this letter introduces a new steerable robotic catheter system that integrates magnetic location sensing and magnetic actuation. The proposed catheter uses a magnetic tip to enable magnetic steering. In addition, a cylindrical array of magnetic sensors is used to measure the field from the catheter tip to enable real-time catheter localization. To enable improved localization accuracy, a novel nested calibration algorithm for sensor positions and magnet dipole strength is introduced. This letter further proposes a novel integration of magnetic actuation and magnetic localization in MSRC systems, where fluoroscopic imaging is only required during catheter steering at bifurcations in the vasculatures. The proposed methodology is tested with an MSRC prototype, where the magnet location estimation algorithm is implemented for real-time visual feedback to the operator with a low latency of 400 ms. Experiments show that an average localization error of 0.95 mm can be achieved a"
Semi-Autonomous Robotic Manipulator for Minimally Invasive Aortic Valve Replacement,"Izadyar Tamadon, S.m.hadi Sadati, Virginia Mamone, Vincenzo Ferrari, Christos Bergeles, Arianna Menciassi","University of Twente,King's College London,University of Pisa, EndoCAS,Università di Pisa,Scuola Superiore Sant'Anna - SSSA",Surgical Robotics III,"Aortic valve surgery is the preferred procedure for replacing a damaged valve with an artificial one. The ValveTech robotic platform comprises a flexible articulated manipulator and surgical interface supporting the effective delivery of an artificial valve by tele-operation and endoscopic vision. This manuscript presents our recent work on force-perceptive safe semi-autonomous navigation of the ValveTech platform prior to valve implantation. First, we present a force observer that transfers forces from the manipulator body and tip to a haptic interface. Second, we demonstrate how hybrid forward/inverse mechanics together with endoscopic visual servoing lead to autonomous valve positioning. Benchtop experiments and an artificial phantom quantify the performance of the developed robot controller and navigator. Valves can be autonomously delivered with a 2.0Â±0.5 mm position error, and minimal misalignment of 3.4Â±0.9Â°. The hybrid Force/Shape Observer (FSO) algorithm was able to predict distributed external forces on the articulated manipulator body with an average 0.09 N error. FSO can also estimate loads on tip with an average accuracy of 3.3%. The presented system can lead to better"
Robotic Needle Insertion with 2D Ultrasound â€“ 3D CT Fusion Guidance,"Long Lei, Baoliang Zhao, Xiaozhi Qi, Rui Mi, Hai Ye, Peng Zhang, Qiong Wang, Pheng Ann Heng, Ying Hu","Shenzhen Institute of Advanced Technology, Chinese Academy of Sc,Shenzhen Institutes of Advanced Technology, Chinese Academy of S,Department of Radiology, Shenzhen University General Hospital, S,Shenzhen Institutes of Advanced Technology,ChineseAcademyofScien,The Chinese University of Hong Kong,Shenzhen Institute of Advanced Technology, ShenZhen, China",Surgical Robotics III,"Puncture robots pave a new way for stable, accurate and safe percutaneous liver tumor puncture operation. However, affected by respiratory motion, intraoperative accurate location of the tumor and its surrounding anatomical structures remains a difficult problem in existing robot-assisted puncture operations. In this paper, a dual-arm robotic needle insertion system with guidance of intraoperative 2D ultrasound (US) and preoperative 3D computed tomography (CT) fusion is proposed, addressing the shortcomings of existing puncture robots. To deal with the challenge of cross-modal and cross-dimensional registration between 2D US and 3D CT, a decoupled two-stage registration approach combining initial vessel structure-based 3D US â€“ 3D CT registration with intraoperative intensity-based 2D US - 3D US registration is proposed. To achieve fast and robust ultrasound probe calibration, a method based on an improved N-wire phantom is proposed. Twenty puncture experiments are performed in different breath-holding positions on a respiratory motion simulation platform, and experimental results show that the mean puncture error is 2.48 mm, which can meet the requirements in a wide of clinical scenarios."
MoRC - a Modular Robot Controller,"Carsten Oldemeyer, Matthias Hellerer, Matthias Reiner, Bernhard Thiele, Patrick Weber, Tobias Bellmann","German Aerospace Center,German Aerospace Center (DLR)",Software for Robotic and Automation,"MoRC is a high-performance modular robot controller based on the Functional Mock-up Interface (FMI) standard. The goal is to control any (industrial) robot with electrical drives using a customizable vendor-agnostic control cabinet and an innovative, self-developed software architecture based on exchangeable multi-rate real-time control components with standardized interfaces. On the hardware side, the use of EtherCAT (Ethernet for Control Automation Technology) allows connecting a freely selectable number of COTS (commercial off-the-shelf) electrical drives and sensors. On the software side, this is matched with exchangeable control software modules based on the FMI standard. Those can be interconnected for forming user-defined multi-rate control structures which can be executed as synchronized real-time threads on a central Linux-based multi-core computing unit. That unlocks additional computational potential for advanced high-frequency control algorithms. Control structures can be switched at runtime to handle highly diverse control tasks. This paper presents the architectural concepts as well as first experiments on an industrial robot testbed."
Enabling the Deployment of Any-Scale Robotic Applications in Microservice Architectures through Automated Containerization,"Jean-pierre Busch, Lennart Reiher, Lutz Eckstein","RWTH Aachen University,Institute for Automotive Engineering, RWTH Aachen University",Software for Robotic and Automation,"In an increasingly automated world â€“ from warehouse robots to self-driving cars â€“ streamlining the development and deployment process and operations of robotic applications becomes ever more important. Automated DevOps processes and microservice architectures have already proven successful in other domains such as large-scale customer-oriented web services (e.g., Netflix). We recommend to employ similar microservice architectures for the deployment of small- to large-scale robotic applications in order to accelerate development cycles, loosen functional dependence, and improve resiliency and elasticity. In order to facilitate involved DevOps processes, we present and release a tooling suite for automating the development of microservices for robotic applications based on the Robot Operating System (ROS). Our tooling suite covers the automated minimal containerization of ROS applications, a collection of useful machine learning-enabled base container images, as well as a CLI tool for simplified interaction with container images during the development phase. Within the scope of this paper, we embed our tooling suite into the overall context of streamlined robotics deployment and compare it to alternative solutions. We release our tools as open-source software at https://github.com/ika-rwth-aachen/dorotos."
Plugâ€™n Play Task-Level Autonomy for Robotics Using POMDPs and Probabilistic Programs,"Or Wertheim, Dan Rouven Suissa, Ronen Brafman","ben gurion university of the negev,Ben-Gurion University of the Negev,Ben-Gurion University",Software for Robotic and Automation,"We describe AOS, the first general-purpose system for model-based control of autonomous robots using AI planning that fully supports partial observability and noisy sensing. The AOS provides a code-based language for specifying a generative model of the system, making model specification easier and model sampling efficient. It also provides a language for specifying the relationship between the model and the actual code, using which it auto-generates all required integration code. This allows Plug'n Play behavior, which facilitates incremental and modular system design. Extensive experiments on real and simulated robotic platforms demonstrate these advantages."
CoBRA: A Composable Benchmark for Robotics Applications,"Matthias Mayer, Jonathan Külz, Matthias Althoff","Technical University of Munich,Technische Universität München",Software for Robotic and Automation,"Selecting an optimal robot, its base pose, and trajectory for a given task is currently mainly done by human expertise or trial and error. To evaluate automatic approaches to this combined optimization problem, we introduce a benchmark suite encompassing a unified format for robots, environments, and task descriptions. Our benchmark suite is especially useful for modular robots, where the multitude of robots that can be assembled creates a host of additional parameters to optimize. We include tasks such as machine tending and welding in synthetic environments and 3D scans of real-world machine shops. All benchmarks are accessible through cobra.cps.cit.tum.de, a platform to conveniently share, reference, and compare tasks, robot models, and solutions."
GSL-Bench: High Fidelity Gas Source Localization Benchmarking Tool,"Hajo Erwich, Bart Duisterhof, Guido De Croon","Delft University of Technology,Carnegie Mellon University",Software for Robotic and Automation,"Gas Source Localization (GSL) is a challenging field of research within the robotics community, with high-stakes search-and-rescue applications. Existing methods vary widely and each has its strengths and weaknesses. Comparisons of different methods are limited due to the lack of a broadly adopted and standardized testing methodology. Existing GSL evaluations vary in environment size, wind conditions, and gas simulation fidelity. They also lack photo-realistic rendering for the integration of obstacle avoidance. In this paper, we propose GSL-Bench, a benchmarking tool that can evaluate the performance of existing GSL algorithms. GSL-Bench features high-fidelity graphics and gas simulation, featuring NVIDIA's Isaac Sim and OpenFOAM computational fluid dynamics software (CFD). Realism is further increased by simulating relevant gas and wind sensors. Scene generation is simplified with the introduction of AutoGDM+, capable of procedural environment generation, CFD and particle-based gas dispersion simulation. To illustrate GSL-Bench's capabilities, three algorithms are compared in six warehouse settings of increasing complexity: E. Coli, dung beetle, and a random walker. Our results demonstrate GSL-Bench's ability to provide valuable insights into algorithm performance."
Cook2LTL: Translating Cooking Recipes to LTL Formulae Using Large Language Models,"Angelos Mavrogiannis, Christoforos Mavrogiannis, Yiannis Aloimonos","University of Maryland, College Park,University of Michigan,University of Maryland",Software for Robotic and Automation,"Cooking recipes are challenging to translate to robot plans as they feature rich linguistic complexity, temporally-extended interconnected tasks, and an almost infinite space of possible actions. Our key insight is that combining a source of cooking domain knowledge with a formalism that captures the temporal richness of cooking recipes could enable the extraction of unambiguous, robot-executable plans. In this work, we use Linear Temporal Logic (LTL) as a formal language expressive enough to model the temporal nature of cooking recipes. Leveraging a pretrained Large Language Model (LLM), we present Cook2LTL, a system that translates instruction steps from an arbitrary cooking recipe found on the internet to a set of LTL formulae, grounding high-level cooking actions to a set of primitive actions that are executable by a manipulator in a kitchen environment. Cook2LTL makes use of a caching scheme that dynamically builds a queryable action library at runtime. We instantiate Cook2LTL in a realistic simulation environment (AI2-THOR), and evaluate its performance across a series of cooking recipes. We demonstrate that our system significantly decreases LLM API calls (-51%), latency (-59%), and cost (-42%) compared to a baseline that queries the LLM for every newly encountered action at runtime."
Toward Automated Programming for Robotic Assembly Using ChatGPT,"Nicholas Cote, Annabella Macaluso, Sachin Chitta","Autodesk, Inc,University of California, San Diego,Autodesk Inc.",Software for Robotic and Automation,"Despite significant technological advancements, the process of programming robots for adaptive assembly remains labor-intensive, demanding expertise in multiple domains and often resulting in task-specific, inflexible code. This work explores the potential of Large Language Models (LLMs), like ChatGPT, to automate this process, leveraging their ability to understand natural language instructions, generalize examples to new tasks, and write code. In this paper, we suggest how these abilities can be harnessed and applied to real-world challenges in the manufacturing industry. We present a novel system that uses ChatGPT to automate the process of programming robots for adaptive assembly by decomposing complex tasks into simpler subtasks, generating robot control code, executing the code in a simulated workcell, and debugging syntax and control errors, such as collisions. We outline the architecture of this system and strategies for task decomposition and code generation. Finally, we demonstrate how our system can autonomously program robots for various assembly tasks in a real-world project."
A Method for Multi-Robot Asynchronous Trajectory Execution in MoveIt2,"Pascal Stoop, Tharaka Ratnayake, Giovanni Toffetti","OST,Zurich Applied Science University,Zurich University of Applied Sciences (ZHAW)",Software for Robotic and Automation,"This paper introduces a method that enables the parallel independent execution of trajectories for multi-robot multi-arm systems in a shared workspace in MoveIt2. The proposed method leverages a centralized scheduler in a distributed set up to prevent collisions while the robots move independently. We argue that this approach is better suited than the state of the art (i.e., synchronous execution) for flexible/adaptive robotic tasks where the actions to be performed may vary in planning and execution time depending on sensor data (e.g., pick and place with inspection, assembly) as it is able to reduce the total execution time w.r.t. current approaches leveraging a single arm or multiple arms with synchronous motion planning."
Improving the ROS 2 Navigation Stack with Real-Time Local Costmap Updates for Agricultural Applications,"Ettore Sani, Antonio Sgorbissa, Stefano Carpin","University of Genova,University of California, Merced",Software for Robotic and Automation,"The ROS 2 Navigation Stack (Nav2) has emerged as a widely used software component providing the underlying basis to develop a variety of high-level functionalities. However, when used in outdoor environments such as orchards and vineyards, its functionality is notably limited by the presence of obstacles and/or situations not commonly found in indoor settings. One such example is given by tall grass and weeds that can be safely traversed by a robot, but that can be perceived as obstacles by LiDAR sensors, and then force the robot to take longer paths to avoid them, or abort navigation altogether. To overcome these limitations, domain specific extensions must be developed and integrated into the software pipeline. This paper presents a new, lightweight approach to address this challenge and improve outdoor robot navigation. Leveraging the multi-scale nature of the costmaps supporting Nav2, we developed a system that using a depth camera performs pixel level classification on the images, and in real time injects corrections into the local cost map, thus enabling the robot to traverse areas that would otherwise be avoided by the Nav2. Our approach has been implemented and validated on a Clearpath Husky and we demonstrate that with this extension the robot is able to perform navigation tasks that would be otherwise not practical with the standard components."
Automated Non-Invasive Analysis of Motile Sperms Using Cross-Scale Guidance Network,"Wei Dai, Zixuan Wu, Jiaqi Wang, Rui Liu, Min Wang, Tianyi Wu, Junxian Zhou, Zhuoran Zhang, Jun Liu","City University of Hong Kong,The Chinese University of HongKongï¼ŒShenzhen,The Chinese University of Hong Kong, Shenzhen",Microrobotics for Biology,"Unbiased measurement of sperm morphometric and motility parameters is essential for assessing fertility potential and guiding visual feedback for microrobotic manipulation. Automated analysis of multiple sperms and selection of an optimal sperm is crucial for in vitro fertilisation treatment such as robotic intracytoplasmic sperm injection. However, conventional image processing methods have limitations in analysing small sperm objects under microscopic imaging. The emergence of convolutional neural networks (CNNs) has offered promising advancements in microscopic image analysis. However, previous CNN methods have struggled to accurately segment tiny objects, requiring staining or fluorescence techniques to enhance visual contrast between sperm and culture medium, leading to clinical impracticality. To address these limitations, we introduce a novel segmentation network named the cross-scale guidance (CSG) network for accurate and efficient segmentation of minute sperm objects. The CSG network employs innovative modules, including collateral multi-scale convolution, cross-scale feature map guide, and multi-scale feature fusion, to preserve essential sperm details despite their small size. Experimental results indicate that the CSG network surpassed the state-of-the-art models designed for small object segmentation, achieving a significant increase up to 18.62% higher mean intersection over union (mIoU). Additionally, the CSG network excelled in sperm morphometric analysis, achieving errors below 20%. Moreover, sperm motility parameters were further derived from the segmentation results for comprehensive sperm fertility analysis."
Multi-Scale Visual Servoing Framework for Optical Microscopy Based on SIFT Matching,"Yameng Zhang, Ao Xu, Yuhan Chen, Max Qing Hu Meng, Li Liu","The Chinese University of Hong Kong,Southern University of Science and Technology",Microrobotics for Biology,"This paper introduces an innovative multi-scale visual servoing framework for optical microscopy, engineered to automatically reposition the microscope for high-magnification target view across multiple magnifications, thereby facilitating repetitive and accurate histologic biopsies. The framework encompasses an active microscope-camera system equipped with both auto-calibration and multi-scale visual servoing capabilities. The auto-calibration technique addresses the challenges posed by the limited depth of field and pattern requirements of the microscope-camera system, and determines its intrinsic and hand-eye parameters through a two-step algorithm. The calibration data is then utilized to execute a SIFT matching-based visual servoing control at progressively increasing magnifications, using only a single high-magnification target view as a reference, ultimately enabling rapid and precise repositioning of the microscope. Experimental results demonstrate the precision and stability of the auto-calibration method, as well as the robustness of the visual servoing method against occlusion, blur, and low illumination."
Robotic Capillary Insertion to the Xenopus Oocyte Using Microscopic Image Analysis and QCR Force Sensor,"Kazusa Otani, Hirotaka Sugiura, Shiro Watanabe, Bilal Turan, Satoshi Amaya, Fumihito Arai","The University of Tokyo,Nagoya University",Microrobotics for Biology,"This paper presented the three-dimensional oocyte manipulation system for the two-electrode voltage clamp (TEVC) experiment under stereomicroscopy. We firstly developed a sequential calibration method to correlate the workspace of the stereomicroscopy with the image and the micromanipulator. Even though the focal depth of the microscopy was limited, the proposed method functioned the three-dimensional position detection and calculated the homogeneous transformation matrix. We secondly employed hybrid use of the image-based manipulation and the quartz crystal resonator (QCR) force sensor. The imaging technique was used to detect the tip of the glass capillary and the contact to the cell membrane, whereas the QCR force sensor was incorporated to detect the force interaction between the sample and the glass capillary. Using the system and proposed technique, we demonstrated the automatic capillary insertion for TEVC experiment, at which the low insertion depth was preferable. The results indicated that the coordination calibration technique provided the positioning accuracy of the capillary tip on the order of 10 um. The imaging technique could detect the contact to the elastic objects and cell membrane. QCR force sensor achieved quite small force measurement and feedback control at the control frequency of 100 Hz without latency."
Robotic Mosaic Atomic Force Microscopy through Sequential Imaging and Multiview Iterative Closest Points Method,"Freddy Romero Leiro, Stéphane Régnier, Frederic Delarue, Mokrane Boudaoud","Sorbonne Université - Institut des Systèmes Intélligents et Robo,Sorbonne University,Sorbonne Université",Microrobotics for Biology,"This paper presents a functionality that has been developed for the home-made AFM-in-SEM robotic system at the ISIR laboratory. The method allows extending the range of an Atomic Force Microscope (AFM) and dealing with drift issues by fusing multiple individually AFM topography patches. The merging of the patches into a single image is done through a Generalized Procrustes Analysis Iterative Closest Point (GPA-ICP) algorithm. To validate the effectiveness of the approach, an AFM image of a TGX1 calibration grid and a 3.4- billion-year-old organic-walled microfossil are reconstructed by automatically merging 50 AFM elementary topography patches of dimension 0.9 Î¼m Ã— 1.2 Î¼m based on feature matching. The overlap between two adjacent patches is 50 % and 33 % in the X and Y axes respectively. The result is a coherent 3.2 Î¼m Ã— 3.0 Î¼m drift-free long range AFM topography without significant artifacts. The method is tested using an AFM-in- SEM system based on a 3-DOF cartesian robot equipped with inertial piezoelectric actuators. This method can be used to extend the range of any type of AFM with a dual XY stage setup. Thus, it opens the door for high-resolution long-range AFM by adding a long-range coarse resolution stage to a preexisting AFM system all without needing to actuate both stages simultaneously."
Automated Sperm Immobilization with a Clinically-Compatible and Compact XYZ Stage,"Haocong Song, Wenyuan Chen, Changsheng Dai, Guanqiao Shan, Steven Yang, Aojun Jiang, Zhuoran Zhang, Yu Sun","University of Toronto,university of Toronto,Dalian University of Technology,The Chinese University of Hong Kong, Shenzhen",Microrobotics for Biology,"Automated positioning systems play a pivotal role in micro-scale cell manipulation. In clinical intracytoplasmic sperm injection (ICSI) of in vitro fertilization (IVF) treatment, a motile sperm needs to be immobilized by glass micropipette tapping for subsequent surgical steps. The process requires accurate tracking of the target sperm and precise alignment between the sperm tail and the micropipette. Manual sperm immobilization suffers from inconsistent success rates, and current robotic systems developed for the task fail to comply with the standard clinical setup. Instead of using a motorized micromanipulator as in existing robotic systems, this paper presents an automated, compact three-dimensional positioning stage for sperm immobilization that can be seamlessly integrated into standard clinical platforms. Based on the analysis of the sperm head orientation, an adaptive tail tapping planning strategy is established to avoid the risk of touching the sperm head where DNA is contained. A visual servo controller equipped with a dynamic sperm motion observer is employed to achieve precise tracking and positioning of the target sperm three-dimensionally. Experimental results revealed the system achieved a success rate of 93.5% and a time cost of 5.5 s for automated sperm immobilization."
Automated Sperm Morphology Analysis Based on Instance-Aware Part Segmentation,"Wenyuan Chen, Haocong Song, Changsheng Dai, Aojun Jiang, Guanqiao Shan, Hang Liu, Yanlong Zhou, Khaled Abdalla, Shivani N Dhanani, Moosavi Katy Fatemeh, Shruti Pathak, Clifford Librach, Zhuoran Zhang, Yu Sun","university of Toronto,University of Toronto,Dalian University of Technology,Henan University,CReATe Fertility Centre,Create Fertility Center,The Chinese University of Hong Kong, Shenzhen",Microrobotics for Biology,"Traditional sperm morphology analysis is based on tedious manual annotation. Automated morphology analysis of a high number of sperm requires accurate segmentation of each sperm part and quantitative morphology evaluation. State-of-the-art instance-aware part segmentation networks follow a â€œdetect-then-segmentâ€ paradigm. However, due to spermâ€™s slim shape, their segmentation suffers from large context loss and feature distortion due to bounding box cropping and resizing during ROI Align. Moreover, morphology measurement of sperm tail is demanding because of the long and curved shape and its uneven width. This paper presents automated techniques to measure sperm morphology parameters automatically and quantitatively. A novel attention-based instance-aware part segmentation network is designed to reconstruct lost contexts outside bounding boxes and to fix distorted features, by refining preliminary segmented masks through merging features extracted by feature pyramid network. An automated centerline-based tail morphology measurement method is also proposed, in which an outlier filtering method and endpoint detection algorithm are designed to accurately reconstruct tail endpoints. Experimental results demonstrate that the proposed network outperformed the state-of-the-art top-down RP-R-CNN by 9.2% AP_vol^p, and the proposed automated tail morphology measurement method achieved high measurement accuracies of 95.34%,96.39%,91.20% for length, width and curvature, respectively."
Fast Photoacoustic Microscopy with Robot Controlled Microtrajectory Optimization,"Yating Luo, Yuxuan Liu, Jiasheng Zhou, Sung-liang Chen, Yao Guo, Guang-Zhong Yang","Shanghai Jiao Tong University,Shanghai Jiao Tong Univerity",Microrobotics for Biology,"Photoacoustic Microscopy (PAM) is a relatively new imaging modality in biomedicine. However, point-by-point raster scanning in PAM suffers from low imaging speed. Sparse sampling has been studied in recent years and with the development of deep learning algorithms, extensive efforts have been devoted to sparse image reconstruction while little attention has been paid to sparse sampling trajectory design required for actual implementation. The use of real-time adaptive robotically controlled sampling with micro-scale accuracy with due consideration of physical constraints can pave the way for using PAM for robot-assisted microsurgery. This work proposes a fast PAM scheme with robot-controlled microtrajectory optimization. The proposed method is adaptive to imaging details of different regions of interest (ROI) and detailed experiments have been conducted on both simulation and in-vivo settings. Results show that our proposed method can achieve faster scanning speed than traditional raster scanning and improved image quality in ROI than the standard spiral trajectory, which demonstrates the effectiveness of our proposed method and its potential to be deployed in other point-by-point scanning systems."
Acoustically Driven Micropipette for Hydrodynamic Manipulation of Mouse Oocytes,"Zhaofeng Zuo, Xiaoming Liu, Zhuo Chen, Yuyang Li, Xiaoqing Tang, Dan Liu, Qiang Huang, Tatsuo Arai","Beijing institute of technology,Beijing Institute of Technology,University of Electro-Communications",Microrobotics for Biology,"Micromanipulation techniques that can achieve controlled fine operations at the micro scale play an important role in biomedical fields including embryo engineering, gene engineering, drug screening, and cell analysis. However, micromanipulation of biological micro-objects, such as cells and micro tissues, suffers from mechanical damage and low efficiency. Several techniques have been introduced to manipulate cells more easily, but most of them are restricted by expensive devices, limited work area, and potential damage to cellular structure. Here we develop a hydrodynamic manipulation method to rotate and transport mouse oocytes, which utilizes acoustic waves and micropipette to generate acoustic radiation force and excite microstreaming. This method can accomplish rotational and translational operations precisely and controllably. We tested the process of trapping, rotation, and transportation of the mouse oocytes, and measured rotational and translational speed with a range of applied voltage. The method was able to shorten the cost time of delivery and posture adjustment before oocyte injection. Our study provides an easy-to-use technique for oocyte manipulation without contact, and it has the potential to be universally applied in many cellular studies."
Remote Control of Untethered Magnetic Robots within a Lumen Using X-Ray-Guided Robotic Platform,"Leendert-Jan Wouter Ligtenberg, Nicole Christina Antoinetta Rabou, Sander Lars Peters, Trishal Sai Srinivas Vengetela, Schut Vincent, Herman Remco Liefers, Michiel Warle, Islam S. M. Khalil","University of Twente,Universiteit Twente,Saxion University,Radboud University Medical Center",Microrobotics for Biology,"Until now, the potential of untethered magnetic robots (UMRs), propelled by external time-periodic magnetic fields, has been hindered by the limitations of wireless manipulation systems or noninvasive imaging techniques combined. The need for simultaneous actuation and noninvasive localization imposes a strict constraint on both functionalities. This study addresses this challenge by substantiating the feasibility through experimental validation, showcasing the direct teleoperation of UMRs within a fluid-filled lumen. This teleoperation capability is facilitated by a scalable X-ray-guided robotic platform, extendable to match the dimensions required for in vivo applications, marking a noteworthy advancement. Our methodology is demonstrated by teleoperating a 12-mm-long screw-shaped UMR (5 mm in diameter) within a bifurcated lumen, filled with blood. This navigation is achieved using controlled rotating magnetic fields, guided by real-time Xray Fluoroscopy images. Incorporating a two-degree-of-freedom control system, we demonstrate the operatorâ€™s capability to use X-ray Fluoroscopy images to keep the UMR coupled with the external field during wireless teleoperations, resulting in a success rate of 76.6% when moving along the intended pathways, with a mean absolute position error of 1.6 Â± 2.1 mm."
TELESIM: A Modular and Plug-And-Play Framework for Robotic Arm Teleoperation Using a Digital Twin,"Florent Audonnet, Jonathan Grizou, Andrew Hamilton, Gerardo Aragon-Camarasa","University of Glasgow,School of Computing Science, University of Glasgow",Telerobotics and Teleoperation II,"Teleoperating robotic arms can be a challenging task for non-experts, particularly when using complex control devices or interfaces. To address the limitations and challenges of existing teleoperation frameworks, such as cognitive strain, control complexity, robot compatibility, and user evaluation, we propose TELESIM, a modular and plug-and-play framework that enables direct teleoperation of any robotic arm using a digital twin as the interface between users and the robotic system. Due to TELESIM's modular design, it is possible to control the digital twin using any device that outputs a 3D pose, such as a virtual reality controller or a finger-mapping hardware controller. To evaluate the efficacy and user-friendliness of TELESIM, we conducted a user study with 37 participants. The study involved a simple pick-and-place task, which was performed using two different robots equipped with two different control modalities. Our experimental results show that most users were able to succeed by building at least a tower of 3 cubes in 10 minutes, with only 5 minutes of training beforehand, regardless of the control modality or robot used, demonstrating the usability and user-friendliness of TELESIM."
Synchronized Human-Humanoid Motion Imitation,"Antonin Dallard, Mehdi Benallegue, Fumio Kanehiro, Abderrahmane Kheddar","LIRMM,AIST Japan,National Inst. of AIST,CNRS-AIST",Telerobotics and Teleoperation II,"We present a tele-operation control framework that (i) enhances the upper motion synchrony between a user and a robot using the minimum-jerk model coupled with a recursive least-square filter, and (ii) synchronizes the walking pace by predicting user's stepping frequency using motion capture data and a deep learning model. By integrating (i) and (ii) in a task-space whole-body controller, we achieve full-body synchronization. We assess our humanoid-to-human whole-body synchronized motion model on the HRP-4 humanoid robot in forward, lateral and backward walks with concurrent upper limbs motions experiments."
SPOTS: Stable Placement of Objects with Reasoning in Semi-Autonomous Teleoperation Systems,"Joonhyung Lee, Sangbeom Park, Jeongeun Park, Kyungjae Lee, Sungjoon Choi","Korea University,Chung-Ang University",Telerobotics and Teleoperation II,"Pick-and-place is one of the fundamental tasks in robotics research. However, the attention has been mostly focused on the ``pick'' task, leaving the ``place'' task relatively unexplored. In this paper, we address the problem of placing objects in the context of a teleoperation framework. Particularly, we focus on two aspects of the place task: stability robustness and contextual reasonableness of object placements. Our proposed method combines simulation-driven physical stability verification via real-to-sim and the semantic reasoning capability of large language models. In other words, given place context information (e.g., user preferences, object to place, and current scene information), our proposed method outputs a probability distribution over the possible placement candidates, considering the robustness and reasonableness of the place task. Our proposed method is extensively evaluated in two simulation and one real world environments and we show that our method can greatly increase the physical plausibility of the placement as well as contextual soundness while considering user preferences. Code, video, and details are available at: https://joonhyung-lee.github.io/spots/"
Online Minimization of the Robot Silhouette Viewed from Eye-To-Hand Camera,"Giovanni Cortigiani, Bernardo Brogi, Alberto Villani, Tommaso Lisini Baldi, Nicole D'aurizio, Domenico Prattichizzo",University of Siena,Telerobotics and Teleoperation II,"Redundant robots have the potential to perform internal joints motion without modifying the pose of the end-effector by exploiting the null-space of the Jacobian matrix. Capitalizing on that feature, we developed a control technique for minimizing the robot visual appearance when observed from an eye-to-hand camera. Such algorithm is instrumental in contexts where quickly adjusting the perspective to see objects obstructed by the robot is impractical (e.g., teleoperation in narrow environment). Diminished reality techniques are frequently employed in these cases to mitigate the robot intrusion into the environment, although these techniques may sometimes compromise the perceived realism. The experimental evaluation confirmed the effectiveness of our control algorithm, demonstrating an average reduction of 4.67% of the area covered by the robot within the frame when compared to the case without the optimization action."
IRoCo: Intuitive Robot Control from Anywhere Using a Smartwatch,"Fabian Clemens Weigend, Xiao Liu, Shubham Dilip Sonawani, Neelesh Kumar, Venugopal Vasudevan, Heni Ben Amor","Arizona State University,Procter and Gamble,Procter & Gamble",Telerobotics and Teleoperation II,"This paper introduces iRoCo (intuitive Robot Control) - a framework for ubiquitous human-robot collaboration using a single smartwatch and smartphone. By integrating probabilistic differentiable filters, iRoCo optimizes a combination of precise robot control and unrestricted user movement from ubiquitous devices. We demonstrate and evaluate the effectiveness of iRoCo in practical teleoperation and drone piloting applications. Comparative analysis shows no significant difference between task performance with iRoCo and gold-standard control systems in teleoperation tasks. Additionally, iRoCo users complete drone piloting tasks 32% faster than with a traditional remote control and report less frustration in a subjective load index questionnaire. Our findings strongly suggest that iRoCo is a promising new approach for intuitive robot control through smartwatches and smartphones from anywhere, at any time. The code is available at www.github.com/wearable-motion-capture"
Integrating Open-World Shared Control in Immersive Avatars,"Patrick Naughton, James Seungbum Nam, Andrew Stratton, Kris Hauser","University of Illinois at Urbana-Champaign,University of Michigan",Telerobotics and Teleoperation II,"Teleoperated avatar robots allow people to transport their manipulation skills to environments that may be difficult or dangerous to work in. Current systems are able to give operators direct control of many components of the robot to immerse them in the remote environment, but operators still struggle to complete tasks as competently as they could in person. We present a framework for incorporating open-world shared control into avatar robots to combine the benefits of direct and shared control. This framework preserves the fluency of our avatar interface by minimizing obstructions to the operator's view and using the same interface for direct, shared, and fully autonomous control. In a human subjects study (N=19), we find that operators using this framework complete a range of tasks significantly more quickly and reliably than those that do not."
Hierarchical Deep Learning for Intention Estimation of Teleoperation Manipulation in Assembly Tasks,"Mingyu Cai, Karankumar Patel, Soshi Iba, Songpo Li","University of California Riverside,Honda Research Institute,Honda Research Institute USA",Telerobotics and Teleoperation II,"In human-robot collaboration, shared control presents an opportunity to teleoperate robotic manipulation to improve the efficiency of manufacturing and assembly processes. Robots are expected to assist in executing the user's intentions. To this end, robust and prompt intention estimation is needed, relying on behavioral observations. The framework presents an intention estimation technique at hierarchical levels i.e., low-level actions and high-level tasks, by incorporating multi-scale hierarchical information in neural networks. Technically, we employ hierarchical dependency loss to boost overall accuracy. Furthermore, we propose a multi-window method that assigns proper hierarchical prediction windows of input data. An analysis of the predictive power with various inputs demonstrates the predominance of the deep hierarchical model in the sense of prediction accuracy and early intention identification. We implement the algorithm on a virtual reality (VR) setup to teleoperate robotic hands in a simulation with various assembly tasks to show the effectiveness of online estimation."
Dynamic Mobile Manipulation Via Whole-Body Bilateral Teleoperation of a Wheeled Humanoid,"Amartya Purushottam, Christopher Xu, Yeongtae Jung, Joao Ramos","University of Illinois, Urbana-Champaign,University of Illinois Urbana-Champaign,Jeonbuk National University,University of Illinois at Urbana-Champaign",Telerobotics and Teleoperation II,"Humanoid robots have the potential to help human workers by realizing physically demanding manipulation tasks such as moving large boxes within warehouses. We define such tasks as Dynamic Mobile Manipulation (DMM). This paper presents a framework for DMM via whole-body teleoperation, built upon three key contributions: Firstly, a teleoperation framework employing a Human Machine Interface (HMI) and a bi-wheeled humanoid, SATYRR, is proposed. Secondly, the study introduces a dynamic locomotion mapping, utilizing human-robot reduced order models, and a kinematic retargeting strategy for manipulation tasks. Additionally, the paper discusses the role of whole-body haptic feedback for wheeled humanoid control. Finally, the system's effectiveness and mappings for DMM are validated through locomanipulation experiments and heavy box pushing tasks. Here we show two forms of DMM: grasping a target moving at an average speed of 0.4 m/s, and pushing boxes weighing up to 105% of the robot's weight. By simultaneously adjusting their pitch and using their arms, the pilot adjusts the robot pose to apply larger contact forces and move a heavy box at a constant velocity of 0.2 m/s."
3D Autocomplete: Enhancing UAV Teleoperation with AI in the Loop,"Batool Ibrahim, Imad Elhajj, Daniel Asmar","American University of Beirut AUB,American University of Beirut",Telerobotics and Teleoperation II,"Manually teleoperating a flying robot can be a demanding task, especially for users with limited levels of experience. This is primarily due to the non-linear properties of such robots in addition to the difficulty of controlling various degrees of freedom at the same time. 3D Autocomplete helps mitigate such limitations by assisting the users in teleoperation. It aids in teleoperating 3D motions, such as helical motions, which are more challenging to the users. The proposed framework uses Artificial Intelligence (AI) to predict just-in-time the user's intended motion and then, if the user accepts, completes it autonomously in 3D. The AI component of 3D Autocomplete was presented in our previous work, where we introduced a deep learning model and an algorithm to predict as early as possible the user's desired motion. Moving forward in this work, we focus on synthesizing and completing the user-intended motion autonomously. Also, we introduce a Mixed Reality (MR) user interface for better human-robot interaction. Finally, we evaluate our system subjectively and objectively through human-subject experiments. Autocomplete outperformed traditional method on all criteria with at least 30% improvement in all objective measures."
Control-Barrier-Aided Teleoperation with Visual-Inertial SLAM for Safe MAV Navigation in Complex Environments,"Siqi Zhou, Sotiris Papatheodorou, Stefan Leutenegger, Angela P. Schoellig","Technical University of Munich,TU Munich",Perception and Autonomy III,"In this paper, we consider a Micro Aerial Vehicle (MAV) system teleoperated by a non-expert and introduce a perceptive safety filter that leverages Control Barrier Functions (CBFs) in conjunction with Visual-Inertial Simultaneous Localization and Mapping (VI-SLAM) and dense 3D occupancy mapping to guarantee safe navigation in complex and unstructured environments. Our system relies solely on onboard IMU measurements, stereo infrared images, and depth images and autonomously corrects teleoperated inputs when they are deemed unsafe. We define a point in 3D space as unsafe if it satisfies either of two conditions: (i) it is occupied by an obstacle, or (ii) it remains unmapped. At each time step, an occupancy map of the environment is updated by the VI-SLAM by fusing the onboard measurements, and a CBF is constructed to parameterize the (un)safe region in the 3D space. Given the CBF and state feedback from the VI-SLAM module, a safety filter computes a certified reference that best matches the teleoperation input while satisfying the safety constraint encoded by the CBF. In contrast to existing perception-based safe control frameworks, we directly close the perception-action loop and demonstrate the full capability of safe control in combination with real-time VI-SLAM without any external infrastructure or prior knowledge of the environment. We verify the efficacy of the perceptive safety filter in real-time MAV experiments using exclusively onboard sensing and computation and show that the teleoperated MAV is able to safely navigate through unknown environments despite arbitrary inputs sent by the teleoperator."
Flow Shadowing: A Method to Detect Multiple Flow Headings Using an Array of Densely Packed Whisker-Inspired Sensors,"Teresa Kent, Sarah Bergbreiter",Carnegie Mellon University,Perception and Autonomy III,"Understanding airflow around a drone is critical for performing advanced maneuvers while maintaining flight stability. Recent research has worked to understand this flow by employing 2D and 3D flow sensors to measure flow from a single source like wind or the droneâ€™s relative motion. Our current work advances flow detection by introducing a strategy to distinguish between two flow sources applied simultaneously from different directions. By densely packing an array of flow sensors (or whiskers), we alter the path of airflow as it moves through the array. We have named this technique â€œflow shadowingâ€ because we take advantage of the fact that a downstream whisker shadowed (or occluded) by an upstream whisker receives less incident flow. We show that this relationship is predictable for two whiskers based on the percent of occlusion. We then show that a 2x2 spatial array of whiskers responds asymmetrically when multiple flow sources from different headings are applied to the array. This asymmetry is direction-dependent, allowing us to predict the headings of flow from two different sources, like wind and a droneâ€™s relative motion."
Onboard Dynamic-Object Detection and Tracking for Autonomous Robot Navigation with RGB-D Camera,"Zhefan Xu, Xiaoyang Zhan, Yumeng Xiu, Christopher Suzuki, Kenji Shimada",Carnegie Mellon University,Perception and Autonomy III,"Deploying autonomous robots in crowded indoor environments usually requires them to have accurate dynamic obstacle perception. Although plenty of previous works in the autonomous driving field have investigated the 3D object detection problem, the usage of dense point clouds from a heavy Light Detection and Ranging (LiDAR) sensor and their high computation cost for learning-based data processing make those methods not applicable to small robots, such as vision-based UAVs with small onboard computers. To address this issue, we propose a lightweight 3D dynamic obstacle detection and tracking (DODT) method based on an RGB-D camera, which is designed for low-power robots with limited computing power. Our method adopts a novel ensemble detection strategy, combining multiple computationally efficient but low-accuracy detectors to achieve real-time high-accuracy obstacle detection. Besides, we introduce a new feature-based data association and tracking method to prevent mismatches utilizing point clouds' statistical features. In addition, our system includes an optional and auxiliary learning-based module to enhance the obstacle detection range and dynamic obstacle identification. The proposed method is implemented in a small quadcopter, and the results show that our method can achieve the lowest position error (0.11m) and a comparable velocity error (0.23m/s) across the benchmarking algorithms running on the robot's onboard computer."
APACE: Agile and Perception-Aware Trajectory Generation for Quadrotor Flights,"Xinyi Chen, Yichen Zhang, Boyu Zhou, Shaojie Shen","The Hong Kong University of Science and Technology,Sun Yat-sen University,Hong Kong University of Science and Technology",Perception and Autonomy III,"Various perception-aware planning approaches have attempted to enhance the state estimation accuracy during maneuvers, while the feature matchability among frames, a crucial factor influencing estimation accuracy, has often been overlooked. In this paper, we present APACE, an Agile and Perception-Aware trajeCtory gEneration framework for quadrotors aggressive flight, that takes into account feature matchability during trajectory planning. We seek to generate a perception-aware trajectory that reduces the error of visual-based estimator while satisfying the constraints on smoothness, safety, agility and the quadrotor dynamics. The perception objective is achieved by maximizing the number of covisible features while ensuring small enough parallax angles. Additionally, we propose a differentiable and accurate visibility model that allows decomposition of the trajectory planning problem for efficient optimization resolution. Through validations conducted in both a photorealistic simulator and real-world experiments, we demonstrate that the trajectories generated by our method significantly improve state estimation accuracy, with root mean square error (RMSE) reduced by up to an order of magnitude. The source code will be released to benefit the community."
SpECULARIA: Towards Fully Autonomous Robotic Indoor Farming System,"Marsela Polic, Barbara Arbanas, Jelena Vuletic, Matko Orsag","University of Zagreb,University of Zagreb, Faculty of Electrical Engineering and Comp",Perception and Autonomy III,"To support the hypothesis that embracing robotics has the potential to address farming challenges and at the same time replace large and complex farm machinery, this paper proposes designing a farm around a heterogeneous robotic system dubbed SpECULARIA. Within this multi-robot system, mobile robots are deployed to work just like in a warehouse, moving plants grown in containers to make sure every plant receives optimal care and ideal growing conditions. By structuring the work cell environment around a stationary dual arm manipulator, the system can plan and execute procedures to control every plant's growth and hygiene, from seed to harvest. Such a system surpasses current farming robots in scalability and versatility. We showcase compliance control algorithms combined with artificial intelligence which help us build a functional model of the plant. The same approach is used to program different plant treatments. Finally we benchmark the proposed setup with a classical mobile manipulation approach, demonstrating its feasibility."
End-To-End Thermal Updraft Detection and Estimation for Autonomous Soaring Using Temporal Convolutional Networks,"Christian Gall, Walter Fichter, Aamir Ahmad",University of Stuttgart,Perception and Autonomy III,"Exploiting thermal updrafts to gain altitude can significantly extend the endurance of fixed-wing aircraft, as has been demonstrated by human glider pilots for decades. In this work, we present a novel end-to-end deep learning approach for the simultaneous detection of multiple thermal updrafts and the estimation of their properties - a key capability to let autonomous unmanned aerial vehicles soar as well. In contrast to previous works, our approach does not require separate algorithms for the detection of individual updrafts. Instead, a sequence of sensor measurements from a time window of interest can be directly fed into our temporal convolutional network, which estimates the position, strength, and spread of the encountered updrafts. We demonstrated in simulations that our approach can reliably detect updrafts solely based on measurements of the aircraft's position and the local vertical wind velocity. Nevertheless, our method can additionally make use of measurements of the roll moment induced by updrafts, which improves the precision further. Compared with a particle-filter-based method, we can determine the correct number of encountered updrafts with an accuracy of 99.99% instead of 79.50%, significantly improve the precision of strength as well as spread estimates, and reduce the computational demand."
SANet: Small but Accurate Detector for Aerial Flying Object,"Xunkuai Zhou, Benyun Zhao, Guidong Yang, Jihan Zhang, Li Li, Ben M. Chen","Tongji University,The Chinese University of Hong Kong,Chinese University of Hong Kong",Perception and Autonomy III,"This paper proposes SANet, a small but accurate detector for aerial flying objects. The detector introduces an attention module into the feature extraction module (FEM) for enhancing the accuracy. This FEM with fewer convolutional kernel channels can reduce the parameters, speed up the inference time, and mitigate the computational burden.Furthermore, we optimize the Spatial Pyramid Pooling (SPP) module to enhance both the accuracy and speed. By analyzing the structure characteristic of the ResNet and RepVGG network that are usually utilized to extract features, a feature fusion module named RepNeck is designed to comprehensively fuse features extracted by the FEM, further enhancing the speed and accuracy. Eventually, we develop a neural network with an impressively small model size of only 4.5M. This network can achieve the state-of-the-art performance on three challenging datasets. Apart from its superior performance, our approach enjoys a real-time detection speed of 14.8 frames per second (fps) and power consumption of only 2.9W while the CPU and GPU temperatures are maintained below 50â—¦C even on an edge- computing device, highlighting the practicality of our approach for long-duration flying object detection and monitor tasks."
N-QR: Natural Quick Response Codes for Multi-Robot Instance Correspondence,"Nathaniel Glaser, Rajashree Ravi, Zsolt Kira","Georgia Institute of Technology,Bowery Farming",Perception and Autonomy III,"Image correspondence serves as the backbone for many tasks in robotics, such as visual fusion, localization, and mapping. However, existing correspondence methods do not scale to large multi-robot systems, and they struggle when image features are weak, ambiguous, or evolving. In response, we propose Natural Quick Response codes, or N-QR, which enables rapid and reliable correspondence between large-scale teams of heterogeneous robots. Our method works like a QR code, using keypoint-based alignment, rapid encoding, and error correction via ensembles of image patches of natural patterns. We deploy our algorithm in a production-scale robotic farm, where groups of growing plants must be matched across many robots. We demonstrate superior performance compared to several baselines, obtaining a retrieval accuracy of 88.2%. Our method generalizes to a farm with 100 robots, achieving a 12.5x reduction in bandwidth and a 20.5x speedup. We leverage our method to correspond 700k plants and confirm a link between a robotic seeding policy and germination."
Containerized Vertical Farming Using Cobots,"Dasharadhan Mahalingam, Aditya Patankar, Khiem Phi, Nilanjan Chakraborty, Ryan Mcgann, Iv Ramakrishnan","Stony Brook University,CubicAcres LLC",Robotics and Automation in Agriculture and Forestry IV,"Containerized vertical farming is a type of vertical farming practice using hydroponics in which plants are grown in vertical layers within a mobile shipping container. Space limitations within shipping containers make the automation of different farming operations challenging. In this paper, we explore the use of cobots (i.e., collaborative robots) to automate two key farming operations, namely, the transplantation of saplings and the harvesting of grown plants. Our method uses a single demonstration from a farmer to extract the motion constraints associated with the tasks, namely, transplanting and harvesting, and can then generalize to different instances of the same task. For transplantation, the motion constraint arises during insertion of the sapling within the growing tube, whereas for harvesting, it arises during extraction from the growing tube. We present experimental results to show that using RGBD camera images (obtained from an eye-in-hand configuration) and one demonstration for each task, it is feasible to perform transplantation of saplings and harvesting of leafy greens using a cobot, without task-specific programming."
INoD: Injected Noise Discriminator for Self-Supervised Representation Learning in Agricultural Fields,"Julia Hindel, Nikhil Gosala, Kevin Bregler, Abhinav Valada","University of Freiburg,Fraunhofer IPA",Robotics and Automation in Agriculture and Forestry IV,"Perception datasets for agriculture are limited both in quantity and diversity which hinders effective training of supervised learning approaches. Self-supervised learning techniques alleviate this problem, however, existing methods are not optimized for dense prediction tasks in agriculture domains which results in degraded performance. In this work, we address this limitation with our proposed Injected Noise Discriminator (INoD) which exploits principles of feature replacement and dataset discrimination for self-supervised representation learning. INoD interleaves feature maps from two disjoint datasets during their convolutional encoding and predicts the dataset affiliation of the resultant feature map as a pretext task. Our approach enables the network to learn unequivocal representations of objects seen in one dataset while observing them in conjunction with similar features from the disjoint dataset. This allows the network to reason about higher-level semantics of the entailed objects, thus improving its performance on various downstream tasks. Additionally, we introduce the novel Fraunhofer Potato 2022 dataset consisting of over 16,800 images for object detection in potato fields. Extensive evaluations of our proposed INoD pretraining strategy for the tasks of object detection, semantic segmentation, and instance segmentation on the Sugar Beets 2016 and our potato dataset demonstrate that it achieves state-of-the-art performance."
Unsupervised Generation of Labeled Training Images for Crop-Weed Segmentation in New Fields and on Different Robotic Platforms,"Yue Linn Chong, Jan Weyler, Philipp Lottes, Jens Behley, Cyrill Stachniss",University of Bonn,Robotics and Automation in Agriculture and Forestry IV,"Agricultural robots have the potential to improve the efficiency and sustainability of existing agricultural practices. Most autonomous agricultural robots rely on machine vision systems. Such systems,however, often perform worse in new fields or when the robotic platforms change. While we can alleviate the performance degradation by manually labeling more data obtained in the new setup, this procedure is labor and cost-intensive. Therefore, we propose an approach to improve the performance of machine vision systems for new fields and different robotic platforms without additional manual labeling. In an unsupervised manner, our approach can generate images and corresponding labels to train machine vision systems. We use StyleGAN2 to generate images that appear like they are from desired new field or robotic platform. Additionally, we propose a label refinement method to generate labels corresponding to the generated images. We show that our approach can improve the performance of the crop-weed segmentation task in new fields and on different robotic platforms without additional manual labeling."
Log Loading Automation for Timber-Harvesting Industry,"Elie Ayoub, Heshan Fernando, William Larrivée-hardy, Nicolas Lemieux, Philippe Giguere, Inna Sharf","FPInnovations,FP Innovations,Laval University,Université Laval,McGill University",Robotics and Automation in Agriculture and Forestry IV,"The timber-harvesting industry is lagging its peer industries, such as mining and agriculture, with respect to deployment of robotic, AI and autonomous technologies. In this paper, we tackle automation of a critical task that arises in transporting logs from the forest to the sawmill: the log loading operation. This work is motivated by the acute shortages of human operators and the need to improve the efficiencies of timber-harvesting processes. To this end, we demonstrate the full autonomy pipeline for the log loading operation with a fixed-base manipulator (a.k.a., the crane), starting with perception of logs around the machine, then grasp planning for where to grasp logs, through motion planning and control of the log loading maneuver. Our main contribution is in the full integration of the necessary elements to achieve a completely autonomous loading cycle, where the crane picks up and loads all logs within its reach on a trailer. Notable features of our implementation are a generalizable perception stack, a grasp planner to pick up multiple logs at a time and an extensive experimental campaign conducted outdoors, on a commercial log loader retrofitted for autonomy. Our results demonstrate an overall 87% success rate of the log loading operation, with primary failure cases due to log segmentation errors and deficiencies in the final height adjustment algorithm for grasping logs. We also present detailed timing results of the main parts of the autonomy pipeline, which support the feasibility of deployment in operational environment."
Region-Determined Localization Method for Unmanned Ground Vehicle under Pole-Like Feature Environment,"Yu-hsiang Lai, Chia-yun Chuang, Yu-qiang Chen, Feng-li Lian","National Taiwan university,National Taiwan University",Robotics and Automation in Agriculture and Forestry IV,"In this paper, a region-determined navigation method applied for unmanned ground vehicles (UGVs) is presented. The method aims to solve GNSS-denied localization problem using pole-like feature such as trees or street lights. The approach includes three parts: mapping, bounding, and localization. To map and reconstruct the environment, the hector mapping approach and circle-fitting method are adopted for the occupancy mapping and feature mapping. To bound out the available working region, we define the intersection area of features' enlarged radius and desired operating area as negative and positive virtual boundaries. While the robot is cruising, the likelihood detection method is adopted for obstacle searching and comparing. Using the detection's searching results as feedback reference, the Extended Kalman Filter (EKF) can modify the shifting between the GNSS signal and the true waypoints of the mowing robot. Three cruising demonstrations are presented to show the mapping and optimizing results. Different cases of demonstration represent different situations and potential issues."
Tree Instance Segmentation and Traits Estimation for Forestry Environments Exploiting LiDAR Data Collected by Mobile Robots,"Meher Venkata Ramakrishna Malladi, Tiziano Guadagnino, Luca Lobefaro, Matias Mattamala, Holger Griess, Janine Schweier, Nived Chebrolu, Maurice Fallon, Jens Behley, Cyrill Stachniss","University of Bonn,University of Oxford,Swiss Federal Institute for Forest, Snow and Landscape Research",Robotics and Automation in Agriculture and Forestry IV,"Forests play a crucial role in our ecosystems, functioning as carbon sinks, climate stabilizers, biodiversity hubs, and sources of wood. By the very nature of their scale, monitoring and maintaining forests is a challenging task. Robotics in forestry can have the potential for substantial automation toward efficient and sustainable foresting practices. In this paper, we address the problem of automatically producing a forest inventory by exploiting LiDAR data collected by a mobile platform. To construct an inventory, we first extract tree instances from point clouds. Then, we process each instance to extract forestry inventory information. Our approach provides per-tree geometric traits such as diameter at breast height together with the individual tree locations in a plot. We validate our results against manual measurements collected by foresters during field trials. Our experiments show strong segmentation and tree trait estimation performance, underlining the potential for automating forestry services."
Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning,"Nicholas Harrison, Nathan Wallace, Salah Sukkarieh","The University of Sydney: The Australian Centre for Field Roboti,University of Sydney",Robotics and Automation in Agriculture and Forestry IV,"The efficient collection of samples is an important factor in outdoor information gathering applications on account of high sampling costs such as time, energy, and potential destruction to the environment. Utilization of available a-priori data can be a powerful tool for increasing efficiency. However, the relationships of this data with the quantity of interest are often not known ahead of time, limiting the ability to leverage this knowledge for improved planning efficiency. To this end, this work combines transfer learning and active learning through a Multi-Task Gaussian Process and an information-based objective function. Through this combination it can explore the space of hypothetical inter-quantity relationships and evaluate these hypotheses in real-time, allowing this new knowledge to be immediately exploited for future plans. The performance of the proposed method is evaluated against synthetic data and is shown to evaluate multiple hypotheses correctly. Its effectiveness is also demonstrated on real datasets. The technique is able to identify and leverage hypotheses which show a medium or strong correlation to reduce prediction error by a factor of 1.4--3.4 within the first 7 samples, and poor hypotheses are quickly identified and rejected eventually having no adverse effect."
Decentralized Multi-Phase Formation Control for Cattle Herding,"Dac Dang Khoa Nguyen, Gavin Paul, Alen Alempijevic",University of Technology Sydney,Robotics and Automation in Agriculture and Forestry IV,"Herding is performed by people or trained animals to control the movement of livestock under the desired direction of a operator. This paper presents a novel decentralized control strategy for a group of robots to herd animals which consists of two phases, a surrounding phase and a driving phase. In the surrounding phase, a custom artificial potential field is employed to simultaneously guide the robots to encircle the herd by tracking the outmost animals and maintaining a safe distance with other neighboring robots. Once the encirclement is complete, the robots transition to drive the animals toward a designated goal by simply maintaining their initial formation and traverse to it. Unlike existing works on herding using flocking control, local observations of the nearest animals and communication with other robots within the sensing range are the only requirements for the robots to effectively surround and herd the animals. Moreover, the animal-robot behavior model resembles interaction of livestock under the presence of an external predatory threat, where robots act as predators. An analytical proof and empirical results collected from different simulators demonstrate that the proposed control enables the robots to converge around the boundary of the animals and guide them toward the designated goal."
Quantized Visual-Inertial Odometry,"Yuxiang Peng, Chuchu Chen, Guoquan Huang",University of Delaware,Localization and Mapping II,"As edge devices equipped with cameras and inertial measurement units (IMUs) are emerging, it holds huge implications to endow these mobile devices with spatial computing capability. However, ultra-efficient visual-inertial estimation at the size, weight, and power (SWAP)-constrained edge devices to provide accurate 3D motion tracking remains challenging. This is exacerbated by data transfer (between different processors and memory) that consumes significantly more energy than computing itself. To push the state of the art, this paper proposes the first-of-its-kind quantized visual-inertial odometry (QVIO) to offer energy-efficient 3D motion tracking. In particular, we first quantize raw visual measurements in an intuitive way with a given small number of bits and then perform an EKF update with these quantized measurements (termed zQVIO). To improve this ad-hoc quantizer (although it works well in practice), we systematically quantize each measurement residual into a single bit and perform maximum-a-posterior (MAP) estimation. Thanks to these quantizers, the proposed QVIO estimators significantly reduce the data transfer and thus improve energy efficiency. As shown in our extensive experiments, the proposed residual-quantized VIO (rQVIO) achieves remarkably competing performance even when using an average of only 3.7 bits per measurement, equivalent to a data reduction of 8.6 times compared to transmitting single-precision measurements."
OCC-VO: Dense Mapping Via 3D Occupancy-Based Visual Odometry for Autonomous Driving,"Heng Li, Yifan Duan, Xinran Zhang, Haiyi Liu, Jianmin Ji, Yanyong Zhang",University of Science and Technology of China,Localization and Mapping II,"Visual Odometry (VO) plays a pivotal role in autonomous systems, with a principal challenge being the lack of depth information in camera images. This paper introduces OCC-VO, a novel framework that capitalizes on recent advances in deep learning to transform 2D camera images into 3D semantic occupancy, thereby circumventing the traditional need for concurrent estimation of ego poses and landmark locations. Within this framework, we utilize the TPV-Former to convert surround view cameras' images into 3D semantic occupancy. Addressing the challenges presented by this transformation, we have specifically tailored a pose estimation and mapping algorithm that incorporates Semantic Label Filter, Dynamic Object Filter, and finally, utilizes Voxel PFilter for maintaining a consistent global semantic map. Evaluations on the Occ3D-nuScenes not only showcase a 20.6% improvement in Success Ratio and a 29.6% enhancement in trajectory accuracy against ORB-SLAM3, but also emphasize our ability to construct a comprehensive map. Our implementation is open-sourced and available at: https://github.com/USTCLH/OCC-VO."
NF-Atlas: Multi-Volume Neural Feature Fields for Large Scale LiDAR Mapping,"Xuan Yu, Yili Liu, Sitong Mao, Shunbo Zhou, Rong Xiong, Yiyi Liao, Yue Wang","Zhejiang University,ShenZhen Huawei Cloud Computing Technologies Co., Ltd.,The Chinese University of Hong Kong",Localization and Mapping II,"LiDAR Mapping has been a long-standing problem in robotics. Recent progress in neural implicit representation has brought new opportunities to robotic mapping. In this paper, we propose the multi-volume neural feature fields, called NF-Atlas, which bridge the neural feature volumes with pose graph optimization. By regarding the neural feature volume as pose graph nodes and the relative pose between volumes as pose graph edges, the entire neural feature field becomes both locally rigid and globally elastic. Locally, the neural feature volume employs a sparse feature Octree and a small MLP to encode the signed distance function (SDF) of the submap with an option of semantics. Learning the map using this structure allows for end-to-end solving of maximum a posteriori (MAP) based probabilistic mapping. Globally, the map is built volume by volume independently, avoiding catastrophic forgetting when mapping incrementally. Furthermore, when a loop closure occurs, with the elastic pose graph based representation, only updating the origin of neural volumes is required without remapping. Finally, these functionalities of NF-Atlas are validated. Thanks to the sparsity and the optimization based formulation, NF-Atlas shows competitive performance in terms of accuracy, efficiency and memory usage on both simulation and real-world datasets. The project page is: https://yuxuan1206.github.io/NFAtlas/."
Dusk Till Dawn: Self-Supervised Nighttime Stereo Depth Estimation Using Visual Foundation Models,"Madhu Vankadari, Samuel Hodgson, Sangyun Shin, Kaichen Zhou, Andrew Markham, Niki Trigoni","University of Oxford,Oxford University",Localization and Mapping II,"Self-supervised depth estimation algorithms rely heavily on frame-warping relationships, exhibiting substantial performance degradation when applied in challenging circum- stances, such as low-visibility and nighttime scenarios with varying illumination conditions. Addressing this challenge, we introduce an algorithm designed to achieve accurate self- supervised stereo depth estimation focusing on nighttime conditions. Specifically, we use pretrained visual foundation models to extract generalised features across challenging scenes and present an efficient method for matching and integrating these features from stereo frames. Moreover, to prevent pixels violating photometric consistency assumption from negatively affecting the depth predictions, we propose a novel masking approach designed to filter out such pixels. Lastly, addressing weaknesses in the evaluation of current depth estimation algorithms, we present novel evaluation metrics. Our experiments, conducted on challenging datasets including Oxford RobotCar and Multi- Spectral Stereo, demonstrate the robust improvements realized by our approach."
SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields for Robotic Inspection,"Yifu Tao, Yash Sanjay Bhalgat, Lanke Frank Tarimo Fu, Matias Mattamala, Nived Chebrolu, Maurice Fallon",University of Oxford,Localization and Mapping II,"We present a neural-field-based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photo-realistic textures. This system adapts the state-of-the-art neural radiance field (NeRF) representation to also incorporate lidar data which adds strong geometric constraints on the depth and surface normals. We exploit the trajectory from a real-time lidar SLAM system to bootstrap a Structure-from-Motion (SfM) procedure to both significantly reduce the computation time and to provide metric scale which is crucial for lidar depth loss. We use submapping to scale the system to large-scale environments captured over long trajectories. We demonstrate the reconstruction system with data from a multi-camera, lidar sensor suite onboard a legged robot, hand-held while scanning building scenes for 600 metres, and onboard an aerial robot surveying a multi-storey mock disaster site-building. Website: https://ori.ox.ac.uk/labs/drs/nerf-mapping/"
LESS-Map: Lightweight and Evolving Semantic Map in Parking Lots for Long-Term Self-Localization,"Liu Mingrui, Xinyang Tang, Yeqiang Qian, Jiming Chen, Liang Li","Zhejiang University,Shanghai Jiao Tong University,Zhejiang Univerisity",Localization and Mapping II,"Precise and long-term stable localization is essential in parking lots for tasks like autonomous driving or autonomous valet parking, textit{etc}. Existing methods rely on a fixed and memory-inefficient map, which lacks robust data association approaches. And it is not suitable for precise localization or long-term map maintenance. In this paper, we propose a novel mapping, localization, and map update system based on ground semantic features, utilizing low-cost cameras. We present a precise and lightweight parameterization method to establish improved data association and achieve accurate localization at centimeter-level. Furthermore, we propose a novel map update approach by implementing high-quality data association for parameterized semantic features, allowing continuous map update and refinement during re-localization, while maintaining centimeter-level accuracy. We validate the performance of the proposed method in real-world experiments and compare it against state-of-the-art algorithms. The proposed method achieves an average accuracy improvement of 5cm during the registration process. The generated maps consume only a compact size of 450 KB/km and remain adaptable to evolving environments through continuous update."
Observation Time Difference: An Online Dynamic Objects Removal Method for Ground Vehicles,"Rongguang Wu, Chenglin Pang, Xuankang Wu, Zheng Fang",Northeastern University,Localization and Mapping II,"In the process of urban environment mapping, the sequential accumulations of dynamic objects will leave a large number of traces in the map. These traces will usually have bad influences on the localization accuracy and navigation performance of the robot. Therefore, dynamic objects removal plays an important role for creating clean map. However, conventional dynamic objects removal methods usually run offline. That is, the map is reprocessed after it is constructed, which undoubtedly increases additional time costs. To tackle the problem, this paper proposes a novel method for online dynamic objects removal for ground vehicles. According to the observation time difference between the object and the ground where it is located, dynamic objects are classified into two types: suddenly appear and suddenly disappear. For these two kinds of dynamic objects, we propose downward retrieval and upward retrieval methods to eliminate them respectively. We validate our method on SemanticKITTI dataset and author-collected dataset with highly dynamic objects. Compared with other state-of-the-art methods, our method is more efficient and robust, and reduces the running time per frame by more than 60% on average. Our method is open-sourced on GitHub"
RO-MAP: Real-Time Multi-Object Mapping with Neural Radiance Fields,"Xiao Han, Houxuan Liu, Yunchao Ding, Lu Yang",University of Electronic Science and Technology of China,Localization and Mapping II,"Accurate perception of objects in the environment is important for improving the scene understanding capability of SLAM systems. In robotic and augmented reality applications, object maps with semantic and metric information show attractive advantages. In this paper, we present RO-MAP, a novel multi-object mapping pipeline that does not rely on 3D priors. Given only monocular input, we use neural radiance fields to represent objects and couple them with a lightweight object SLAM based on multi-view geometry, to simultaneously localize objects and implicitly learn their dense geometry. We create separate implicit models for each detected object and train them dynamically and in parallel as new observations are added. Experiments on synthetic and real-world datasets demonstrate that our method can generate semantic object map with shape reconstruction, and be competitive with offline methods while achieving real-time performance (25Hz). The code and dataset will be available at: https://github.com/XiaoHan-Git/RO-MAP"
OctoMap-RT: Fast Probabilistic Volumetric Mapping Using Ray-Tracing GPUs,"Heajung Min, Kyungmin Han, Young Jun Kim","Ewha Womans University,Ewha Woman's Univeristy",Localization and Mapping II,"A 3D occupancy map that is accurately modeled after real-world environments is essential for reliably performing robotic tasks. Probabilistic volumetric mapping (PVM) is a well-known environment mapping method using volumetric voxel grids that represent the probability of occupancy. The main bottleneck of current CPU-based PVM, such as OctoMap, is determining voxel grids with occupied and free states using ray-shooting. In this paper, we propose an octree-based PVM, called OctoMap-RT, using a hybrid of off-the-shelf ray-tracing GPUs and CPUs to substantially improve CPU-based PVM. OctoMap-RT employs massively parallel ray-shooting using GPUs to generate occupied and free voxel grids and to update their occupancy states in parallel, and it exploits CPUs to restructure the PVM using the updated voxels. Our experiments using various large-scale real-world benchmarking environments with dense and high-resolution sensor measurements demonstrate that OctoMap-RT builds maps up to 41.2 times faster than OctoMap and 9.3 times faster than the recent SuperRay CPU implementation. Moreover, OctoMap-RT constructs a map with 0.52% higher accuracy, in terms of the number of occupancy grids, than both OctoMap and SuperRay."
VICAN: Very Efficient Calibration Algorithm for Large Camera Networks,"Gabriel Moreira, Manuel Marques, Joao Paulo Costeira, Alexander Hauptmann","Carnegie Mellon University,Instituto Superior Técnico,Insituto Superior Tecnico",SLAM VI,"The precise estimation of camera poses within large camera networks is a foundational problem in computer vision and robotics, with broad applications spanning autonomous navigation, surveillance, and augmented reality. In this paper, we introduce a novel methodology that extends state-of-the-art Pose Graph Optimization (PGO) techniques. Departing from the conventional PGO paradigm, which primarily relies on camera-camera edges, our approach centers on the introduction of a dynamic element - any rigid object free to move in the scene - whose pose can be reliably inferred from a single image. Specifically, we consider the bipartite graph encompassing cameras, object poses evolving dynamically, and camera-object relative transformations at each time step. This shift not only offers a solution to the challenges encountered in directly estimating relative poses between cameras, particularly in adverse environments, but also leverages the inclusion of numerous object poses to ameliorate and integrate errors, resulting in accurate camera pose estimates. Though our framework retains compatibility with traditional PGO solvers, its efficacy benefits from a custom-tailored optimization scheme. To this end, we introduce an iterative primal-dual algorithm, capable of handling large graphs. Empirical benchmarks, conducted on a new dataset of simulated indoor environments, substantiate the efficacy and efficiency of our approach."
Tightly-Coupled LiDAR-Visual-Inertial SLAM and Large-Scale Volumetric-Occupancy Mapping,"Simon Boche, Sebastián Barbas Laina, Stefan Leutenegger","Technical University of Munich,TU Munich",SLAM VI,"Autonomous navigation is one of the key requirements for every potential application of mobile robots in the real-world. Besides high-accuracy state estimation, a suitable and globally consistent representation of the 3D environment is indispensable. We present a fully tightly-coupled LiDAR-Visual-Inertial SLAM system and 3D mapping framework applying local submapping strategies to achieve scalability to large-scale environments. A novel and correspondence-free, inherently probabilistic, formulation of LiDAR residuals is introduced, expressed only in terms of the occupancy fields and its respective gradients. These residuals can be added to a factor graph optimisation problem, either as frame-to-map factors for the live estimates or as map-to-map factors aligning the submaps with respect to one another. Experimental validation demonstrates that the approach achieves state-of-the-art pose accuracy and furthermore produces globally consistent volumetric occupancy submaps which can be directly used in downstream tasks such as navigation or exploration."
Active Visual Localization for Multi-Agent Collaboration: A Data-Driven Approach,"Matthew Hanlon, Boyang Sun, Marc Pollefeys, Hermann Blum",ETH Zurich,SLAM VI,"Rather than having each newly deployed robot create its own map of its surroundings, the growing availability of SLAM-enabled devices provides the option of simply localizing in a map of another robot or device. In cases such as multi-robot or human-robot collaboration, localizing all agents in the same map is even necessary. However, localizing e.g. a ground robot in the map of a drone or head-mounted MR headset presents unique challenges due to viewpoint changes. This work investigates how active visual localization can be used to overcome such challenges of viewpoint changes. Specifically, we focus on the problem of selecting the optimal viewpoint at a given location. We compare existing approaches in the literature with additional proposed baselines and propose a novel data-driven approach. The result demonstrates the superior performance of our data-driven approach when compared to existing methods, both in controlled simulation experiments and real-world deployment."
Autonomous Implicit Indoor Scene Reconstruction with Frontier Exploration,"Jing Zeng, Yanxu Li, Jiahao Sun, Qi Ye, Yunlong Ran, Jiming Chen",Zhejiang University,SLAM VI,"Implicit neural representations have demonstrated significant promise for 3D scene reconstruction. Recent works have extended their applications to autonomous implicit reconstruction through the Next Best View (NBV) based method. However, the NBV method cannot guarantee complete scene coverage and often necessitates extensive viewpoint sampling, particularly in complex scenes. In the paper, we propose to 1) incorporate frontier-based exploration tasks for global coverage with implicit surface uncertainty-based reconstruction tasks to achieve high-quality reconstruction. and 2) introduce a method to achieve implicit surface uncertainty using color uncertainty, which reduces the time needed for view selection. Further with these two tasks, we propose an adaptive strategy for switching modes in view path planning, to reduce time and maintain superior reconstruction quality. Our method exhibits the highest reconstruction quality among all planning methods and superior planning efficiency in methods involving reconstruction tasks. We deploy our method on a UAV and the results show that our method can plan multi-task views and reconstruct a scene with high quality."
Probabilistic Active Loop Closure for Autonomous Exploration,"He Yin, Jong Jin Park, Marcelino Almeida, Martin Labrie, James Zamiska, Richard Kim","Amazon.com, Inc.,Amazon Lab,,,,University of Texas at Austin,Amazon,Amazon, Lab,,,",SLAM VI,"When a mobile robot autonomously explores an indoor space to produce a localization and navigation map, it is important to create both a stable pose graph and a high-quality occupancy map that covers all the navigable areas. In this work, we propose a novel probabilistic active loop closure framework which attempts to maximally reduce pose graph uncertainty during exploration and improves occupancy map quality. We calculate a probabilistic reward of getting a loop closure at any pose on a pose graph, which considers both how much pose graph uncertainty would be reduced by getting a loop closure there, and the robotâ€™s travel cost to navigate to that pose. By choosing poses that provide the largest rewards, we can maximally reduce pose graph uncertainty while avoiding long travel times. The effectiveness of the method is illustrated through on-device testing in various floor plans."
CARE: Confidence-Rich Autonomous Robot Exploration Using Bayesian Kernel Inference and Optimization,"Yang Xu, Ronghao Zheng, Senlin Zhang, Meiqin Liu, Shoudong Huang","Zhejiang University,University of Technology, Sydney",SLAM VI,
Event-Based Stereo Visual Odometry with Native Temporal Resolution Via Continuous-Time Gaussian Process Regression,"Jianeng Wang, Jonathan Gammell",University of Oxford,SLAM VI,"Event-based cameras asynchronously capture individual visual changes in a scene. This makes them more robust than traditional frame-based cameras to highly dynamic motions and poor illumination. It also means that every measurement in a scene can occur at a unique time. Handling these different measurement times is a major challenge of using event-based cameras. It is often addressed in visual odometry (VO) pipelines by approximating temporally close measurements as occurring at one common time. This grouping simplifies the estimation problem but, absent additional sensors, sacrifices the inherent temporal resolution of event-based cameras. This paper instead presents a complete stereo VO pipeline that estimates directly with individual event-measurement times without requiring any grouping or approximation in the estimation state. It uses continuous-time trajectory estimation to maintain the temporal fidelity and asynchronous nature of event-based cameras through Gaussian process regression with a physically motivated prior. Its performance is evaluated on the MVSEC dataset, where it achieves 7.9Â·10-3 and 5.9Â·10-3 RMS relative error on two independent sequences, outperforming the existing publicly available event-based stereo VO pipeline by two and four times, respectively."
MSCEqF: A Multi State Constraint Equivariant Filter for Vision-Aided Inertial Navigation,"Alessandro Fornasier, Pieter Van Goor, Eren Allak, Robert Mahony, Stephan Weiss","University of Klagenfurt,The Australian National University,Australian National University,Universität Klagenfurt",SLAM VI,"This letter re-visits the problem of visual-inertial navigation system (VINS) and presents a novel filter design we dub the multi state constraint equivariant filter (MSCEqF, in analogy to the well known MSCKF). We define a symmetry group and corresponding group action that allow specifically the design of an equivariant filter for the problem of visual-inertial odometry (VIO) including IMU bias, and camera intrinsic and extrinsic calibration states. In contrast to state-of-the-art invariant extended Kalman filter (IEKF) approaches that simply tack IMU bias and other states onto the SE2(3) group, our filter builds upon a symmetry that properly includes all the states in the group structure. Thus, we achieve improved behavior, particularly when linearization points largely deviate from the truth (i.e., on transients upon state disturbances). Our approach is inherently consistent even during convergence phases from significant errors without the need for error uncertainty adaptation, observability constraint (OC), or other consistency enforcing techniques. This leads to greatly improved estimator behavior for significant error and unexpected state changes during, e.g., long-duration missions. We evaluate our approach with a multitude of different experiments using three different prominent real-world datasets."
L-VIWO: Visual-Inertial-Wheel Odometry Based on Lane Lines,"Bin Zhao, Yunzhou Zhang, Junjie Huang, Xichen Zhang, Zeyu Long, Yulong Li",Northeastern University,SLAM VI,"To achieve precise localization for autonomous vehicles and mitigate the problem of accumulated drift error in odometry, this paper proposes L-VIWO, a Visual-Inertial-Wheel Odometry based on lane lines. This method effectively utilizes the lateral constraints provided by lane lines to eliminate and relieve the incrementally accumulated pose errors. Firstly, we introduce a lane line tracking method that enables multi-frame tracking of the same lane line, thereby obtaining multi-frame data of a lane line. Then, we utilize multi-frame data of the lane lines and the curvature characteristics of adjacent lane lines to optimize the positions of the lane line sample points, thus building a reliable lane line map. Finally, we use the built local lane line map to correct the position of the vehicle. Based on the corrected position and prior pose from the odometry, we build a graph optimization model to optimize the pose of vehicle. Through localization experiments on the KAIST dataset, it has been demonstrated that the proposed method effectively enhances the localization accuracy of odometry, thus confirming the effectiveness of the method."
Fast Force-Closure Grasp Synthesis with Learning-Based Sampling,"Wei Xu, Weichao Guo, Xu Shi, Xinjun Sheng, Xiangyang Zhu",Shanghai Jiao Tong University,Multifingered Hands,"Anthropomorphic robotic hands have been widely investigated to dexterously manipulate objects because of their anatomical similarity to the human hand. However, the large dimension of configuration space challenges the real-time performance of existing grasp planning methods and drastically limits the application of anthropomorphic hands. In this letter, we propose a fast force-closure grasp synthesis (FFCGS) method for the anthropomorphic hand to efficiently grasp unknown objects. The FFCGS is implemented by using a signed distance field (SDF) as input. Firstly, a network that samples feasible 6D wrist poses is trained in an end-to-end fashion to reduce the dimension of search space. Furthermore, a fast optimization algorithm is presented to find finger configurations for force-closure precision grasp based on the differentiable Q-distance metric. We validate our method in both a simulated and a real-world environment. Experiment results show that the proposed FFCGS achieves a significantly improved performance in terms of time efficiency (5 times faster), grasp quality metrics, and success rate (5%-10% improvement) over benchmark methods. The outcomes of this study have great significance in promoting the motion planning of robot hand-arm systems and upper-limb prostheses."
"The New Dexterity Modular, Dexterous, Anthropomorphic, Open-Source, Bimanual Manipulation Platform: Combining Adaptive and Hybrid Actuation Systems with Lockable Joints","Che-ming Chang, Felipe Sanches, Geng Gao, Minas Liarokapis","University of Auckland,Acumino inc,The University of Auckland",Multifingered Hands,"This work introduces the New Dexterity modular, dexterous, anthropomorphic, open-source, bimanual manipulation platform (OpenBMP) that is designed for research and rapid experimentation in robot grasping, dexterous manipulation, and bimanual manipulation. The platform combines adaptive and hybrid actuation systems with lockable joints, facilitating transitions between the execution of delicate and forceful tasks. Antagonistic tendon-driven elbows and inline actuator transmissions, reduce the system's inertial mass while enhancing energy efficiency and overall performance. Leveraging 3D printing and carbon fiber reinforced manufacturing of core parts the platform is easy to replicate and highly modular. This paper presents the details of the design, the actuation principles, and the experimental validation of the efficiency of the platform with the execution of complex teleoperation and telemanipulation tasks. The designs, the electronics, and the code are open-sourced to allow replication by others."
Fully 3D Printable Robot Hand and Soft Tactile Sensor Based on Air-Pressure and Capacitive Proximity Sensing,"Sean Taylor, Kyungseo Park, Sankalp Yamsani, Joohyung Kim","University of Illinois at urbana champaign,Daegu Gyeongbuk Institute of Science and Technology (DGIST),University of Illinois Urbana-Champaign,University of Illinois at Urbana-Champaign",Multifingered Hands,"Soft tactile sensors can enable robots to grasp objects easily and stably by simultaneously providing tactile data and mechanical compliance to robotic hands. If there are low-cost and easy-to-build robotic hands equipped with soft tactile sensors, they would be highly accessible and facilitate many robotics projects. To this end, we propose an accessible robot hand capable of tactile sensing, which can be produced through digital fabrication. We made the robot hand using commercial servo motors as well as components 3D printed from PETG, TPU, and conductive TPU. These materials allow the robot hand to have a soft, durable, and even functional structure. Specifically, the soft fingertip was crafted from TPU and conductive TPU, and their mechanical and electrical properties enable easy implementation of tactile sensing capabilities, such as force and capacitive touch, simply by adding off-the-shelf sensors (air-pressure and capacitance). The proposed robot hand could effectively sense interaction forces and proximity to conductive objects, and its utilization in various tasks was also demonstrated successfully."
TPGP: Temporal-Parametric Optimization with Deep Grasp Prior for Dexterous Motion Planning,"Haoming Li, Qi Ye, Yuchi Huo, Qingtao Liu, Shijian Jiang, Tao Zhou, Xiang Li, Yang Zhou, Jiming Chen","Zhejiang University,OPPO US RESEARCH CENTER,OPPO",Multifingered Hands,"Grasping motion planning aims to find a feasible grasping trajectory in the configuration space given an input target grasp. While optimizing grasp motion with two or three-fingered grippers has been well studied, the study on natural grasp motion planning with a dexterous hand remains a very challenging problem due to the high dimensional working space. In this work, we propose a novel temporal-parametric grasp prior (TPGP) optimization method to simplify the difficulty of grasping trajectory optimization for the dexterous hand while maintaining smooth and natural properties of the grasping motion. Specifically, we formulate the discrete trajectory parameters into a temporal-based parameterization, where the prior constraint provided by a hand poser network, is introduced to ensure that hand pose is natural and reasonable throughout the trajectory. Finally, we present a joint target optimization strategy to enhance the target pose for more feasible trajectories. Extensive validations on two public datasets show that our method outperforms state-of-the-art methods regarding grasp motion on various metrics."
A Wearable Robotic Hand for Hand-Over-Hand Imitation Learning,"Dehao Wei, Huazhe Xu",Tsinghua University,Multifingered Hands,"Dexterous manipulation through imitation learning has gained significant attention in robotics research. The collection of high-quality expert data holds paramount importance when using imitation learning. The existing approaches for acquiring expert data commonly involve utilizing a data glove to capture hand motion information. However, this method suffers from limitations as the collected information cannot be directly mapped to the robotic hand due to discrepancies in their degrees of freedom or structures. Furthermore, it fails to accurately capture force feedback information between the hand and objects during the demonstration process. To overcome these challenges, this paper presents a novel solution in the form of a wearable dexterous hand, namely Hand-over-hand Imitation learning wearable Robotic Hand(HIRO Hand), which integrates expert data collection and enables the implementation of dexterous operations. This HIRO Hand empowers the operator to utilize their own tactile feedback to determine appropriate force, position, and actions, resulting in more accurate imitation of the expertâ€™s actions. We develop both non-learning and visual behavior cloning-based controllers allowing HIRO Hand successfully achieves grasping and in-hand manipulation ability."
WARABI Hand: Five-Fingered Robotic Hand with Flexible Skin and Force Sensors for Social Interaction,"Aoi Nakane, Iori Yanokura, Shun Hasegawa, Naoya Yamaguchi, Kunio Kojima, Kei Okada, Masayuki Inaba","The Univeersity of Tokyo,University of Tokyo,The University of Tokyo",Multifingered Hands,"A robotic hand for social interaction should be capable of comfortable touch with humans. However, it is difficult to mount skin, tactile sensors, and driving mechanism required for human contact, especially holding hands, on a slender finger. In addition, in order to unitize the hand for easy use with any robot and maintainability, the mechanism must be contained within the small space of the fingers and palms. In this paper, we propose a human-sized five-fingered robotic hand named WARABI Hand. It is covered with multi-layored rubber skin to realize human-like soft and pleasant feel. Force sensors on each finger link detect contact with humans and adjust gripping force. We conducted experiments in which a humanoid equipped with WARABI Hand grasped forearm, held hands, and interlocked fingers with a person. The performance for object grasping was also evaluated. We demonstrated that our proposed hand is useful for interaction with humans including receiving and handing over things."
Sensorized Soft Skin for Dexterous Robotic Hands,"Jana Egli, Benedek Forrai, Thomas Jakob Konrad Buchner, Jiangtao Su, Xiaodong Chen, Robert Kevin Katzschmann","ETHZ,ETH Zürich,ETH Zurich,Nanyang Technological University",Multifingered Hands,"Conventional industrial robots often use two-fingered grippers or suction cups to manipulate objects or interact with the world. Because of their simplified design, they are unable to reproduce the dexterity of human hands when manipulating a wide range of objects. While the control of humanoid hands evolved greatly, hardware platforms still lack capabilities, particularly in tactile sensing and providing soft contact surfaces. In this work, we present a method that equips the skeleton of a tendon-driven humanoid hand with a soft and sensorized tactile skin. Multi-material 3D printing allows us to iteratively approach a cast skin design which preserves the robotâ€™s dexterity in terms of range of motion and speed. We demonstrate that a soft skin enables firmer grasps and piezoresistive sensor integration enhances the handâ€™s tactile sensing capabilities."
Identifying Expert Behavior in Offline Training Datasets Improves Behavioral Cloning of Robotic Manipulation Policies,"Qiang Wang, Robert Mccarthy, David Cordova Bulens, Francisco Roldan Sanchez, Kevin Mcguinness, Noel O'Connor, Stephen Redmond","University College Dublin,CeADAR - Ireland’s Centre for Applied AI, University College Dub,Dublin City University",Multifingered Hands,"This paper presents our solution for the Real Robot Challenge III, aiming to address dexterous robotic manipulation tasks through learning from offline data. In this competition, participants were given two types of datasets for each task: expert and mixed. Each expert dataset is collected by a high-skill policy, whereas the mixed dataset is collected using both expert and non-expert policies. We found that vanilla behavioural cloning (BC) can learn a very proficient policy with minimal human intervention when trained on expert datasets. Notably, BC outperformed even the most advanced offline reinforcement learning (RL) algorithms. However, when applied to mixed datasets, the performance of BC deteriorates; the performance of offline RL algorithms is also less than satisfactory. Upon examining the provided datasets, it was apparent that each mixed dataset contained a significant proportion of expert data, which should enable the training of a proficient BC agent. However, the expert data is not labelled in the datasets. As a result, we propose a classifier to identify the pattern of the expert behaviour within a mixed dataset and then utilize it to isolate the expert data. To further boost the BC performance, we take advantage of the geometric symmetry of the arena to augment the training dataset through mathematical transformations. Our submission outperformed that of other participants. Site: https://github.com/wq13552463699/Real-Robot-Challenge-III-Winning-Solution"
Development of a Versatile Robotic Hand Toward Jig-Less Assembly of a Shaft-Shaped Part,"Kohei Shibata, Hiroki Dobashi",Wakayama University,Multifingered Hands,"Jig-less assembly of a shaft-shaped part with a single versatile robotic hand requires several functions of the hand to achieve a series of operations such as alignment, picking, reorientation, and positioning of the part. In this research, we propose a novel robotic hand with these functions and corresponding finger mechanisms. Moreover, we propose a manipulation strategy for grasping shaft-shaped parts with the proposed hand, and experimentally verify the feasibility of desired operations with the proposed method as well as the versatility of the hand for several different parts."
AHPPEBot: Autonomous Robot for Tomato Harvesting Based on Phenotyping and Pose Estimation,"Xingxu Li, Nan Ma, Yiheng Han, Shun Yang, Siyi Zheng","Beijing University of Technology, Beijing, China,Beijing University of Technology,Beijing AIForceTech Technology Co., Ltd,Beijing AIForce Technology Co., Ltd.",Perception for Grasping and Manipulation III,"To address the limitations inherent to conventional automated harvesting robots specifically their suboptimal success rates and risk of crop damage, we design a novel bot named AHPPEBot which is capable of autonomous harvesting based on crop phenotyping and pose estimation. Specifically, In phenotyping, the detection, association, and maturity estimation of tomato trusses and individual fruits are accomplished through a multi-task YOLOv5 model coupled with a detection-based adaptive DBScan clustering algorithm. In pose estimation, we employ a deep learning model to predict seven semantic keypoints on the pedicel. These keypoints assist in the robot's path planning, minimize target contact, and facilitate the use of our specialized end effector for harvesting. In autonomous tomato harvesting experiments conducted in commercial greenhouses, our proposed robot achieved a harvesting success rate of 86.67%, with an average successful harvest time of 32.46 s, showcasing its continuous and robust harvesting capabilities. The result underscores the potential of harvesting robots to bridge the labor gap in agriculture."
Unknown Object Grasping for Assistive Robotics,"Elle Miller, Maximilian Durner, Matthias Humt, Gabriel Quere, Wout Boerdijk, Ashok Meenakshi Sundaram, Freek Stulp, Joern Vogel","University of Edinburgh,German Aerospace Center DLR,German Aerospace Center (DLR), Technical University Munich (TUM),DLR,German Aerospace Center (DLR),DLR - Deutsches Zentrum für Luft- und Raumfahrt e.V.",Perception for Grasping and Manipulation III,"We propose a novel pipeline for unknown object grasping in shared robotic autonomy scenarios. State-of-the-art methods for fully autonomous scenarios are typically learning-based approaches optimised for a specific end-effector, that generate grasp poses directly from sensor input. In the domain of assistive robotics, we seek instead to utilise the user's cognitive abilities for enhanced satisfaction, grasping performance, and alignment with their high level task-specific goals. Given a pair of stereo images, we perform unknown object instance segmentation and generate a 3D reconstruction of the object of interest. In shared control, the user then guides the robot end-effector across a virtual hemisphere centered around the object to their desired approach direction. A physics-based grasp planner finds the most stable local grasp on the reconstruction, and finally the user is guided by shared control to this grasp. In experiments on the DLR EDAN platform, we report a grasp success rate of 87% for 10 unknown objects, and demonstrate the method's capability to grasp objects in structured clutter and from shelves."
Liquids Identification and Manipulation Via Digitally Fabricated Impedance Sensors,"Junyi Zhu, Young Joong Lee, Yiyue Luo, Tianyu Xu, Chao Liu, Daniela Rus, Stefanie Mueller, Wojciech Matusik","Massachusetts Institute of Technology,Massachusetts Institute of Technology, Google,MIT,MIT CSAIL",Perception for Grasping and Manipulation III,"Despite recent exponential advancements in computer vision and reinforcement learning, it remains challenging for robots to interact with liquids. These challenges are particularly pronounced due to the limitations imposed by opaque containers, transparent liquids, fine-grained splashes, and visual obstructions arising from the robot's own manipulation activities. Yet, there exists a substantial opportunity for robotics to excel in liquid identification and manipulation, given its potential role in chemical handling in laboratories and various manufacturing sectors such as pharmaceuticals or beverages. In this work, we present a novel approach for liquid class identification and state estimation leveraging electrical impedance sensing. We design and mount a digitally embroidered electrode array to a commercial robot gripper. Coupled with a customized impedance sensing board, we collect data on liquid manipulation with a swept frequency sensing mode and a frequency-specific impedance measuring mode. Our developed learning-based model achieves an accuracy of 93.33% in classifying 9 different types of liquids (8 liquids + air), and 97.65% in estimating the liquid state. We investigate the effectiveness of our system with a series of ablation studies. These findings highlight our work as a promising solution for enhancing robotic manipulation in liquid-related tasks."
Learning to Grasp in Clutter with Interactive Visual Failure Prediction,"Michael Murray, Abhishek Gupta, Maya Cakmak",University of Washington,Perception for Grasping and Manipulation III,"Modern warehouses process millions of unique objects which are often stored in densely packed containers. To automate tasks in this environment, a robot must be able to pick diverse objects from highly cluttered scenes. Real-world learning is a promising approach, but executing picks in the real world is time-consuming, can induce costly failures, and often requires extensive human intervention, which causes operational burden and limits the scope of data collection and deployments. In this work, we leverage interactive probes to visually evaluate grasps in clutter without fully executing picks, a capability we refer to as Interactive Visual Failure Prediction (IVFP). This enables autonomous verification of grasps during execution to avoid costly downstream failures as well as autonomous reward assignment, providing supervision to continuously shape and improve grasping behavior as the robot gathers experience in the real world, without constantly requiring human intervention. Through experiments on a Stretch RE1 robot, we study the effect that IVFP has on performance - both in terms of effective data throughput and success rate, and show that this approach leads to grasping policies that outperform policies trained with human supervision alone, while requiring significantly less human intervention."
Kinesthetic-Based In-Hand Object Recognition with an Underactuated Robotic Hand,"Julius Arolovitch, Osher Azulay, Avishai Sintov","Carnegie Mellon University,Tel Aviv University,Tel-Aviv University",Perception for Grasping and Manipulation III,"Tendon-based underactuated hands are intended to be simple, compliant and affordable. Often, they are 3D printed and do not include tactile sensors. Hence, performing in-hand object recognition with direct touch sensing is not feasible. Adding tactile sensors can complicate the hardware and introduce extra costs to the robotic hand. Also, the common approach of visual perception may not be available due to occlusions. In this paper, we explore whether kinesthetic haptics can provide in-direct information regarding the geometry of a grasped object during in-hand manipulation with an underactuated hand. By solely sensing actuator positions and torques over a period of time during motion, we show that a classifier can recognize an object from a set of trained ones with a high success rate of almost 95%. In addition, the implementation of a real-time majority vote during manipulation further improves recognition. Additionally, a trained classifier is also shown to be successful in distinguishing between shape categories rather than just specific objects."
Fit-NGP: Fitting Object Models to Neural Graphics Primitives,"Marwan Taher, Ignacio Alzugaray, Andrew J Davison",Imperial College London,Perception for Grasping and Manipulation III,"Accurate 3D object pose estimation is key to enabling many robotic applications that involve challenging object interactions. In this work, we show that the density field created by a state-of-the-art efficient radiance field re-construction method is suitable for highly accurate and robust pose estimation for objects with known 3D models, even when they are very small and with challenging reflective surfaces. We present a fully automatic object pose estimation system based on a robot arm with a single wrist-mounted camera, which can scan a scene from scratch, detect and estimate the 6-Degrees of Freedom (DoF) poses of multiple objects within a couple of minutes of operation. Small objects such as bolts and nuts are estimated with accuracy on order of 1mm."
Efficient Object Rearrangement Via Multi-View Fusion,"Dehao Huang, Chao Tang, Hong Zhang","Southern University of Science and Technology,SUSTech",Perception for Grasping and Manipulation III,"The prospect of assistive robots aiding in object organization has always been compelling. In an image-goal setting, the robot rearranges the current scene to match the single image captured from the goal scene. The key to an image-goal rearrangement system is estimating the desired placement pose of each object based on the single goal image and observations from the current scene. In order to establish sufficient associations for accurate estimation, the system should observe an object from a viewpoint similar to that in the goal image. Existing image-goal rearrangement systems, due to their reliance on a fixed viewpoint for perception, often require redundant manipulations to randomly adjust an object's pose for a better perspective. Addressing this inefficiency, we introduce a novel object rearrangement system that employs multi-view fusion. By observing the current scene from multiple viewpoints before manipulating objects, our approach can estimate a more accurate pose without redundant manipulation times. A standard visual localization pipeline at the object level is developed to capitalize on the advantages of multi-view observations. Simulation results demonstrate that the efficiency of our system outperforms existing single-view systems. The effectiveness of our system is further validated in a physical experiment. For videos, please visit https://sites.google.com/view/multi-view-rearr."
EDOPT: Event-Camera 6-DoF Dynamic Object Pose Tracking,"Arren Glover, Luna Gava, Zhichao Li, Chiara Bartolozzi","Istituto Italiano di Tecnologia,University of Genova",Perception for Grasping and Manipulation III,"High-frequency, low-latency, 6-DoF object tracking is useful for grasping objects in motion, taking robots beyond pick-and-place tasks. We propose using an event-camera for tracking the objects to leverage the low-latency and continuous (i.e. not fixed-rate) data capture for high-frequency tracking. We propose the EDOPT algorithm, which maintains real-time operation with a variable event-rate (which occurs due to variation in camera velocity and scene texture) and avoids frame-jumps and motion-blur which are problematic in traditional computer vision solutions. EDOPT uses a strong object prior, leading to a novel solution possible only with the event-camera. To our knowledge, this is the first method for 6-DoF object pose estimation with only the event-camera. The proposed method achieves comparable results to a state-of-the-art DNN technique that fuses frames, depth, and events. We demonstrate smooth, online object pose tracking with a live camera feed at >300 Hz."
Attention-Based Cloth Manipulation from Model-Free Topological Representation,"Kevin Galassi, Bingbing Wu, Julien Perez, Gianluca Palli, Jean-michel Renders","Università di Bologna,Naver Labs Europe,NAVER LABS EUROPE,University of Bologna",Perception for Grasping and Manipulation III,"The robotic manipulation of deformable objects, such as clothes and fabric, is known as a complex task from both the perception and planning perspectives. Indeed, the stochastic nature of the underlying environment dynamics makes it an interesting research field for statistical learning approaches and neural policies. In this work, we introduce a novel attention-based neural architecture capable of solving a smoothing task for such objects by means of a single robotic arm. To train our network, we leverage an oracle policy, executed in simulation, which uses the topological description of a mesh of points for representing the object to smooth. In a second step, we transfer the resulting behavior in the real world with imitation learning using the cloth point cloud as decision support, which is captured from a single RGBD camera placed egocentrically on the wrist of the arm. This approach allows fast training of the real-world manipulation neural policy while not requiring scene reconstruction at test time, but solely a point cloud acquired from a single RGBD camera. Our resulting policy first predicts the desired point to choose from the given point cloud and then the correct displacement to achieve a smoothed cloth. Experimentally, we first assess our results in a simulation environment by comparing them with an existing heuristic policy, as well as several baseline attention architectures. Then, we validate the performance of our approach in a real-world scenario."
WLST: Weak Labels Guided Self-Training for Weakly-Supervised Domain Adaptation on 3D Object Detection,"Tsung Lin Tsou, Tsung-han Wu, Winston Hsu",National Taiwan University,Object Detection V,"In the field of domain adaptation (DA) on 3D object detection, most of the work is dedicated to unsupervised domain adaptation (UDA). Yet, without any target annotations, the performance gap between the UDA approaches and the fully-supervised approach is still noticeable, which is impractical for real-world applications. On the other hand, weakly-supervised domain adaptation (WDA) is an underexplored yet practical task that only requires few labeling effort on the target domain. To improve the DA performance in a cost-effective way, we propose a general weak labels guided self-training framework, WLST, designed for WDA on 3D object detection. By incorporating autolabeler, which can generate 3D pseudo labels from 2D bounding boxes, into the existing self-training pipeline, our method is able to generate more robust and consistent pseudo labels that would benefit the training process on the target domain. Extensive experiments demonstrate the effectiveness, robustness, and detector-agnosticism of our WLST framework. Notably, it outperforms previous state-of-the-art methods on all evaluation tasks."
Towards a Robust Sensor Fusion Step for 3D Object Detection on Corrupted Data,"Maciej Wozniak, Viktor Karefjard, Marko Thiel, Patric Jensfelt","KTH Royal Institute of Technology,Hamburg University of Technology (TUHH),KTH - Royal Institute of Technology",Object Detection V,"Multimodal sensor fusion methods for 3D object detection have been revolutionizing the autonomous driving re- search field. Nevertheless, most of these methods heavily rely on dense LiDAR data and accurately calibrated sensors which is often not the case in real-world scenarios. Data from LiDAR and cameras often comes misaligned due to the miscalibration, de- calibration, or different frequencies of the sensors. Additionally, some parts of the LiDAR data may be occluded and parts of the data may be missing due to hardware malfunction or weather conditions. This work presents a novel fusion step that addresses data corruptions and makes sensor fusion for 3D object detection more robust. Through extensive experiments, we demonstrate that our method performs on par with state-of-the-art approaches on normal data and outperforms them on misaligned data."
TerrainSense: Vision-Driven Mapless Navigation for Unstructured Off-Road Environments,"Bilal Hassan, Arjun Sharma, Nadya Abdel Madjid, Majid Khonji, Jorge Dias","Khalifa University, Abu Dhabi,Khalifa University",Object Detection V,"Navigating autonomous vehicles efficiently across unstructured and off-road terrains remains a formidable challenge, often requiring intricate mapping or multi-step pipelines. However, these conventional approaches struggle to adapt to dynamic environments. This paper presents TerrainSense, an end-to-end framework that overcomes these limitations. By utilizing a transformers, TerrainSense detects lane semantics and topology from camera images, enabling mapless path planning without the reliance on highly detailed maps. The efficacy of TerrainSense was rigorously assessed on six diverse datasets, evaluating its efficacy in detection, segmentation, and path prediction using various metrics. Notably, it outperforms the other state-of-the-art methods by 9.32% in precisely predicting the path with 18.28% faster inference time."
RCM-Fusion: Radar-Camera Multi-Level Fusion for 3D Object Detection,"Jisong Kim, Minjae Seong, Geonho Bang, Dongsuk Kum, Jun Won Choi","Hanyang university,Hanyang University,KAIST,Seoul National University",Object Detection V,"While LiDAR sensors have been successfully applied to 3D object detection, the affordability of radar and camera sensors has led to a growing interest in fusing radars and cameras for 3D object detection. However, previous radarcamera fusion models could not fully utilize the potential of radar information. In this paper, we propose Radar-Camera Multi-level fusion (RCM-Fusion), which attempts to fuse both modalities at feature and instance levels. For feature-level fusion, we propose a Radar Guided BEV Encoder which transforms camera features into precise BEV representations using the guidance of radar Birdâ€™s-Eye-View (BEV) features and combines the radar and camera BEV features. For instance-level fusion, we propose a Radar Grid Point Refinement module that reduces localization error by accounting for the characteristics of the radar point clouds. The experiments on the public nuScenes dataset demonstrate that our proposed RCM-Fusion achieves state-of-the-art performances among single framebased radar-camera fusion methods in the nuScenes 3D object detection benchmark. The code will be made publicly available."
One-Vs-All Semi-Automatic Labeling Tool for Semantic Segmentation in Autonomous Driving,Jing Gu,Expleo Germany GmbH,Object Detection V,"Semantic image segmentation plays a pivotal role in creating High-Definition (HD) maps for autonomous driving, where every pixel in an image is assigned a label from a specific semantic class. However, obtaining dense pixel-level annotations for model training is a laborious and expensive process. Active learning holds promise as a method to reduce the human annotation effort needed for semantic segmentation. However, existing active learning methods often perform well in the majority classes but struggle with the minority classes, negatively impacting segmentation performance. To tackle this challenge, we propose a novel One-vs-All (OVA) active learning framework, known as OVAAL. This paper explains how OVAAL can shift more attention towards the minority classes and thoroughly analyzes its contributions to performance enhancement. Additionally, we introduce an OVA-based semi-supervised learning method for post-processing, referred to as OVAAL+. Our results demonstrate that both OVAAL and OVAAL+ lead to significant improvements, with mean Intersection over Union (mIoU) gains of 4.55% and 6.38%, respectively, compared to the state-of-the-art active learning method Pixelpick on the Cityscapes semantic segmentation benchmark. These improvements are achieved while maintaining an economical annotation budget of 1.44% of the training data. We foresee further research exploring the potential of OVA-based active selection to address challenges in cold start scenarios and resource-constrained training environments."
LiRaFusion: Deep Adaptive LiDAR-Radar Fusion for 3D Object Detection,"Jingyu Song, Lingjun Zhao, Katherine A. Skinner","University of Michigan,University of Michigan - Ann Arbor",Object Detection V,"We propose LiRaFusion to tackle LiDAR-radar fusion for 3D object detection to fill the performance gap of existing LiDAR-radar detectors. To improve the feature extraction capabilities from these two modalities, we design an early fusion module for joint voxel feature encoding, and a middle fusion module to adaptively fuse feature maps via a gated network. We perform extensive evaluation on nuScenes to demonstrate that LiRaFusion leverages the complementary information of LiDAR and radar effectively and achieves notable improvement over existing methods."
Discwise Active Learning for LiDAR Semantic Segmentation,"Ozan Unal, Dengxin Dai, Ali Tamer Unal, Luc Van Gool","ETH Zurich,Bosphorus University",Object Detection V,"While LiDAR data acquisition is easy, labeling for semantic segmentation remains highly time consuming and must therefore be done selectively. Active learning (AL) provides a solution that can iteratively and intelligently label a dataset while retaining high performance and a low budget. In this work we explore AL for LiDAR semantic segmentation. As a human expert is a component of the pipeline, a practical framework must consider common labeling techniques such as sequential labeling that drastically improve annotation times. We therefore propose a discwise approach (DiAL), where in each iteration, we query the region a single frame covers on global coordinates, labeling all frames simultaneously. We then tackle the two major challenges that emerge with discwise AL. Firstly we devise a new acquisition function that takes 3D point density changes into consideration which arise due to location changes or ego-vehicle motion. Next we solve a mixed-integer linear program that provides a general solution to the selection of multiple frames while taking into consideration the possibilities of disc intersections. Finally we propose a semi-supervised learning approach to utilize all frames within our dataset and improve performance."
BaSAL: Size Balanced Warm Start Active Learning for LiDAR Semantic Segmentation,"Jiarong Wei, Yancong Lin, Holger Caesar","Delft University of Technology,Delft University of Technology (TU Delft)",Object Detection V,"Active learning strives to reduce the need for costly data annotation, by repeatedly querying an annotator to label the most informative samples from a pool of unlabeled data, and then training a model from these samples. We identify two problems with existing active learning methods for LiDAR semantic segmentation. First, they overlook the severe class imbalance inherent in LiDAR semantic segmentation datasets. Second, to bootstrap the active learning loop when there is no labeled data available, they train their initial model from randomly selected data samples, leading to low performance. This situation is referred to as the cold start problem. To address these problems we propose BaSAL, a size-balanced warm start active learning model, based on the observation that each object class has a characteristic size. By sampling object clusters according to their size, we can thus create a size-balanced dataset that is also more class-balanced. Furthermore, in contrast to existing information measures like entropy or CoreSet, size-based sampling does not require a pretrained model, thus addressing the cold start problem effectively. Results show that we are able to improve the performance of the initial model by a large margin. Combining warm start and size-balanced sampling with established information measures, our approach achieves comparable performance to training on the entire SemanticKITTI dataset, despite using only 5% of the annotations, outperforming existing active learning methods. We also match the existing state-of-the-art in active learning on nuScenes."
Trajectory-Prediction-Based Dynamic Tracking of a UGV to a Moving Target under Multi-Disturbed Conditions,"Jinge Si, Bin Li, Yongkang Xu, Liang Wang, Chencheng Deng, Shoukun Wang, Junzheng Wang","Beijing institute of technology,Beijing Institute of Technology",Object Detection V,"Tracking dynamic targets poses a significant challenge for Unmanned Ground Vehicles (UGVs). Existing methods often lack research on multi-disturbed conditions. To address this issue, we propose a trajectory-prediction-based dynamic tracking scheme, which includes target localization, trajectory prediction, and UGV control. Firstly, an estimation algorithm based on the Extended Kalman Filter (EKF) is employed to mitigate noise and estimate the absolute states of the target accurately. To enhance robustness, we present an Adaptive Trajectory Prediction (ATP) algorithm based on prediction anchors. In this method, a quantization standard for trajectory disturbance is designed for adaptive control. Subsequently, we iteratively solve prediction anchor points based on two motion models to robustly predict the target trajectory even in the presence of unknown disturbances. Finally, the Linear Time-Varying Model Predictive Control (LTV-MPC) is utilized in the UGV controller for dynamic tracking. Experimental results demonstrate that the ATP exhibits superior prediction robustness and accuracy in perturbed environments compared to other prediction algorithms. In addition, the proposed scheme effectively achieves dynamic tracking of the Unmanned Aerial Vehicle (UAV) by the UGV under multi-disturbed conditions. Specifically, when the target moves at a speed of 1.0 m/s, the UGV can maintain a tracking error within 0.346 m."
Sim-To-Real Robotic Sketching Using Behavior Cloning and Reinforcement Learning,"Biao Jia, Dinesh Manocha","University of Maryland at College Park,University of Maryland",AI Robotics,"Robotic sketching in real-world scenarios poses a challenging problem with diverse applications in art, robotics, and digital design. We present a novel approach that bridges the gap between digital and robotic sketching, leveraging behavior cloning and reinforcement learning techniques. This paper intro- duces an approach aimed at bridging the gap between simulated and real-world robotic sketching through the integration of behavior cloning and reinforcement learning techniques. Our approach trains painting policies that operate effectively in both virtual environments and real-world robotic sketching systems. We have implemented a robotic sketching system featuring an UltraArm robot equipped with a RealSense D415 camera, closely emulating the MyPaint virtual environment. Our system can perceive its environment and adapt painting policies to natural painting media. Our results highlight the effectiveness of our agent in terms of acquiring policies for high-dimensional continuous action spaces, enabling the seamless transfer of brush manipulation techniques from simulation to practical robotic sketching. Furthermore, we demonstrate our robotic sketching systemâ€™s capability to generate complex images and strokes using various configurations."
Safe Table Tennis Swing Stroke with Low-Cost Hardware,"Francesco Cursi, Marcus Kalander, Shuang Wu, Xidi Xue, Yu Tian, Guangjian Tian, Xingyue Quan, Jianye Hao","Imperial College London,Huawei Technologies,Huawei,The Chinese University of Hong Kong,Noah's Ark Lab",AI Robotics,"Playing table tennis with a human player is a challenging robotic task due to its dynamic nature. Despite a number of researches being devoted to developing robotic table tennis systems, most of the works have demanding hardware requirements and ignore safety measures when generating the swing stoke. To address these issues, we propose a safe motion planning framework that fully pushes the robotic hardware performance limits to play table tennis. In particular, we propose a pipeline to generate manipulator joint trajectories with environmental safety constraints and scale the trajectories to satisfy joint movement limitations. We use three different agents to validate the planning algorithm with our handmade robot platform in both simulation and real-world environments."
Pluck and Play: Self-Supervised Exploration of Chordophones for Robotic Playing,"Michael Görner, Norman Hendrich, Jianwei Zhang",University of Hamburg,AI Robotics,"Existing robotic musicians utilize detailed handcrafted instrument models to generate or learn policies for playing because model-free or inaccurate policy rollouts might easily damage or wear out fragile instruments. We introduce an approach to characterize geometric models of chordophones and their audio onset responses directly through audio-tactile exploration with a physical robot arm. Initially, the system refines prior estimates of string positions, provided by kinesthetic teaching or visual estimation, through repeated attempts to pluck individual strings. A subsequent stage implements a Safe Active Exploration paradigm based on Gaussian Processes to explore and characterize the audio onset response of feasible plucking motions while minimizing invalid attempts. The resulting models can be used to actuate an imprecise robotic arm to play sequences of notes with varying loudness on a Chinese Guzheng."
MBot: A Modular Ecosystem for Scalable Robotics Education,"Petre Gaskell, Jana Pavlasek, Tom Gao, Abhishek Narula, Stanley Lewis, Odest Chadwicke Jenkins",University of Michigan,AI Robotics,"The Michigan Robotics MBot is a low-cost mobile robot platform that has been used to train over 1,400 students in autonomous navigation since 2014 at the University of Michigan and our collaborating colleges. The MBot platform was designed to meet the needs of teaching robotics at scale to match the growth of robotics as a field and an academic discipline, spanning all levels of undergraduate and graduate experiences. Transformative advancements in robot navigation over the past decades have led to a significant demand for skilled roboticists across industry and academia. This demand has sparked a need for robotics courses in higher education. Incorporating real robot platforms into such courses and curricula is effective for sparking student motivation and conveying the unique challenges of programming embodied agents in real-world environments. However, teaching with real robots remains challenging due to the cost of hardware and the development effort involved in adapting existing hardware for a new course. In this paper, we describe the design and evolution of the MBot platform, and keys to success in terms of its scalability and flexibility."
SO(2)-Equivariant Downwash Models for Close Proximity Flight,"Henry Smith, Ajay Shankar, Jennifer Gielis, Jan Blumenkamp, Amanda Prorok","University of Cambridge,University of Cambridge, UK,University of Cambrdige",AI Robotics,"Multirotors flying in close proximity induce aerodynamic wake effects on each other through propeller downwash. Conventional methods have fallen short of providing adequate 3D force-based models that can be incorporated into robust control paradigms for deploying dense formations. Thus, learning a model for these downwash patterns presents an attractive solution. In this paper, we present a novel learning-based approach for modelling the downwash forces that exploits the latent geometries (i.e. symmetries) present in the problem. We demonstrate that when trained with only 5 minutes of real world flight data, our geometry-aware model outperforms state-of-the-art baseline models trained with more than 15 minutes of data. In dense real-world flights with two vehicles, deploying our model online improves 3D trajectory tracking by nearly 36% on average (and vertical tracking by 56%)."
Hierarchical Meta-Learning-Based Adaptive Controller,"Fengze Xie, Guanya Shi, Michael O'connell, Yisong Yue, Soon-Jo Chung","California Institute of Technology,Carnegie Mellon University,Caltech",AI Robotics,"We study how to design learning-based adaptive controllers that enable fast and accurate online adaptation in changing environments. In these settings, learning is typically done during an initial (offline) design phase, where the vehicle is exposed to different environmental conditions and disturbances (e.g., a drone exposed to different winds) to collect training data. Our work is motivated by the observation that real-world disturbances fall into two categories: 1) those that can be directly monitored or controlled during training, which we call ``manageable''; and 2) those that cannot be directly measured or controlled (e.g., nominal model mismatch, air plate effects, and unpredictable wind), which we call ``latent''. Imprecise modeling of these effects can result in degraded control performance, particularly when latent disturbances continuously vary. This paper presents the Hierarchical Meta-learning-based Adaptive Controller (HMAC) to learn and adapt to such multi-source disturbances. Within HMAC, we develop two techniques: 1) Hierarchical Iterative Learning, which jointly trains representations to caption the various sources of disturbances, and 2) Smoothed Streaming Meta-Learning, which learns to capture the evolving structure of latent disturbances over time (in addition to standard meta-learning on the manageable disturbances). Experimental results demonstrate that HMAC exhibits more precise and rapid adaptation to multi-source disturbances than other adaptive controllers."
A Novel Wide-Area Multiobject Detection System with High-Probability Region Searching,"Xianlei Long, Hui Zhao, Chao Chen, Fuqiang Gu, Qingyi Gu","Chongqing University,College of Computer Science, China University of Geoscience,Institute of Automation, Chinese Academy of Sciences",AI Robotics,"In recent years, wide-area visual detection systems have been widely applied in various industrial and security sectors. These systems, however, face significant challenges when implementing multi-object detection due to conflicts arising from the need for high-resolution imaging, efficient object searching, and accurate localization. To address these challenges, this paper presents a hybrid system that incorporates a wide-angle camera, a high-speed search camera, and a galvano-mirror. In this system, the wide-angle camera offers panoramic images as prior information, which helps the search camera in capturing detailed images of the targeted objects. This integrated approach enhances the overall efficiency and effectiveness of wide-area visual detection systems. Specifically, in this study, we introduce a wide-angle camera-based method to generate a panoramic probability map (PPM) for estimating high-probability regions of target object presence. Then, we propose a probability searching module that uses the PPM-generated prior information to dynamically adjust the sampling range and refine target coordinates based on uncertainty variance computed by the object detector. Finally, the integration of PPM and the probability searching module yields an efficient hybrid vision system capable of achieving 120 fps multi-object search and detection. Extensive experiments are conducted to verify the system's effectiveness and robustness."
Real-Time Robot Navigation and Manipulation with Distilled Vision-Language Models,"Kangcheng Liu, Xinhu Zheng, Chaoqun Wang, Lin Wang, Hesheng Wang, Lei Zhu, Haoang Li, Xiaodong Han, Pengcheng Hu, Ming Liu, Kai Tang","ETH Zurich,The HongKong University of Science and Technology (Guangzhou),Shandong University,HKUST,Shanghai Jiao Tong University,The Hong Kong University of Science and Technology (Guangzhou),The Chinese University of Hong Kong,Minjiang University,Hong Kong University of Science and Technology (Guangzhou),,Hong Kong University of Science and Technology",AI Robotics,"Autonomous robot navigation within the dynamic unknown environment is of crucial significance for mobile robotic applications including robot navigation in last-mile delivery and robot-enabled automated supplies in industrial and hospital delivery applications. Current solutions still suffer from limitations, such as the robot cannot recognize unknown objects in real time and cannot navigate freely in a dynamic, narrow, and complex environment. We propose a complete software framework for autonomous robot perception and navigation within very dense obstacles and dense human crowds. First, we propose a framework that accurately detects and segments open-world object categories in a zero-shot manner, which overcomes the over-segmentation limitation of the current SAM model. Second, we proposed the distillation strategy to distill the knowledge to segment the free space of the walkway for robot navigation without the label. In the meantime, we design the trimming strategy that works collaboratively with distillation to enable lightweight inference to deploy the neural network on edge devices such as NVIDIA-TX2 or Xavier NX during autonomous navigation. Integrated into the robot navigation system, extensive experiments demonstrate that our proposed framework has achieved superior performance in terms of both accuracy and efficiency in robot scene perception and autonomous robot navigation. The related codes, data, and models will be made publicly available at https://github.com/KangchengLiu to benefit the community."
Implicit Point Function for LiDAR Super-Resolution in Autonomous Driving,"Minseong Park, Haengseon Son, Euntai Kim","Yonsei University,Korea Electronics Technology Institute",Autonomous Vehicle Navigation III,"LiDAR super-resolution is a relatively new problem in which we seek to fill in the blanks between measured points when a low-resolution LiDAR is given, making a high-resolution LiDAR or even a resolution-free LiDAR. Recently, several research works have been reported regarding LiDAR super-resolution. However, most of the works on LiDAR super-resolution have the drawback that they first transform 3D LiDAR point cloud into 2D depth map and upsample the LiDAR output by applying the image super-resolution method, ignoring the 3D geometric information of the point cloud obtained from a LiDAR. To solve the above problem, we propose a new deep learning network named as implicit point function (IPF). The basic idea of IPF is that when we are given low-resolution point cloud and a query ray, we generate the 3D target point embeddings on the query ray using on-the-ray positional embedding and local features, preserving the 3D geometric information of the given point cloud. Then, we aggregate them into one target point via the attention mechanism. IPF enables us to learn continuous representation of 3D space from low-resolution LiDAR and upsample a small number of layers to any number that we want. Finally, our IPF is applied to large-scale synthetic dataset and real dataset, and its validity is demonstrated by comparing with the previous methods."
Circular Accessible Depth: A Robust Traversability Representation for UGV Navigation,"Shikuan Xie, Ran Song, Yuenan Zhao, Xueqin Huang, Yibin Li, Wei Zhang",Shandong University,Autonomous Vehicle Navigation III,"In this paper, we present the Circular Accessible Depth (CAD), a robust traversability representation for an unmanned ground vehicle (UGV) to learn traversability in various scenarios containing irregular obstacles. To predict CAD, we propose a neural network, namely CADNet, with an attention-based multi-frame point cloud fusion module, Stability-Attention Module (SAM), to encode the spatial features from point clouds captured by LiDAR. CAD is designed based on the polar coordinate system and focuses on predicting the border of traversable area. Since it encodes the spatial information of the surrounding environment, which enables a semi-supervised learning for the CADNet, and thus desirably avoids annotating a large amount of data. Extensive experiments demonstrate that CAD outperforms baselines in terms of robustness and precision. We also implement our method on a real UGV and show that it performs well in real-world scenarios."
Robots That Can See: Leveraging Human Pose for Trajectory Prediction,"Tim Salzmann, Lewis Chiang, Markus Ryll, Dorsa Sadigh, Carolina Parada, Alex Bewley","Technical University Munich,Google Deepmind,Stanford University,Google",Autonomous Vehicle Navigation III,"Anticipating the motion of all humans in dynamic environments such as homes and offices is critical to enable safe and effective robot navigation. Such spaces remain challenging as humans do not follow strict rules of motion and there are often multiple occluded entry points such as corners and doors that create opportunities for sudden encounters. In this work, we present a Transformer based architecture to predict human future trajectories in human-centric environments from input features including human positions, head orientations, and 3D skeletal keypoints from onboard in-the-wild sensory information. The resulting model captures the inherent uncertainty for future human trajectory prediction and achieves state-of-the-art performance on common prediction benchmarks and a human tracking dataset captured from a mobile robot adapted for the prediction task. Furthermore, we identify new agents with limited historical data as a major contributor to error and demonstrate the complimentary nature of 3D skeletal poses in reducing prediction error in such challenging scenarios."
Uncertainty-Aware Reinforcement Learning for Autonomous Driving with Multimodal Digital Driver Guidance,"Wenhui Huang, Zitong Shan, Shanhe Lou, Chen Lv","NanYang Technological University,Jilin University,Nanyang Technological University",Autonomous Vehicle Navigation III,"While existing Learning from intervention (LfI) methods within the human-in-the-loop reinforcement learning (HiL-RL) paradigm mainly operate on the assumption that human policies are homogeneous and deterministic with low variance, natural human driving behaviors are multimodal with intrinsic uncertainties, and hence, accommodating diverse human capabilities is significant for its practical applications. This work proposes an enhanced LfI approach for learning the optimal RL policy by leveraging multimodal human behaviors in the setting of N-driver concurrent interventions. Specifically, we first learn the N number of human digital drivers from the multi-human demonstration dataset, wherein each driver possesses its own policy distribution. Then, the post-trained drivers will be kept in the training loop of the RL algorithms, providing diverse driving guidance whenever the intervention is required. Additionally, to better utilize the provided guidance, we augment the RL regarding the fundamental architecture and optimization objectives to facilitate the proposed uncertainty-aware reinforcement learning (UnaRL) algorithm. The pro-posed approach, which won 2nd place in the Alibaba Future Car Innovation Challenge 2022, is solidly compared in two challenging autonomous driving scenarios against state-of-the-art (SOTA) LfI baselines, and results of both simulation and real-world experiment confirm the superiority of our method in terms of learning robustness and driving performance. Videos and source code are provided."
Boosting Offline Reinforcement Learning for Autonomous Driving with Hierarchical Latent Skills,"Zenan Li, Fan Nie, Qiao Sun, Fang Da, Hang Zhao","Tsinghua University,Shanghai Jiao Tong University,Shanghai QiZhi Institute,QCraft",Autonomous Vehicle Navigation III,"Vehicle planning is receiving increasing attention with the emergence of diverse driving simulators and large-scale driving datasets. While offline reinforcement learning (RL) is well suited for these safety-critical tasks, it still struggles to plan over extended periods. In this work, we present a skill-based framework that enhances offline RL to overcome the long-horizon vehicle planning challenge. Specifically, we design a variational autoencoder (VAE) to learn skills from offline demonstrations. To mitigate the posterior collapse problem, we introduce a two-branch sequence encoder to capture both discrete options and continuous variations of the complex driving skills. The final policy treats learned skills as actions and can be trained by any off-the-shelf offline RL algorithms. This facilitates a shift in focus from per-step actions to temporally extended skills, thereby enabling long-term reasoning into the future. Extensive results on CARLA prove that our model consistently outperforms strong baselines at both training and new scenarios. Additional visualizations and experiments demonstrate the interpretability and transferability of extracted skills."
A Framework for Real-Time Generation of Multi-Directional Traversability Maps in Unstructured Environments,"Tao Huang, Gang Wang, Hongliang Liu, Jun Luo, Lang Wu, Tao Zhu, Huayan Pu, Jun Luo, Shuxin Wang","Chongqing University,Huazhong University of Science and Technology,Chongqing university,Shanghai University,Tianjin University",Autonomous Vehicle Navigation III,"In complex unstructured environments, accurate terrain traversability analysis is a fundamental requirement for the successful execution of any movements of ground robots, especially given that terrain traversability often exhibits anisotropy. However, the difficulty in obtaining multi-directional terrain labels hinders the emergence of end-to-end multi-directional traversability network. This paper introduces a framework for real-time multi-directional traversability maps (MTraMap) generation tailored for unstructured environments. It involves pre-training a uni-directional traversability classifier, termed UniTraT, through self-supervised learning using ground robot travel simulation. Furthermore, it employs Uni-directional to Multi-directional Traversability Distillation (UMTraDistill) to distill a multi-directional traversability network, termed MultiTCNN, which is capable of directly generating MTraMap. We evaluated both networks on our traversability dataset, achieving an 89% accuracy in terrain traversability classification with the UniTraT. Compared to UniTraT, the accuracy of the MultiTCNN distilled via UMTraDistill only decreases by 1.8%, and it can process 10 m Ã— 10 m elevation map at a speed of 74 fps. Field robotics experiments were also conducted and showed that MultiTCNN can generate MTraMap of the surrounding 20 m Ã— 20 m environment at a rate of 9.39 fps, with a slight reduction of 0.61 fps compared to the lidar data publishing rate, and the generated MTraMap can clearly delineate the multi-directional traversability of the surrounding environments."
Cross-Modal Registration Using Adaptive Modeling in Infrastructure-Based Vehicle Localization,"Fei Wang, Yuesheng He, Hanyang Zhuang, Chenxi Yang, Ming Yang","Shanghai JiaoTong University,Shanghai Jiao Tong University",Autonomous Vehicle Navigation III,"Infrastructure-based vehicle localization, in comparison to single-agent approaches, offers several advantages including reduced system cost, extended perception range, enhanced data fusion capabilities, and energy savings. Many conventional approaches impose limitations on the types of objects due to the need for specific object-end modifications, such as applying perceptual markers like color-labeled plates and reflective balls. LiDAR presents a solution in terms of object arbitrariness, as it addresses the challenges of feature-free object modeling and continuous registration. However, achieving complete environmental coverage with LiDAR remains prohibitively expensive, particularly in extensive areas. Hence, this study proposes a cross-modal localization approach using adaptive modeling, employing LiDAR for object modeling and cost-effective sensor cameras for object tracking through image-point-cloud registration. Accurate correspondence between the model and observation can be estimated in real-time. The experiments are conducted in a typical scenario that requires adaptive modeling: Autonomous Valet Parking (AVP). Results demonstrate that the proposed system achieves comparable performance with significantly reduced system costs, highlighting its potential for large-scale deployment."
Efficient Gas Source Active Search in Unfamiliar Environments,"Yu Zhai, Yanzi Miao","China University of mining and technology,China University of Mining and Technology",Autonomous Vehicle Navigation III,"Searching Gas Source actively and efficiently in unknown hazard environments is an important but challenging issue. Using mobile robots to autonomously search and navigate to gas source location provides a promising way. Existing methods are mostly based on the modularization framework which investigates the gas-source search and robot navigation tasks independently, leading to a decoupled approach that results in higher collision risks and lower navigation efficiency. Moreover, existing robot navigation techniques grapple with the intricacies of navigating through unknown environments. To tackle these complexities, we introduce an integrated framework that merges gas source localization with robot navigation. This unified structure, underpinned by an end-to-end learning approach, resolves the inherent conflicts between gas exploration and collision avoidance. Our approach aggregates the local observations (raw 3D-LiDAR data) and the expert guidance information (gas distribution), and directly generates navigation actions by implementing the reinforcement learning with a novel reward function based on region dynamic guidances, thus effectively addressing the challenges of active gas source searching in unknown environments. Simulation results underscore the adaptability of our method to diverse unknown environments, along with its superior gas source searching capabilities compared to conventional approaches. Finally, we conduct real-world experiments to demonstrate our feasibility."
RGBD-Based Image Goal Navigation with Pose Drift: A Topo-Metric Graph Based Approach,"Shuhao Ye, Yuxiang Cui, Hao Sha, Sha Lu, Yu Zhang, Rong Xiong, Yue Wang",Zhejiang University,Autonomous Vehicle Navigation III,"Image-goal navigation in unknown environments with sensor error is of considerable difficulty for autonomous robots. In this paper, we propose a drift-resisting topo-metric graph to map the environment and localize the robot using only relative poses. The error-sharing mechanism under this representation effectively reduces the impact of accumulated drifts commonly encountered in navigation tasks. A Reinforcement Learning based policy was proposed for sub-goal selection on this topo-metric graph, which improves navigation efficiency by handling task-driven features taking both image correlation and topological layout into account. We adopt a modular system design with this map representation and graph policy, leaving the low-level motion planning problems to classical controllers for better stability and generalizability. Experimental results demonstrate that our method can achieve robust navigation performance in a variety of unknown environments and even 50% higher success rate over existing methods in complex environments with odometry drift."
MonoOcc: Digging into Monocular Semantic Occupancy Prediction,"Yupeng Zheng, Xiang Li, Pengfei Li, Yuhang Zheng, Bu Jin, Chengliang Zhong, Xiaoxiao Long, Hao Zhao, Qichao Zhang","School of Artificial Intelligence, University of Chinese Academy,Department of Computer Science and Technology, Tsinghua Universi,Institute for AI Industry Research (AIR), Tsinghua University,Beihang University,Institute of Automation, Chinese Academy of Sciences,Tsinghua University,The University of Hong Kong",Image-Based Navigation II,"Monocular Semantic Occupancy Prediction aims to infer the complete 3D geometry and semantic information of scenes from only 2D images. It has garnered significant attention, particularly due to its potential to enhance the 3D perception of autonomous vehicles. However, existing methods rely on a complex cascaded framework with relatively limited information to restore 3D scenes, including a dependency on supervision solely on the whole network's output, single-frame input, and the utilization of a small backbone. These challenges, in turn, hinder the optimization of the framework and yield inferior prediction results, particularly concerning smaller and long-tailed objects. To address these issues, we propose MonoOcc. In particular, we (i) improve the monocular occupancy prediction framework by proposing an auxiliary semantic loss as supervision to the shallow layers of the framework and an image-conditioned cross-attention module to refine voxel features with visual clues, and (ii) employ a distillation module that transfers temporal information and richer knowledge from a larger image backbone to the monocular semantic occupancy prediction framework with low cost of hardware. With these advantages, our method yields state-of-the-art performance on the camera-based SemanticKITTI Scene Completion benchmark. Codes and models can be accessed at at https://github.com/ucaszyp/MonoOcc."
ShaSTA: Modeling Shape and Spatio-Temporal Affinities for 3D Multi-Object Tracking,"Tara Sadjadpour, Jie Li, Rares Ambrus, Jeannette Bohg","Stanford University,Toyota Research Institute",Image-Based Navigation II,"Multi-object tracking (MOT) is a cornerstone capability of any robotic system. Tracking quality is largely dependent on the quality of input detections. In many applications, such as autonomous driving, it is preferable to over-detect objects to avoid catastrophic outcomes due to missed detections. As a result, current state-of-the-art 3D detectors produce high rates of false-positives to ensure a low number of false-negatives. This can negatively affect tracking by making data association and track lifecycle management more challenging. Additionally, occasional false-negative detections due to difficult scenarios like occlusions can harm tracking performance. To address these issues in a unified framework, we propose ShaSTA which learns shape and spatio-temporal affinities between tracks and detections in consecutive frames. The affinity is a probabilistic matching that leads to robust data association, track lifecycle management, false-positive elimination, false-negative propagation, and sequential track confidence refinement. We offer the first self-contained framework that addresses all aspects of the 3D MOT problem. We quantitatively evaluate ShaSTA on the nuScenes tracking benchmark with 5 metrics, including the most common tracking accuracy metric called AMOTA, to demonstrate how ShaSTA may impact the ultimate goal of an autonomous mobile agent. ShaSTA achieves 1st place amongst LiDAR-only trackers that use CenterPoint detections. The open-source code for reproducing"
Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources,"Jinlong Li, Baolu Li, Xinyu Liu, Runsheng Xu, Jiaqi Ma, Hongkai Yu","cleveland state university,Cleveland State University,UCLA,University of California, Los Angeles",Image-Based Navigation II,"The diverse agents in multi-agent perception systems may be from different companies. Each company might use the identical classic neural network architecture based encoder for feature extraction. However, the data source to train the various agents is independent and private in each company, leading to the Distribution Gap of different private data for training distinct agents in multi-agent perception system. The data silos by the above Distribution Gap could result in a significant performance decline in multi-agent perception. In this paper, we thoroughly examine the impact of the distribution gap on existing multi-agent perception systems. To break the data silos, we introduce the Feature Distribution-aware Aggregation (FDA) framework for cross-domain learning to mitigate the above Distribution Gap in multi-agent perception. FDA comprises two key components: Learnable Feature Compensation Module and Distribution-aware Statistical Consistency Module, both aimed at enhancing intermediate features to minimize the distribution gap among multi-agent features. Intensive experiments on the public OPV2V and V2XSet datasets underscore FDAâ€™s effectiveness in point cloud-based 3D object detection, presenting it as an invaluable augmentation to existing multi-agent perception systems. The code is available at https://github.com/jinlong17/BDS-V2V."
AdvGPS: Adversarial GPS for Multi-Agent Perception Attack,"Jinlong Li, Baolu Li, Xinyu Liu, Jianwu Fang, Felix Juefei-xu, Qing Guo, Hongkai Yu","cleveland state university,Cleveland State University,Xian Jiaotong University,Meta AI,Agency for Science, Technology and Research (A*STAR)",Image-Based Navigation II,"The multi-agent perception system collects visual data from sensors located on various agents and leverages their relative poses determined by GPS signals to effectively fuse information, mitigating the limitations of single-agent sensing, such as occlusion. However, the precision of GPS signals can be influenced by a range of factors, including wireless transmission and obstructions like buildings. Given the pivotal role of GPS signals in perception fusion and the potential for various interference, it becomes imperative to investigate whether specific GPS signals can easily mislead the multiagent perception system. To address this concern, we frame the task as an adversarial attack challenge and introduce ADVGPS, a method capable of generating adversarial GPS signals which are also stealthy for individual agents within the system, significantly reducing object detection accuracy. To enhance the success rates of these attacks in a black-box scenario, we introduce three types of statistically sensitive natural discrepancies: appearance-based discrepancy, distribution-based discrepancy, and task-aware discrepancy. Our extensive experiments on the OPV2V dataset demonstrate that these attacks substantially undermine the performance of state-of-the-art methods, showcasing remarkable transferability across different point cloud based 3D detection systems. This alarming revelation underscores the pressing need to address security implications within multi-agent perception systems, thereby underscoring a critical area of research. The code is available at https://github.com/jinlong17/AdvGPS."
Towards Motion Forecasting with Real-World Perception Inputs: Are End-To-End Approaches Competitive?,"Yihong Xu, Loick Chambon, Eloi Zablocki, Mickaël Chen, Alexandre Alahi, Matthieu Cord, Patrick Perez","Valeo.ai,Valeo,valeo,EPFL,Sorbonne Université, Valeo.ai",Image-Based Navigation II,"Motion forecasting is crucial in enabling autonomous vehicles to anticipate the future trajectories of surrounding agents. To do so, it requires solving mapping, detection, tracking, and then forecasting problems, in a multi-step pipeline. In this complex system, advances in conventional forecasting methods have been made using curated data, i.e., with the assumption of perfect maps, detection, and tracking. This paradigm, however, ignores any errors from upstream modules. Meanwhile, an emerging end-to-end paradigm, that tightly integrates the perception and forecasting architectures into joint training, promises to solve this issue. So far, however, the evaluation protocols between the two methods were incompatible and their comparison was not possible. In fact, and perhaps surprisingly, conventional forecasting methods are usually not trained nor tested in real-world pipelines (e.g., with upstream detection, tracking, and mapping modules). In this work, we aim to bring forecasting models closer to real-world deployment. First, we propose a unified evaluation pipeline for forecasting methods with real-world perception inputs, allowing us to compare the performance of conventional and end-to-end methods for the first time. Second, our in-depth study uncovers a substantial performance gap when transitioning from curated to perception-based data. In particular, we show that this gap (1) stems not only from differences in precision but also from the nature of imperfect inputs provided by perception modules, and that (2) is not trivially reduced by simply finetuning on perception outputs. Based on extensive experiments, we provide recommendations for critical areas that require improvement and guidance towards more robust motion forecasting in the real world. We will release an evaluation library to benchmark models under standardized and practical conditions."
QUEST: Query Stream for Practical Cooperative Perception,"Siqi Fan, Haibao Yu, Wenxian Yang, Jirui Yuan, Zaiqing Nie","Tsinghua University,The University of Hong Kong",Image-Based Navigation II,"Cooperative perception can effectively enhance individual perception performance by providing additional viewpoint and expanding the sensing field. Existing cooperation paradigms are either interpretable (result cooperation) or flexible (feature cooperation). In this paper, we propose the concept of query cooperation to enable interpretable instance-level flexible feature interaction. To specifically explain the concept, we propose a cooperative perception framework, termed QUEST, which let query stream flow among agents. The cross-agent queries are interacted via fusion for co-aware instances and complementation for individual unaware instances. Taking camera-based vehicle-infrastructure perception as a typical practical application scene, the experimental results on the real-world dataset, DAIR-V2X-Seq, demonstrate the effectiveness of QUEST and further reveal the advantage of the query cooperation paradigm on transmission flexibility and robustness to packet dropout. We hope our work can further facilitate the cross-agent representation interaction for better cooperative perception in practice."
Towards Visibility Estimation and Noise-Distribution-Based Defogging for LiDAR in Autonomous Driving,"Jie Zhan, Yucong Duan, Junfeng Ding, Xuzhong Hu, Xiao Huang, Jie Ma","Huazhong University of Science and Technology,huazhong university of science and technology,China ship Development and Design Center",Image-Based Navigation II,"Point clouds play a crucial role in robots and intelligent vehicles. Noise caused by fog droplets seriously degrades the quality of point clouds. Previous research has shown that the extent of degradation is correlated with visibility. The fog attenuation coefficient is associated with visibility. In light of this background, this paper proposes a noise-distribution based defogging method for point clouds. Our approach hinges on the estimation of fog attenuation coefficient, facilitated by road-based prior knowledge. Subsequently, our method integrates the fog-induced noise distribution inferred from the LiDAR imaging model with the spatially non-uniform distribution of point clouds caused by LiDAR structure. The fused results are input to a statistical filter based on the relative sparsity of noise to achieve defogging. This paper is one of the early works focusing on point cloud defogging. It's core insight lies in the estimation of the attenuation coefficient and the employment of fog-induced noise distribution for defogging. Experiments demonstrate that our method can accurately mitigate the impact of fog and meanwhile enhance the performance of 3D object detection network."
CenterCoop: Center-Based Feature Aggregation for Communication-Efficient Vehicle-Infrastructure Cooperative 3D Object Detection,"Linyi Zhou, Zhongxue Gan, Jiayuan Fan",Fudan University,Image-Based Navigation II,"Vehicle-Infrastructure Cooperative (VIC) 3D object detection is a challenging task for balancing communication bandwidth and detection performance simultaneously. Intermediate fusion is recently studied to reach a better balance by transferring feature maps. Existing works mainly perform spatial-wise fusion and adopt feature compression to alleviate bandwidth cost by high-resolution feature maps, which would inevitably lead to information loss. Besides, overlapping observations between the two sensors would lead to near-duplicate detections, making trivial improvement to cooperative task while causing unnecessary bandwidth cost. To mitigate these problems, we propose a novel feature aggregation framework called CenterCoop, which first encodes the informative clues from the whole Bird's Eye View (BEV) context into compact center representations, enabling feature aggregation at sequence-level to significantly reduce the communication cost. Furthermore, to tackle the redundancy of transmitted data, we incorporate communication-aware regularization which enforces the network to extract complementary and beneficial cues for collaboration task. From an information-theoretic perspective, the proposed auxiliary constraints facilitate cooperative-view independence mining, resulting in enlarged perception range within the limited bandwidth. Extensive experiments on the DAIR-V2X dataset demonstrate the superior performance-bandwidth trade-off of CenterCoop, which achieves the state-of"
Probabilistic 3D Multi-Object Cooperative Tracking for Autonomous Driving Via Differentiable Multi-Sensor Kalman Filter,"Hsu-kuang Chiu, Chien-yi Wang, Min-hung Chen, Stephen F. Smith","Carnegie Mellon University,NVIDIA",Image-Based Navigation II,"Current state-of-the-art autonomous driving vehicles mainly rely on each individual sensor system to perform perception tasks. Such a framework's reliability could be limited by occlusion or sensor failure. To address this issue, more recent research proposes using vehicle-to-vehicle (V2V) communication to share perception information with others. However, most relevant works focus only on cooperative detection and leave cooperative tracking an underexplored research field. A few recent datasets, such as V2V4Real, provide 3D multi-object cooperative tracking benchmarks. However, their proposed methods mainly use cooperative detection results as input to a standard single-sensor Kalman Filter-based tracking algorithm. In their approach, the measurement uncertainty of different sensors from different connected autonomous vehicles (CAVs) may not be properly estimated to utilize the theoretical optimality property of Kalman Filter-based tracking algorithms. In this paper, we propose a novel 3D multi-object cooperative tracking algorithm for autonomous driving via a differentiable multi-sensor Kalman Filter. Our algorithm learns to estimate measurement uncertainty for each detection that can better utilize the theoretical property of Kalman Filter-based tracking methods. The experiment results show that our algorithm improves the tracking accuracy by 17% with only 0.037x communication costs compared with the state-of-the-art method in V2V4Real."
Design and Implementation of a Robotic Testbench for Analyzing Pincer Grip Execution in Human Specimen Hands,"Nikolas Jakob Wilhelm, Claudio Glowalla, Sami Haddadin, Julian Schote, Hannes Hoeppner, Patrick Van Der Smagt, Maximilian Karl, Rainer Burgkart","Technical University of Munich,Department of Orthopaedics and Sports Orthopaedics, Klinikum rec,Berliner Hochschule für Technik, BHT,Volkswagen Group,Volkswagen AG,Technische Universität München",Motion Analysis and Planning,"This study presents an innovative test rig engineered to explore the kinematic and viscoelastic characteristics of human specimen hands. The rig features eight force-controlled motors linked to muscle tendons, enabling precise stimulation of hand specimens. Hand movements are monitored through an optical tracking system, while a force-torque sensor quantifies the resultant fingertip loads. Employing this setup, we successfully demonstrated a pincer grip using a cadaver hand and measured both muscle forces and grip strength. Our results reveal a nonlinear relationship between tendon forces and grip strength, which can be modeled by an exponential fit. This investigation serves as a nexus between biomechanical and robotics-focused research, providing critical insights for the advancement of robotic hand actuation and therapeutic interventions."
Human Gait Cost Function Varies with Walking Speed: An Inverse Optimal Control Study,"Jiacheng Weng, Ehsan Hashemi, Arash Arami","University of Waterloo,University of Alberta",Motion Analysis and Planning,"This work investigates the optimal cost function composition for human gait at different walking speeds. Kinematic and kinetic data for walking at four walking speeds were collected from five able-bodied individuals. The data was then used to recover optimal cost functions in a predictive simulation environment with musculoskeletal models. 20 inverse optimal control (IOC) problems were solved for cost function weight tuning using the previously developed and validated Adaptive Reference IOC (AR-IOC) algorithm. Given the walking speed range examined (0.6-1.5m/s), the converged cost function weights suggest that the increase in walking speed attributes to a reduction of foot sliding penalty weight and weight increase for the center of mass (CoM) acceleration and stability as confirmed by several experiments. Furthermore, we did not observe any significant weight shift in effort reduction between the upper and the lower body with respect to walking speed. The obtained results from this study can be used in a toolbox for obtaining subject- and task-specific cost functions and assisting the development of personalized rehabilitation technologies."
LORIS: A Lightweight Free-Climbing Robot for Extreme Terrain Exploration,"Paul Nadan, Spencer Backus, Aaron Johnson","Carnegie Mellon University,NASA Jet Propulsion Lab",Motion Analysis and Planning,"Climbing robots can investigate scientifically valuable sites that conventional rovers cannot access due to steep terrain features. Robots equipped with microspine grippers are particularly well-suited to ascending rocky cliff faces, but most existing designs are either large and slow or limited to relatively flat surfaces such as walls. We present a novel free-climbing robot to bridge this gap through innovations in gripper design and force control. Fully passive grippers and wrist joints allow secure grasping while reducing mass and complexity. Forces are distributed among the robot's grippers using an optimization-based control strategy to minimize the risk of unexpected detachment. The robot prototype has demonstrated vertical climbing on both flat cinder block walls and uneven rock surfaces in full Earth gravity."
MST-Q: Micro Suction Tape Quadruped Robot with High Payload Capacity,"Jichun Xiao, Lina Hao, Hongzhi Xu, Xu Zhang, Xing Li, Zhi Li","Northeastern University,the State Key Laboratory of Synthetical Automation for Process I,School of Electrical Engineering & Intelligentization, Dongguan ",Motion Analysis and Planning,"Payload capacity is a crucial factor for climbing robots, as it directly affects their ability to carry and transport heavy loads during various climbing tasks. However, many dry adhesion-based legged robots prioritize foot design from a bionic perspective to accomplish various climbing tasks while overlooking payload capacity. To address this issue, a novel adhesion foot structure utilizing micro-suction tape (MST) and a detachment mechanism has been proposed. This design was implemented in the construction and experimental testing of the quadrupedal climbing robot MST-Q with a high payload carrying capacity of up to 2.8 kg for vertical climbing and 0.5 kg for inverted climbing. The robot also demonstrated the ability to traverse obstacles with a payload in the inverted climbing tasks."
Floating-Base Manipulation on Zero-Perturbation Manifolds,"Brian Bittner, Jason Reid, Kevin C. Wolfe","JHUAPL,Jet Propulsion Laboratory,Johns Hopkins University Applied Physics Laboratory",Motion Analysis and Planning,"To achieve high-dexterity motion planning on floating-base systems, the base dynamics induced by arm motions must be treated carefully. In general, it is a significant challenge to establish a fixed-base frame during tasking due to forces and torques on the base that arise directly from arm motions (e.g. arm drag in low Reynolds environments and arm momentum in high Reynolds environments). While thrusters can in theory be used to regulate the vehicle pose, it is often insufficient to establish a stable pose for precise tasking, whether the cause be due to underactuation, modeling inaccuracy, suboptimal control parameters, or insufficient power. We propose a solution that asks the thrusters to do less high bandwidth perturbation correction by planning arm motions that induce zero perturbation on the base. We are able to cast our motion planner as a nonholonomic rapidly-exploring random tree (RRT) by representing the floating-base dynamics as pfaffian constraints on joint velocity. These constraints guide the manipulators to move on zero-perturbation manifolds (which inhabit a subspace of the tangent space of the internal configuration space). To invoke this representation (termed a perturbation map) we assume the body velocity (perturbation) of the base to be a joint-defined linear mapping of joint velocity and describe situations where this assumption is realistic (including underwater, aerial, and orbital environments). The core insight of this work is that when perturbation of the floating-base has affine structure with respect to joint velocity, it provides the system a class of kinematic reduction that permits the use of sample-based motion planners (specifically a nonholonomic RRT). We show that this allows rapid, exploration-geared motion planning for high degree of freedom systems in obstacle rich environments, even on floating-base systems with nontrivial dynamics."
Towards Geometric Motion Planning for High-Dimensional Systems: Gait-Based Coordinate Optimization and Local Metrics,"Yanhao Yang, Capprin Bass, Ross Hatton",Oregon State University,Motion Analysis and Planning,"Geometric motion planning offers effective and interpretable gait analysis and optimization tools for locomoting systems. However, due to the curse of dimensionality in coordinate optimization, a key component of geometric motion planning, it is almost infeasible to apply current geometric motion planning to high-dimensional systems. In this paper, we propose a gait-based coordinate optimization method that overcomes the curse of dimensionality. We also identify a unified geometric representation of locomotion by generalizing various nonholonomic constraints into local metrics. By combining these two approaches, we take a step towards geometric motion planning for high-dimensional systems. We test our method in two classes of high-dimensional systems - low Reynolds number swimmers and free-falling Cassie - with up to 11-dimensional shape variables. The resulting optimal gait in the high-dimensional system shows better efficiency compared to that of the reduced-order model. Furthermore, we provide a geometric optimality interpretation of the optimal gait."
See through the Real World Haze Scenes: Navigating the Synthetic-To-Real Gap in Challenging Image Dehazing,"Jiayuan Fan, Shijie Chen, Tao Chen, Mohammad Mahdizadeh, Chong Yu","Fudan University,Fudan University & NVIDIA",Visual Perception and Learning I,"Dehazing and enhancing visibility in real-world hazy images pose significant challenges due to the physical complexity of haze, the variability in haze conditions, the tendency to capture more details in noisy scenes, and the risk of overexposure. Many existing single RGB image dehazing methods tend to perform well in synthetic hazy scenarios but struggle in real-world situations. It stems from the fact that these methods often rely solely on deep learning techniques or classical approaches. In addition, they neglect overall image quality improvement. To partially address these challenges, we introduce an innovative approach that harnesses the strengths of both modalities to dehaze and enhance visibility in a single real-world hazy RGB image. First, both Low-level and deep features are extracted, and then a pre-trained vector quantization GAN is employed to create a discrete codebook of well-detailed data patches. A decoder component, enhanced with a normalized module, effectively utilizes these high-quality features to produce clear results. Additionally, a controllable operation is introduced to improve feature matching. To further enhance dehazing and generalizability, the decoder's output undergoes a sequence of gamma-correction operations and generates a sequence of multi-exposure images that are combined to create a haze-free, visually pleasing, and higher-quality final image. The method effectively reduces haziness, enhances sharpness, preserves natural colors, and minimizes artifacts in challenging real-world scenarios. The proposed approach surpasses five SOTA methods in both qualitative and quantitative evaluations across three key metrics, utilizing two real-world and three synthetic hazy image datasets. Notably, it achieves a substantial improvement in real-world datasets over the second-best method, with gains of 0.5702 and 0.129 in FADE metrics for the RTTS and Fattal datasets, respectively."
